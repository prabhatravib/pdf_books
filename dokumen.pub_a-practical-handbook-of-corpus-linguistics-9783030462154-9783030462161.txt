
A Practical Handbook of Corpus Linguistics 
Magali Paquot • Stefan Th. Gries Editors 

A Practical Handbook of Corpus Linguistics 
123 
Editors  
Magali Paquot  Stefan Th. Gries  
FNRS  Department of Linguistics  
Centre for English Corpus Linguistics,  University of California  
Language and Communication Institute  Santa Barbara, CA, USA  
UCLouvain Louvain-la-Neuve, Belgium  Justus Liebig University Giessen Giessen, Germany  

ISBN 978-3-030-46215-4 ISBN 978-3-030-46216-1 (eBook) 
https://doi.org/10.1007/978-3-030-46216-1 
© Springer Nature Switzerland AG 2020 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microflms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specifc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
Introduction 
Corpus linguistics is “a whole system of methods and principles” (McEnery et al. 2006: 7f) that can be applied to answer research questions related to language use and variation in a wide variety of domains of linguistic enquiry and beyond. Over the last decades, it has been “among the fastest-growing methodological disciplines in linguistics” (Gries 2015: 93) and is now also developing as a key methodology in the humanities and social sciences. 
The tasks of corpus linguists are manifold and complex. They can be grouped into minimally three different, though of course interrelated, areas: 
• 
Corpus design, which requires knowledge about corpus compilation (e.g., the notions of sampling and/or representativeness), data processing for corpus annotation (e.g., tagging, lemmatizing, parsing), and corpus architecture (e.g., representing corpus data in a maximally useful way); then, once there is a corpus, 

• 
Corpus searching/processing, which requires knowledge of, ideally, general data processing (e.g., fle management, dealing with different annotation formats, using regular expressions to defne character strings that lead to good search results) as well as corpus query tools and methods (from off-the-shelf tools to programming) to address the specifcities of various data types (e.g., time alignment of spoken and multimodal corpora or bitext alignment of parallel corpora); then, once there are results from a corpus, 

• 
Statistical analysis to get the most out of the corpus data, which requires knowl­edge of statistical data wrangling/processing (e.g., establishing subgroups in data or determining whether transformations are necessary), statistical analysis techniques (e.g., signifcance testing or alternative approaches, regression model­ing, or exploratory data analysis), and visualization (e.g., representing the results of complex statistical analysis in ways non-quantitatively minded readers can understand). 


This handbook aims to address all these areas with contributions by many of their leading experts, to be a comprehensive practical resource for junior and more senior corpus linguists, and to represent the whole research cycle from corpus creation, method, and analyses to reporting results for publication. It is divided into six parts. In Part I, the frst three chapters focus on corpus design and address issues related to corpus compilation, corpus annotation, and corpus architecture. Part II deals with corpus methods: Chapters 4–9 provide an overview of the most commonly used methods to extract linguistic and frequency information from corpora (frequency lists, keywords lists, dispersion measures, co-occurrence frequencies and concordances) as well as an introduction on the added value of programming skills in corpus linguistics. Chapters 10–16 in Part III review different corpus types (diachronic corpora, spoken corpora, parallel corpora, learner corpora, child language corpora, web corpora, and multimodal corpora), with each chapter focusing on the specifc methodological challenges associated with the analysis of each type of corpora. 
Parts IV–VI aim to offer a user-friendly introduction to the variety of statistical techniques that have been used, or have started to be used, more extensively in corpus linguistics. As each chapter under Part IV–VI uses R for explaining and exemplifying the statistics, Part IV starts with an introductory chapter on how to use R for descriptive statistics and visualization. Chapters 18 and 19 focus on exploratory techniques, i.e., cluster analysis and the multidimensional exploratory approaches of correspondence analysis, multiple correspondence analysis, principal component analysis, and exploratory factor analysis. Part V focuses on hypothesis-testing (classical monofactorial tests, fxed-effects regression modeling, mixed-effects regression modeling, generalized additive mixed models, bootstrapping techniques and conditional inference trees and random forests). It is important to note that the chapters on mixed effects regression modeling and generalized additive mixed models in particular are primarily meant for readers to get a grasp of what these techniques are (more and more corpus linguistic papers rely on such methods and it is important for corpus linguists to understand the current literature). However, a single chapter can of course not provide all that is required to get started with statistics of such a level of complexity. If the reader is interested to know more and wants to use these statistics for their own purposes, they will necessarily need to read more on the topic. 
Part VI aims to pull everything together by providing guidelines for how to write a corpus linguistic paper and how to meta-analyze corpus linguistic research. 
Chapters in Parts IV and V as well as Chaps. 7, 9 and 27 come with online additional material (R code with datasets). 
It is our hope that this handbook will serve to help students and colleagues expand their methodological toolbox. We certainly learned a lot while editing this volume! 
Louvain-la-Neuve, Belgium Magali Paquot Santa Barbara, CA, USA Stefan Th. Gries 
References 
Gries, S. T. (2015). Some current quantitative problems in corpus linguistics and a sketch of some solutions. Language and Linguistics, 16(1), 93–117. 
McEnery, T., Xiao, R., & Tono, Y. (2006). Corpus-based language studies: An advanced resource book. London/New York: Routledge. 

Contents 
Part I Corpus Design 
1 Corpus Compilation ........................................................ 3 AnnelieÄdel 
2 Corpus Annotation ......................................................... 25 JohnNewman andChristopher Cox 
3 Corpus Architecture........................................................ 49 Amir Zeldes 
Part II Corpus methods 
4 Analysing Frequency Lists ................................................ 77 Don Miller 
5 Analyzing Dispersion....................................................... 99 Stefan Th. Gries 
6 Analysing Keyword Lists .................................................. 119 Paul Rayson andAmandaPotts 
7 Analyzing Co-occurrence Data............................................ 141 Stefan Th. Gries and Philip Durrant 
8 Analyzing Concordances .................................................. 161 Stefanie Wulff and Paul Baker 
9 Programming for Corpus Linguistics .................................... 181 Laurence Anthony 
Part III Corpus types 
10 Diachronic Corpora ........................................................ 211 Kristin Davidse and Hendrik DeSmet 
11  Spoken Corpora ............................................................ Ulrike Gut  235  
12  Parallel Corpora ............................................................ Marie-Aude Lefer  257  
13  Learner Corpora ........................................................... Gaëtanelle Gilquin  283  
14  Child-Language Corpora .................................................. Sabine Stoll and Robert Schikowski  305  
15  Web Corpora ................................................................ Andrew Kehoe  329  
16  Multimodal Corpora ....................................................... Dawn Knight and Svenja Adolphs  353  
Part IV  Exploring Your Data  
17  Descriptive Statistics and Visualization with R ......................... Magali Paquot and Tove Larsson  375  
18  Cluster Analysis ............................................................ Hermann Moisl  401  
19  Multivariate Exploratory Approaches ................................... Guillaume Desagulier  435  
Part V  Hypothesis-Testing  
20  Classical Monofactorial (Parametric and Non-parametric) Tests .... Vaclav Brezina  473  
21  Fixed-Effects Regression Modeling ....................................... Martin Hilpertand Damián E. Blasi  505  
22  Mixed-Effects Regression Modeling ...................................... RolandSchäfer  535  
23  Generalized Additive Mixed Models ..................................... R. HaraldBaayen and Maja Linke  563  
24  Bootstrapping Techniques ................................................. Jesse Egbert and Luke Plonsky  593  
25  Conditional Inference Trees and Random Forests ...................... NataliaLevshina  611  

Part VI Pulling Everything Together 
26 Writing up a Corpus-Linguistic Paper ................................... 647 Stefan Th. Gries and Magali Paquot 
27 Meta-analyzing Corpus Linguistic Research ............................ 661 AtsushiMizumoto, LukePlonsky, and Jesse Egbert 

Part I Corpus Design 
Chapter 1 Corpus Compilation 
Annelie Ädel 
Abstract This chapter deals with the fundamentals of corpus compilation, approached from a practical perspective. The topics covered follow the key phases of corpus compilation, starting with the initial considerations of representativeness and balance. Next, issues in collecting corpus data are covered, including ethics and metadata. Technical aspects involving formatting and annotation are then presented, followed by suggestions for sharing the corpus with others. Corpus comparison is also discussed, as it merits some refection when a corpus is created. To further illustrate key concepts and exemplify the varying roles of the corpus in specifc research projects, two sample studies are presented. The chapter closes with a brief consideration of future directions in corpus compilation, focusing on the importance of compensating for the inevitable loss of complex information and taking the increasingly multimodal nature of discourse as a case in point. 
1.1 Introduction 
Given that linguistics is descriptive at its core, many linguists study how language is used based on some linguistic sample. Finding the right material to use as the basis for a study is a key aspect of the research process: we are expected to use material that is appropriate for answering our research questions, and not make claims that go beyond what is supported by the material. This chapter covers the basics of compiling linguistic material in the form of a corpus. Corpus compilation involves “designing a corpus, collecting texts, encoding the corpus, assembling and storing the relevant metadata, marking up the texts where necessary and possibly adding linguistic annotation” (McEnery and Hardie 2012:241). In the process of putting together linguistic data in a corpus, researchers need to make a series of decisions at different steps. The process is described in a general way in this chapter, while more 
A. Ädel (•) Dalarna University, Falun, Sweden e-mail: annelie.adel@du.se 
© Springer Nature Switzerland AG 2020 3 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_1 
A. Ädel 
in-depth discussion relating to the compilation of specifc types of corpora follows in Chaps. 10–16. Specifcs on corpus annotation and corpus architecture follow in Chaps. 2 and 3, respectively. 

1.2 Fundamentals 
1.2.1 Representativeness 
The most basic question to consider when compiling a corpus involves representa­tiveness: what type of speakers/variety/discourse is the corpus meant to represent? In many of the well-known corpora of English, the ambition has been to cover a general and very common type of discourse (such as ‘conversation in a variety of English’) or a very large population (such as ‘second-language learners of English’). However, such a comprehensive aim is beyond the scope for most researchers and should be reserved for large groups of researchers with plenty of resources at their disposal (see e.g. Aston and Burnard (1998) for discussions on how the British National Corpus was designed, or Johansson et al. (1978)onthe Lancaster-Oslo/Bergen Corpus). In small-scale projects, the aims regarding representativeness need to be more modest by comparison, for example with a focus on a specialized type of discourse used by a relatively restricted group of speakers. 
The general sense of the word ‘sample’ is simply a text or a text extract, but in its more specifc and statistical sense it refers to “a group of cases taken from a population that will, hopefully, represent that population such that fndings from the sample can be generalised to the population” (McEnery and Hardie 2012:250).1 The aim in compiling a corpus is that it should be a maximally representative—in practice, this translates into acceptably representative—sample of a population of language users, a language variety, or a type of discourse. In most linguistic studies, we have to make do with studying merely a sample of the language use, or variety, as a whole. It is only in rare cases, and when the research question is quite delimited, that it is possible to collect all of the linguistic production of the population or type of discourse we are interested in. As an example, it may be possible for a researcher in Languages for Specifc Purposes to retrieve all of the emails sent and received in a large company to use as a basis for studying the typical features of this specifc type of communication in that company. 
The corpus builder needs to consider very carefully how to collect samples that maximally represent the target discourse or population. One of the ways of selecting material for a corpus is by stratifed sampling, where the hierarchical structure (or ‘strata’) of the population is determined in advance. For example, a researcher 
1Samples in the sense ‘text extracts’ are occasionally used in corpora to avoid having one type of text dominate, just because it happens to be long. There are many arguments for using complete texts, however. See e.g. Douglas (2003) and Sinclair (2005). 
who is interested in spoken workplace discourse could document demographic information about speakers’ job titles and ages and whether interactions involve peers or managers/subordinates, and then include in the corpus a predetermined proportion of texts from each category. In the detailed sampling process, it is decided exactly what texts or text chunks to include. 
There is a range of possible considerations to take in deciding about sampling procedures for a corpus, one of which concerns to what extent to organize the overall design around text production or text reception. For illustration, this is what the compilers of the British National Corpus (Aston and Burnard 1998:28) concluded with respect to the written part of the corpus: 
In selecting texts for inclusion in the corpus, account was taken of both production, by sampling a wide variety of distinct types of material, and reception, by selecting instances of those types which have a wide distribution. Thus, having chosen to sample such things as popular novels, or technical writing, best-seller lists and library circulation statistics were consulted to select particular examples of them. 
A concept that is intrinsically related to representativeness is balance, which has to do with the proportions of the different samples included in the corpus. In a balanced corpus, “the relative sizes of each of [the subsections] have been chosen with the aim of adequately representing the range of language that exists in the population of texts being sampled” (McEnery and Hardie 2012:239). In the case of ‘conversation in a variety of English’, the researcher would need a principled way of deciding what proportions to include, for example, of conversations among friends versus among strangers, or unplanned versus preplanned conversations (an interview is an example of the latter), or conversations from institutional/public/private-personal settings, and so on. Such decisions could be based on some assessment of how commonly these different confgurations occur or of their relative importance (however this may be defned). Balancing decisions could even be based on comparability with some other corpus: for example, in a diachronic corpus of English (cf. Chap. 
10) fction writing may be deliberately overrepresented and religious writing underrepresented in earlier periods to allow for easier comparison to present-day English. 
The notions of representativeness and balance are scalar and vague (see e.g. Leech 2007), so there are no hard and fast rules for achieving representativeness and balance in a corpus. The frst step is to map out the available types of discourse, in order to fnd useful categorizations of the different ways of communicating used in the target community. The point that the most important consideration in corpus compilation is “a thorough defnition of the target population” which is able to describe the “different situations, purposes, and functions of text in a speech community” was made by Biber (1993:244–245) in a classic piece on representativeness in corpus design. Added to this are “decisions concerning the method of sampling” (Biber 1993:244), as the next step is to fnd some principled way of representing these different ways of communicating. For some of the early standard corpora, this was done by drawing on classifcations from library science, where there is a long tradition of cataloguing written publications. For example, 
A. Ädel 
a list of the collection of books and periodicals in the Brown University Library and the Providence Athenaeum was used as a sampling frame for the pioneering Brown corpus, aiming to represent written American English in general (published in 1961); see Francis and Kucera (1979).2 Using stratifed random sampling, a one-million word corpus was produced, consisting of 500 texts including 2,000 words each. 
However, if the available types of discourse are not already classifed in some reliable way, as in the case of spoken language, it means that the corpus builder will have to dedicate a great deal of time to researching the characteristics of the target discourse in order to develop valid and acceptable selection criteria. Douglas (2003) describes this type of situation and includes a useful discussion about the collection of The Scottish Corpus of Texts and Speech. 
With a defnition of representativeness as the extent to which a corpus refects “the full range of variability in a population” (Biber 1993:243), it has been suggested that representativeness can be assessed by the degree to which it captures not only the range of text types in a language (external criteria), but also the range of linguistic distributions in a language (internal criteria). Since different linguistic features—vocabulary, grammar, lexicogrammar—vary in frequency and are distributed differently “within texts, across texts, across text types” (ibid.), the corpus should make possible analysis of such distributions. In fact, Biber (1993) suggests a cyclical method for corpus compilation, including as key components theoretical analysis of relevant text types (which is always primary) and empirical investigation of the distributions of linguistic features. However, few corpus projects have attempted this. 
The literature on corpus design sometimes contrasts ‘principled’ ways of building a corpus to ‘opportunistic’ ones. An opportunistic corpus is said to “represent nothing more nor less than the data that it was possible to gather for a specifc task”, with no attempts made “to adhere to a rigorous sampling frame” (McEnery and Hardie 2012:11). It is, however, very diffcult not to include some element of opportunism in corpus design, as we do not have boundless resources. This is especially true of single-person MA or PhD projects, where time constraints may present a major issue. What is absolutely not negotiable, however, is that the criteria for selecting material for the corpus be clear, consistent and transparent. Indeed, transparency is key in selecting material for the corpus. The criteria used when selecting material also need to be explicitly stated when reporting to others about a study—it is a basic principle in research and a matter of making it possible for others to replicate the study. The selection criteria are typically biased with respect to specifc research interests behind a given corpus project, which should also be spelled out in the documentation about the corpus. 
2Biber (1993:244) defnes a sampling frame as “an operational defnition of the population, an itemized listing of population members from which a representative sample can be chosen”. 

1.2.2 Issues in Collecting Data for the Corpus 
Corpus compilation involves a series of practical considerations having to do with the question ‘Given the relative ease of access, how much data is it feasible to collect for the corpus?’. Indeed, this needs addressing before it is possible to determine fully the design of a corpus. Relevant spoken or written material may of course be found in many different places, and the effort required to collect it may vary considerably. Some types of discourse are meant to be widely distributed, and are even in the public domain, while others are relatively hidden, and are even confdential or secret. In an academic setting, for example, written course descriptions and spoken lectures target a large audience, while teacher feedback and committee discussions about the ranking of applicants for a post target a restricted audience. 
Once the data have been collected, varying degrees of data management will be required depending on the nature and form of the data. If spoken material is to be included in the corpus, it needs to be transcribed, that is, rendered in written form to be searchable by computer. The transcription needs to be appropriately detailed for the research question (see Chap. 11 for key issues involved in compiling spoken corpora). If written material is to be included in the corpus, there are practical considerations regarding how it is encoded. For example, if it can be accessed as plain text fles at the time of collection, it will save time. If it is only available on paper, it will need to be scanned using OCR (Optical Character Recognition) in order for the text to be retrieved. If it is only available on parchment, it will need very careful handling indeed by the historical corpus compiler, including manual typing and annotation to represent it. Even modern text fles which are available in pdf format may not be retrievable as plain text at all, or it may be possible to convert the pdf to text, but only with a varying degree of added symbols and garbled text, requiring additional ‘cleaning’.3 Section 1.2.5 on Formatting the corpus discusses some of these issues more fully. 
Nowadays there are massive amounts of material on the web, which are already in an electronic format. As a consequence, it has become popular among corpus builders to include material from online sources (see Chap. 15), which represent a great variety of genres, ranging from research articles to blogs. It is important, however, to make the relevance of the material to the research question a priority over ease of access, and carefully consider questions such as “How do we allow for the unavoidable infuence of practicalities such as the relative ease of acquiring public printed language, e-mails and web pages as compared to the labour and expense of recording and transcribing private conversations or acquiring and keying personal handwritten correspondence?” (Sinclair 2005). 
Even if material is available on the web, it does not necessarily mean that it is easy to access—at least not in the way texts need to be accessed for corpus work. Online newspapers are a case in point. While they often make it possible to search 
3There are tools that automatically convert pdf fles to simple text, such as AntFileConverter http:// www.laurenceanthony.net/software/antfleconverter/. Accessed 24 May 2019. 
A. Ädel 
the archive, they may not make the text fles downloadable other than one by one by clicking a hyperlink. The work of clicking the link, copying and saving each individual article manually is then left to the user. This is no small task, but it tends to be underestimated by beginner corpus compilers. Fortunately, there are ways of speeding up and automatizing the process in order to avoid too much manual work; Chap. 15 offers suggestions. 
Corpus compilers who are able to collect relevant material in the public domain still need to check the accuracy and adequacy of the material. Consider the case of a research group seeking the answer to the question ‘To what extent is (a) the spoken dialogue in the fctional television series X (dis)similar to (b) authentic non-scripted conversation?’. They may go to the series’ website to search for material, following the logic that an offcial website is likely to be a more credible source for transcripts than a site created by anonymous fans. Before any material can be included in the corpus, however, each transcript needs to be checked against the recorded episode to ensure that the transcription is not only correct, but also suffciently detailed for the specifc research purposes. When collecting material from the web, there may also be copyright restrictions to take into account; see e.g. the section on Ethical considerations below and Section 3.2 in McEnery and Hardie (2012) on legal issues in collecting such data. 
Beginner corpus researchers often fnd themselves confounded by the question ‘How much data do I need in order for my study to be valid?’. There is no rule of thumb for corpus size, except for the general principle ‘the more, the better’. That said, it requires more data to be able to make valid observations about a large group of people and a general type of discourse than a small group of people and a specifc type of discourse. It also requires more data to investigate rare rather than common linguistic features. Thus, the appropriate amount of data depends on the aim of the research. Each study, however, needs to be considered in its context. There are always going to be practical restrictions on how much time a given researcher is able to put into a project. Researchers who fnd themselves in a situation of not being able to collect as much data as planned will need to adjust their research questions accordingly. With less data—a smaller sample—the claims one is able to make based on one’s corpus fndings will be more modest. But most importantly, as discussed above, the issue of representativeness needs to be addressed before a corpus, regardless of size, can be considered appropriate for a given study. 

1.2.3 Ethical Considerations 
Corpus compilation involves different types of ethical considerations depending on the type of data. For data in the public domain, such as published fction or online newspaper text, it is not necessary to secure consent. However, such data may be protected by copyright. For data that is collected from scratch by the researcher, it is necessary to obtain the informants’ informed consent and it may be necessary to ask for institutional approval. 
In the case of already published material, permission may be needed from a publisher or some other copyright holder. There are grey areas in copyright law and copyright infringement is looked at in different ways in different parts of the world, so it is diffcult to fnd universally valid advice on the topic, but generally speaking copyright may prove quite a hindrance for corpus compilation. To a certain extent, restrictions on copyright may be alleviated through concepts such as ‘fair use’, as texts in a corpus are typically used for research or teaching purposes only, with no bearing on the market.4 However, copyright holders and judges are likely to distinguish between material that is used by a single researcher only and material that is distributed to other researchers, so it may matter whether or not the corpus is made available to the wider research community. In addition to the potential difference between data gathering for a single use versus data distribution for repeated use by many different people, copyright holders may be more likely to grant permission to use an extract rather than a complete text. 
In the case of collecting data from informants, approval may be needed from an institutional ethics review board before the project can begin. Even if institutional approval is not needed, consent needs to be sought from the informants in order to collect and use the data for research purposes. Asking for permission to use material for a corpus is often done by means of a consent form, which is signed by each informant, or by the legal guardians in the case of children (see Chap. 14). A consent form should clearly state what the data will be used for so that an informed decision can be made. It needs to be clear that the decision to give consent is completely voluntary. It is important how the consent form is worded, so it is useful to consider forms used in similar corpus projects for comparison.5 If a participant does not give his or her consent, the data will have to be removed from the corpus. In the case of multi-party interactions, it may still be worth including the data if most participants have given their consent, while blanking out contributions from the non­consent-giving participant. See Crasborn (2010) for a problematized view of consent in connection with online publication of data. 
Once permission has been obtained to use data for a corpus, the informants’ integrity needs to be protected in different ways, such as by anonymizing the material. An initial step may be to not reveal the identity of the informants by not showing their real names, for example through ‘pseudonymisation’, whereby personal data is transformed in such a way that it cannot be attributed to a specifc informant without the use of additional information, which is kept separately. A second step may be to manipulate the actual linguistic data (that is, what the 
4Fair use is measured through the purpose and character of the use, the nature of the copyrighted work, the amount and substantiality of the portion taken, and the effect of the use on the potential market. For more information, see https://fairuse.stanford.edu/overview/fair-use/four­
factors/. Accessed 24 May 2019. 
5For sample templates, see the forms from the Bavarian Archive for Speech Signals at http://www.phonetik.uni-muenchen.de/Bas/BasTemplateInformedConsent_en.pdf, or Newcas­tle University at https://www.ncl.ac.uk/media/wwwnclacuk/research/fles/Example%20Consent %20Form.pdf. Accessed 29 May 2019. 
people represented in the corpus said or wrote) by changing also names and places mentioned which could in some way give away the source. In the case of image data, this would involve masking participants’ identity in various ways. 
Confdential data needs to be stored in a safe way. Sensitive information may have to be destroyed if there is a risk that others may access information which informants have been promised will not be revealed. For further reading on ethical perspectives on data collection, see e.g. BAAL’s Recommendations on Good Practice in Applied Linguistics.6 It complicates matters that regulations may differ by region. While ethics review boards have been in place for quite some time at universities in the United States, linguists in Europe have been relatively free to collect data. It is not clear, however, what long-term effects the General Data Protection Regulation (https://www.eugdpr.org/; effective as of 2018) will have for data collected in the European Union. 

1.2.4 Documenting What Is in the Corpus 
As language use is characterized by variability, factors which may have an impact on the way in which language is used should be recorded in some way—these may include demographic information about the speakers/writers, or situational information such as the purpose of the communication or the type of relationship between the discourse participants. Even if the corpus compilers are deeply familiar with the material, it is still the case that memory is both short and fallible, so if they want to use the corpus in a few years’ time, important details of the specifc context of the data may well have been forgotten. In addition, if the corpus is made available to others, they need to know what is in it in order to make an informed decision about whether the design of the corpus is appropriate for answering their specifc research questions. 
Anybody who wants the claims made based on a corpus to be accepted by the research community needs to show in some way that the corpus material is appropriate for the type of research done. With incomplete description of the corpus, people will be left wondering whether the material was in fact valid for the study. There are several different ways in which information about the corpus design can be disseminated. It can be done through a research publication, such as a research article or an MA thesis, which includes a section or chapter describing the material (for more on this, see Chap. 26). Corpus descriptions are sometimes published in peer-reviewed journals, especially if the corpus is breaking new ground (as is the case in Representative Study 2 below), so that the research community can beneft from discussions on corpus design. It can also be done by writing a report solely dedicated to describing the corpus (and possibly how to use it), which is made 
6See https://baalweb.fles.wordpress.com/2016/10/goodpractice_full_2016.pdf. Accessed 24 May 2019. 
available either as a separate fle stored together with the corpus itself, or online. Corpora often come with “read me” fles where the corpus design is accounted for. Some large corpus projects intended to attract large numbers of users, such as the British National Corpus (BNC) and the Michigan Corpus of Academic Spoken English (MICASE), provide relatively detailed reports online.7 There are also published books which offer even more detailed documentation of corpora and recommendations for how to use them (e.g. Aston and Burnard’s (1998) The BNC Handbook and Simpson-Vlach and Leicher’s (2006) The MICASE Handbook). 
Another reason for documenting what is in the corpus is to enable researchers to draw on various variables in a systematic way when analyzing data from the corpus. As an example, see Chap. 8 and the subsection on quantitative analysis of concordance lines. In a study of that-complementation in English, for each hit in the corpus, the researchers considered external variables such as the L1 of the speaker who had produced the hit and whether the hit came from a written or spoken mode. 
Through the inclusion of ‘metadata’—data about the data—about the type of discourse represented in the corpus, the corpus user can keep track of or investigate different factors that may infuence language use, which may explain differences observed in different types of data. Metadata can consist of different types of information. For example, the corpus compiler may include information based on interviews with participants or participant observation. A common way of collecting metadata is by asking corpus participants to fll out a questionnaire which has been carefully designed by the corpus compiler so as to include information likely to be relevant with respect to the specifc context of the discourse included and the people represented. An example of metadata based on a questionnaire from the International Corpus of Learner English (ICLE) is summarized in Fig. 1.1.The ICLE is a large-scale project with collaborators from several different countries. (For more information on learner corpora, see Chap. 13.) The corpus includes metadata about the type of discourse included (written essays) and about the language users represented (university students), collected through a questionnaire called a ‘learner profle’, as the contributors are all learners of English. In a language-learning context, some of the variables likely to be relevant include what the learner’s frst language is (2e), what the medium of instruction was in school (2i; 2j), how much exposure the learner has had to the second language—whether through instruction in a school context (2l) or through spending time in a context where the second language is spoken (2q). 
Based on metadata from the questionnaire, it is possible to select a subset of the ICLE corpus, for example to study systematically potential differences in language use between learners who have and who have not spent any time abroad in a country where the target language is spoken natively—and thus test a hypothesis from Second Language Acquisition research. 
7See http://www.natcorp.ox.ac.uk/docs/URG/ (Accessed 24 May 2019) and https://web.archive. org/web/20130302203713/http://micase.elicorpora.info/fles/0000/0015/MICASE_MANUAL.pdf 
(Accessed 24 May 2019). 
Metadata about the discourse (essay)  
1a  Title:  
1b  Approximate length required: -500 words/+500 words  
1c  Conditions: timed/untimed  
1d  Examination: yes/no  
1e  Reference tools: yes/no  
1f  -> What reference tools? Bilingual dictionary / English monolingual dictionary / Grammar / Other(s)  
Metadata about the informant (university student)  
2a  Surname, First names:  
2b  Age:  
2c  Gender: M/F  
2d  Nationality:  
2e  Native language:  
2f  Father’s mother tongue:  
2g  Mother’s mother tongue:  
2h  Language(s) spoken at home: (if more than one, give average % use of each)  
2i  Primary school - medium of instruction:  
2j  Secondary school - medium of instruction:  
2k  Current studies:  
2l  Current year of study:  
2m  Institution:  
2n  Medium of instruction: English only / Other language(s) (specify) / Both  
2o  Years of English at school:  
2p  Years of English at university:  
2q  Stay in an English-speaking country: -> Where? When? How long?  
2r  Other foreign languages in decreasing order of proficiency:  
Fig. 1.1 An example of metadata collected for a corpus: The learner profle for the ICLE. (Adapted from https://uclouvain.be/en/research-institutes/ilc/cecl/corpus-collection-guidelines. html. Accessed 24 May 2019) 


Three different documents that are commonly used in corpus compilation have been brought up above: (i) the consent form from the participants, (ii) the questionnaire asking for various types of metadata about the participants and the discourse and (iii) a text, possibly in a “read me” fle, which documents what is in the corpus. Corpus compilers who are collecting publicly available data in such a way that they do not need (i) or (ii), may still choose to compile metadata to help track for instance various types of sociolinguistic information about the corpus participants. However, if both (i) and (ii) are needed, it is a good idea to investigate the possibility of setting them up electronically, such as on a website, to avoid having to type in all the responses manually. 

1.2.5 Formatting and Enriching the Corpus 
There is a great deal to be said about how best to format corpus material, but this section will merely offer a few hints on technicalities. (More detailed information is found in Chaps. 2 and 3.) Researchers’ computational needs and programming skills vary. Those who are reasonably computer literate and whose corpus needs are relatively simple are likely to be able to do all the formatting themselves. However, those who wish to compile a corpus involving complex types of information or do advanced types of (semi-)automatic corpus searches would be helped by collaborating with a computational linguist or computer programmer (see Chap. 9). 
A plain text format (such as .txt) is often used for corpus fles. MS Word formats are avoided, as these add various types of information to the fle and do not work with corpus tools such as concordance programs. When naming fles for the corpus, it is useful to have the fle name in some way refect what is in the fle. For example, the fle name ‘BIO.G0.02.3’ in a corpus of university student writing across disciplines and student levels (Michigan Corpus of Upper-level Student Papers, MICUSP; see Rer and O’Donnell 2011), consists of an initial discipline code (‘Biology’), a student level code (‘G0’ stands for fnal year undergraduate; while ‘G1’ stands for frst year graduate, etc.), followed by a student and paper number (‘02.3’ refers to the third paper submitted by the second student at that level). Codes not only make it easier for the analyst to select the relevant fles, but are also useful when analyzing concordance results, as the codes may help reveal patterns in the data. For example, in studying adverbial usage in student writing, the analyst may fnd that all of the hits for the relatively informal adverbial maybe come from texts coded with the lowest student level (‘G0’). 
It may be necessary, or just a good investment of time, to add markup, that is, “codes inserted into a corpus fle to indicate features of the original text rather than the actual words of the text. In a spoken text, markup might include utterance breaks, speaker identifcation codes, and so on; in a written text, it might include paragraph breaks, indications of omitted pictures and other aspects of layout” (McEnery and Hardie 2012:246). If we take an example from the corpus of university student writing mentioned above, one of the marked-up features is quoted material. This makes it possible to exclude quotations when searching the running text, based on the logic that most corpus users would be primarily interested in text produced by novice academics themselves, and not material brought in from primary or secondary sources. 
Markup allows the corpus builder to include important information about each fle in the corpus. Various types of metadata can be placed in a separate fle or in a ‘header’, so that a computer script or web-based tool for example will be able to use the information in systematic ways when counting frequencies, searching for or displaying relevant data. If we consider the metadata from the ICLE (Fig. 1.1 above) again, it makes it possible to distinguish for instance between those essays which were timed versus untimed, or between essays written by learners who have never 
3a 3b 3c 3d 3e 3f 3g 3h 3i 3j 3k 3l 3m 3n 3o 3p 3q 3r 3s 3t 3u 3v 3x 3y 3z  <s n="652"> <w c5="CJC" hw="but" pos="CONJ">But </w> <w c5="AV0" hw="like" pos="ADV">like</w> <c c5="PUN">, </c> <w c5="PNP" hw="i" pos="PRON">I</w> <w c5="VBD" hw="be" pos="VERB">was</w> <seg Quotatives="QTG"> <w c5="VVG" hw="think" pos="VERB">thinking</w> </seg> <pause/> <seg Reporting_modes="MDD"> <w c5="DT0" hw="this" pos="ADJ">this </w> <w c5="VBZ" hw="be" pos="VERB">is </w> <w c5="VVG" hw="gon" pos="VERB">gon</w> <w c5="TO0" hw="na" pos="PREP">na </w> <w c5="VBI" hw="be" pos="VERB">be </w> <w c5="AV0" hw="so" pos="ADV">so </w> <w c5="AJ0" hw="embarrassing" pos="ADJ">embarrassing </w> <w c5="AV0" hw="like" pos="ADV">like </w> <w c5="PRP" hw="in" pos="PREP">in </w> <w c5="ZZ0" hw="p" pos="SUBST">P </w> <w c5="ZZ0" hw="e" pos="SUBST">E</w> <c c5="PUN">!</c> </seg> </s>  
Fig. 1.2 An illustration of XML annotation: Sentence (a) from the corpus in Representative Study 


2. (Based on Rlemann and O’Donnell 2012:337) 
stayed in a country where the target language is spoken versus learners who have reported on relatively extensive stays in such a context. 
Another way of adding useful information to a corpus is through annotation, or “codes within a corpus that embody one or more linguistic analyses of the language in the corpus” (McEnery and Hardie 2012:238). Annotation can be done manually or (semi-)automatically (see Chap. 2 for information about automatic annotation). Annotation helps to make the data more interesting and useful. It can be done at any linguistic level, including for example classifcation of word class for each word in the corpus (POS-tagging; see Fig. 1.2), indication of prosodic features of spoken data, or pragmatic marking of politeness phenomena. Representative study 2 presents annotations of narratives in conversation, which for example involved adding a code for the degree to which an utterance is represented as verbatim or indirect. Example (a) from the corpus includes a sentence from an utterance where the underlined unit is coded ‘MDD’ (3k in Fig. 1.2) for a verbatim presentation mode. 
(a) But like, I was thinking this is gonna be so embarrassing like in P E! 
The contemporary standard for corpus markup and annotation is XML (eXtensi­ble Markup Language), where added information is indicated by angle brackets <>, as illustrated in Fig. 1.2, which represents the above sentence. The sentence opens with an <s> tag including a number which uniquely identifes it (3a), and closes with an end tag </s> (3z). Each word also has an opening <w> tag, giving information about lemma forms and part of speech (‘pos’), and a closing </w> tag. The quotative verb (3h), for example, is labelled “VERB” and, more specifcally, “VVG” to mark the –ing form of a lexical verb. We can also see, for example, that 3f (was), 3m (is) and 3p (be) instantiate different forms of the lemma BE. 
XML is ideal “because of its standard nature” and “because so much corpus software is (at least partially) XML-aware” (Hardie 2014:77–78). This does not mean, however, that it is necessary to use in corpus building. While Representative Study 2 is at the advanced end regarding corpus formatting, Representative Study 1 uses raw corpus texts and does not even mention XML or annotation. The degree to which a corpus is enriched will depend partly on the research objectives. MICUSP was mentioned above as an example of a corpus created with the aim of mapping advanced student writing across different levels and disciplines. As mentioned, quoted material was marked up to enable automatic separation between the students’ own writing and writing from other sources. It is also an example of a corpus that is distributed to others, which means that the compilers put a greater effort into marking up the data for a range of potential future research projects. For those wishing to learn more about XML for corpus construction, Hardie (2014:73) is a good place to start. 
Even more fundamental than markup or annotation is encoding, which refers to “the process of representing a text as a sequence of characters in computer memory” (McEnery and Hardie 2012:243). We want corpus texts to be rendered and recognized the same way regardless of computer platform or software, but for example accented characters in Western European languages (such as ç and ä) may cause problems if standard encoding formats are not used. How characters are encoded may be an issue especially for non-alphabetical languages. A useful source on the fundamentals of character encoding in corpus creation is McEnery and Xiao (2005), who recommend the format UTF-8 for corpus construction, as it represents “a universal format for data exchange” in the Unicode standard. Unicode is “a large character set covering most of the world’s writing systems, offering a way of standardizing the hundreds of different encoding systems for rendering electronic text in different languages, which were often conficting” (Baker et al. 2006:163) in the past. Unicode and XML together currently form a standard in corpus building. 
There are many considerations for formatting corpus material in ways that follow current standards and best practice. An authoritative source is the Text Encoding Initiative (TEI),8 which represents a collective enterprise for developing and maintaining international guidelines. The TEI provides recommendations for different aspects of corpus building, ranging from how to transcribe spoken data to what to put in the ‘header’. As mentioned above, some corpus projects make use of ‘headers’ placed at the top of each corpus fle. A TEI-conformant header 
8See https://tei-c.org/. Accessed 24 May 2019. 
should at least document the corpus fle with respect to the text itself, its source, its 
encoding, and its (possible) revisions. This type of information can be used directly by linguists searching the corpus texts, but most often it is processed automatically by corpus tools to help the linguist pre-select fles, visualize the distribution of variables, display characters correctly, and so on. 

1.2.6 Sharing the Corpus 
One of many ways in which corpora vary is in how extensively and long-term they are intended to be used. A corpus can be designed to be the key material for many different research projects for a long time to come, or it can be created with a single project in mind, with no concrete plan to make it available to others. In the former category, we fnd ‘standard’ corpora, which are widely distributed and which form the basis for a large body of research. This type of corpus is designed to be representative of a large group of speakers, typically adopting “the ambitious goal of representing a complete language”, as Biber (1993:244) puts it. In the latter category, we fnd a large and ever-growing number of corpora created on a much more modest scope, focusing on a small subset of language. These are oftentimes used by a single researcher to answer one specifc set of research questions, as in the case of Representative Study 1. 
Even in the context of a small-scale corpus project, it is considered good practice in research to make one’s data available to others. It supports the principle of replicability in research and it fosters generosity in the research community. Our time will be much better invested if more than one person actually uses the material we have put together so meticulously. Certain types of data will be of great interest to not only researchers or teachers and students, but also the producer community itself, as in the case of sign language corpora (e.g. Crasborn 2010). Sharing one’s corpus is in fact to an increasing extent a requirement; some bodies of research funding make ‘open access’ a precondition for receiving any funding. When sharing a corpus, it is common to apply licensing. Making a corpus subject to a user licence agreement provides a way of keeping a record of the users and of enforcing specifc terms of use. Corpora published online may for example be made available to others through a Creative Commons licence in order to prohibit proft-making from the material.9 However, even with such a licence in place, it may be diffcult for corpus compilers to enforce compliance, which is another reason for taking very seriously the protection of informants’ integrity. 
Even if open access is not a requirement, in a case where a researcher is applying for funding to compile a corpus for a research project, it may be a good idea to include an entry in the budget for eventually making the corpus available. If, say for various reasons related to copyright, it is not possible to make the complete set of 
9See https://creativecommons.org/licenses/. Accessed 24 May 2019. 
corpus fles available to others, the corpus could still be made searchable online and concordance lines from the corpus be shown. 
Another consideration in sharing corpus resources involves how to make these accessible to others and how to preserve digital data. The easiest option is to fnd an archive for the corpus, such as The Oxford Text Archive or CLARIN.10 

1.2.7 Corpus Comparison 
Corpus data are typically studied quantitatively in some capacity. This means that the researcher will have various numbers to which to relate, which typically give rise to questions such as ‘Is a frequency of X a lot or a little?’. Such questions are diffcult to answer in a vacuum, but are more usefully explored by means of comparison—for example by studying the target linguistic phenomenon not just in one context, but contrasting it across different contexts. Statistics can then be used to support the interpretation of results across two or more corpora, or to assess the similarity between two or more corpora (see e.g. Kilgarriff (2001) for a classic paper taking a statistical approach to measuring corpus similarity). 
The researcher may go on to ask qualitative questions such as ‘How is phenomenon X used?’ and systematically study similarities and differences in (sub-)corpus A and (sub-)corpus B. Even if frequencies are similar in cross-corpus comparison, it may be the case that, once you scratch the surface and do a qualitative analysis of how the individual examples are actually used, considerable differences emerge. In order for the comparison to be valid, however, the two sets ((sub-)corpus A and (sub-)corpus B) need to be maximally comparable with regard to all or most factors, except for the one being contrasted. 
Some corpora are intentionally constructed for comparative studies (this includes parallel corpora, covered in Chap. 12). In contrastive studies of different languages or varieties, for example, it is useful to have a so-called comparable corpus, which “contains two or more sections sampled from different languages or varieties of the same language in such a way as to ensure comparability” (McEnery and Hardie 2012:240). The way in which the texts included in the corpora have been chosen should be identical or similar—that is, covering the same type of discourse, taken from the same period of time, etc.—to avoid comparing apples to oranges. 
Having considered some of the fundamentals of corpus compilation, we will next turn to the two sample studies, which will illustrate further many of the concepts mentioned in this section. 
10See https://ota.ox.ac.uk/ (Accessed 24 May 2019) and https://www.clarin-d.net/en/corpora (Accessed 24 May 2019). 
Representative Study 1 
Jaworska, S. 2016. A comparison of Western and local descriptions of hosts in promotional tourism discourse. Corpora 11(1): 83–111. 
Jaworska (2016:84) makes the point that “corpus tools and methods [are] increasingly used to study discursive constructions of social groups, espe­cially the social Other—that is, groups that have been marginalised and discriminated against”.11 In this study, corpus methods are used to investigate promotional tourism discourse and ways in which local people (hosts) are represented. Previous research in the area is based on small samples of texts and looks at representations in one destination or region, so there is typically no comparison across contexts. The research questions for the study are: 
1. 
How are hosts represented in tourism promotional materials produced by Western versus local tourist industries? 

2. 
To what extent do these representations differ? 

3. 
What is the nature of the relationship between the representations found in the data and existing stereotypical, colonial, and often gendered ideolo­gies? 


To answer these questions, two corpora were created, consisting of written texts promoting tourist destinations that have a history of being colonised. The two corpora represent, on the one hand, a Western, ‘external’ perspective and, on the other, a local, ‘internal’ perspective, which are contrasted in the study. They are labelled the External Corpus (EC) and the Internal Corpus (IC). 
To create the EC, texts were manually taken from the websites of “some of the largest tourism corporations operating in Western Europe”. A selection of 16 destinations was made, based on the most popular destinations as identifed by the companies themselves during the period of data collection—however excluding Southern European destinations, as the focus of the research was on post-colonial discursive practices. To create the IC, offcial tourism websites were sourced from the 16 countries selected in the process of creating the EC. All of the websites are listed in an appendix to the article. 
A restriction imposed on the data selection for both corpora was to include only “texts that describe the countries and its main destinations (regions and towns)” rather than specifc resorts or hotels or information on how to get there. This was to make the two corpora as comparable as possible. However, one way in which they differ is with respect to size, with the IC being three 
(continued) 
11A similar example is reported in Chap. 8 and involves a study of how foreign doctors are represented in a corpus of British press articles. 
times as big as the EC, as “local tourism boards [offer] longer descriptions and more details” (92). The solution to comparing corpora of different sizes was to normalise the numbers, rather than reduce the size of the IC. The author’s rationale was that reducing the IC would have “compromise[d] the context and the discourse of local tourism boards in that some valuable textual data could have been lost” (92). 
The corpora were compared by extracting lists of the most frequent nouns (cf. Chap. 4). From these lists were identifed the most frequent items used to refer to local people (e.g. people, locals, man/men, woman/women, fshermen). Careful manual analysis was required in order to check that each instance was relevant, that is, actually referring to hosts/local people. The word people, for example, was also sometimes used to refer to tourists. It was found that the IC had not only more tokens of such references, but also more types (F = 68) compared to the EC (F = 20). The tokens were further classifed into socio-semantic groups of social actors based on an adapted taxonomy from the literature, for example based on ‘occupation’ (fsherman, butler), ‘provenance’ (locals, inhabitants), ‘relationship’ (tribe, citizens), ‘religion’ (devotees, pilgrims), ‘kinship’ (son/s, child/ren) and ‘gender’ (man/men, woman/women). 
The corpora were compared qualitatively as well, by identifying patterns in the concordance lines and analysing the context (“collocational profles”) of the references to hosts, specifcally of people and locals, which occurred in both corpora. The pattern found for locals was that local people were represented “on an equal footing with tourists” in the IC, while in the EC they were portrayed as “docile, friendly and smiley servants [,] reproduc[ing] and maintain[ing] the ideological colonial asymmetry” (104). 
Representative Study 2 
Rlemann, C. and O’Donnell, M.B. 2012. The creation and annotation 
of a corpus of conversational narratives. Corpus Linguistics and Lin­
guistic Theory 8(2): 313–350. 
Rlemann and O’Donnell’s (2012) article Introducing a corpus of conversa­tional stories: Construction and annotation of the Narrative Corpus describes the main features of a corpus of conversational narratives. Research has shown that it is extremely common for people to tell stories in everyday conversation. The authors hope that the use of the corpus “will advance the linguistic theory of narrative as a primary mode of everyday spoken interaction” (315). 
(continued) 
Previous work on this type of discourse has been based not on corpus data, but on elicited interviews or narratives told by professional narrators. 
The corpus comprises selected extracts of narratives, 153 in all, for a total of around 150,000 words, taken from the demographically sampled ‘casual conversations’ section of the BNC, which is balanced by sex, age group, region and social class, and which totals approximately 4.5 million words. This example is somewhat unusual in that the authors do not collect the data themselves, but instead use a selection of data from an existing corpus. However, given that the intended audience of this handbook is expected to have limited resources for corpus compilation, it seems useful to provide an example of a study where it was possible to use part of an already existing corpus. The NC is only about 3% of the original collection from BNC, so the authors have put a great deal of effort into selecting the data, which is done in a transparent and principled way. In the article, they describe (i) the extraction techniques, (ii) selection criteria and (iii) sampling methods used in constructing the corpus. In order to (i) retrieve narratives, they (a) read the fles manually and (b) used a small set of lexical forms (e.g. it was so funny/weird; did I tell you; reminds me) that tend to occur in narratives based on the literature or based on analysis of their own data. In (ii) deciding what counts as a conversational narrative, they used three selection criteria: First, some kind of ‘exosituational orientation’ needed to be present in the discourse, that is, “linguistic evidence of the fact that stories relate sequences of events that happened in a situation remote from the present, story-telling, situation” (317)—this includes for example the use of past tense verbs; items with past time reference as in yesterday; reference to locations not identical to the location of speaking. A second criterion was that at least two narrative clauses be present, which are temporally related so that frst one event takes place and then another. A third criterion involved consensus, so that at least two researchers agreed that a given example was in fact a narrative. With respect to (iii) sampling, the authors retained the sociological balance from the demographically sampled BNC by choosing two texts from each fle insofar as this was possible. 
The NC is not only a carefully selected subset of the demographically sampled BNC, but it is also annotated. The corpus builders have thus augmented the existing data by adding various types of information—about the speakers (sex, age, social class, region of origin, educational background), about the text (type of narrative; whether a stand-alone story or part of a ‘narrative chain’) and about the utterance (the roles of the participants vis-à-vis the narration; type of quotative verb used to signal who said what in a narrative; to what degree the discourse is represented as being verbatim or more or less indirect). The authors stress that all of the annotation is justifed in some way by the literature on conversational narrative, so the rationale for 
(continued) 
including a layer of analysis to the corpus text is to enable researchers to answer central research questions in a systematic fashion. 
The corpus design makes it possible to use the demographic information about the speakers—such as sex—and consider how it is distributed in relation to the number of words uttered by the speakers who are involved in the narratives, as exemplifed in Table 1.1. Note the presence of a category of “unknown”, which is useful when relevant metadata is missing. 
Each narrative in the corpus is classifed also based on a taxonomy of narrative types. This type of information is highly useful, as it not only makes it possible to study and compare different types of narrative, but it also shows how the corpus is balanced (or not) with respect to type of narrative. The classifcation is justifed by an observation from the literature that “we are probably better off [] considering narrative genre as a continuous cline, consisting of many subgenres, each of which may need differential research treatment” (Ervin-Tripp and Ktay 1997:139, cited in Rlemann and O’Donnell 2012:321). The annotation includes two features: experiencer person (whether frst person or third person, that is, direct involvement by nar­rator versus hearsay) and type of experience (personal experiences; recurrent generalized experiences; dreams; fantasies; jokes; mediated experiences). The last subcategory refers to the common practice of retelling a flm or a novel. 
At the time of creation, the NC was the frst corpus of conversational narra­tives to be annotated, so there was no established practice to follow regarding what analytical categories to annotate. However, the authors were able to follow some general guidelines, for example Leech’s (1997) ‘standards’ for corpus annotation concerning how to design the labels in the tagsets (e.g. they should be (a) easy to interpret and (b) concise, consisting of no more than three characters). 
Table 1.1 Distributions of male and female narrative participants involved in narratives [based on a subset of the total corpus] 
Sex  Number of participants  %  Number of words  %  
Female  212  42  44,476  56  
Male  173  35  24,268  31  
Unknown  115  23  10,079  13  
Total  500  100  78,823  100  

Rlemann and O’Donnell (2012:320) 


1.3 Critical Assessment and Future Directions 
The representation of a group of language users/variety of language/type of dis­course in a corpus inevitably involves simplifcation and loss of complex contextual information. If we consider the future of corpus building from the perspective of the loss of complex information, it is interesting to note that few existing corpora refect a feature which many present-day types of discourses exhibit: that of multimodality. It represents information of a kind that many corpus creators have expressed an interest in, but which few corpus projects have included (see Chap. 16 for more information). If we take the two sample studies as an example, they would both have beneftted from multimodal data. In Jaworska (2016:105), this is explicitly commented on by the author, who says that “given that images are an integral part of tourism promotional discourse, further studies would need to complement a quantitative textual analysis with a multi-modal approach based on a systematic examination of the visual material in order to reveal other semiotic resources”. In the case of the corpus of narratives described in Rlemann and O’Donnell (2012), it was constructed based on the BNC, for which there is no information other than the speech signal (sound from recordings) and the transcriptions of this in the case of the spoken data. This is critiqued in a review of a monograph by Rlemann, where the reviewer makes the point that “[t]he one glaring limitation to using pre­existing transcribed texts such as these from the BNC is the paucity of information on the paralinguistics going on during storytelling, including glance, gesture, tone of voice and, since the central topic of the volume is narrative co-construction and recipient feedback, this is a signifcant absence” (Partington 2015:169). 
Regarding the inevitable loss of contextual information in the making of a corpus, it is important to attempt to compensate for this by means of rich metadata that describe the material. With better metadata about individual texts and speakers, we will be in a better position to understand the data, not only to correlate metadata to variation, but also to see more precisely how corpora differ in the case of comparison. Corpus enrichement is an important way forward, and this applies not only to metadata but also to linguistic annotation. Some of the possibilities of corpus annotation are presented in the next chapter. In order to promote and make better use of corpus enrichment, there is a need for collaborative work between linguists with a deep knowledge of the needs to different areas such as Second Language Acquisition or Historical Linguistics and experts in Computational Linguistics or Natural Language Processing. 
Further Reading 
Biber, D. 1993. Representativeness in Corpus Design. Literary and Linguistic Computing 8(4): 243–257. 
Biber’s work is signifcant not only in having had quite an impact on the feld, but also in its attempt to develop empirical methods for evaluating corpus representa­tiveness. 
Wynne, M. (Editor). 2005. Developing Linguistic Corpora: a Guide to Good Practice. Oxford: Oxbow Books. http://ota.ox.ac.uk/documents/creating/dlc/. 
There is a surprising dearth of reference works on corpus compilation. Even if this collection of chapters is not recent, it is still worth reading. 
References 
Aston, G., & Burnard, L. (1998). The BNC handbook: Exploring the British National Corpus with SARA. Edinburgh: Edinburgh University Press. 
Baker, P., Hardie, A., & McEnery, T. (2006). A glossary of corpus linguistics. Edinburgh: Edinburgh University Press. 
Biber, D. (1993). Representativeness in corpus design. Literary and Linguistic Computing, 8(4), 243–257. 
Crasborn, O. (2010). What does ‘informed consent’ mean in the internet age? Publishing sign language corpora as open content. Sign Language Studies, 10(2), 276–290. 
Douglas, F. M. (2003). The Scottish corpus of texts and speech: Problems of corpus design. Literary and Linguistic Computing, 18(1), 23–37. 
Francis, W. N., & Kucera, H. (1964/1979). Manual of information to accompany a standard corpus of present-day edited American English, for use with digital computers. Department of Linguis­tics, Brown University. http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM. Accessed 24 May 2019. 
Hardie, A. (2014). Modest XML for Corpora: Not a standard, but a suggestion. ICAME Journal, 38, 72–103. 
Jaworska, S. (2016). A comparative corpus-assisted discourse study of the representations of hosts in promotional tourism discourse. Corpora, 11(1), 83–111. https://doi.org/10.3366/ cor.2016.0086. 
Johansson, S., Leech, G., & Goodluck, H. (1978). Manual of information to accompany the Lancaster-Oslo/Bergen Corpus of British English, for use with digital computers. Department of English, University of Oslo. http://clu.uni.no/icame/manuals/LOB/INDEX.HTM. Accessed 24 May 2019. 
Kilgarriff, A. (2001). Comparing corpora. International Journal of Corpus Linguistics, 6(1), 97– 133. 
Leech, G. (1997). Introducing corpus annotation. In R. Garside, G. Leech, & T. McEnery (Eds.), Corpus annotation: Linguistic information from computer text corpora (pp. 1–18). London: Longman. 
Leech, G. (2007). New resources, or just better old ones? In M. Hundt, N. Nesselhauf, & C. Biewer (Eds.), Corpus linguistics and the web (pp. 134–149). Amsterdam: Rodopi. 
McEnery, T., & Hardie, A. (2012). Corpus linguistics: Method, theory and practice. Cambridge: Cambridge University Press. 
McEnery, T., & Xiao, R. (2005). Character encoding in corpus construction. In M. Wynne (Ed.), Developing linguistic corpora: A guide to good practice (pp. 47–58). Oxford: Oxbow Books. 
Partington, A. (2015). Review of Rlemann (2014) Narrative in English conversation: A corpus analysis of storytelling. ICAME Journal, 39. https://doi.org/10.1515/icame-2015-0011. 
Rer, U., & O’Donnell, M. B. (2011). From student hard drive to web corpus (part 1): The design, compilation and genre classifcation of the Michigan Corpus of Upper-level Student Papers (MICUSP). Corpora, 6(2), 159–177. 
Rlemann, C., & O’Donnell, M. B. (2012). Introducing a corpus of conversational stories. Construction and annotation of the Narrative Corpus. Corpus Linguistics and Linguistic Theory, 8(2), 313–350. https://doi.org/10.1515/cllt-2012-0015. 
Simpson-Vlach, R., & Leicher, S. (2006). The MICASE handbook: A resource for users of the Michigan Corpus of Academic Spoken English. Ann Arbor: University of Michigan Press. 
Sinclair, J. (2005). Corpus and text – basic principles. In M. Wynne (Ed.), Developing linguistic corpora: A guide to good practice (pp. 1–16). Oxford: Oxbow Books. 


Chapter 2 Corpus Annotation 
John Newman and Christopher Cox 
Abstract In this chapter, we provide an overview of the main concepts relating to corpus annotation, along with some discussion of the practical aspects of creating annotated texts and working with them. Our overview is restricted to automatic annotation of electronic text, which is the most common kind of annotation in the context of contemporary corpus linguistics. We focus on the annotation of texts which typically follow established orthographic principles and consider the follow­ing four main types of annotation, using English for the purposes of illustration: 
(1) part-of-speech (POS) tagging, (2) lemmatization, (3) syntactic parsing, and (4) semantic annotation. The accuracy of annotation is a key factor in any evaluation of annotation schemes and we discuss methods to verify annotation accuracy, including precision and recall measures. Finally, we briefy consider newer developments in two broad areas: the annotation of multimodal corpora and the annotation of Indigenous and endangered language materials. Both of these developments refect changing priorities on the part of linguistic researchers, and both present signifcant challenges when it comes to automated annotation. 
2.1 Introduction 
Annotation provides ways to enhance the value of a corpus by adding to the corpus information about parts of the corpus. While there may be a variety of types of annotation, including, for example, adding information about persons or places referenced in historical texts, our focus here is linguistic annotation. Such annotation 
J. Newman (•) University of Alberta, Edmonton, Canada 
Monash University, Melbourne, Australia e-mail: john.newman@ualberta.ca 
C. Cox Carleton University, Ottawa, Canada e-mail: christopher.cox@carleton.ca 
© Springer Nature Switzerland AG 2020 25 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_2 
J. Newman and C. Cox 
most typically takes the form of adding linguistically relevant information about words, phrases, and clausal/sentential units, though other linguistic units can also be annotated, e.g., morphemes, intonation units, conversational turns, and paragraphs. The reality of contemporary corpus linguistics is that the corpora we rely on, in most cases, are simply too large for manually adding annotation, and the automated annotation of electronic texts has become the primary focus in the development of annotation methods. Consequently, it is automated linguistic annotation that we will be concerned with in this chapter (see Part III of this volume for discussion of manual annotation in certain kinds of corpora, e.g., annotation of errors in a learner corpus). In order to simplify the discussion that follows, we illustrate our points about the fundamentals of annotation using primarily English data. 
While the raw text of an unannotated corpus has its own unique value, (wisely) annotated corpora offer great advantages over the raw text when it comes to the investigation of linguistic phenomena. Most linguistic phenomena of interest to linguists are couched in terms of linguistic constructs (the plural morpheme, the passive construction, time adverbials, the subject of a verb, etc.), rather than orthographic words. A corpus that has been annotated with the needs of linguists in mind can greatly facilitate the exploration of such phenomena by reducing the time and effort involved. Even if an automatically annotated corpus is unlikely to meet all the expectations of a researcher in terms of its categories of annotation, it can still be an invaluable resource. 

2.2 Fundamentals 
In the following sub-sections, we deal with a few main types of annotation (part­of-speech tagging, lemmatization, syntactic parsing, and semantic annotation), the accuracy of annotation, and the practicalities of carrying out annotation of texts. 

2.2.1 Part-of-Speech Tagging 
Part-of-speech (POS) tagging is a common form of linguistic annotation that labels or “tags” each word of a corpus with information about that word’s grammatical category (e.g., noun, verb, adjective, etc.). Any such tagging assumes prior tok­enization of the text, i.e., division of the text into units appropriate for analysis and annotation. Tokenization in fact is a prerequisite for most kinds of annotation and presents challenges in its own right. Although it is convenient to simply refer here to the tagging of “words”, tokenizing a text into word units subsumes a number of critical decisions that we put aside here (see Chap. 3 for further discussion of tokenization and related issues). 
There are many POS tagsets currently used in English corpus analysis, varying in degree of differentiation of POS categories and in the nature of the categories themselves (see Atwell 2008 for an overview of English POS tagsets). One commonly used tagset is CLAWS (Constituent Likelihood Automatic Word-tagging System), available in different versions (e.g., CLAWS 5 contains just over 60 tags, while CLAWS 7 contains over 160). (1a) is an example of a sentence that has been tagged using the CLAWS 7 tagset and (1b) shows the descriptions of the tags used in (1a), as given in the CLAWS documentation.1 Most of these tags correspond to familiar parts of speech from traditional grammatical analysis of English (article, infnitive, singular common noun etc.), though some other tags are less familiar (e.g., after-determiner). A key consideration in preparing tagged corpora is that the tag appears in some predictable format along with the word it is associated with, as is the case in (1a) where the POS tag is appended to a word using the underscore as a separator (see Chap. 3 for further discussion of the integration of POS information into a corpus, including “stand-off” POS annotation). In this version of CLAWS, punctuation marks like a comma are trivially tagged as that punctuation mark. 
(1) 
a. If_CS the_AT government_NN1 continues_VVZ to_TO behave_VVI in_II this_DD1 way_NN1 ,_, it_PPH1 will_VM fnd_VVI itself_PPX1 facing_VVG opposition_NN1 from_II those_DD2 who_PNQS have_VH0 been_VBN supporting_VVG it_PPH1 many_DA2 years_NNT2 ._. 

b. 
AT = article CS = subordinating conjunction DA2 = plural after-determiner DD1 = singular determiner DD2 = plural determiner II = general preposition NN1 = singular common noun NNT2 = temporal noun, plural PNQS = subjective wh-pronoun (who) PPH1 = 3rd person singular neuter pronoun (it) PPX1 = singular refexive personal pronoun TO = infnitive marker (to) VBN = been VH0 = have, base form VM = modal auxiliary VVI = infnitive VVG = -ing participle of lexical verb VVZ = -s form of lexical verb 


1The full CLAWS 7 tagset can be found at http://ucrel.lancs.ac.uk/claws7tags.html. Accessed 25 May 2019. 
J. Newman and C. Cox 
Contracted forms of English show some peculiarities when tagged with CLAWS. For example, in the British National Corpus, tagged with CLAWS 5, gonna is segmented into gon (VVG) and na (TO), tagged just like the unreduced equivalent going to would be. Similarly, the reduced form of isn’t, inn’t, appears as the sequence in (VBZ, a tag otherwise reserved for the is form of the verb be), n (XX0, a negative particle), and it PNP (personal pronoun) (see Chap. 3 for more information about how to map word forms and annotations in multiple layers or annotation graphs in corpus architecture). 
Sometimes, it is useful to allow multiple tags to be associated with the same word. The CLAWS tagger, for example, assigns a hyphenated POS tag, referred to as an “ambiguity tag”, when its tagging algorithm is unable to unambiguously assign a single POS to a word. For example, singing in the sentence She says she couldn’t stop singing, even if she wanted to try is tagged in the British National Corpus by CLAWS 5 as VVG-NN1. The hyphenated tag in this case indicates that the algorithm was unable to decide between VVG (the -ing form of a verb) and NN1 (the singular of a common noun), but the preference is for the VVG tag which appears as the frst element of the hyphenated tag. In some cases, genuine multiple readings of a sentence may be possible, e.g., the sentence the duchess was entertaining from the Penn Treebank, where entertaining could justifably be tagged either as an adjective or as a present participle. Hyphenated tags also have a useful role to play in the tagging of diachronic corpora where a word may come to be associated with different parts of speech or different functions through time (cf. Meurman-Solin 2007 and Chap. 10). 
POS tags may also be attached to sequences of words. So, for example, in the British National Corpus, sequences such as of course, all at once, and from now on are tagged as adverbs, while instead of, in pursuit of, and in accordance with are tagged as prepositions. These tags, of which there are many, are assigned as part of an “idiom tagging” step after the initial assignment of POS tags and involve matching the sequences of POS-tagged words in the corpus against templates of multiword sequences.2 Indeed, there has been a growing interest in multiword expressions as part of the annotation of a corpus, beyond just multiword parts of speech. The term multiword expression, it should be noted, is used in a great variety of ways in the literature, including, but not limited to, relatively fxed idiomatic phrases (easy as pie), whole proverbs (beggars can’t be choosers), verb-particle phrases (pick up), and light verb constructions (make decisions) (see Schneider et al. 2014 for an overview of multiword expressions as understood in the literature). 
Each POS tagset, either explicitly or implicitly, embodies some theory of grammar, even if the theory is simply traditional grammar. It would be unrealistic, therefore, to expect that different POS-tagging algorithms for the same language will produce identical results. Consider the tags assigned to rid in the three sentences 
2For more on the treatment of multiword expressions in the British National Corpus, see the section on “Automatic POS-Tagging of the Corpus” at http://ucrel.lancs.ac.uk/bnc2/bnc2autotag. htm. Accessed 25 May 2019. 
I am now completely rid of such things  You are well rid of him  I got rid of the rubbish  
CLAWS7 taggera  Past participle  Past participle  Past participle  
Infogisticsb  Verb base  Verb base  Past participle  
FreeLingc  Adjective  Verb base  Past participle  
(Brill-based) GoTaggerd  Adjective  Adjective  Adjective  

aAvailable at http://ucrel.lancs.ac.uk/claws/trial.html. Accessed 25 May 2019 bAvailable at http://www.infogistics.com/posdemo.htm. Accessed 25 May 2019 cAvailable at https://github.com/TALP-UPC/FreeLing. Accessed 25 May 2019 dAvailable at https://github.com/adsva/go-tagger. Accessed 25 May 2019 
in Table 2.1, based on four automatic tagging programs, where it can be seen that there is no uniform assignment of the part of speech of rid for any of the three sentences given. Here we see indications of an earlier historical shift in grammatical status, from a past participle to an adjective, with different (unambiguous) solutions provided by the different POS taggers. 

2.2.2 Lemmatization 
Another common kind of annotation found in modern-day corpora is lemmatization. In lemmatization, each orthographic word encountered in a corpus is assigned a lemma, or ‘base form’, which provides a level of abstraction from any infection that might appear in the original orthographic word. If we were to lemmatize this paragraph up to this point, for instance, we would see that the resulting nouns would appear without plural marking, and many verbs without agreement with their subjects, as in (2). Here, the lemmas have replaced the original words, but lemmas could also be added as additional information to the infected forms (see Chap. 3). Note, too, that in example (2), prior tokenization of the text plays a role in what base forms are recognized. The word modern-day in this example has been tokenized into separate units (modern,-, day), allowing each part to be annotated individually. 
(2) another common kind of annotation fnd in modern – day corpus be lemmatization . in lemmatization , each orthographic word encounter in a corpus be assign a lemma , or ‘ base ’ form , which provide a level of abstraction from any infection that may appear in the original orthographic word . if we be to lemmatize this paragraph up to this point , for instance , we will see that the resulting noun will appear without plural marking , and many verb without agreement with their subject ,asin(2). 
These kinds of lemmas often resemble the headwords found in dictionaries. Like the lemmas found in corpora, dictionary headwords often aim to represent a base word form (e.g., toaster, shine, be), rather than provide separate entries for each distinct 
J. Newman and C. Cox 
infected word form (e.g., toasters, shines / shone / shining, is / am / are / were / was / been / being). Both the headwords used in dictionaries and the lemmas found in corpora serve a similar purpose: they allow researchers to locate information more readily, particularly when searching by individual infected word forms might otherwise make it diffcult to fnd features of interest. 
In corpus linguistic studies, lemmas can be particularly useful in making complex searches more tractable, especially when a single word appears in many forms. Searches based on lemmas can be invaluable when working with corpora of languages with rich infection, such as Romance languages like French or Spanish, where a single regular verb may have dozens of distinct infected forms. While searches such as these can also often be conducted using regular expressions, lemmas generally make these searches more straightforward, and help to ensure that no relevant surface forms are inadvertently overlooked. English is not as richly infected as some languages, but the variation found in infected forms of, say, lexical verbs is still considerable (cf. the infected forms of bring, sing, drive, send, stand, etc.) and being able to search for all instances of such verbs on the basis of the lemmas will save time and effort.3 
For some researchers, it is linguistic patterning at the higher lemma level that is of most interest, but for other researchers the individual infected forms can be the target of interest. Rice and Newman (2005), for example, take a closer look at the infectional ‘profles’ of individual verbs in English (e.g., think, rumour, go, etc.), with an eye to how infected forms are distributed across particular tense, aspect, and subject person categories across different genres. They fnd that individual verbs (and classes of verbs) often have distinctively skewed distributions of infection across these categories. A verb like think, for instance, shows a marked tendency across all tense and aspects to appear with frst-person subjects (e.g., I think, Iwas thinking, I thought, etc.). These ‘infectional islands’, as Rice and Newman call them, would be more challenging to study for morphologically complex verbs like think if it were necessary to run individual searches for each distinct infected form (e.g., think, thinks, thought, etc.). In a lemmatized corpus, however, all instances of think can be retrieved with a single search on the lemma THINK—essentially using lemmatization as a way of getting back to the full range of infected forms found in the corpus. 
The choice of lemmatization software often depends on the kinds of language found in the corpus materials. Lemmatization can be an extremely useful tool in the corpus builder’s toolkit, especially when searches of the annotated corpus may need to be run over many infected forms. Although current English-language lemmatization tools make this process much easier to carry out on large bodies of text, it is often worth bearing in mind that even the most sophisticated lemmatization software will inevitably run into cases that are not entirely clear-cut (e.g., should the 
3Obviously, it will also beneft researchers if any spelling variation in a corpus (older vs. newer spellings, American vs. British spellings etc.) can be “normalized”, though we take this to be distinct from lemmatization (cf. Chap. 10). 
lemma of the noun axes be AXE or AXIS?) and where the resulting lemmas are not necessarily what one might expect. Different analyses can thus lead to different lemma assignments; accordingly, many lemmatization tools are careful to document the language-specifc lemmatization guidelines that they follow. As when using other corpus annotations that have been produced by automatic or semi-automatic procedures, understanding the limitations of automatic lemmatization and treating its outputs accordingly with a degree of circumspection is often a necessary part of the corpus annotation and analysis process. 

2.2.3 Syntactic Parsing 
The preceding sections have focused on adding annotations to corpus sources that focused on the properties of individual words—in the case of part-of-speech tagging, on their membership in particular grammatical classes, or in the case of lemmatization, on their association with particular headwords. When annotating a corpus, we may also be interested in adding information about the relationships that exist between elements in our texts above the level of the individual word that may help shed light on larger patterns in our corpus. Identifying particular multiword expressions, as mentioned in the preceding section, is an example of this. Another kind of higher-level annotation is syntactic parsing, which aims to provide information about the grammatical structure of sentences in a corpus. 
While syntactic structures can be assigned to sentences manually (and may be necessary when developing a corpus for a language for which few syntactically-annotated corpus resources have already been developed; see Sect. 2.3), it is more common for syntactic annotations to be added automatically by a syntactic parser, a program that provides information about different kinds of syntactic relationships that exist between words in a given text (parses). In the past, syntactic parsers were typically developed following deterministic approaches, often applying sets of carefully crafted syntactic rules or constraints to determine syntactic structure in a fully predictable way. In the 1990s, a new wave of syntactic parsers emerged that adopted a range of novel, probabilistic approaches (see Collins 1999 for an overview). These parsers began by analyzing large numbers of syntactically annotated sentences (a treebank), attempting to learn how syntactic structures are typically assigned to input sentences. Once trained on a particular sample of sentences, a probabilistic parser could then use that information to determine what the most likely parses would be for any new sentences it encountered, weighing the likelihood of different possible analyses in consideration. These probabilistic parsers have become increasingly common in corpus and computational linguistics, and generally work quite well for annotating arbitrary corpus texts in many 
J. Newman and C. Cox 
languages. One such parser is the Stanford Parser (Klein and Manning 2003),4 and a sample output from this parser is shown in (3), representing the phrase structure tree of She ran over the hill. The phrase structure tree provided by the Stanford Parser also includes automatic part-of-speech tagging, in this case using the tagset from the Penn Treebank (Marcus et al. 1993). The nested parentheses in (3) capture the parent-child relationships between higher and lower-level constituents (e.g., the determiner the and the noun hill both appearing within a larger noun phrase). 
(3) Parse of She ran over the hill. (ROOT 
(S 
(NP (PRP She)) 
(VP (VBD ran) 
(PP (IN over) (NP (DT the) (NN hill)))) (. .))) 
Another common form of syntactic information in corpora are dependency annotations, which indicate the grammatical relationships between sets of words. The online Stanford Parser is able to provide dependency parses for input sentences, as well. Example (4) shows the dependency parse for our previous example sentence, She ran over the hill. In this representation, each word in the input is numbered according to its position in the original sentence: the frst word, She,is marked with -1, while the ffth word, hill, is marked as -5. Each word appears in a three-part structure that gives the name of the grammatical relationship, followed by the governing and dependent elements (e.g., the nominal subject (nsubj)of the second word in the sentence, ran, is the frst word in the sentence, She;the determiner of the ffth word, hill, is the fourth word, the, etc.). By tracing these relationships, it is possible to extract features of corpus sentences—subjects of active and passive verbs, objects of prepositions, etc.—that would be diffcult to retrieve from searches of unannotated text alone. 
(4) nsubj(ran-2, She-1) 
root(ROOT-0, ran-2) 
case(hill-5, over-3) 
det(hill-5, the-4) 
nmod(ran-2, hill-5) 
As informative as these compact, textual representations of constituency and dependency structures can be, it can also be useful to be able to visualize syntactic annotations, especially whenreviewing annotated corpus materials for accuracy. 
4Available at https://nlp.stanford.edu/software/lex-parser.shtml. Accessed 25 May 2019. 

Several freely available tools are able to produce graphical representations of the kinds of dependency and constituency structures seen here. In Fig. 2.1,the dependency structure for (4) has been visualized using the Stanford CoreNLP online tool.5 

2.2.4 Semantic Annotation 
Semantic annotation refers to the addition of semantic information about words or multiword units to a corpus. An example of an annotation model like this is the UCREL Semantic Analysis System (USAS).6 A full explanation of USAS can be found in Archer et al. (2004) and Piao et al. (2005).7 USAS relies on a classifcation of the lexicon into twenty-one broad categories, called discourse felds, represented by the letters of the alphabet, as displayed in Fig. 2.2. These felds are intended to be as general as possible, suitable for working with as many kinds of text as possible and providing an intuitive, immediately understandable breakdown of our conceptual world. Archer et al. (2004) point to similarities in the taxonomies utilized by USAS and the Collins English Dictionary (2001), an indication of how the taxonomy in Fig. 2.2 refects more a common ‘folk understanding’ of our conceptual world rather than a top-down classifcation one arrived at by strict psychological or philosophical criteria. The set of felds includes a category Z assigned to names and function words, with narrower sub-categories indicated in tags by additional delimiting numbers. In (5) we illustrate narrower categories of the Time category in USAS, along with examples. Notice in (5) that words belonging to different parts of speech can be assigned the same semantic tag. The category of T.1.1.1, for example, includes nouns (history), verbs (harked,asin harked back to), adjectives (nostalgic) and adverbs (already). 
5Available at http://corenlp.run/. Accessed 25 May 2019. 
6UCREL stands for “University Centre for Computer Corpus Research on Language” at Lancaster University. 7Further information on USAS can also be found online at http://ucrel.lancs.ac.uk/usas/. Accessed 
25 May 2019. 
34  A F  B G  
 C H  J. Newman and C. Cox E I  

 K O  L P  
 M Q  N S  

 T  W  
 X  Y  

 Z  
 
 
Fig. 2.2 The 21 major discourse felds underlying the USAS semantic tagset 


(5) T1 Time 
T1.1 General T1.1.1 Past: history, medieval, nostalgic T1.1.2 Present: yet, now, present T1.1.3 Future: shall, will, next 
T1.2 Momentary: midnight, sunrise, sunset 
T1.3 Period: years, century, 1940s T2 Beginning/ending: still, began, continued T3 Old/new/young; age: new, old, children T4 Early/late: early, later, premature 
An example of text annotated in accordance with USAS is shown in (6a). The annotated text comes from the Canadian component of the International Corpus of English. Notice that in some cases there are multiple semantic categories associated with one word. Children, for example, is associated with a portmanteau tag, consisting of both S (Social actions, states and processes) and T (Time). The labels “m” and “f” for male and female can also form part of the semantic tag, e.g., girls includes the “f” label, while children includes both “m” and “f”. (6b) illustrates how some multi-word units, e.g., at a time (as in build houses two at a time), have a unique identifer that is associated with each word in the multi-word unit, here i165.3, with each member of the three-word unit assigned an additional suffx 1, 2, or 3. 
(6) a. The_Z5 ending_T2-of_Z5 the_Z5 poem_Q3 may_A7+ seem_A8 to_Z5 be_A3+ contradictory_A6.1-because_Z5/A2.2 both_N5 girls_S2.1f marry_S4 and_Z5 have_A9+ children_S2mf/T3-;_PUNC thereby_Z5 flling_N5.1+ the_Z5 traditional_S1.1.1 female_S2.1 role_I3.1 ._PUNC 
b. at_T1.1.2[i165.3.1 a_T1.1.2[i165.3.2 time_T1.1.2[i165.3.3 
The process of annotating a corpus with USAS tags is carried out automatically, relying on a complex combination of lexicon and rules, including prior tokenization and POS tagging (Rayson 2007), and a number of corpora in the International Corpus of English have been semantically annotated using USAS. 

2.2.5 Annotation Accuracy 
Automated annotation is subject to errors and consequently the accuracy of annotation models is a key consideration for researchers either choosing a model to apply to a corpus or working with a corpus already annotated. A common measure of (token) accuracy is to report the number of correct tags in a corpus as a percentage of the total number of tags in the corpus, or more likely in some sample of the whole corpus extracted for the explicit purpose of checking accuracy (cf. Marcus et al. 1993). An accuracy of more than 90% is typical for automatic lemmatization, POS tagging, parsing, and semantic annotation of general English. Another kind of measure of accuracy of POS taggers would be the percentage of sentences in a corpus or corpus sample which have been correctly tagged, i.e., sentence-accuracy, and then the accuracy drops to around 55–57% (cf. Manning 2011). This might seem a marginal and severe measure, but as Manning points out, even one incorrect tag in a sentence can seriously impact subsequent automatic parsing of a sentence. 
The concepts of precision and recall (cf. van Halteren 1999:82) are also relevant when evaluating an annotation scheme. Precision is the extent to which the retrieved objects in a query are correctly tagged, e.g., the extent to which the results from searching on the preposition tag consists, in fact, of prepositions. Recall describes the extent to which the objects matching the query retrieve all the target objects in the corpus, e.g., the extent to which searching on a preposition tag successfully retrieves all the objects that the researcher would identify as prepositions. Recall and precision ratios for parts of speech in the British National Corpus have been calculated and show an overall precision rate of 96.25% and an overall recall rate of 98.85% (based on a 50,000 word sample of the whole corpus).8 Hempelmann et al. (2006) investigated the accuracy of the Stanford Parser (Klein and Manning 2003), along with other parsers, where the accuracy measurements took into account both 
8See the section “POS-tagging Error Rates” in the BNC at http://ucrel.lancs.ac.uk/bnc2/bnc2error. htm (last accessed 25 May 2019) for information on recall and precision rates for individual parts of speech. 
J. Newman and C. Cox 
Table 2.2 Precision and recall of labeled constituents, based on the Stanford Parser 
Precision  Recall  
WSJ text  84.41  87.00  
Expository text  75.38  85.12  
Narrative text  62.65  87.56  

tags and constituents. The authors reported not just on the accuracy of a commonly used “Gold Standard” test corpus drawn from a parsed excerpt of the Wall Street Journal (WSJ) corpus, but also on a selection of narrative and expository scientifc texts, including textbooks. The results are shown in Table 2.2 where one can see that there is a considerable drop in precision as one moves from the WSJ text to the other types of text. 
The accuracy rates reported above give some sense of what is possible with state-of-the-art automatic annotation. For one thing, the more fne-grained an annotation system, the more diffcult it will be to achieve high accuracy. Clearly, the accuracy of a model will also vary depending on the type of text it is applied to, and even within formal written genres of English, accuracy of annotation can vary quite a bit, as seen in Table 2.2 above. The more the text differs, lexically and structurally, from the kind of text that the annotation has been originally trained on (most often, written text consciously planned, such as newspaper text), the more one can expect the accuracy to drop. Panunzi et al. (2004) highlight problems faced in the automatic annotation of Italian spontaneous speech which included regional and dialectal words, onomatopoeia, pause fllers etc. and report on the token accuracy of a selection of 3,069 tokens. Their adaptation of an existing annotation tool intended for written texts resulted in a token accuracy of 90.36%, compared with 97% when applied to written texts of offcial European Union documents. Learner corpora, where the users of language are still acquiring the lexicon and structures of a language, can be expected to present special diffculties when it comes to automatic annotation (see Thouësny 2011 for a combination of methods that achieve signifcant improvements in accuracy of annotation of a corpus of learners of French; see also Chap. 13). Transcriptions of highly informal and fragmentary texts such as Twitter present specifc challenges to annotation models (Derczynski et al. 2013; Eisenstein 2013; Farzindar and Inkpen 2015; cf. also the issue of ‘noisy’ data in web corpora generally in Chap. 15). 

2.2.6 Practicalities of Annotation 
While the preceding discussion has focused on introducing different kinds of automatic annotation that are commonly assigned to linguistic corpora, it is also reasonable to ask how these methods can be applied in practice when creating a new corpus. At the outset, this involves making decisions as to what kinds of annotations should be added, what conventions should be followed for representing that information consistently, and what tools will be used to apply those conventions to corpus source materials. All these decisions and the availability of existing conventions and annotation tools can make a signifcant difference to the overall process of annotation that follows. In the case of relatively well-resourced languages like English, for which many corpus annotation standards and tools exist, many common annotation tasks can be accomplished with ‘off-the-shelf’ software tools and minimal customization of annotation standards or procedures. In contrast, for many lesser-studied languages and varieties, these same tasks may require the development of annotation conventions that ‘ft’ the linguistic features of the source materials, as well as the implementation of these conventions in existing annotation tools, adding additional complexity to the overall corpus annotation workfow (see Sect. 2.2.3). 
In this section, we consider the example of applying POS tagging to a collection of unannotated corpus source materials. As mentioned in Sect. 2.2.1,thisisa common task in corpus development, and one on which other forms of linguis­tic annotation (e.g., lemmatization, syntactic annotation) often rely. In the most straightforward case, it may be possible to use existing annotation software to automatically apply a given set of POS tagging conventions to corpus source materials. If one were aiming to create an English-language corpus of the novels of Charles Dickens (1812–1870), one might begin by retrieving plain-text copies of these works from Project Gutenberg,9 then load these unannotated sources into an annotation tool such as TagAnt (Anthony 2015), which provides a graphical user interface for the TreeTagger annotation package (Schmid 1994). Example (7) shows the frst paragraph of Dickens’s 1867 novel Great Expectations before and after being loaded into TagAnt, which applies TreeTagger’s existing English POS annotation model and saves the annotated text for further use. 
(7) My father’s family name being Pirrip, and my Christian name Philip, my infant tongue could make of both names nothing longer or more explicit than Pip. So, I called myself Pip, and came to be called Pip. My_PP$ father_NN ’s_POS family_NN name_NN being_VBG Pirrip_NN ,_, and_CC my_PP$ Christian_JJ name_NN Philip_NP ,_, my_PP$ infant_JJ tongue_NN could_MD make_VV of_IN both_DT names_NNS nothing_NN longer_RBR or_CC more_RBR explicit_JJ than_IN Pip_NP ._SENT So_RB ,_, I_PP called_VVD myself_PP Pip_NP ,_, and_CC came_VVD to_TO be_VB called_VVN Pip_NP ._SENT 
Graphical tools such as TagAnt can be useful for exploring corpus annotation, particularly when creating smaller or specialized corpora for languages with existing annotation conventions. However, not all annotation procedures offer graphical user interfaces, and, for larger corpora, it may not be practical to manually load thousands (or millions) of corpus source documents into such an interface for annotation. In these cases, it is often necessary to integrate annotation as part of a larger, automatic workfow. This is sometimes accomplished by invoking individual annotation tools such as TreeTagger in scripts that contain a series of commands that carry out tasks 
9See http://www.gutenberg.org/ebooks/author/37. Accessed 25 May 2019. 
J. Newman and C. Cox 
for which general-purpose tools have not yet been designed to accomplish (cf. Chap. 9). Example (8) presents an excerpt of one such script (written in the standard sh(1) command language available on Unix-like operating systems such as macOS and Linux), which calls on TreeTagger to automatically tag all of the given English-language source documents and save the resulting POS-tagged versions under the same name preceded by the label ‘TAGGED-’: 
(8) for iin $* ; do 
tree-tagger-english “$i” > “TAGGED-$i”; done 
For many larger-scale or more complicated corpus construction projects, it is common for annotation to be implemented in custom software, which may in turn be integrated into larger ‘pipelines’ of natural language processing (NLP) tools that feed corpus source documents through successive stages of annotation and analysis (cf. Chap. 3).10 Libraries of common corpus annotation functions exist for many programming languages (e.g., NLTK for Python; Bird et al. 2009; Perkins 2014), which can greatly facilitate the process of developing custom corpus annotation procedures. 
While this discussion has assumed that appropriate tagsets and tagging models exist for the language or variety represented in your corpus, in many cases, existing sets of POS categories may not be suitable for the data, or may not exist at all. In situations like these, it is possible to train some kinds of annotation software to apply new tagging conventions to your corpus sources. This involves manually annotating samples of corpus text as examples of how the new annotation categories should be assigned. These examples are then provided to training software that attempts to learn how these categories should be assigned to other texts, often by assessing the probability of particular sequences of words and annotations occurring together (e.g., is dog more likely to be a noun or a verb when preceded by the words the big?). Creating a new POS tagging model is often an iterative process: small samples of text are manually annotated and used as training data to create a provisional POS tagger, which then automatically annotates more texts. These texts are then reviewed and their POS tags corrected, creating more data for the next round of training and annotation (cf. Cox 2010). While “bootstrapping” a new POS tag assignment model in this way requires some effort, it allows these techniques to be extended to some languages and varieties for which POS-tagged corpora do not already exist. 
Once POS annotations have been added to a collection of texts, there are several ways in which they can be used in queries. Some corpus search tools, such as 
10Frameworks such as GATE (Cunningham et al. 2013), UIMA (Ferrucci and Lally 2004), and Stanford CoreNLP (Manning et al. 2014) provide support for creating complex sequences of text annotation and analysis which are particularly valuable for larger-scale corpus construction projects involving multiple forms of annotation. References to several of these frameworks are provided in the Resources section below. 
AntConc (Anthony 2016), allow users to search for particular POS tags and tag sequences through a graphical user interface. In other cases, the regular expression search facilities integrated into many common programming languages and text editing tools can often be used to target particular tags without relying on specialized corpus linguistic software. For example, if we wanted to retrieve all of the nouns that appear immediately after the adjective brilliant in an English-language corpus with Penn Treebank-style POS tags, we could search the corpus using regular expression “brilliant/JJ . * ?/NNS?”. This would identify all of the instances in the corpus where the word brilliant has been tagged as an adjective (JJ) and is followed by some number of characters (.* ?) that have been tagged as a noun (NNS?). More information about applying regular expression searches in corpus linguistic studies can be found in Chap. 9 as well as Weisser (2015, Chap. 6) and Gries (2017). 
Representative Study 1 
Newman, J. 2011. Corpora and cognitive linguistics. Brazilian Journal of Applied Linguistics 11(2): 521–559. Special Issue on Corpus Studies: Future Directions, ed. Gries, S.T. 
To illustrate some advantages of a POS-tagged corpus, we draw upon Newman’s (2011) investigation into the lexical preferences for nouns in two English constructions: EXPERIENCE N and EXPERIENCE OF N, where N stands for a noun and the small caps for lemmas. What does a corpus teach us about the noun preferences in each of these two constructions – constructions which at frst glance seem rather similar in the semantic relations implied? The analysis relies on the collostructional analysis approach pioneered by Stefanowitsch and Gries (2003), although we will not report all the details of the statistical computations here. For this study, Mark Davies’ POS-tagged online Corpus of Contemporary American English (COCA; Davies 2008–) was used. The corpus contained well over 400 million words at the time of the study, allowing for solid statistical calculations. All frequencies reported here are from the 2011 version of COCA. 
As an initial attempt to establish noun preferences in the two constructions, one could simply retrieve all instances of EXPERIENCE N and EXPERIENCE OF N from the corpus and identify the most frequently occurring nouns in each construction. We say “simply”, but this task is only simply carried out when the corpus is POS-tagged and one is able to search for the verb EXPERIENCE immediately followed by a noun. In the COCA interface the relevant search expression is: [experience].[vv* ][nn* ].11 If the corpus were not POS-tagged, one would have to fnd some alternative way of retrieving the desired forms, 
(continued) 
11Lemma forms are retrieved by square brackets around the search term in the COCA interface. 
J. Newman and C. Cox 
e.g., by retrieving all tokens of any form of the lemma EXPERIENCE in their sentential context and then working manually through these results to isolate the verbal uses of EXPERIENCE followed by a noun. Given that there are more than 14,000 forms of EXPERIENCE in the corpus, this is not a very appealing task. 
Instead of relying solely on raw frequency of occurrence of nouns in each construction, however, it has become commonplace to invoke more sophisticated measures of attraction of words to a construction (see Chap. 7 for more information on collostructional analysis). Typically, these measures take into account frequencies in the whole corpus. The relevant statistic that we rely on for this more nuanced investigation into the preferences for the nouns (collexemes) in each of these constructions is called collostructional strength. Put simply, collostructional strength is a measure of the overuse or underuse of a word in a construction in light of expected frequencies. The higher the values, the greater the attraction of the collexeme to the construction.12 Table 2.3 shows the 20 nouns with the strongest collostructional strength in each of the two constructions. 
Looking at the top 20 collexemes of EXPERIENCE N in Table 2.3,it can be easily seen that the great majority of them are nouns which in fact share a kind of negative nuance: diffculty, depression, pain, stress, anxiety etc. Results for the collostructional analysis of the EXPERIENCE OF N construction, on the other hand, are quite different. The top 20 collexemes in this construction include a mixture of negatively nuanced concepts and more abstract, philosophical concepts, e.g., reality, oneness, modernity, transcendence, otherness. The collostructional profles in Table 
2.3 provide a sophisticated way of demonstrating the quite different types of nouns attracted to the two constructions and lend support to treating the two constructions as objects of study in their own right. In addition, this example shows how much more straightforward and replicable this analysis becomes once the analyst can rely on existing POS-tagging. 
(continued) 
12To be more precise, the collostructional strength in Table 2.3 is the negative log10-values of the p-values of the Fisher-Yates exact test (cf. Chap. 7). A collostructional strength >3 is signifcant at the p<0.001 level. 
Table 2.3 Top 20 nouns attracted to the EXPERIENCE Nand EXPERIENCE OF N construct 
ions, based on all genres of COCA, ranked by collostructional strength 
RANK  Top 20 nouns attracted to EXPERIENCE N construction  COLL. STRENGTH  Top 20 nouns attracted to EXPERIENCE OF N construction  COLL. STRENGTH  
1  Diffculty  58.7  Reality  34.4  
2  Success  36.9  Oneness  28.7  
3  Diffculties  31.4  Modernity  28.1  
4  Depression  30.1  Life  27.5  
5  Symptoms  29.9  Depression  18.4  
6  Feelings  28.5  Combat  17.5  
7  Pain  27.4  Jealousy  15.7  
8  Stress  24.6  Slavery  15.2  
9  Anxiety  22.4  Trauma  15.0  
10  Life  17.2  Pain  13.7  
11  Problems  15.9  Stress  13.5  
12  Wash-out  15.3  Giftedness  12.9  
13  Pleasure  15.2  Oppression  12.4  
14  Discrimination  14.5  Suffering  11.9  
15  Boredom  13.9  Disability  11.8  
16  Discomfort  13.9  Transcendence  11.7  
17  Nausea  12.7  Reading  11.7  
18  Frustration  12.2  Childbirth  11.0  
19  Orgasm  12.0  Otherness  10.9  
20  Burnout  11.7  Colonialism  10.5  

Representative Study 2 
Granger, S. 1997. Automated retrieval of passives from native and learner 
corpora: precision and recall. Journal of English Linguistics 25(4): 
365–374. 
This case study illustrates ways to refne search results from a corpus study using the notions of precision and recall. The focus of the investigation is what Granger (1997) calls the “central passive”, with BE as the auxiliary and a past-participle, and excluding various categories of pseudo-passives such as It is nearly fnished. 
Part of Granger’s study is concerned with passives in a native-speaker corpus, the Louvain Corpus of Native English Essays (LOCNESS), consisting of samples of American university students’ essays (40,820 words). The 
(continued) 
J. Newman and C. Cox 
corpus had been tagged with the TOSCA tagger as developed for the International Corpus of English (Oostdijk 1991). The tagset seems well suited for an investigation of the English passive since the set includes a passive subcategory of the auxiliary tag associated with BE and GET,asin they were given a fairly good write-up, That’s why I got sent home etc. Relying solely on the automatic tagging of the corpus, Granger found 562 passives. Rather than simply rely on the presence of the passive tag, however, one can identify the specifc kinds of passives, the so-called “central passives”, that may or may not be tagged as passives by the automatic tagger. So, for example, the corpus sentences in (9) contain structures that Granger considered central passives (shown in bold), but were not tagged as passives. 
(9)  a  Children in orphanages were not taken from their families, but  
voluntarily placed there by parents who couldn’t adequately care for  
them.  
b  The white students are expected to be reverent and respectful.  
c  The reader is lead to believe that he is hiding something.  

In (9a), there is ellipsis of were before the past participle placed in the second conjunct and since the passive tag attaches only to the be auxiliary in this tagging system, the passive structure in the second conjunct is not identifed automatically. In (9b), are expected to is tagged as a semi-auxiliary, not a passive, like be about to, have to etc. In (9c), lead (a misspelling of the past-participle led) is tagged as a noun, following the copula is. On the other hand, pseudo-passives such as I am very tired or it is nearly fnished are present in the results from the automated search (38 instances), along with other results containing tagging errors (32 instances). This leaves just 492 genuine central passives in the search results. Having identifed the genuine central passives, Granger was able to determine the precision and recall rates from the automated search, using the formulae in (10) and (11). Both precision and recall rates are less than 90%. Note that the whole LOCNESS corpus was the basis for calculating these rates. 
(10) 
number of automatically retrieved central passives 
Precision of passive results = 
number of automatically retrieved Aux (pass) instances = 492/562 = 87.5% (11) 
number of automatically retrieved central passives 
Recall of passive results = 
number of central passives in the corpus = 492/556 = 88.5% 
(continued) 
Granger also adopted the same procedure on writing samples of Swedish, Finnish, and French learners of English, taken from the ICLE (International Corpus of Learner English). Like the LOCNESS corpus, this corpus has been tagged using the TOSCA tagger. Based on these samples of the ICLE amounting to 59,640 words, the precision rate for the automatic retrieval of central passives dropped to 77.2%, while the recall rate was slightly higher at 89.4%. The lower precision rate from the learner data was due to the presence of pseudo-passives in the learner results and also tagging errors. The precision rates point to signifcant underuse of the central passive by learners. Apart from establishing this interesting result about learner English, the study illustrates the desirability of combining automated searches on annotated corpora with some manual post-editing, rather than relying on automated searches alone. 

2.3 Critical Assessment and Future Directions 
While annotation facilitates linguistic research and enables more immediate access to certain kinds of patterns in a corpus, one should acknowledge the potential for valuable linguistic research to be carried out even on unannotated corpora (cf. Chap. 8). Particularly if a researcher is oriented towards certain “bottom-up” approaches to language analysis (as in some kinds of corpus-driven linguistics; cf. Leech 2005; Gries and Berez 2017:403), the raw form, or at least the orthographic representation of speech, may present a natural starting point. Note, too, that even when an annotated corpus is available, there may still be a certain amount, even a considerable amount, of additional work required to refne search patterns or edit the search results in order to arrive at the desired outcome (cf. Representative Study 2 above and the references to the need to weed out “thousands” of irrelevant hits when obtaining data on an into-causative or as the as-predicative constructions in Gries et al. 2005:644). Some construction types may not be easily captured in terms of the available annotation or a researcher’s idea of a certain category (a lemma, a part of speech, parsing etc.) may differ from that subsumed by the annotation system. Any POS tagset or algorithms for syntactic parsing or lemmatization relies on some implicit or explicit grammatical theory and, as such, is always likely to present problems to a researcher who does not subscribe to that particular theory. 
Looking ahead to the future, we see two areas that present particular challenges, refecting newer trends in the feld of linguistics. One of these trends is the rise of interest in the multimodality of language, combining audio, visual, and textual components of communication, as presenting particularly exciting new challenges when it comes to annotating corpora consisting of such material (see also Chap. 16). 
J. Newman and C. Cox 
Conceivably, researchers might wish to have such corpora annotated for elements such as hand gestures, head movement, gaze, motion, speed of body movements, facial expressions, and bodily stance aligned with audio and textual elements. With the increased attention now given to multimodal analysis of language, establishing agreed upon standards and good practice for creating and working with annotated multimodal corpora will be a priority (cf. the discussion of multimodal annotation in Wittenburg 2014:673–683). Automatic and semi-automatic annotation of multimodal materials is currently an area of active research,13 with contact between corpus linguistics and work in computer vision and machine learning already producing valuable new annotation tools (cf. De Beugher et al. 2017 for one recent example involving gesture annotation). 
A second signifcant challenge for future corpus annotation efforts concerns smaller and endangered languages. Over the past two decades, researchers in corpus and computational linguistics have made efforts to include a wider range of the world’s languages in their scope, viewing this as one of the standing challenges for these felds in the twenty-frst century (McEnery and Ostler 2000). While research in documentary and descriptive linguistics commonly produces extensive collections of audiovisual and textual data for smaller languages (Himmelmann 2012), and indi­
vidual researchers in this area have begun to draw on corpus linguistic techniques in their analyses (cf. Mosel 2014), most of the corpus annotation methods discussed in this chapter are currently applicable only to languages for which large, annotated corpora already exist.14 This division between the manual and semi-automatic anno­tation procedures typically followed in documentary linguistics and the increasingly automatic annotation methods that are common in corpus linguistics represents one aspect of a deeper bifurcation in how corpus development proceeds in both felds (cf. Cox 2011). As corpus linguistics and language documentation continue their engagement with lesser-studied languages—and as the resources and tools that both felds produce continue to grow in signifcance, both as supports for linguistic research and for endangered language maintenance and revitalization—further work will be needed to ensure that the kinds of corpus annotation procedures outlined in this chapter can be readily applied to corpora of these languages, as well. 
13See, for example, recent research associated with the Distributed Little Red Hen Lab, an international consortium of institutions and individual researchers involved in multi-modal corpus development: http://www.redhenlab.org/. Accessed 25 May 2019. 
14In part, this is due to assumptions that many of the annotation techniques surveyed here make about the typological profles of the languages they annotate. Many probabilistic annotation methods assume that individual word forms will recur with enough frequency in a corpus to learn their overall distributions. However, as Gotti et al. (2005) point out, this assumption does not hold for polysynthetic languages like Inuktitut (iku), where complex infectional and derivational processes can result in texts that consist overwhelmingly of apparent hapax legomena (words that occur only once in a corpus). 

2.4 Tools and Resources 
1. 
TagAnt (Anthony 2015) is a free, downloadable, user-friendly cross-platform GUI that adds POS tags to texts in several languages, either singly or as a directory of fles. http://www.laurenceanthony.net/software/tagant/. Accessed 25 May 2019. 

2. 
TreeTagger (Schmid 1994) is one of the most established and accessible tools relevant to annotation, including lemma and POS tagging, available for Win­dows, Mac, and Unix systems. One important feature of TreeTagger is its ability to be used with a wide range of languages (historical and modern Indo-European languages, as well as for several non-Indo-European languages (e.g., Hausa, Swahili, Mandarin). http://www.cis.uni-muenchen.de/~{}schmid/tools/ TreeTagger/. Accessed 25 May 2019. 

3. 
Natural language processing pipelines provide one means of applying separate annotation tasks to texts in user-defned sequences (e.g., applying tokenization, then part-of-speech tagging, then syntactic parsing to unannotated materials), which can greatly assist in constructing annotated corpora of all sizes. Many such frameworks exist today, including Apache OpenNLP (https://opennlp.apache. org/) (accessed 25 May 2019), GATE (Cunningham et al. 2013; https://gate.ac. uk/) (accessed 25 May 2019), Stanford CoreNLP (Manning et al. 2014; https:// stanfordnlp.github.io/CoreNLP/) (accessed 25 May 2019), and UIMA (Ferrucci and Lally 2004; https://uima.apache.org/) (accessed 25 May 2019). 

4. 
Web-based interfaces have been developed for many of the corpus annotation tools surveyed in this chapter. These can be useful for quickly testing corpus annotation procedures, especially when the associated offine software compo­nents require some technical expertise to confgure. USAS semantic taggers are available online for several languages (http://ucrel.lancs.ac.uk/usas/) (accessed 25 May 2019), while Stanford CoreNLP (http://corenlp.run) (accessed 25 May 2019) offers users a range of annotation procedures (part-of-speech tagging, lemmatization, dependency and constituency parsing, etc.) and associated visu­alization options. 

5. 
Tools for Corpus Linguistics provides an up-to-date list of software packages for corpus annotation and analysis, and includes information about their pricing and the operating systems that they support. https://www.corpus-analysis.com/. Accessed 25 May 2019. 


J. Newman and C. Cox 
Further Reading 
Ide, N., and Pustejovsky, J. (Eds.). 2017. Handbook of Linguistic Annotation. Dordrecht: Springer Science+Business Media. 
This is a comprehensive survey of all aspects of linguistic annotation, including accounts of thirty-nine case studies of linguistic annotation projects. 
Kler, S., and Zinsmeister, H. 2015. Corpus Linguistics and Linguistically Annotated Corpora. London / New York: Bloomsbury. 
A recent introduction to working with annotated corpora, with particularly detailed discussion of different forms of annotation (ranging from the level of individual words to larger discourse features) and current software tools for querying them. 
Weisser, M. 2015. Practical Corpus Linguistics: An Introduction to Corpus-based Language Analysis. Hoboken, NJ: Wiley-Blackwell. 
Chapter 7, “Understanding part-of-speech tagging and its uses”, provides a practical guide to adding POS annotation to English texts, in particular the Penn Treebank and CLAWS 7 POS-tagsets. The chapter includes exercises exploring annotation and tagsets. 
References 
Anthony, L. (2015). TagAnt (Version 1.2.0). Tokyo: Waseda University. http:// www.laurenceanthony.net/software/tagant/. Accessed 25 May 2019. Anthony, L. (2016). AntConc (Version 3.4.4). Tokyo: Waseda University. http:// www.laurenceanthony.net/software/antconc/. Accessed 25 May 2019. Archer, D., Rayson, P., Piao, S., & McEnery, T. (2004). Comparing the UCREL Semantic Annotation Scheme with lexicographical taxonomies. In G. Williams & S. Vessier (Eds.), 
Proceedings of the 11th EURALEX (European Association for Lexicography) International Congress (Euralex 2004) (Vol. III, pp. 817–827). Lorient: Université de Bretagne Sud. 
Atwell, E. (2008). Tokenizing and part-of-speech tagging. In A. Leling & M. Kyt(Eds.), Corpus linguistics: An international handbook. Volume 1 (pp. 527–551). Berlin/New York: Walter de Gruyter. 
Bird, S., Loper, E., & Klein, E. (2009). Natural language processing with Python. Sebastopol: O’Reilly Media. Collins, M. (1999). Head-driven statistical models for natural language processing. Unpublished 
dissertation, Philadelphia: University of Pennsylvania. Collins English Dictionary. (2001). 5th ed. Glasgow: Harper-Collins Publishers. Cox, C. (2010). Probabilistic tagging of minority language data: a case study using Qtag. In S. 
T. Gries, S. Wulff, & M. Davies (Eds.), Corpus linguistic applications: Current studies, new 
directions (pp. 213–231). Amsterdam: Rodopi. Cox, C. (2011). Corpus linguistics and language documentation: Challenges for collaboration. In 
J. Newman, R. H. Baayen, & S. Rice (Eds.), Corpus-based studies in language use, language learning, and language documentation (pp. 239–264). Amsterdam: Rodopi. 
Cunningham, H., Tablan, V., Roberts, A., & Bontcheva, K. (2013). Getting more out of biomedical documents with GATE’s full lifecycle open source text analytics. PLoS Computational Biology, 9(2), e1002854. https://doi.org/10.1371/journal.pcbi.1002854. 
Davies, M. (2008–). The corpus of contemporary American English: 520 million words, 1990– present. Provo: Brigham Young University. http://corpus.byu.edu/coca/. Accessed 25 May 2019. 
De Beugher, S., Bre, G., & Goedemé, T. (2017). A semi-automatic annotation tool for unob­trusive gesture analysis. Language Resources and Evaluation, 1–28. https://doi.org/10.1007/ s10579-017-9404-9. 
Derczynski, L., Ritter, A., Clark, S., & Bontcheva, K. (2013). Twitter part-of-speech tagging for all: Overcoming sparse and noisy data. In G. Angelova, K. Bontcheva, & R. Mitkov (Eds.), Proceedings of the International Conference Recent Advances in Natural Language Processing (RANLP) 2013 (pp. 198–206). Shoumen: INCOMA. http://www.aclweb.org/anthology/R13­1026. Accessed 25 May 2019. 
Eisenstein, J. (2013). What to do about bad language on the Internet. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Tech­nologies (NAACL-HLT) (pp. 359–369). Atlanta: Association for Computational Linguistics. http://www.aclweb.org/anthology/N13-1037. Accessed 25 May 2019. 
Farzindar, A., & Inkpen, D. (2015). Natural language processing for social media (Synthesis lectures on human language technologies 30) (G. Hirst, Ed). San Rafael: Morgan & Claypool. 
Ferrucci, D., & Lally, A. (2004). UIMA: An architectural approach to unstructured information processing in the corporate research environment. Natural Language Engineering, 10(3–4), 327–348. https://doi.org/10.1017/S1351324904003523. 
Gotti, F., Patry, A., Cao, G., & Langlais, P. (2005). A look at English-Inuktitut word alignment. Paper presented at the 3rd Computational Linguistics in the North-East (CLiNE) Workshop, Gatineau, QC, August 26, 2005. http://rali.iro.umontreal.ca/rali/?q=en/node/751. Accessed 25 May 2019. 
Granger, S. (1997). Automated retrieval of passives from native and learner corpora: Precision and recall. Journal of English Linguistics, 25(4), 365–374. 
Gries, S. T. (2017). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). New York: Routledge. 
Gries, S. T., & Berez, A. L. (2017). Linguistic annotation in/for corpus linguistics. In N. Ide & J. Pustejovsky (Eds.), Handbook of linguistic annotation (pp. 379–409). Dordrecht: Springer. 
Gries, S. T., Hampe, B., & Schefeld, D. (2005). Converging evidence: Bringing together exper­imental and corpus data on the association of verbs and constructions. Cognitive Linguistics, 16(4), 635–676. 
Hempelmann, C. F., Rus, V., Graesser, A. S., & McNamara, D. S. (2006). Evaluating state-of-the­art treebank-style parsers for Coh-Metrix and other learning technology environments. Natural Language Engineering, 12(2), 131–144. 
Himmelmann, N. P. (2012). Linguistic data types and the interface between language documenta­tion and description. Language Documentation and Conservation, 6, 187–207. 
Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (Vol. 1, pp. 423–430). Sap­poro: Association for Computational Linguistics. https://doi.org/10.3115/1075096.1075150. 
Leech, G. (2005). Adding linguistic annotation. In M. Wynne (Ed.), Developing linguistic corpora: A guide to good practice (pp. 17–29). Oxford: Oxbow Books. http://ota.ox.ac.uk/documents/ creating/dlc/chapter2.htm. Accessed 25 May 2019. 
Manning, C. D. (2011). Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In A. F. Gelbukh (Ed.), Computational linguistics and intelligent text processing. 12th International conference, CICLing 2011, proceedings, part I. Lecture notes in computer science 6608 (pp. 171–189). Berlin/Heidelberg: Springer. 
Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. J., & McClosky, D. (2014). The Stanford CoreNLP natural language processing toolkit. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations (pp. 55– 60). http://www.aclweb.org/anthology/P14-5010. Accessed 25 May 2019. 
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2), 313–330. 
J. Newman and C. Cox 
McEnery, T., & Ostler, N. (2000). A new agenda for corpus linguistics – working with all of the world’s languages. Literary and Linguistic Computing, 15(4), 403–419. https://doi.org/ 10.1093/llc/15.4.403. 
Meurman-Solin, A. (2007). The manuscript-based diachronic corpus of Scottish correspondence. In J. C. Beal, K. P. Corrigan, & H. L. Moisl (Eds.), Creating and digitizing language corpora. Volume 2: Diachronic databases (pp. 127–147). Basingstoke/New York: Palgrave Macmillan. 
Mosel, U. (2014). Corpus linguistic and documentary approaches in writing a grammar of a previously undescribed language. In T. Nakayama & K. Rice (Eds.), The art and practice of grammar writing (Language documentation and conservation special publication 8) (pp. 135– 157). Honolulu: University of Hawai’i Press. http://hdl.handle.net/10125/4589. Accessed 25 May 2019. 
Newman, J. (2011). Corpora and cognitive linguistics. Brazilian Journal of Applied Linguistics, 11(2), 521–559. Special Issue on Corpus studies: Future Directions, ed. Gries, S.T. 
Oostdijk, N. (1991). Corpus linguistics and the automatic analysis of English. Amsterdam: Rodopi. 
Panunzi, A., Picchi, E., & Moneglia, M. (2004). Using PiTagger for lemmatization and PoS tagging of a spontaneous speech corpus: C-Oral-Rom Italian. In M. T. Lino, M. F. Xavier, 
F. Ferreira,R.Costa,R.Silva,with thecollaborationofC.Pereira,F.Carvalho, M. Lopes, M. Catarino, S. Barros (eds), Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04) (Vol. 2, pp. 563–566). Paris: European Language Resources Association. http://www.lrec-conf.org/proceedings/lrec2004/pdf/246.pdf/. Accessed 28 May 2019. 
Perkins, J. (2014). Python 3 text processing with NTLK 3 cookbook (2nd ed.). Birmingham: Packt Publishing. 
Piao, S. L., Archer, D., Mudraya, O., Rayson, P., Garside, R., McEnery, T., & Wilson, A. (2005). A large semantic lexicon for corpus annotation. Proceedings of the Corpus Linguistics 2005 con­ference, July 14–17, Birmingham, UK. Proceedings from the Corpus Linguistics Conference Series, 1(1). http://ucrel.lancs.ac.uk/people/paul/publications/cl2005_estlex.pdf. Accessed 25 May 2019. 
Rayson, P. (2007). Wmatrix: A web-based corpus processing environment. Computing Department, Lancaster University. http://ucrel.lancs.ac.uk/wmatrix/. Accessed 25 May 2019. 
Rice, S., & Newman, J. (2005). Infectional islands. Paper presented at the 9th International Cognitive Linguistics Conference, Yonsei University, Seoul, Korea. https://sites.ualberta.ca/ ˜johnnewm/Islands.pdf. Accessed 25 May 2019. 
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of international conference on new methods in language processing (pp. 44–49). Manchester. Schneider, N., Onuffer, S., Kazour, N., Danchik, N. E., Mordowanec, M. T., Conrad, H., & Smith, 
N. A. (2014). Comprehensive annotation of multiword expressions in a social web corpus. In N. Calzolari, K. Choukri, T. Declerck, H. Loftsson, B. Maegaard, J. Mariani, A. Moreno, J. Odijk, & S. Piperidis (Eds.), Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014) (pp. 455–461). Paris: European Language Resources Association. http://www.lrec-conf.org/proceedings/lrec2014/pdf/521_Paper.pdf. Accessed 25 May 2019. 
Stefanowitsch, A., & Gries, S. T. (2003). Collostructions: Investigating the interaction between words and constructions. International Journal of Corpus Linguistics, 8(2), 209–243. 
Thouësny, S. (2011). Modeling second language learners’ interlanguage and its variability: A computer-based dynamic assessment approach to distinguishing between errors and mistakes. Unpublished dissertation. Dublin: Dublin City University. http://doras.dcu.ie/16559/. Accessed 31 May 2019. 
van Halteren, H. (1999). Performance of taggers. In H. van Halteren (Vol. Ed.) & Ide N, & Véronis J (Series Ed.), Syntactic wordclass tagging. (Text, speech, and language series 9) (pp. 81–94). Dordrecht: Kluwer. 
Weisser, M. (2015). Practical corpus linguistics: An introduction to corpus-based language analysis. Hoboken: Wiley-Blackwell. 
Wittenburg, P. (2014). Preprocessing multimodal corpora. In A. Leling & M. Kyt(Eds.), Corpus linguistics: An international handbook. Volume I (pp. 664–685). Berlin/New York: Walter de Gruyter. 
Chapter 3 Corpus Architecture 
Amir Zeldes 
Abstract Corpus architecture refers to the set of design decisions taken in the conceptual division and interrelations of types of objects contained in a corpus, such as texts, annotations and metadata, and the ways in which we choose to represent these. The architecture of a corpus can be as simple as a list of unanalyzed text fles, or as complex as a web of trees or graphs connecting words, sentences and documents, or even parts of words and individual letters. The architecture chosen has direct consequences for the types of analyses researchers can apply to the data, such as deciding whether metadata can contain nested metadata, or whether multiple words can occur at ‘the same time’ for dialog data. For complex corpora containing many annotations or documents which are interrelated in non-trivial ways, choosing the right architecture early in a project is crucial, but the end result of many decisions is not always obvious in advance. This chapter presents some of the key characteristics distinguishing different corpus architectures, with examples illustrating the tradeoffs between complexity and fexibility. The focus is on abstract data models and the ways in which they are realized in concrete formats for corpus representation, as well as the usability of the resulting corpora in different scenarios. 
3.1 Introduction 
The architecture chosen for a certain corpus refers to the conceptual division of different types of objects contained in a corpus, such as texts, annotations and metadata. As presented in Chaps. 1 and 2, annotations and metadata are sometimes relatively simple: markup can be added to set them apart from the text and control their inventories. However once annotations become more complex they often form a hierarchical architecture of added information which can be represented in multiple ways, with varying consequences. This chapter presents some of the key 
A. Zeldes (•) Georgetown University, Washington, DC, USA e-mail: amir.zeldes@georgetown.edu 
© Springer Nature Switzerland AG 2020 49 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_3 
characteristics distinguishing different corpus architectures. The focus is on abstract data models and the ways in which they are realized in concrete formats for corpus representation, as well as consequences for the usability of the resulting corpora. 

3.2 Fundamentals 
The fundamentals of corpus architecture begin with conceptualizing corpora as collections of documents, possibly arranged in subcorpora, and often carrying metadata. Within each document, we must consider how primary data is represented, such as textual data, transcribed dialogue with or without multiple overlapping speakers, and aligned multimodal data. For example, although spoken language is considered ‘primary’ in many senses, corpus architectures usually treat aligned audio/video information (A/V for short) as a type of annotation, and this can have consequences for corpus architecture. As we will explore below, cases breaking the idea of text as a sequence of adjacent tokens, such as overlapping data and multiple or conficting tokenization, can arise, which will have complex effects and different possible solutions (see Sauer and Leling 2016). Some of the more complex solutions amount to annotation graphs (roughly, webs of interconnected analyses), which can also be encoded in different ways. As we will see in this section, the choice of architecture determines how much information can be expressed in a corpus, from simple token annotations, such as part of speech (POS) tags, to complex multilayer corpora with conficting hierarchies encoding syntax, semantics and more in dozens of annotation layers. 


3.2.1 Corpus Macro-structure 
If a corpus is “a collection of pieces of language that are selected and ordered according to explicit linguistic criteria in order to be used as a sample of the language”,1 then the frst component of corpus architecture, before considering analyses within each ‘piece’, is the organization of the collection of ‘pieces of language’. A minimal corpus macro-structure is therefore a single or ‘top-level’ corpus object, directly containing the ‘pieces’, which can be referred to as ‘docu­ments’, as shown on the left in Fig. 3.1. Documents are not necessarily complete texts: for example, they can be samples of n tokens (see Biber 1993 on sample size selection), a situation that can arise due to copyright restrictions forbidding full publication of the source text, or due to resource limitations when only a subpart of a longer text can be feasibly annotated in a given project. Documents usually 
1Defnition from the Expert Advisory Group on Language Engineering Standards (EAGLES); see Chap. 1 as well as Calzolari and McNaught (1994), and McEnery et al. (2006:4–5) for discussion. 

correspond to contiguous text taken from some source, with few exceptions.2 In many corpus search architectures, the defnition of the document plays an important role in determining the boundaries of the search space for queries: often, if users want to search for certain words ‘within 10 words’, they intend for the result to come from one document, and would not want to see a search result containing the last word of one text followed by a word from the beginning of an unrelated text. Although this issue is often overlooked, the defnition of the document can thus affect search results. For example, in a corpus of the works of Charles Dickens, what are the boundaries of a document? A single book? Or each chapter within each book? While each defnition may seem reasonable, they are not identical. 
Very often, corpora are constructed according to design criteria which assign documents to different categories (see Chap. 1). In these cases, the most common corpus macro-structure is the one in the middle of Fig. 3.1: a tree of subcorpora, each containing documents. Subcorpora can be arranged hierarchically, for example a corpus can have written and spoken subcorpora (e.g. corpora in the International Corpus of English, ICE, Greenbaum 1996) and the latter subcorpus may further con­
tain conversation and broadcast news subcorpora, before reaching actual documents. In more complex designs, shown on the right of Fig. 3.1, criteria cross-classify across documents, meaning that documents belong to several categories at once. This is often achieved by labeling documents with metadata categories, with the intention of creating dynamic or virtual subcorpora. For example, metadata may be used to classify spoken data as a conversation or monologue, and at the same time as private or public speech. It is then possible to dynamically construct a subcorpus containing all private spoken data, or all monologue data, etc. 
The term virtual subcorpus is also used sometimes to refer to querying subsets of earlier queries, i.e. one can dynamically design a ‘subcorpus’ containing all documents matching an arbitrary query (e.g. the subcorpus of documents containing the word ‘snow’), and work further with these documents (see Kupietz et al. 2010:1852). 
2One example could be in the case of historical corpora of fragmentary texts, in which a document corresponds to everything we have from a certain work that was originally a contiguous text. 
While the structures in Fig. 3.1 cover the bulk of corpus resources, some more complex situations deserve special mention. Firstly, in parallel corpora (see Chap. 
12) containing aligned text, the concept of ‘document’ is further complicated. Alignment is most often the result of translation corpora, in which each document may exist in more than one language, with alignment either simply at the document level, or more fne-grained forms of alignment, such as section, paragraph, sentence or word alignment (see Romary and Bonhomme 2000 for an extensive discussion). There are also more unusual types of alignment, such as partial alignment (e.g. multilingual corpora of parallel Wikipedia articles which are similar in content, but not actual translations, Smith et al. 2010), corpora aligning editing differences (e.g. corpora of aligned draft revisions, Lee et al. 2015) or corpora containing non-native texts next to aligned target hypotheses of what native annotators believe a non-native speaker is trying to say in the standard target language (see Reznicek et al. 2013). All of these situations complicate the notion of a tree-like graph with simple documents as leaves: in such cases, the leaves themselves may have a complex macro-structure. 

3.2.2 Primary Data and Text Representation 
As collections of textual data (in the broad sense, whether written, or transcribed from speech, see Wichmann 2008, or even multimodal corpora of sign language utterances, see Crasborn and Sloetjes 2008, Schembri et al. 2013; cf. also Chaps. 11 and 16), the most fundamental concern in modelling corpora is how text is represented within each document. I will refer to the text being represented as the ‘primary data’.3 While this may seem uncomplicated, it is actually a substantial challenge in many cases. In the frst instance, corpus architectures differ in whether or not, or to what extent, they preserve features of the original source data. One of the most frequent violations of primary data integrity in written corpora is white-space preservation. Consider the following example, as formatted, where underscores mark otherwise invisible spaces, and the arrow indicates a tab symbol 
(1) Mark agreed. This was, then, the end._ _ |–>But I cannot accept it._ _ 
Early corpus architectures were aimed at capturing and separating word form tokens, using spaces between token units, often followed by a separator and 
3Note that although A/V signals in multimodal corpora logically precede their transcription, corpus architectures usually implement aligned A/V signals as annotations anchored to the transcription using timestamps. In other words, in much the same way as the POS tag ‘noun’ might apply to the position in the text of a word like ‘bread’, a recording of this word is also a type of datum that can be thought of as happening at the point in which ‘bread’ is uttered. In continuous primary data representations (see below), A/V timestamp alignment therefore ‘tiles’ the text (no span of audio is left without alignment to some token). 
annotations, as in (2), where a separator ‘/’ marks the beginning of a POS tag (see also Chap. 2). 
(2) Mark/NNP agreed/VBD./SENT This/DT was/VBD,/, then/RB,/, the/DT end/NN ./SENT But/RB I/PRP can/MD not/RB accept/VB it/PRP./SENT 
For many linguistic research questions, the representation in (2) is adequate, for example for vocabulary studies: one can extract type/token ratios to study vocabulary size in different texts, fnd vocabulary preferences of certain authors, etc. 
However for many other purposes, the loss of information about the original text from (1) is critical. To name but a few examples: 
• 
Tokens with ambiguous spacing: both ‘can not’ and ‘cannot’ are usually tokenized as two units, but to study variation between these forms, one needs to represent whitespace somehow. 

• 
Training automatic sentence/document/subsection splitters: Position and number of spaces, as well as presence of tab characters are very strong cues for such programs. For example TextTiling, a classic approach to automatic document segmentation, makes use of tabs as predictors (Hearst 1997). 

• 
Stylometry and authorship attribution: even subtle cues found in whitespace can distinguish authors and styles. For example, US authors are much more likely to use double spaces after a sentence fnal period than UK authors, and specifc combinations of whitespace practices can sometimes uniquely identify authors (see Kredens and Coulthard 2012:506–507). Proportion of white space has also been used in authorship and plagiarism detection (Canales et al. 2011). 


Whitespace and other features of the original primary data can therefore be important, and some corpus architectures employ formats which preserve and separate the underlying data from processes of tokenization and annotation, often using ‘stand-off’ XML formats. In stand-off formats, different layers of information are stored in separate fles using a referencing mechanism which allows us, for example, to leave an original text fle unchanged. One can then add e.g. POS annotations in a separate fle specifying the character offsets in the text fle at which the annotations apply (e.g. marking that a NOUN occurred between characters 4– 10; see ‘Tools and Resources’ for more details). 
A second important issue in representing language data is the tokenization itself, which requires detailed guidelines, and is usually executed automatically, possibly with manual correction (see Schmid 2008 for an overview). Although a working defnition of ‘tokens’ often equates them with “words, numbers, punctuation marks, parentheses, quotation marks, and similar entities” (Schmid 2008:527), a more precise defnition of tokens is simply “the smallest unit of a corpus” (Krause et al. 2012:2), where units can also be smaller than a word, e.g. in a corpus treating each syllable as a token. In other words, tokens are minimal, indivisible or ‘atomic’ units, and any unit to which we want to apply annotations cannot be smaller than a token (see Representative Corpus 2). 
(3) wf  I’ll  do  it  
norm  I  will  do  it  
pos  PRP  MD  VB  PRP  
tok  I  ’ll  do  it  
(4) wf  I  won’t  do  it  then  
norm  I  will  not  do  it  then  
pos  PRP  MD  RB  VB  PRP  RB  
tok  I  wo  n’t  do  it  then  

(5) wf  I’m  a  do  it  
norm  I  am  going  to  do  it  
pos  PRP  VBP  VBG  TO  VB  PRP  
tok  I  ’m  a  do  it  
Fig. 3.2 Tokenization, normalization and POS tags for word forms in (3)–(5) 


In English, word forms and tokens usually coincide, and tokenization is closely related to prevalent part of speech tagging guidelines (the Penn tag set, Santorini 1990 and CLAWS, Garside et al. 1987, both ultimately going back to the Brown tag set, Ku.
cera and Francis 1967; cf. also Chap. 2). However, modals, negations and other items which sometimes appear as clitics are normally tokenized apart, as in the clitics ‘ll and n’t in (3) and (4). These are represented as separate in the ‘tok’ (token) rows of Fig. 3.2, but are fused on the ‘wf’ (word form) level. In (3), separating the clitic ‘ll allows us to tag it as a modal on the ‘pos’ layer (MD), just like a normal will. The other half of the orthographic sequence I’ll is retained unproblematically as I. In (4), by contrast, separating the negation n’t produces a segment wo, which is not a ‘normal’ word in English, but is nevertheless tagged as a modal. 
(3) 
I’ll do it 

(4) 
I won’t do it then 


In order to make all instances of the lexical item will fndable, some corpora rely on lemmatization (the lemma of all of these is will), while other corpora use explicit normalization. This distinction becomes more crucial in corpora with non-standard orthography, as in example (5), featuring the contraction I’m a (frequent in, but not limited to African American Vernacular English, Green 2002:196). 
(5) I’m a do it (i.e. I’m going to do it) 
This last example clearly shows that space-delimited orthographic borders, tokenization, and annotations at the word form level may not coincide. To do justice to examples such as (5), a corpus architecture must be capable of mapping word forms and annotations to any number of tokens, in the sense of minimal units. In some cases these tokens may even be empty, as in the position following a in the ‘tok’ layer for (5) – what matters is not necessarily that ‘tok’ contains some segmentation of the text in ‘wf’, but rather that the positions and borders that are required for the annotation table are delimited correctly in order to allow the interpretation of a as corresponding to the ‘norm’ sequence going (tagged VBG) and to (tagged TO), assuming this is the desired analysis.4 
For multimodal data in which speakers may overlap, the situation is even more complex and an architecture completely separating the concepts of tokens as minimal units and word forms becomes necessary. An example is shown in Fig. 3.3. 
The example shows several issues: towards the end, two speakers overlap with word forms that only partially occur at the same time, meaning that borders are needed corresponding to these offsets; in the middle of the excerpt, there is a moment of silence, which has a certain duration; and fnally, there is an extra linguistic event (a phone ringing) which takes place in part during speaker A’s dialogue, and in part during the silence between speech acts. 
An architecture using the necessary minimal units can still represent even this degree of complexity, provided that one draws the correct borders at the minimal transitions between events, and add higher level spans for each layer of information. In cases like these, the concept of minimal token is essentially tantamount to timeline indices, and if these have explicit references to time (as in the seconds and milliseconds in the ‘time’ layer of Fig. 3.3), then they can be used for A/V 
spkA  I  see  but  actually  I  
posA  PRP  VBP  PRP  RB  PRP  
spkB  you  know  
posB  PRP  VBP  
events  [phone rings]  
time  00:03.1  00:04  00:04.2  00:05.6  00:07  00:07.5  00:08  00:08.1  
Fig. 3.3 Multiple layers for dialog data with a minimally granular timeline 


4Some architectures go even further and use an ‘empty’ token layer, using tokens solely as ordered positions or time-line markers, not containing text (e.g. the RIDGES corpus, Odebrecht et al. 2017, or REM, Klein and Dipper 2016). In such cases, tools manipulating the data can recover the covered text for each position from an aligned primary text. 
signal alignment as well. More generally, an architecture of this kind, which is used by concrete speech corpus transcription tools such as ELAN (Brugman and Russel 2004) or EXMARaLDA (Schmidt and Wner 2009), can also be thought of as an annotation graph (see Sect. 3.2.3). 
A fnal consideration in cases such as these is the anchoring or coupling of specifc layers of information in the data model: in the example above, the two ‘pos’ layers belong to the different speakers. A user searching for all word forms coinciding with a verbal tag in the corpus would be very surprised to fnd the word I, which might be found if all VBP tags coinciding with a word form are retrieved (since the second I overlaps with the other speaker’s word know). What is meant in such situations is to only look at combinations of POS and word form information from either speaker A or speaker B. In other situations, however, one might want to look at any layers containing some speaker (e.g. search for anyone saying um), in which case some means of capturing the notion of ‘any transcription layer’ is required. These concepts of connecting annotation layers (posA belongs to spkA) and applying multiple segmentations to the data will be discussed below in the context of graph models for corpus annotations. 

3.2.3 Data Models for Document Annotations 
The central concern of annotations is ‘adding interpretative, linguistic information to an electronic corpus’ (Leech 1997:2), such as adding POS tags to word forms (see Chap. 2). However, as we have seen, one may also want to express relationships between annotations, grouping together multiple units into larger spans, building structures on top of these, and annotating them in turn. For example, multiple minimal tokens annotated as morphemes may be grouped together to delineate a complex word form, several such word forms may be joined into phrases or sentences, and each of these may carry annotations as well. Additionally, some annotations ‘belong together’ in some sense, for example by relating to the same speaker in a dialogue. If a document contains these kinds of data, the resulting structure is then no longer a fat table such as Fig. 3.2, but rather a graph with explicit hierarchical connections. For planning and choosing a ftting corpus architecture, it is important to understand the components of annotation graphs at an abstract level, since even if individual XML formats under consideration for a corpus vary substantially (see Sect. 3.4), at an underlying level, the most important factor is which elements of an annotation graph they can or cannot represent. 
At its most general formulation, a graph is just a collection of nodes connected by edges: for example an ordered sequence of words, each word connected to the next, with some added nodes connected to multiple words (e.g. a sentence node grouping some words, or smaller phrase nodes). Often these nodes and edges will be annotated with labels, which usually have a category name and a value 
(e.g. POS=NOUN); in some complex architectures, annotations can potentially include more complex data types, such as hierarchical feature structures, in which annotations can contain not only simple values, but also further nested annotations (see ISO 24612 for a standardized representation for such structures in corpus markup). Additionally, some data models add grouping mechanisms to annotation graphs, often referred to as ‘annotation layers’, which can be used to lump together annotations that are somehow related.5 
Given the basic building blocks ‘nodes’, ‘edges’, ‘annotations’ and ‘layers’, there are many different constraints that can be imposed on the combinations of these elements. Some data models allow us to attach annotations only to nodes, or also to edges; some data models even allow annotations of annotations (e.g. Dipper 2005), which opens up the possibility of annotation sub-graphs expressing, for example, provenance (i.e. who or what created an annotation and when, see Eckart de Castilho et al. 2017) or certainty of annotations (e.g. an ‘uncertain’ label, or numerical likelihood estimate of annotation accuracy). Another annotation model constraint is whether multiple instances of the same annotation in the same position are allowed 
(e.g. conficting versions of the same annotation, such as multiple POS tags or even syntax trees, see Kountz et al. 2008). This can be relevant not only for fne-grained manual annotations, but also for the application and comparison of multiple automatic tools (several POS taggers, parsers, etc.). Layers too can have different constraints, including whether layers can be applied only to nodes, or also to edges and annotations, and whether layer-element mapping is 1:1 or whether an element can belong to multiple layers. Search engines sometimes organize visualizations by layers, i.e. using a dedicated syntax tree visualization for a ‘syntax’ layer, and other modules for annotations in other layers. 
Basic annotation graphs, such as syntactically annotated treebanks, can be described in simple inline formats. However, as the corpus architecture grows more complex or ‘multilayered’, the pressure to separate annotations into different fles and/or more complex formats grows. To see why, one can consider the Penn Treebank’s (Marcus et al. 1993) bracketing format, which was developed to encode constituent syntax trees. The format uses special symbols to record not only the primary text, but also empty categories, such as pro (for dropped subject pronouns), PRO (for infnitive subjects), traces (for postulated movement), and more. In the following tree excerpt from the Wall Street Journal portion of the Penn Treebank, there are two ‘empty categories’, at the two next to last tokens: a zero ‘0’ tagged as -NONE-standing in for an omitted that (i.e. researchers said *that*), and a trace ‘*T*-2’, indicating that a clause has been fronted (i.e. the text is “crocidolite is ... resilient ..., researchers said”, which can be considered to be fronted from a form such as “researchers said the crocidolite ...”): 
5In some formats, XML namespaces form layers to distinguish annotations from different inventories, such as tags from the TEI vocabulary (Text Encoding Initiative, http://www.tei-c.org/) (Accessed 28 May 2019) versus corpus specifc tags (see Her 2012 for an example). A formal concept of layers to group annotations is provided in the Salt data model (Zipser and Romary 2010), and UIMA Feature Structure Types in the NLP tool-chain DKPro (Eckart de Castilho and Gurevych 2014). NLP tool chain components are often thought of as creating implicit layers (e.g. a parser component adds a syntactic annotation layer), see e.g. GATE Processing Resources or CREOLE Modules in GATE (Cunningham et al. 1997), Annotators components in CoreNLP (Manning et al. 2014) or WebLicht Components (Hinrichs et al. 2010). 
( (S 
(S-TPC-2 
(NP-SBJ 
… 
(NP (NN crocidolite) ) 
(, ,)) 
( VP (VBZ is) 
(ADJP-PRD (RB  unusually) (JJ resilient) ) 
… 
(, ,) 
(NP-SBJ ( NNS researchers)  ) 
(VP (VBD said) 
(SBAR (-NONE-0) (S (-NONE-*T*-2) ))) (. .) )) 
This syntax tree defnes a hierarchically nested annotation graph, with nodes corresponding to the tokens and bracketing nodes, and annotations corresponding to parts of speech and syntactic category labels (NP, VP etc.). However much of the information is rather implicit; the edges of the tree are marked by nested brackets: the NP dominates the noun ‘crocidolite’, etc. Annotations are pure value labels (VBD, VP etc.), and one must infer readings for their keys (POS, phrase category). Another ‘edge’ represented by co-indexing the trace with its location at S-TPC-2 depends on our understanding that *T*-2 is not just a normal token (marked only by a special POS tag -NONE-). This is especially crucial for the dropped ‘that’, since the number 0 can also appear as a literal token, for example in the following case, also from the Wall Street Journal section of the Penn Treebank: 
(NP 
(NP (DT a) (NN vote) ) 
(PP (IN of) 
(NP 
(NP (CD 89) ) 
(PP (TO to) 
(NP (CD 0) ))))) 
At the very latest, once information unrelated to the syntax tree is to be added to the corpus, such as temporal annotation, coreference resolution or named entity tags, multiple annotation fles will be needed. In fact, the OntoNotes corpus (Hovy et al. 2006), which contains parts of the Penn Treebank extended with multiple additional layers, is indeed serialized in multiple fles for each document, expressing unrelated or only loosely connected layers of annotation. A corpus containing unrelated layers in this fashion is often referred to as a ‘multilayer’ corpus, and data models and technology for such corpora are an active area of research (see Leling et al. 2005; Burchardt et al. 2008; Zeldes 2017, 2018). 
Because of the complexity inherent in annotation graphs, complex tools are often needed to annotate and represent multilayer data, and the choice of search and visualization tools with corresponding support becomes more limited (see Tools and Resources). In the case of formats for data representation, the situation is somewhat less critical, since, as already noted, different types of information can be saved in separate fles. This also extends into the choice of annotation tools, as one can use separate tools, for example to annotate syntax trees, typographical properties of source documents, or discourse annotations. The greater challenge begins once these representations need to be merged. This is often only possible if tools ensuring consistency across layers are developed (e.g. the underlying text, and perhaps also tokenization must be kept consistent across tools and formats). 
As a merged representation for complex architectures, stand-off XML formats are often used (see Sect. 3.3), and Application Programming Interfaces (APIs) are often developed in tandem with such corpora to implement validation, merging and conversion of separate representations of the same data (for example, the ANC Tool, used to convert data in the American National Corpus and its richly annotated subcorpus, MASC, Ide et al. 2010). For search and visualization of multilayer architectures, either a complex tool can be used, such as ANNIS (Krause and Zeldes 2016; see also Sect. 3.4), or a combination of tools is used for each layer. For example in the TXM text mining platform, Heiden (2010) proposes to use a web interface to query the Corpus Workbench (Christ 1994) for ‘fat’ annotations, TigerSearch (Lezius 2002) for syntax trees, and XQuery for hierarchical XML. The advantage of this approach is that it can use off-the-shelf tools for a variety of annotation types, and that it can potentially scale better for large corpora, since each tool has only a limited share of the workload. The disadvantage is that a data model merging results from all components can only be generated after query retrieval has occurred in each component. This prevents complex searches across all annotation layers: for example, it is impossible to fnd sentences with certain syntactic properties, such as clefts, which also contain certain XML tags, such as entity annotations denoting persons, and also have relational edges with components of other sentences, such as coreference with a preceding or following entity annotation. These kinds of combinations can be important for example for studying the interplay between syntax and semantics, especially at the levels of discourse and pragmatics: for example, to predict variables such as word order 
in practice, we must often be aware of the grammatical functions, semantic roles, information status of entity mentions, degree of formality and politeness, and more. 
Representative Corpus 1 
The GUM corpus 
The Georgetown University Multilayer corpus (GUM, Zeldes 2017), is a freely available corpus of English Web genres, created using ‘class­sourcing’ as part of the Linguistics curriculum at Georgetown University. The corpus, which is expanded every year and currently contains over 129,000 tokens, is collected from eight open access sources: Wikinews news reports, biographies, fction, reddit forum discussions, Wikimedia interviews, wikiHow how-to guides and Wikivoyage travel guides. Its architecture can therefore be considered to follow the common tree-style macro-structure with eight subcorpora, each containing simple, unaligned documents. The complexity of the corpus architecture results from its annotations: as the data is collected, student annotators iteratively apply a large number of annotation schemes to their data using different formats and tools, including document structure in TEI XML, POS tagging, syntactic parsing, entity and coreference annotations and discourse parses in Rhetorical Structure Theory. The complete corpus covers over 50 annotation types (see http://corpling. uis.georgetown.edu/gum/). A single tokenized word in GUM therefore often carries an annotation graph of dozens of nodes and annotations, illustrated using only two tokens from the corpus in Fig. 3.4, which shows the two tokens I know. 

(continued) 
At an abstract level, the interconnected units in Fig. 3.4 all represent nodes in an annotation graph. The two solid rectangles at the bottom of the image are special nodes representing actual tokens from a text: they are unique in that they carry not only a number of annotations, but also references to primary text data (I and know in bold at the top of each box). Their annotations include two distinct POS tags using different tag sets (Penn Treebank and CLAWS), as well as lemmas. These token nodes also function as anchors for the remaining nodes in the graph: every other node in the fgure is attached directly or indirectly to the tokens via edges. Layers are represented by dashed rectangles: in this case, each annotation belongs to exactly one layer (rectangles do not overlap). Node annotations are represented as key-value pairs with an ‘=’ sign (e.g. entity = person), while edge annotations look the same but are given in italics next to the edge they annotate. 
For example, there is a dependency edge connecting the two tokens and carrying a label (func = nsubj, since I is the nominal subject of know), belonging to a layer ‘dependencies’. The single node in the layer ‘sentence types’ above the tokens is annotated as s_type = decl (declarative sentence), and is attached to both tokens, but the edges attaching it are unannotated (no labels). Finally, some layers, such as ‘constituent syntax’, contain a complex subgraph: an NP node is attached to the token I, and a VP node is attached to know, and together they attach to the S node denoting the clause. Similarly, the ‘discourse’ layer, of which we only see one incoming edge, is the entry point into the discourse annotation part of the graph, which places multiple tokens in segments, and then constructs a sub-graph made of sentences and clauses based on Rhetorical Structure Theory (RST, Mann and Thompson 1988). The edge is annotated as ‘relname = background’, indicating this clause gives background information for some other clause (not pictured). 
Note that it is the corpus designer’s decision which elements are grouped in a layer. For example, the constituent S representing the clause has a similar meaning to the sentence annotation in the ‘sentence types’ layer, but these have been modeled as separate. As a result, it is at least technically possible for this corpus to have conficting constituent trees and sentence span borders for sentence type annotation. If these layers are generated by separate automatic or manual annotation tools, then such conficts are in fact likely to occur over the course of the corpus. Similarly, a speaker annotation (‘sp_who’) is attached to both tokens, as is the sentence annotation, but it is conceivable that these may confict hierarchically: a single sentence annotation may theoretically cover tokens belonging to different speakers, which may or may not be desirable (e.g. for annotating one speaker completing another’s sentence). The graph-based data model allows for completely independent annotation layers, united only by joint reference to the same primary text. 
Representative Corpus 2 

MERLIN 
The MERLIN project (Multilingual Platform for European Reference Levels: Interlanguage Exploration in Context, Boyd et al. 2014) makes three learner corpora available in the target languages Czech, German and Italian, which are richly annotated and follow a comparable architecture to allow for cross-target language and native language comparisons. The project was conceived to collect, study and make available learner texts across the levels of the Common European Framework of Reference for Languages (CEFR), which places language learners at levels ranging from A1 (also called ‘Breakthrough’, the most basic level) to C2 (‘Mastery’). Although these levels are commonly used in language education and studies of second language acquisition, it is not easily possible to fnd texts coming from these levels, neither for researchers nor for learners looking for reference examples of writing at specifc levels. The MERLIN corpora fll this gap by making texts at the A1-C1 levels publicly available in the three target languages above. 
To see how the MERLIN corpora take advantage of their architecture in order to expose learner data across levels we must frst consider how users may want to access the data, and what the nature of the underlying primary textual data is. On one level, researchers, language instructors and other users would like to be able to search through learner data directly: the base text is, trivially, whatever a learner may have written. However, at the same time the problems discussed in Sect. 3.2.2 make searching through non-native data, which potentially contains many errors,6 non-trivial. For example, the excerpt from one Italian text in (6) contains multiple errors where articles should be combined with prepositions: once, da ‘from’ is used without an article in da mattina ‘from (the) morning’ for dalla ‘from the (feminine)’, and once, the form da is used instead of dal ‘from the (masculine)’. The data comes from a Hungarian native speaker, rated at an overall CEFR ranking of B2, as indicated by document metadata in the corpus. 
(6) Da mattina al pomerrigio? Da prossima mese posso lavorare? from morning to.the afternoon ? from next month can.1.SG/ work? ‘From morning to the afternoon? From next month I can work?’ 
(continued) 
6This is not to say that native data does not contain errors from a normative perspective, and indeed some corpora, such as GUM in Representative Corpus 1, do in fact annotate native data for errors. 
This data is invaluable to learners and educators interested in article errors. However users interested in fnding all usages of da in the L2 data will not be able to distinguish correct cases of da from cases that should have dal or dalla. At the same time, less obvious errors may render some word forms virtually unfndable. For example, the word pomeriggio ‘afternoon’ is misspelled in this example, and should read pomerrigio (the ‘r’ should be double, the ‘g’ should not be). As a result, users who cannot guess the actual spelling of words they are interested in will not be able to fnd such cases. 
In order to address this, MERLIN includes layers of target hypotheses (TH, see Reznicek et al. 2013). These provide corrected versions of the learner texts: At a minimum, all subcorpora include a span annotation called TH1, which gives a minimally grammatical version of the learner utterance, cor­recting only as much as necessary to make the sentence error-free, but without improving style or correcting for meaning.7 Figure 3.5 shows the learner utterances on the ‘learner’ layer, while the TH1 layer shows the minimal correction: preposition+article forms have been altered, and a word-order error in the second utterance has been corrected (the sentence should begin Posso lavorare ‘can I work’). The layer TH1Diff further notes where word form changes have occurred (the value ‘CHA’), or where material has been moved, using ‘MOVS’ (moved, source) and ‘MOVT’ (moved, target).8 These ‘difference tags’ allow users to fnd all cases of discrepancies between the learner text and TH1 without specifying the exact forms being searched for. 

error annotations, visualized using ANNIS 
(continued) 
7See Reznicek et al. (2012) for the closely related Falko corpus of L2 German, which developed minimal TH annotation guidelines. Like Falko, a subset of MERLIN also includes an ‘extended’ TH layer, called TH2, on which semantics and style are also corrected. A closely related concept to TH construction which is relevant to historical corpora is that of normalization: non-standard historical spellings can also be normalized to different degrees, and similar questions about the desired level of normalization often arise. 
8Further tags include ‘DEL’ for deleted material, and ‘INS’ for insertions. 
One consequence of using a TH layer for the architecture of the corpus is that the data may now in effect have two conficting tokenizations: on the ‘learner’ layer, the frst ‘?’ and the second ‘Da’ stand at adjacent token positions; on the TH1 layer, they do not. To make it possible to fnd ‘?’ followed by ‘Da’ in this instance, while ignoring or including TH layer gaps, MERLIN’s architecture explicitly fags these annotation layers as ‘segmentations’, allowing a search engine to use either one for the purpose of determining adjacency as well as context display size (i.e. what to show when users request a windows of +/- 5 units). 
One shortcoming of TH annotations is that they cannot generalize over common error types which are of interest to users: for example, they do not directly encode a concept of ‘article errors’. To remedy this, MERLIN includes a wide range of error annotations, with a major error-annotation category layer (EA_category, e.g. G_Morphol_Wrong for morphological errors), and more fne grained layers, such as G_Morphol_Wrong_type. The latter indicates a ‘gender’ error on prossima ‘next (feminine)’ in Fig. 3.5, which should read prossimo ‘next (masculine)’ to agree with mese ‘month’. Note however that the architecture allows multiple conficting annotations at the same position: two ‘EA_category’ annotations overlap under prossima, indicating the presence of two concurrent errors, and there is no real indi­cation, except for the length of the span, that the ‘gender’ error is somehow paired with the shorter EA_category annotation. Additionally, the EA layers cannot encode all foreseeable errors of interest: for example, there is no specifc category for cases where da should be dal (but not dalla). This type of query can only be addressed using the running TH layer.9 
Finally it should be noted that both tokens and annotations, including TH layers, can be used as entry points for more complex annotation graphs. In the case of MERLIN, an automatically generated dependency syntax parse layer was added on top of the learner layer, as shown in Fig. 3.6. 
If the corpus architecture has successfully expressed all annotations including the parse in a single graph, then it is possible to query syntax trees in conjunction with other layers. For example we can obtain syntactic information, such as the most common grammatical functions and distance between words associated with movements (MOVS/MOVT) across gaps on the TH layer. This would not be possible if TH analysis had been implemented 
(continued) 
9A more minimal type of TH analysis is also possible, in which only erroneous tokens are given a correction annotation (see e.g. Tenfjord et al. 2006 for a solution using TEI XML). A limitation of this approach is that the TH layer itself cannot be annotated as a complete independent text (e.g. to compare POS tag distributions in the original and TH text), and that gaps of the type seen in Fig. 
3.5 cannot be represented. 

in separate fles, without consideration for the alignment of each annotation’s structures or the handling of gaps and segmentation conficts. Similar addi­tional graphs would also be conceivable, for example to link specifc MOVS and MOVT locations, but these have not yet been implemented – TH1Diffs are currently expressed as fat annotations whose interconnections are left unexpressed in the data model. 


3.3 Critical Assessment and Future Directions 
At the time of writing, corpus practitioners are in the happy position of having a wide range of choices for concrete corpus representation formats and tools. However, few tools or formats can do ‘everything’, and more often than not, the closer they get to this ideal, the less convenient or optimized they are for any one task. To recap some important considerations in choosing a corpus architecture and a corresponding concrete representation format: 
• 
Is preservation of the exact underlying text (e.g. whitespace preservation) important? 

• 
Are annotations very numerous or involve conficting spans to the extent that a stand-off format is needed? 

• 
Are annotations arranged in mutually exclusive spans? Are they hierarchically nested? Are discontinuous annotations required? 

• 
Are complex metadata management and subcorpus structure needed, or can this information be saved separately in a simple table? 

• 
Does the data contain A/V signals? If so, are there overlapping speakers in dialogue? 

• 
Is parallel alignment needed, i.e. a parallel corpus? 


These questions are important to address, but the answers are not always straight­forward. For example, one can represent ‘discontinuous’ annotations slightly less faithfully by making two annotations with some co-indexed naming mechanism (cf. MOVS and MOVT in Representative Corpus 2). This may be unfaithful to our envisioned data model, but will greatly broaden the range of tools that can be used. 
In practice, a large part of the choice of corpus architecture is often dictated by the annotation tools that researchers wish to use, and the properties of their representation formats. Using a more convenient tool and compromising the data model can be the right decision if this compromise does not hurt our ability to approach our research questions or applications. For example, many spoken corpora containing dialogue do not model speaker overlap, instead opting to place overlapping utterances in the order in which they begin. This can be fne for some research questions, for example for a study on word formation in spoken language; but not for others, e.g. for pragmatic studies of speech act interactions in dialogue. Table 3.1 gives a (non-exhaustive) overview of some popular corpus formats and their coverage in terms of the properties discussed above. A good starting point when looking to choose a format is to use this table or construct a similar one, note supported and unsupported features, and rule out formats that are not capable of representing the desired architectural properties. 
CoNLLU is a popular format in the family of tab-delimited CoNLL formats, which is used for dependency treebanks in the Universal Dependencies project (http://universaldependencies.org/). It is enriched with ‘super-token’-like word forms (i.e. multi-token orthographic word forms such as ‘I’m’), open-ended key-value pairs on tokens, and sentence level annotations as key-value pairs. The CWB vertical format (sometimes also called ‘TreeTagger format’, due to its compatibility with the tagger by Schmid 1994), is an SGML format with one token per line, accompanied by tab-delimited token annotations, and potentially conficting, but not hierarchically nested element spans. Elan and EXMARaLDA are two popular grid-based annotation tools, which do not necessarily model a token concept, instead opting for unrestricted layers of spans, some of which can be used to transcribe texts, while others express annotations. They offer excellent support for aligned A/V data and model a concept of potentially multiple speakers, complete with speaker-related metadata, which makes them ideal for dialogue annotation. FoLiA, GrAF and PAULA XML are all forms of graph-based stand-off XML formats, though FoLiA’s implementation is actually contained in a single XML fle, with document internal references. GrAF has the status of an ISO standard (ISO 24612), and has been used to represent the American National Corpus (https://www.anc.org/). FoLiA has the advantage of offering a complete annotation environment (FLAT, http://fat.science. ru.nl/), though PAULA and GrAF can be edited using multi-format annotation tools such as Atomic (http://corpus-tools.org/atomic/). PAULA is the only format of the three which implements support for parallel corpora and overlapping speakers. 
Table 3.1 Data model properties for a range of open corpus formats 
Whitespace  Standoff  Hierarchy  Conf. Spans  Discontinuous  Parallel  Dialogue overlap  Metadata  Subcorpora  Multimodal  
CoNLLU  Yes  No  Depa  No  No  No  No  No  No  No  
CWB  No  No  No  Yes  No  Yes  No  Yes  No  No  
Elan  Yes  Inline  No  Yes  No  No  Yes  Yes  Yes  Yes  
EXMARaLDA  Yes  Inline  No  Yes  No  No  Yes  Yes  Yes  Yes  
FoLiA  Yes  Inline  Yes  Yes  Yes  No  No  Yes  Yes  No  
GrAF  Yes  Yes  Yes  Yes  Yes  No b  No b  Yes  Yes  No  
PAULA XML  Yes  Yes  Yes  Yes  Yes  Yes  Yes  Yes  Yes  Yes  
PTB  No  No  Yes  No  No  No  No  No  No  No  
TCF  Yes  Inline  Yes  Yes  Yes  No  No  Yes  No  No  
TEI XML  Yes  Yesc  Yes  No c  No c  Yes  Yes  Yes  Yes  Yes  
TigerXML  No  No  Yes  No  Yes  No d  No  Yesd  Yes  No  
tiger2  Yes  Yes  Yes  Yes  Yes  No  No  Yes  Yes  No  
WebAnno  Yes  Inline  dep a  Yes  No  No  No  No  No  No  

aThe value ‘dep’ indicates formats with some capacity to express dependency edges betweenfat units (including, e.g. syntactic dependency or coreference annotation), but without complex node hierarchies. bWhile GrAF does not explicitly support multiple overlapping speakers or parallel corpora, there are some conceivable ways of representing these using the available graph structure. However I am not aware of any corpus or tool implementing these with GrAF. cStand-off annotation has been implemented in TEI XML (see Chapter 20.5 of the TEI p5 guidelines, http://www.tei-c.org/) (Accessed 28 May 2019) and can cover a wide range of use cases for discontinuous annotations and hierarchy conficts. However it is not frequently used in the TEI community, and there are some limitations (see Ba´
nski 2010 for analysis). dTigerXML itself does not implement parallel alignment, but an extension format known as STAX has been developed for parallel treebanking in the Stockhold TreeAligner (Lundborg et al. 2007). Metadata in TigerXML is limited to a predetermined set offelds, such as ‘author’, ‘date’ and ‘description’. 
Penn Treebank bracketing (PTB), TigerXML and tiger2 are formats specializing in syntax annotation (treebanks). The PTB format is the most popular way of representing projective constituent trees (no crossing edges) with single node annotations (part of speech or syntactic category). It is highly effcient and readable, but has some limitations (see the ‘crocidolite’ example above). TigerXML is a more expressive XML format, capable of representing multiple node annotations, crossing edges, edge labels and two distinct types of edges. The tiger2 format (Romary et al. 2015) is an extension of TigerXML, outwardly very similar in syntax, but with unlimited edge typing, metadata, multiple/conficting graphs per sentence and other more ‘graph-like’ features. It enjoys an ISO standard status (ISO 24615). 
TCF (Hinrichs et al. 2010) is an exchange format used by CLARIN infrastruc­ture, and in particular the WebLicht NLP toolchain. It is highly expressive for a closed set of multilayer annotations, and has built in concepts for tokenization, sentence segmentation, syntax and entity annotation. It is also one of the supported formats of the popular WebAnno online annotation tool (Yimam et al. 2013), which also supports a variety of formats of its own, including its highly expressive UIMA based format (serializable as an ‘inline stand-off’ XMI format), and a whitespace preserving tab-delimited export, called WebAnno TSV. 
An important trend in corpus building tools looking forward is a move away from saving and exchanging data in local fles on annotators’ computers or private servers. Corpora are increasingly built using public, version-controlled repositories on platforms such as GitHub. For example, the Universal Dependencies project is managed entirely on GitHub, including publicly available data in multiple languages and the use of GitHub pages and issue trackers for annotation guidelines and discussion. Some tools (e.g. the online XML and spreadsheet editor GitDox, Zhang and Zeldes 2017) are opting for online storage on GitHub and similar platforms as their exclusive fle repository. In the future we will hopefully see increasing openness and interoperability between tools which adopt open data models and best practices that allow users to beneft from and re-use existing data and software. 

3.4 Tools and Resources 
An important set of tools infuencing the choice of corpus architecture is NLP pipelines and APIs, which allow users to construct automatically tagged and parsed representations with complex data models (and these can be manually corrected if needed). Some examples include Stanford CoreNLP (Manning et al. 2014), Apache OpenNLP (https://opennlp.apache.org/) (accessed 28 May 2019), Spacy (https:// spacy.io/) (accessed 28 May 2019), the Natural Language Toolkit (NLTK, http:// www.nltk.org/) (accessed 28 May 2019), GATE (Cunningham et al. 1997), DKPro (Eckart de Castilho and Gurevych 2014), NLP4J (https://emorynlp.github.io/nlp4j/) (accessed 28 May 2019) and FreeLing (http://nlp.cs.upc.edu/freeling/) (accessed 28 May 2019). 
The output formats of NLP tools are often not compatible with corpus search architectures, and may not be readily human-readable (for example, .json fles offer very effcient storage, but are only meant to be machine readable). For this reason, NLP tool output must often be converted into corpus formats such as those in Table 3.1. Versatile conversion tools, such as Pepper (http://corpus-tools.org/ pepper/) (accessed 28 May 2019), can be used to convert between a variety of formats and make data accessible to a wider range of tools. Another important feature supported by tools such as Pepper is merging data from several formats into a format capable of expressing the multiple streams of input data. Using a merging paradigm makes it possible to build corpora that require some advanced features 
(e.g. conficting spans, or multimodal time alignment), which are not available simultaneously in the tools we wish to use, but can be represented separately in a range of tools, only to be merged later on. For example, the GUM corpus described above is annotated using fve different tools which are optimized to specifc tasks, and the merged representation is created automatically (this is sometimes called a ‘build bot’ strategy; for an example see https://corpling.uis.georgetown.edu/gum/ build.html) (accessed 28 May 2019). 
Finally, corpus architecture considerations also interact with the choice of search and visualization facilities that one intends to use. Having an annotation tool which supports a complex data model may be of little use if the annotated data cannot be accessed and used in sensible ways later on. Some corpus practitioners use scripts, often in Python or R, to evaluate their data, without using a dedicated search engine (see Chap. 9). While this approach is very versatile, it is also labor intensive: for each new type of information, a new script must be written which traverses the corpus in search of some information. It is therefore often desirable to have a search engine that is capable of extracting data based on a simple query. For corpora that are meant to be publically available to non-expert users, this is a necessity. In public projects, a proprietary search engine tailored explicitly for a specifc corpus is often programmed, which cannot easily be used for other corpora. Here I therefore focus on generic, freely available tools which can be used for a variety of datasets. 
The Corpus Workbench (Christ 1994) and its web interface CQPWeb (Hardie 2012) are amongst the most popular tools for corpus search and visualization, but are not capable of representing hierarchical data, and therefore they cannot be used for treebanks. Grid-like data, e.g. from EXMARaLDA or Elan fles, can be indexed for search using EXMARaLDA’s search engine, EXAKT (http://exmaralda.org/en/ exakt-en/) (accessed 28 May 2019). For treebanks, there are some local user tools 
(e.g. TigerSearch, Lezius 2002, or command line tools such as TGrep2, http:// tedlab.mit.edu/˜dr/Tgrep2/ (accessed 28 May 2019), the successor of the original Penn Treebank tool, or Stanford’s Tregex, (https://nlp.stanford.edu/software/tregex. shtml) (accessed 28 May 2019). There are only a few dedicated web interfaces for treebanks, notably Ghodke and Bird’s (2010) highly effcient Fangorn (for projective, unlabeled constituent trees), and TDRA, the Tingen aNnotated Data Retrieval Application, for TigerXML style trees and dependency trees (Martens 2013). For small-medium sized multilayer corpora, with syntax trees, entity and coreference annotation, discourse parses and more, ANNIS (http://corpus-tools.org/ 
annis/) (accessed 28 May 2019) offers a comprehensive solution supporting highly complex graph queries over hierarchies, conficting spans, aligned A/V data and parallel corpora. For larger datasets, KorAP (Diewald et al. 2016) presents a search engine supporting a substantial subset of graph relations, accelerated for text search using Apache Lucene (https://lucene.apache.org/) (accessed 18 June 2019). 
Further Reading 
McEnery, T., Xiao, R., and Tono, Y. 2006. Corpus-Based Language Studies: 
An Advanced Resource Book. (Routledge Applied Linguistics.) London: 
Routledge. 
This resource book contains both important readings in corpus linguistics and practical guides to corpus compilation and research. It forms a cohesive introduction and is a useful starting point for newcomers to the discipline. Chapters 3 and 4 in part one of the book offer a good brief overview of issues in corpus mark-up and annotation that relate to corpus architecture, and the third part of the book explores practical case studies with real data that can help familiarize readers with some fundamental examples of corpus architectures. 
Leling, A., and Kyt M. (eds.) 2008–2009. Corpus Linguistics. An Interna­
tional Handbook. (Handbooks of Linguistics and Communication Science 
29.) Berlin: Mouton de Gruyter. 
A comprehensive, two volume overview of topics in Corpus Linguistics written by a range of experts. The chapters on corpus types, including Speech, Multimodal, Historical, Parallel and Learner Corpora offer good overviews of issues in specifc corpus type architectures, and the chapters on Annotation Standards and Searching and Concordancing should be of interest with respect to corpus architectures as well. 
Kler, S., and Zinsmeister, H. 2015. Corpus Linguistics and Linguistically Annotated Corpora. London: Bloomsbury. 
This book gives a comprehensive overview of many aspects of complex annotated corpora, including data models and corpus query languages for treebanks and multilayer corpora. A particular focus on concrete corpora and tools makes it a useful practical introduction. 
Zeldes, A. 2018. Multilayer Corpus Studies. (Routledge Advances in Corpus Linguistics 22.) London: Routledge. 
As an intermediate level survey of multilayer corpora and their applications, this two part volume begins by laying out foundations for dealing with large numbers of concurrent annotations, and goes on to explore some of the applications of such corpora across a range of different research questions, primarily in the area of discourse level phenomena. 
References 
Ba´
nski, P. (2010). Why TEI stand-off annotation doesn’t quite work and why you might want to use it nevertheless. Proceedings of Balisage: The markup conference 2010. Montréal. Biber, D. (1993). Representativeness in Corpus design. Literary and Linguistic Computing, 8(4), 243–257. 
Boyd, A., Hana, J., Nicolas, L., Meurers, D., Wisniewski, K., Abel, A., Sche, K., Štindlová, B., & Vettori, C. (2014). The MERLIN Corpus: Learner language and the CEFR. Proceedings of LREC 2014 (pp. 1281–1288). Reykjavik, Iceland. 
Brugman, H., & Russel, A. (2004). Annotating multimedia/multi-modal resources with ELAN. Proceedings of LREC 2004 (pp. 2065–2068). Paris: ELRA. 
Burchardt, A., Pad S., Spohr, D., Frank, A., & Heid, U. (2008). Formalising Multi-layer Corpora in OWL DL – Lexicon Modelling, Querying and Consistency Control. Proceedings of IJCNLP 2008 (pp. 389–396). Hyderabad, India. 
Calzolari, N., & McNaught, J. (1994). EAGLES interim report EAG–EB–IR–2. 
Canales, O., Monaco, V., Murphy, T., Zych, E., Stewart, J., Castro, C.T.A., Sotoye, O., Torres, L., & Truley, G. (2011). A Stylometry system for authenticating students taking online tests. Proceedings of student-faculty research day, CSIS, Pace University, May 6th, 2011 (pp. B4.1– B4.6). White Plains, NY. 
Christ, O. (1994). A modular and fexible architecture for an integrated Corpus query system. 
Proceedings of complex 94. 3rd conference on computational lexicography and text research 
(pp. 23–32). Budapest. Crasborn, O. & Sloetjes, H. (2008). Enhanced ELAN functionality for sign language corpora. 
Proceedings of the 3rd workshop on the representation and processing of sign languages at LREC 2008 (pp. 39–42). Marrakesh, Morocco. 
Cunningham, H., Humphreys, K., Gaizauskas, R., & Wilks, Y. (1997). Software infrastructure for natural language processing. Proceedings of the ffth conference on applied natural language processing (pp. 237–244). Washington, DC. 
Diewald, N., Hanl, M., Margaretha, E., Bingel, J., Kupietz, M., Ba´nski, P., & Witt, A. (2016). KorAP architecture – Diving in the Deep Sea of Corpus data. Proceedings of LREC 2016. Portorož: ELRA. 
Dipper, S. (2005). XML-based stand-off representation and exploitation of multi-level linguistic annotation. Proceedings of Berliner XML Tage 2005 (pp. 39–50). Berlin, Germany. 
Eckart de Castilho, R., & Gurevych, I. (2014). A broad-coverage collection of portable NLP components for building shareable analysis pipelines. Proceedings of the workshop on open infrastructures and analysis frameworks for HLT (pp. 1–11). Dublin. 
Eckart de Castilho, R., Ide, N., Lapponi, E., Oepen, S., Suderman, K., Velldal, E., & Verhagen, 
M. (2017). Representation and interchange of linguistic annotation: An in-depth, side-by-side comparison of three designs. Proceedings of the 11th linguistic annotation workshop (LAW XI) (pp. 67–75). Valencia, Spain. 
Garside, R., Leech, G., & Sampson, G. (Eds.). (1987). The computational analysis of English: A Corpus-based approach. London: Longman. Ghodke, S., & Bird, S. (2010). Fast query for large Treebanks, 267–275. Proceedings of NAACL 2010. Los Angeles, CA. Green, L. J. (2002). African American English: A linguistic introduction. Cambridge: Cambridge University Press. Greenbaum, S. (Ed.). (1996). Comparing English worldwide: The international Corpus of English. Oxford: Clarendon Press. Hardie, A. (2012). CQPweb – Combining power, fexibility and usability in a Corpus analysis tool. International Journal of Corpus Linguistics, 17(3), 380–409. Hearst, M. A. (1997). TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1), 33–64. 
Heiden, S. (2010). The TXM platform: Building open-source textual analysis software compatible with the TEI encoding scheme, 24th Pacifc Asia Conference on Language, Information and Computation (pp. 389–398). Sendai, Japan. 
Hinrichs, E.W., Hinrichs, M., & Zastrow, T. 2010. WebLicht: Web-based LRT services for German. Proceedings of the ACL 2010 system demonstrations (pp. 25–29). Uppsala. 
Her, S. (2012). Annotating ambiguity: Insights from a Corpus-based study on syntactic change in old Swedish. In T. Schmidt & K. Wner (Eds.), Multilingual Corpora and multilingual Corpus analysis (Hamburg studies on multilingualism 14) (pp. 245–271). Amsterdam/Philadelphia: Benjamins. 
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischedel, R. (2006). OntoNotes: The 90% solution. Proceedings of the human language technology conference of the NAACL, companion volume: Short papers (pp.57–60). New York. 
Ide, N., Baker, C., Fellbaum, C., & Passonneau, R. (2010). The manually annotated sub-Corpus: A community resource for and by the people. Proceedings of ACL 2010, (pp. 68–73). Uppsala, Sweden. 
ISO 24612. (2012). Language resource management – Linguistic annotation framework (LAF). London: BSI British Standards (pp. 10). 
ISO 24615. (2010). Language resource management – Syntactic annotation framework (SynAF). London: BSI British Standards (pp. 20). 
Klein, T., & Dipper, S. (2016). Handbuch zum Referenzkorpus Mittelhochdeutsch (Bochumer Linguistische Arbeitsberichte 19). Bochum: Universität Bochum Sprachwissenschaftliches Institut. 
Kountz, M., Heid, U., & Eckart, K. (2008). A LAF/GrAF-based encoding scheme for under­specifed representations of dependency structures. Proceedings of LREC 2008. Marrakesh, Morocco. 
Krause, T., & Zeldes, A. (2016). ANNIS3: A new architecture for generic Corpus query and visualization. Digital Scholarship in the Humanities, 31(1), 118–139. 
Krause, T., Leling, A., Odebrecht, C., & Zeldes, A. (2012). Multiple Tokenizations in a diachronic Corpus. In Exploring Ancient Languages through. Oslo: Corpora. 
Kredens, K., & Coulthard, M. (2012). Corpus linguistics in authorship identifcation. In P. M. Tiersma &L.M.Solan (Eds.), The Oxford handbook of language and law (pp. 504–516). Oxford: Oxford University Press. 
Ku.cera, H., & Francis, W. N. (1967). Computational analysis of present-day English. Providence: Brown University Press. 
Kupietz, M., Belica, C., Keibel H., & Witt, A. (2010). The German reference Corpus DEREKO: A primordial sample for linguistic research. Proceedings of LREC 2010 (pp. 1848–1854). Valletta, Malta. 
Lee, J., Yeung, C. Y., Zeldes, A., Reznicek, M., Leling, A., & Webster, J. (2015). CityU Corpus of essay drafts of English language learners: A Corpus of textual revision in second language writing. Language Resources and Evaluation, 49(3), 659–683. 
Leech, G. N. (1997). Introducing Corpus annotation. In R. Garside, G. N. Leech, & T. McEnery (Eds.), Corpus annotation: Linguistic information from computer text corpora (pp. 1–18). London/New York: Routledge. 
Lezius, W. (2002). Ein Suchwerkzeug f syntaktisch annotierte Textkorpora. PhD thesis, Institut f maschinelle Sprachverarbeitung Stuttgart. 
Leling, A., Walter, M., Kroymann, E., & Adolphs, P. (2005). Multi-level error annotation in learner corpora. Proceedings of Corpus linguistics 2005. Birmingham, UK. 
Lundborg, J., Marek, T., Mettler, M., & Volk, M. (2007). Using the Stockholm TreeAligner. Proceedings of the sixth workshop on Treebanks and Linguistic theories. Bergen. 
Mann, W. C., & Thompson, S. A. (1988). Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3), 243–281. 
Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. J., & McClosky, D. (2014). The Stanford CoreNLP natural language processing toolkit. Proceedings of ACL 2014: System demonstrations (pp. 55–60). Baltimore, MD. 
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building a large annotated Corpus of English: The Penn Treebank. Special Issue on Using Large Corpora, Computational Linguistics, 19(2), 313–330. 
Martens, S. (2013). Tundra: A web application for Treebank search and visualization. Proceedings of the twelfth workshop on Treebanks and Linguistic theories (TLT12) (pp. 133–144). Sofa. 
McEnery, T., Xiao, R., & Tono, Y. (2006). Corpus-based language studies: An advanced resource book (Routledge Applied Linguistics). London/New York: Routledge. 
Odebrecht, C., Belz, M., Zeldes, A., & Leling, A. (2017). RIDGES Herbology – Designing a diachronic multi-layer Corpus. Language Resources and Evaluation., 51(3), 695–725. 
Reznicek, M., Leling, A., Krummes, C., Schwantuschke, F., Walter, M., Schmidt, K., Hirschmann, H., & Andreas, T. (2012). Das Falko-Handbuch. Korpusaufbau und Annotatio­nen. Humboldt-Universität zu Berlin, Technical Report, Version 2.01, Berlin. 
Reznicek, M., Leling, A., & Hirschmann, H. (2013). Competing target hypotheses in the Falko Corpus: A fexible multi-layer Corpus architecture. In A. Díaz-Negrillo, N. Ballier, & P. Thompson (Eds.), Automatic treatment and analysis of learner Corpus data (pp. 101–124). Amsterdam: John Benjamins. 
Romary, L., & Bonhomme, P. (2000). Parallel alignment of structured documents. In J. Véronis (Ed.), Parallel text processing: Alignment and use of translation corpora (pp. 201–217). Dordrecht: Kluwer. 
Romary, L., Zeldes, A., & Zipser, F. (2015). <tiger2/> - Serialising the ISO SynAF syntactic object model. Language Resources and Evaluation, 49(1), 1–18. 
Santorini, B. (1990). Part-of-Speech Tagging Guidelines for the Penn Treebank Project (3rd rev.). Technical report, University of Pennsylvania. 
Sauer, S., & Leling, A. (2016). Flexible multi-layer spoken dialogue corpora. International Journal of Corpus Linguistics, Special Issue on Spoken Corpora, 21(3), 419–438. 
Schembri, A., Fenlon, J., Rentelis, R., Reynolds, S., & Cormier, K. (2013). Building the British sign language Corpus. Language Documentation and Conservation, 7, 136–154. 
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. Proceedings of the conference on new methods in language processing (pp. 44–49). Manchester, UK. 
Schmid, H. (2008). Tokenizing and part-of-speech tagging. In A. Leling & M. Kyt(Eds.), Corpus linguistics. An international handbook (Vol. 1, pp. 527–551). Berlin: Mouton de Gruyter. 
Schmidt, T., & Wner, K. (2009). EXMARaLDA – Creating, analysing and sharing spoken language corpora for pragmatic research. Pragmatics, 19(4), 565–582. 
Smith, J. R., Quirk, C., & Toutanova, K. (2010). Extracting parallel sentences from comparable corpora using document level alignment. Proceedings of NAACL 2010 (pp. 403–411). Los Angeles. 
Tenfjord, K., Meurer, P., & Hofand, K. (2006). The ASK Corpus – A language learner Corpus of Norwegian as a second language. Proceedings of LREC 2006 (pp. 1821–1824). Genoa, Italy. 
Wichmann, A. (2008). Speech corpora and spoken corpora. In A. Leling & M. Kyt(Eds.), Corpus linguistics. An international handbook (Vol. 1, pp. 187–207). Berlin: Mouton de Gruyter. 
Yimam, S. M., Gurevych, I., Eckart de Castilho, R., & Biemann, C. (2013). WebAnno: A fexible, web-based and visually supported system for distributed annotations. Proceedings of ACL 2013 (pp. 1–6). Sofa, Bulgaria. 
Zeldes, A. (2017). The GUM Corpus: Creating multilayer resources in the classroom. Language Resources and Evaluation, 51(3), 581–612. 
Zeldes, A. (2018). Multilayer Corpus studies (Routledge advances in Corpus linguistics 22). London: Routledge. 
Zhang, S., & Zeldes, A. (2017). GitDOX: A linked version controlled online XML editor for manuscript transcription. Proceedings of FLAIRS-30 (pp. 619–623). Marco Island, FL. 
Zipser, F., & Romary, L. (2010). A model oriented approach to the mapping of annotation formats using standards. Proceedings of the workshop on language resource and language technology Standards, LREC-2010 (pp. 7–18). Valletta, Malta. 




Part II Corpus methods 
Chapter 4 Analysing Frequency Lists 
Don Miller 
Abstract As perhaps the most fundamental statistic in corpus linguistics, frequency of occurrence plays a signifcant role in uncovering variation in language. Frequency lists have been designed for a variety of linguistic features and employed in order to address a variety of research questions in diverse felds of inquiry, from language learning and teaching to literary analysis to workplace communication. This chapter provides an overview of fundamental concepts and methods in the development and application of frequency lists, including issues related to the target unit of analysis, and key considerations beyond raw frequency, such as the importance of normalising frequency counts when making comparisons and employing additional measures to gain a more comprehensive picture of distributional characteristics. 
4.1 Introduction 
The use of corpora and corpus-based tools allows for increased effciency in the identifcation of linguistic features that are characteristic of a language variety. A fundamental statistic in assessing the saliency of any linguistic feature is frequency of occurrence, or, simply, the number of times a feature of interest occurs in a data set. 
Probably the most widely known frequency lists are lists of frequently occurring lexical items, such as West’s (1953) General Service List (GSL), Nation’s (2004) lists from the British National Corpus (BNC), or more specialized lists such as Coxhead’s (2000) Academic Word List (AWL). These and other word lists have found wide use in support of language learning and teaching, helping focus efforts on lexical items that will ideally provide the biggest payoff for learners. Indeed, a considerable amount of research has been devoted toward developing and improving such lists (cf. Nation 2016). In addition to helping identify target vocabulary for 
D. Miller (•) Department of Languages and Applied Linguistics, University of California, Santa Cruz, CA, USA e-mail: dpmiller@ucsc.edu 
© Springer Nature Switzerland AG 2020 77 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_4 
study, lexical frequency lists have been used to help educators to better understand the lexical demands of target language uses (e.g. Adolphs and Schmitt 2003; Laufer and Ravenhorst-Kalovski 2010; Nation 2006;) or to assess the developing vocabulary sizes of learners (e.g. Batista and Horst 2016; Laufer and Nation 1995). Language teachers and learners have also benefted from frequency analysis of linguistic units beyond lexis. For example, the frequency of different grammatical constructions has informed the selection and ordering of features included in course textbooks (Biber and Reppen 2002; Conrad and Biber 2009), and frequency of error types have been used to better understand the challenges faced by different groups of second language writers (Doolan and Miller 2012). 
But the use of frequency lists extends well beyond pedagogically oriented appli­cations, helping to address a wide variety of language-related research questions across numerous felds of inquiry. Baker (2011), for example, identifed diachronic variation in British English by analysing word frequencies in corpora refecting four time periods between 1931–2006. Weisser (2016a) examined the frequency of different speech acts in workplace communication as a possible means for differentiating individuals (or groups) and assessing speaker effciency in service encounters. Ikeo (2016) used frequency of multi-word units to assist in literary analysis, to illustrate how language is used in DH Lawrence’s Lady Chatterley’s Lover to establish different characters’ internal states, perceptions, and viewpoints. Frequency of multi-word units has also been used in studies of authorial attribution 
(e.g. Houvardas and Stamatatos 2006). In speech and language pathology, Liu and Sloane (2006) used frequency profles to inform the selection of Chinese characters meriting encoding for an augmentative and alternative communication (AAC) system. These few examples illustrate the wide diversity of applications of frequency analysis. The following section highlights some fundamental issues related to the construction, analysis, and application of frequency lists. 

4.2 Fundamentals 
4.2.1 Zipf’s Law 
A key to understanding both the effcacy and limitations of frequency lists is Zipf’s law (Zipf 1936, 1949). In researching a variety of languages (English, German, Latin, and Chinese), Zipf observed that texts were made up of relatively few frequently occurring words and a large number of words occurring quite rarely. This observation led to Zipf’s law, which very broadly states that, in human language, frequency of occurrence patterns in inverse proportion to frequency rank. If the most frequently occurring word (rank 1) occurs x times, the next most frequent word (rank 
2) occurs approximately half as often. The 10th ranked word occurs approximately twice as often as the 20th ranked word, about 10 times as often as the 100th ranked 
Table 4.1 Sample rank and frequency distribution from the BNC 
Rank  Word  Frequency  
1  the  6,187,267  
2  of  2,941,444  
10  to  917,579  
20  at  478,162  
100  as  91,583  
1000  playing  9738  

Fig. 4.1 Rank vs. frequency plot of word forms in ICE-GB. (I thank Stefan Th. Gries for providing me with this fgure) 
Rank of word form frequency in ICE-GB(binary log) 
0 5 1015 

0 5 1015 World form frequency in ICE-GB (binary log) 
word, and about 100 times as often as the 1000th ranked word. Table 4.1 illustrates this distribution in the BNC (Leech et al. 2001). 
This phenomenon is perhaps best visualized via a log-log plotting of rank vs. frequency such as in Fig. 4.1, which illustrates form/rank distributions in the British component of the International Corpus of English (ICE-GB). 
In practical terms, the distribution identifed in Zipf’s law has two important implications: 
1. 
A relatively small set of words accounts for a large proportion of tokens in a text (or corpus). For example, the 100 most frequently occurring words in the BNC account for approximately 45% coverage of the corpus (Atkins and Rundell 2008). This distributional phenomenon allows researchers, despite immense lexical diversity in language, to achieve high text coverage with a relatively low number of high-frequency words. This is very fortunate for list designers and list users seeking to maximize efforts in language learning. However, ... 

2. 
... a very large proportion of words in a text have a very low frequency of occurrence. Approximately 40% of words in the BNC, for example, are hapax legomena, or words occurring only once in a corpus (Scott and Tribble 2006). Since the vast majority of words in a language are highly infrequent, very large corpora are required to capture and understand the distribution and behaviour of rare lexical features. 



4.2.2 Unit of Analysis 
As noted above, perhaps the most widely known frequency lists are lists of individual words. Even at this level, some basic decisions have to be made in the operationalization of the construct of a “word.” In languages using the Roman alphabet, for example, it may seem obvious that a word can be identifed as one or more letters surrounded by spaces. However, this is not so simple. In English, for instance, decisions must be made regarding whether to treat compounds, either 
(a) 
open (e.g., school bus), closed (e.g., weekend), or hyphenated (e.g., well-being), 

(b) 
transparent (e.g. school bus) or opaque (e.g., deadline), and (c) as one word or two words. This can be especially tricky, as there is often some variation in how these items appear across a corpus (or even within a text); for example, although the hyphenated form, well-being, is clearly the most common option found in the Corpus of Contemporary American English (COCA) (>8000 occurrences), the forms well being and wellbeing each occur several hundred times. Researchers also need to account for different spellings across different varieties of a language (e.g., behaviour vs. behavior) or even commonly misspelled words (e.g., mispelled). Whatever decisions are made, they should be principled, consistent, and clearly explained (cf. Nation 2016, particularly Sect. 4.4.2, for a discussion and suggestions for dealing with such issues). 


In profling lexical frequencies, researchers also have to decide which related forms should be counted as a single lexical item. Operationalizations of the ‘word’ construct can range from treating all orthographic word forms as distinct words (e.g., book and books are two separate words), to treating all word forms sharing a classical root as members of a word family (e.g., help, helping, helpful, unhelpful, helpfulness, etc. are all members of the word family related to the headword HELP). Bauer and Nation (1993) outline a useful seven-level taxonomy for classifying morphological affxation and, ultimately, the relationships between word forms. Until fairly recently, most of the infuential pedagogically oriented word lists have been based on the more inclusive levels of this taxonomy in their operationalization of “word family.” For example, the GSL (West 1953), the BNC 3000 (Nation 2004), and the AWL (Coxhead 2000) are all lists of word families. More recently, however, there has been a trend toward using the lemma, a unit consisting of “lexical forms having the same stem and belonging to the same major word class, differing only in infection and/or spelling” (Francis and Kucera’s 1982, 1), rather than the word family, as the unit of analysis. For example, the lemma FALL (v.) has the members fall (v.), falls (v.), fell (v.), and fallen (v.); the separate lemma FALL (n.) has the members fall (n.) and falls (n.). An example of a lemma-based list is the 5000-word list detailed in Davies and Gardner’s Frequency Dictionary of American English (2010). Among the drivers of this trend, one is technological advances, especially the development of increasingly accurate part-of-speech taggers that allow automatic distinctions to be made (cf. Chap. 2). Another driver is the recognition that using the lemma as the unit of analysis does some of the work in differentiating meaning senses of polysemous words. For example, man (n.) 
meaning “the human species” or “an adult male” would be differentiated from man (v.) “to take charge of something”. Whatever unit is chosen, it should be noted that constructing frequency lists based on sets of related word forms (e.g., lemmas or word families) requires the additional step of frst compiling those sets. For example, in his construction of the BNC lists, Nation (2004) beneftted from the lemmatized list that Leech et al. (2001) had previously compiled, but he still had to build word families from that list. Fortunately, lists of lemmas and word families for many languages are available online, and many software packages used for word list construction include options for unit of analysis (e.g., word form, lemma, or word family) (see Sect. 4.4.4 for examples). 
Yet another robust research paradigm has widened the concept of “lexical item” beyond single words to include multi-word units (MWUs), including collocations, “phrasal verbs (call on, chew out), idioms (rock the boat), fxed phrases (excuse me), and prefabs (the point is)” (Gardner 2007,260). This paradigm is founded on the important role of phraseology in discourse. Research has demonstrated the wide prevalence of multi-word sequences in language (e.g. Biber et al. 1999; Biber et al. 2004; Conklin and Schmitt 2008). Erman and Warren (2000) even propose that multi-word phrases account for over half of both spoken and written English! The ubiquity of MWUs, coupled with their demonstrated psycholinguistic reality (e.g. Conklin and Schmitt 2008; Durrant and Doherty 2010) has led to investigations into MWU frequency distributions in a variety of languages (e.g., English: Shin and Nation 2008; Korean:Kim 2009; Polish: Grabowski 2015), registers (e.g., academic writing: Biber et al. 2004; Simpson-Vlach and Ellis 2010), and disciplines (e.g., engineering:Ward 2007; business:Hsu 2011). 
A great deal of research in this paradigm has focused on identifying and analyz­ing prefabricated ‘chunks’ of formulaic language (e.g., if you look at) (Biber et al. 2004). These contiguous sequences of words of various length are known by many names, including formulaic sequences, lexical bundles,or n-grams (where n = the number of words in the sequence, e.g., bigrams, trigrams, etc.). Conceptually, what separates these units from other MWUs is that, rather than being recognized and interpreted as semantically ‘complete’ units (as with, for example, phrasal verbs or idioms), formulaic sequences may appear semantically or structurally incomplete and are thus typically categorized functionally (e.g., framing: the existence of a; quantifying: in a number of ) or structurally (e.g., PP-based: as a result of; NP-based: the nature of the) (Biber and Barbieri 2007; Biber et al. 1999). From a practical perspective, these contiguous strings of words can be retrieved automatically very easily compared to many other types of MWUs, such as open compound nouns (post offce), phrasal verbs (call on, chew out), or restricted collocations (do + homework), as identifying these latter types of MWUs requires distinguishing them from free combinations (e.g., I called on a student who was raising her hand. vs. I called on Friday, but she wasn’t in the offce.) and/or identifying them across gaps of variable length (e.g., I have to do homework. vs. I have to do my boring algebra homework.) (Gardner 2007). 
In the past decade, increasing attention has been given to non-contiguous chunks of frequently co-occurring language. These “discontinuous sequences in which words form a ‘frame’ surrounding a variable slot (e.g. I don’t * to, it is * to)” (Gray and Biber 2013, 109) have been referred to by different names, including lexical frames (ibid.), gapped n-grams (Cheng et al. 2006), phrase frames,or p-frames (Fletcher 2011; Rer 2010)or skipgrams (Cheng et al. 2006). This paradigm allows for the identifcation of frequently occurring phraseological patterns that would be missed were investigations restricted solely to contiguous strings of words. For example, while the latter would identify the trigram it would be, the former allows for the identifcation of a larger pattern, it would be * to, and to identify common fllers of the slot (e.g., interesting, useful, better)(Rmer 2010). 

4.2.3 Beyond Raw Frequency 
As noted above, frequency of occurrence can be used in understanding the relative salience of linguistic features in a text or discourse domain. It can also be used for comparison across texts or discourse domains. For example, Table 4.2 shows the 30 most frequently occurring nouns in two corpora, each comprising 30 speeches delivered by one of the candidates, Hillary Clinton or Donald Trump, during the 2016 United States Presidential campaign (corpora adapted from Brown 2017). These ranked lists might be used to highlight and compare some of the typical issues raised by the candidates. Juxtaposing these lists reveals some expected overlapping words (e.g., people, country, America, United States, job), but also some words refecting each candidate’s key talking points. For example, Hillary Clinton frequently mentioned the economy (business, work, tax), education (school, college), and her opponent (Trump). Donald Trump often spoke about national security and immigration (border, wall, Mexico, ISIS), his negative assessment of the state of the country and its policies (problem, disaster, Obama, Obamacare) and, in another interesting overlap, himself in the 3rd person (Trump). (See Chap. 6 for discussion of more sophisticated methods for comparing salience of lexical items across corpora.) 
The frequency counts in Table 4.2 are what are referred to as raw frequency, or the actual number of times these items are attested in the corpora. As can be seen from Table 4.2, raw frequency can be used to rank items, and ranked lists can provide some useful insights. However, if the goal is to determine whether a feature occurs more or less often in texts or corpora of unequal size, raw frequency has limitations. In this case, is necessary to normalise frequency counts. 
4.2.3.1 Normalising Frequency Counts 
As can be seen from the ranked lists in Table 4.2,the word job made it into both candidates’ lists of frequently used nouns. In terms of raw frequency, there are more than twice the number of occurrences in the corpus of Donald Trump’s speeches than in the corpus of Hillary Clinton’s speeches (525 compared with 251). While it may seem such a disparity is evidence that Donald Trump was more concerned about the issue of employment, these raw frequencies are not meaningfully comparable without factoring in the size of the corpora in which items occur, or normalising the frequencies. 
Hillary Clinton  Donald Trump  
Rank  Lemma  Frequency  Rank  Lemma  Frequency  
1  People  558  1  People  1254  
2  Country  326  2  Country  800  
3  America  320  3  Hillary  575  
4  Trump  300  4  Job  525  
5  President  285  5  Time  429  
6  Job  251  6  Thing  365  
7  Family  238  7  Year  360  
8  Year  168  8  Way  333  
9  Day  163  9  American  296  
10  Election  158  10  Trump  254  
11  Time  152  11  Percent  240  
12  Business  144  12  Day  236  
13  American  143  13  Deal  233  
14  Way  131  14  World  229  
15  Economy  129  15  Obama  213  
16  Campaign  128  16  United States  208  
17  Thing  124  17  Border  196  
18  Work  118  18  Right  194  
19  World  113  19  Folks  182  
20  Right  110  20  Number  173  
21  Life  104  21  Problem  167  
22  Woman  103  22  ISIS  157  
23  Plan, college  101  23  Trade  151  
24  United States, friend  97  24  Mexico  147  
25  Man  89  25  Disaster  146  
26  Kid, future  88  26  Wall  140  
27  Tax  85  27  Company  136  
28  Place  80  28  Guy  130  
29  Community  75  29  Obamacare  127  
30  School  74  30  Plan  119  

Normalising frequencies is simply a matter of dividing a raw frequency by the total number of words in a text (or corpus) and, optionally, multiplying the result by a meaningful common denominator that is somewhat comparable to the length of a corpus (or texts within a corpus). Thus, as can be seen in Table 4.3, 100,000 has been set as the denominator, as one of the corpora is nearly this size, while the other is not too much larger. A note of caution: Though it is common for research using corpora comprising tens or hundreds of millions of words to use one million or more as a common denominator, care should be taken in choosing this fgure to avoid misrepresenting the frequency (Weisser 2016b). Using the current example, normalising to occurrences per one million words—which is in this case about 10 times the size of the corpora and 100 times the size of the individual texts—might artifcially infate counts for rare features. 
Table 4.3 Computing normalized frequency 

Candidate  Raw frequency of the word jobs  Number of words in corpus  Step 1: Raw frequency/total number of words  Step 2: multiply by common base (here 100,000)  Normalised frequency of occurrence (frequency per 100,000 words)  
Hillary Clinton  251  95,131  252/95,131 = 0.00264  0.00264 × 100,000  264  
Donald Trump  525  167,446  525/167,446 = 0.00313  0.00313 × 100,000  313  

As can be seen in Table 4.3, the corpora are quite different in size. Thus, despite the large difference in raw frequency of occurrence, normalized frequency suggests that the word job actually played a more comparable role – at least in terms of frequency – in both candidate’s speeches than the raw frequencies would suggest. 

4.2.3.2 Range and Dispersion 
In practice, frequency alone—even normalized frequency—is of limited use for understanding the salience of a feature throughout a discourse domain. This point can be illustrated by considering two different content words, each occurring 42 times (per 100,000 words) in the corpus of Donald Trump’s campaign speeches: military (n.), and Virginia (n.). While frequency appears to suggest some sort of parity in salience, these occurrences had very different distributions throughout the corpus. Closer analysis reveals that the word military occurred in 80% of his speeches (24 of 30), suggesting it was one of Donald Trump’s frequent talking points throughout the campaign, whereas the word Virginia occurred in just 7 speeches, illustrating its use was limited to a much narrower set of contexts— perhaps to speeches given in this state. If the goal is to identify Donald Trump’s pet talking points throughout his campaign, it should be clear that frequency alone would be insuffcient. 
For this reason, researchers typically include measures of dispersion in order to better understand the distributional characteristics of a target feature. Measures of dispersion provide a more comprehensive picture of frequency distributions, allowing researchers to determine whether features are generally representative of a target discourse domain or, alternatively, limited to certain contexts or idiosyncrasies of certain language users. 
The simplest measure of dispersion is range, and it is typically operationalized in terms of the number of texts and/or sections in which a feature occurs. Many frequency lists have been designed based on a combination of frequency and range criteria. For example, the words included in Coxhead’s (2000) AWL had to occur 10 times in each of the four main macro-disciplines and in approximately one half of the subdisciplines represented in her Academic Corpus. The formulaic sequences in Hsu’s (2011) Business Formulas List had to occur 10 times per million words, in at least one half of the business disciplines represented in the corpus, and in at least 10% of texts representing each of these disciplines. 
Though range may be the mostly commonly employed measure of dispersion in frequency list design, contemporary researchers are increasingly employing mea­sures of dispersion that take into account how evenly a feature occurs throughout a target discourse domain – not simply the number of texts or sections of a corpus that a feature occurs in, but whether the occurrences are evenly distributed or particularly frequent or infrequent in any of these sections (see Chap. 5 for further discussion). For example, the words in Nation’s (2004) three 1000-word BNC-based frequency lists had to achieve a minimum frequency (=10,000 occurrences in the whole corpus), range (occurrence in 95–98 of 100 one million-word sections), and evenness of distribution across 100 one million-word sections (a dispersion coeffcient—Juilland’s D—of =.80; cf. Chap. 5 for more about this measure and below for a discussion of its potential shortcomings). Davies and Gardner (2010) determined ranks for words in their dictionary by employing yet another method: multiplying each word’s frequency by its dispersion (also Juilland’s D) and ranking words by resulting score. 
The variety of measures and criteria evidenced in these examples illustrate an important point: there are no hard and fast rules regarding the best frequency or dispersion criteria for constructing frequency lists. Rather, the criteria used have often been somewhat arbitrary, refecting researcher’s intuitions about what seems reasonable. Often times, this has meant that researchers simply employ criteria used in previous research. Alternatively, they might experiment with different criteria, observing resulting lists to gauge whether the criteria used lead to lists with desired characteristics (see Representative Study 2). 
Representative Study 1 
Gardner, D., Davies, M. 2014. A new academic vocabulary list. Applied Linguistics 35(3):305–327. 
Gardner and Davies’ study exemplifes a contemporary approach to con­structing and validating a frequency list of academic vocabulary—one based on lemmas rather than word families—by taking advantage of technological advances (e.g., part of speech tagging; the ability to compile substantially larger corpora) and the application of statistics to building frequency lists. 
(continued) 
The list that Davies and Gardner designed, the 3000-word Academic Vocabulary List (AVL), was culled from a 120+ million word subcorpus of written academic English from the Corpus of Contemporary English (COCA). According to Gardner and Davies, the AVL improves upon previous efforts in a number of key ways. First, the AVL is based on a contemporary, balanced corpus that is considerably larger than the 3.5 million word corpus from which its most popular predecessor, the AWL (Coxhead 2000), was culled. Their academic subcorpus of COCA represents nine academic disciplines. Approximately 75% of the corpus comprises academic journals, with the remainder from “academically oriented magazines” and fnancial newspaper articles (p. 313). According to the researchers, the size and breadth of this corpus improve its representativeness of academic writing. 
Secondly, the AVL is a list of lemmas rather than word families. Gardner and Davies argue that using the lemma as the unit of analysis will allow list users to more accurately target the most frequently occurring forms and meaning senses of academic vocabulary. 
Third, the authors included a more comprehensive set of statistical mea­sures related to ratio and dispersion than were employed in previous efforts for identifying academic vocabulary (esp. the AWL). Specifcally, words in the AVL had to meet the following criteria: 
1. 
Ratio: Selected words had to occur at a rate 50% higher (i.e., at 1.5 the ‘expected’ rate of occurrence) in their academic corpus than in a non­academic corpus (the rest of COCA). 

2. 
Range: Selected words had to occur with more than 20% of the expected frequency (cf. Chap. 20) in at least 7 of the 9 disciplines in their academic corpus 

3. 
Dispersion: In order to demonstrate evenness of distribution, selected words had to achieve a Juilland’s D of at least .80. 

4. 
Discipline measure: Selected words could not occur at more than 3 times the expected frequency in any one discipline, in order to avoid including discipline-specifc vocabulary 


Gardner and Davies arrived at each cut-off rate via experimentation, as no guidance was available in previous frequency list research. For example, the 
1.5 ratio rate was chosen because the researchers determined that lower ratios allowed too many general words (e.g. different, large), while higher ratios disallowed many words that intuitively belong in a high-frequency academic word list (e.g., require, create). 
(continued) 
In addition to the methodology used for corpus design and list extraction, two coverage-based analyses were used to provide evidence of the AVL’s validity. First, to demonstrate the specialized nature of the list, Gardner and Davies compared the coverage of the AVL in two academic corpora to that in two non-academic corpora. Results of this analysis demonstrate that the list performs considerably better coverage-wise in academic writing than in other written genres, in both in the academic portion of the corpus from which the list was constructed (13.8%), and a different academic sub-corpus from the BNC (13.7%) (compared to 8.0% for newspaper articles and 3.4% for fction). 
In the second analysis, Gardner and Davies compared the performance of the AVL to that of the AWL in order to demonstrate that it is indeed a more robust list. Because the AWL is a list of 570 word families, they built word families based on the most frequent 570 lemmas in the AVL for this comparison. The resulting 570 word families based on the AVL indeed provided considerably better coverage than the AWL in the two academic corpora—in fact, almost twice the coverage: nearly 14% by the AVL compared to approximately 7% by the AWL. 
The researchers acknowledge a key concern that may be raised about the fairness of the AVL vs. AWL coverage-based comparison in this second analysis. Many specialized lists that have been designed over the past several decades have relied on West’s (1953) GSL in helping to determine the specialized nature of frequency vocabulary (e.g., the AWL). They have essentially used the GSL as a stoplist, focusing only on words that do not appear in this list. This approach has a few drawbacks. First, it inherits any potential shortcomings of the GSL (or any stoplist employed). What is perhaps more important, however, is that, as Davies and Gardner demonstrate (and others have noted previously, see esp., Paquot 2007, 2010), many general service list words do have notable importance in academic writing as well. While they are highly frequent and widely dispersed in general English, they may be especially so in academic writing. 
Nevertheless, because Gardner and Davies used a different methodology— specifcally, not using the GSL as a stoplist—their AVL includes many high frequency items that are also found on the GSL and so were not included in the AWL. While this does not detract from the effcacy of the AVL, it does limit the strength of conclusions that can be drawn from this coverage-based comparison. 
Representative Study 2 
Brezina, V., &, Gablasova, D. 2015. Is there a core general vocabu­lary? Introducing the New General Service List. Applied linguistics 36(1):1–22. 
In this study, Brezina and Gablasova detail their development of an updated general service list, the new-GSL. Of particular note in their study is an additional step that they employ for addressing reliability: comparing fre­quency list items across different corpora. That is, they identify an overlapping core frequency list based on four corpora that differ from each other in terms of size, genre distributions, and age: the one million word Lancaster­Oslo-Bergen Corpus (LOB) corpus from 1961, the 100 million-word British National Corpus (BNC) from the 1990s, the one million-word British English 2006 corpus (BE06) from 2005–6, and the 12 billion-word internet-based EnTenTen12 from 2012. Using Sketch Engine, a sophisticated web-based corpus tool, Brezina and Gablasova culled four lists of 3000 words, one from each of the corpora. 
An important difference between their methodology and West’s (1953) 
is the unit of analysis used. They chose the lemma (i.e., base form + 
infectional variants) rather than the word family for two primary reasons: 
(1) it is beginners, i.e., learners without wide derivational morphological knowledge, who are the most likely users of a general service list; (2) using lemmas—rather than a more inclusive category like word family—can lessen the number of different word senses contained in a single lexical unit. 
Another key difference is that they selected words for their list by using dis­tributional criteria only: words were ordered according to a composite score based on frequency and range, i.e. Average Reduced Frequency (Savickand Hlavá.
cová 2002),1 and the top 3000 lemmas (excluding proper nouns) from each corpus were included in each of the four lists. 
To address and better understand the issue of diachronic change in frequency profles, they extracted and compared lists from the four corpora. Differences among the lists were found primarily in content words, often refecting changes in technology. For example, the BE06 includes a number of words that its earlier counterpart, the LOB, designed 40 years earlier, did not, e.g., CD, computer, email, Internet, mobile, online, video, web, website. 
(continued) 
1West (1953) purposely left out words that he considered highly emotional, potentially offensive, colloquial or slang, regardless of their frequency in the corpus used for constructing the GSL. He felt that users of this list (i.e., language learners), needed to be able to express ideas, not emotions, and he felt that these ideas could be expressed without colloquialisms. Further, to cover as wide a range of notions as possible, West left off words expressing notions already covered by more frequent words and replaced them with less frequent words with different semantic values. 
Notable differences were also found between the BE06 and EnTenTen12 corpora, despite their being so close in age. The list from the EnTenTen12 included a higher proportion of words related to the internet and technology (e.g., download, fle, install, menu, scan, software, upgrade) and business and advertising (e.g., advertising, CEO, competitor, dealer, discount, logo). The researchers note that these differences likely refect the online registers of the internet-based EnTenTen12. 
The authors then compared the frequency lists culled from each corpus to identify an overlapping, “stable” core of general service words—a list not demonstrating bias toward any of the four corpora, and thus exhibiting stability across time. The four lists had 78 to 84 percent overlap, with correlations between ranks at rs = .762 to rs = .870, all p < .001. 2116 lemmas were shared by each of the four lists, thus, according to the authors, constituting a stable core. In order to further modernize their new-GSL, they also added 378 lemmas which were identifed in the two recent corpora, the BE06 and the EnTenTen12, completing the fnal, 2494-word list. 
Finally, Brezina and Gablasova compared the coverage of their new-GSL with that of West’s (1953) GSL in each of the four corpora from which the former was derived. They found coverage to be generally comparable, both lists providing on average a bit over 80%. What is most important in this comparison is that the new-GSL is based on lemmas (2494 lemmas in total), whereas the original GSL’s 2000 word families comprise 4114 lemmas. Thus, the researchers argue, an important beneft of the new-GSL is that it provides comparable coverage with considerably less ‘investment’ for learners (i.e., approximately 40% fewer lemmas). 


4.3 Critical Assessment and Future Directions 
Three critical issues require attention as frequency list research goes forward: (1) dealing with homoforms and multi-word units, (2) the application of dispersion (and other) statistics to frequency list creation, and (3) addressing reliability/stability in validation of frequency lists. 

4.3.1 Dealing with Homoforms and Multi-word Units 
Two critical obstacles that need to be addressed in frequency list research is the handling of homoforms (e.g., river bank vs. investment bank vs. bank as a verb) and multi-word units (e.g., I didn’t care for the movie at all vs. The product is carried at all locations.) (Cobb 2013). These phenomena pose challenges at both the theoretical level (e.g., agreeing on a consistent operational defnition for multi-word units) and the practical level (identifcation of these phenomena and incorporating them into frequency lists) (Nation 2016). 
Identifcation of these units is the frst complex hurdle. Take for example a word from Nation’s frst BNC 1000 list, course. Among its many meanings are (1) a certain path or direction (e.g., a circuitous course); (2) an academic class or program (e.g., a biology course); (3) a part of a meal (e.g., frst course, second course ...); 
(4) to move (e.g., blood coursing through the veins). As Gardner (2007) and Gardner and Davies (2014) note, using the lemma can do some of the work in differentiating meaning senses. For example, meaning 4 of course would be differentiated with a POS tagger. However, this still leaves at least three meanings (1–3) grouped together as course (n.). Yet another challenge is differentiating multi-word units from the same sequences of words not acting as a lexical unit, as discussed above. Differentiation in both instances requires either manual sorting or, to accomplish this task automatically, complex algorithms able to analyse the lexical and grammatical context. 
Such differentiation would no doubt have some effect on frequency lists. Martinez and Schmitt (2012), for example, found 505 multi-word units in the most frequent 5000 BNC words. In their analysis, they determined that, for example, while the word course currently exists in the frst BNC 1000 list, it appears in the BNC more frequently as part of the MWU of course rather than as the single word course. Incorporating this information would put the MWU of course on the BNC 1000 list and shift the single word course to the BNC 3000. Several options have been proposed for incorporating MWUs along with single-word items into frequency lists. O’Donnell (2011), for example, proposes possible methods for adjusting frequencies to account for words’ occurrence in frequently occurring n-grams of various sizes so that this information can be used in frequency list construction. 
The task of identifying and sorting homoforms and multi-word units in corpora and incorporating them into frequency lists is clearly a signifcant one, and this is only part of the challenge. If we then want to use these lists to profle lexis in other texts or corpora, the same procedure is necessary for each new text. Such disambiguation remains both a theoretical and practical challenge for corpus-based frequency list research. 

4.3.2 Application of Dispersion (and other) Statistics 
As discussed in Sect. 4.2.3.2, contemporary operationalizations of lexical item salience typically include some measure of dispersion. The most frequently used measure in contemporary frequency list research is Juilland’s D (e.g. Davies and Gardner 2010; Gardner and Davies 2014; Leech et al. 2001; Lei and Liu 2016). Gries (2008), however, details limitations of Juilland’s D and other “parts-based” measures of dispersion (p. 410). A key issue is that these measures tend to require that parts be of equal size, which Gries notes may lead to less “meaningful” divisions (rather than at, for example, the text level) for corpus parts.2 Gries also notes that these measures may lack the desired sensitivity to be able to distinguish between items with even very different distributional profles (see Chap. 5 for more information). 
Further investigating the limitations raised by Gries (2008), Biber et al. (2016) conducted a number of experiments to determine the sensitivity of Juilland’s D for words with a range of distributional profles in the BNC when the number of corpus parts used in calculation of this statistic were manipulated. What they found was that dispersion scores increased (indicating more even distribution) as the number of parts increased, which would in effect allow even items with highly skewed dispersion to meet selection criteria. They conclude that Juilland’s D is “not a reliable measure of lexical dispersion in a large corpus that has been divided into numerous corpus-parts” (p. 442). However, they were able to demonstrate that an alternative measure of dispersion, Gries’ (2008) Deviation of Proportions (DP)is more functional and reliable. Their experiments confrmed that, apart from allowing calculation using unequal corpus parts, DP does not suffer from the tendency of Juilland’s D to identify highly skewed items as being evenly dispersed. This allows for both the use of natural divisions of parts (e.g., at the text level) and fner grained sampling (see Representative Study 1 in Chap. 5 for more info). 
Biber et al. (2016) conclude their study with an extremely important point: While the increasing application of more robust statistics to corpus-based research should be encouraged, a great deal more research is needed to better understand the benefts and limitations of different statistics for different research questions (cf. Gries 2010 and Chap. 5). 
4.3.3 Addressing Reliability in the Validation of Frequency Lists 
Over the past two decades—and as exemplifed in the two representative studies in this chapter—it would appear that the greatest efforts in designing and validating frequency lists have gone into three areas: corpus design, item selection criteria, and—in the case of word lists—coverage-based demonstrations of list robustness. Corpora are now often much larger and better balanced and, as a result, perhaps more representative than ever before. The application of additional distributional statistics allows for better targeting of items with desired distributions (e.g., Gardner and Davies 2014). Lexical frequency lists are providing ever higher coverage of target texts or achieving such coverage with fewer words (e.g., Brezina and Gablasova 2015). 
2Gries (2008) and Biber et al. (2016) note that there are adjustments that can be made to account for different size parts, but they are rarely (if ever) employed in frequency list research. 
In the midst of these important developments, one issue that deserves more attention is frequency list generalizability. That is, to what degree are corpus-based frequency lists generalizable to target discourse domains? 
At present, evidence of frequency list generalizability tends to come in one or two forms, both of which are indirect. The frst comprises primarily corpus-external evidence, focused on corpus design (i.e., Do samples represent the diversity of text types, topics, etc. in a proportion refecting the target language use domain?). Biber (1993) refers to these critical considerations as situational evidence of corpus representativeness (see also Chap. 1). When it appears that corpus design represents the situational parameters of the target discourse domain, this is often taken to suggest that frequency lists generated from them should be generalizable. 
In word list research, a second form of evidence of generalizability is also typically employed: post-hoc assessment of a list’s coverage in other, similarly purposed corpora. As discussed above, Gardner and Davies (2014) assessed the coverage provided by their Academic Vocabulary List (AVL) in the corpus from which it was extracted as well as in an academic subcorpus from the BNC, and found it to be almost identical in both corpora. While coverage is an unquestionably important factor in assessing the value of a list, it can only serve as an indirect measure of reliability. 
Very rarely do studies include a direct assessment of list generalizability – of the extent to which items on a frequency list produced from one corpus overlap with items on a list extracted from a similarly purposed corpus made up of different texts. Manuals on lexical frequency list research, however, do recommend such comparisons. Nation and Webb (2010), for example, suggest that list designers “cross-check the resulting list on another corpus or against another list to see if there are any notable omissions or unusual inclusions or placements” (p. 135). Nation (2006), for example, showed that the set of words in each frequency band provided greater coverage than the set of words in each subsequent frequency bands in both the BNC and a comparison corpus. While this analysis provides important evidence regarding the proper ordering of each frequency band, Nation acknowledges that “this approach does not show that each word family member is in the right list” (2006, p. 64). In other words, while BNC 1 K did in fact provide more coverage than the BNC 2 K in both the BNC and the comparison corpus, there may be words in the BNC 2 K—or even in the BNC 3 K, 4 K, etc. —which provide higher coverage in the comparison corpus than do certain words in the BNC 1 K. 
For this particular reason, Nation (2016) recommends checking lists “against competing lists not just for coverage but also for overlapping and non-overlapping words” (p. 132), as was done in Brezina and Gablasova’s (2015) study (cf. Representative Study 2). In this analysis, they found 78–84% overlap in pairwise comparisons between their four lists, and 71% overlap among all four. While this level of overlap may or may not be surprising, it is important to consider that lists of high frequency general vocabulary, as was the focus here, would likely have the greatest stability across different corpora. The generalizability of lists of lower frequency features, e.g., specialized vocabulary, MWUs, etc., may be more limited without certain methodological adjustments (e.g., to corpus design, selection criteria, etc.). 
More research is needed to better understand the effects of different method­ologies employed on the ability to capture reliable, generalizable frequency lists. Current practice has been to design new lists by adopting previously used method­ologies or to update lists by manipulating several variables (e.g., compiling larger, more principally balanced corpora; changing the unit of measurement; employing different measures of dispersion). But is it unclear whether newly employed methodologies are leading to (more) reliable lists, and, if so, which (combination of) methodological adjustments account for seeming improvements. Future research should continue to investigate how different variables such as frequency profles of target features, selection criteria used, or corpus design may affect the reliability of lists. What combination of selection criteria ensure the greatest list reliability (Park 2015)? What size and composition of corpus is required to capture a reliable, generalizable frequency list (Brysbaert and New 2009; Miller and Biber 2015)? (How) does this requirement change for different features (e.g., lemmas, word families, MWUs) or discourse domains (Biber 1990, 1993)? In what ways might statistical techniques such as bootstrapping (cf. Chap. 24, this volume, esp. Sect. 
24.2.2.5) be applied to help researchers better understand the extent to which their corpora represent the distributions of target features in target discourse domains? Answers to these questions would go a long way towards helping researchers maximize efforts in designing reliable, generalizable frequency lists. 

4.4 Tools and Resources 
This section highlights three tools that are highly useful for lexical frequency profling and the construction of frequency lists, including single words and n-grams. 
AntConc Anthony, L. 2014. AntConc (Version 3.4.3) [Computer Software]. Tokyo, Japan: 
Waseda University. http://www.laurenceanthony.net/. Accessed 7 June 2019. 
AntConc is very easy-to-use yet powerful freeware which allows users to quickly construct frequency lists of word forms, lemmas, or n-grams based on their own uploaded texts. A nice feature is that AntConc allows users to build frequency lists based on lemmas, by using lists of lemmas provided in the software for English, French, or Spanish or by uploading their own. 
AntWordProfler 
Anthony, L. 2014. AntWordProfler (Version 1.4.1) [Computer Software]. Tokyo, 
Japan: Waseda University. http://www.laurenceanthony.net/. Accessed 7 June 
2019. 
AntWordProfler is an update of Heatley and Nation’s (1994) Range program which, as the original name implies, profles not only the frequency but also the range of 
each word across user-uploaded fles. Results can easily be exported into an Excel fle, allowing for further calculations of dispersion statistics. 
#LancsBox: Lancaster University Corpus Toolbox Brezina, V., McEnery, T., & Wattam, S. 2015. Collocations in context: A 
new perspective on collocation networks. International Journal of Corpus 
Linguistics, 20(2), 139–173. http://corpora.lancs.ac.uk/lancsbox/. Accessed 7 
June 2019. 
Another example of useful freeware for corpus analysis, LancsBox includes the functionalities of the Ant-ware tools described and several more. Users can upload their own texts or use any of the half dozen corpora already embedded into the software. Also embedded into the program is the TreeTagger, allowing for part-of­speech tagging and lemmatization of more than a dozen languages. LancsBox also includes the ability to calculate several measures of dispersion. Yet another useful feature is the option to graphically visualize dispersion across corpora via interactive illustrations. 
Further Reading 
Nation, I.S.P. 2016. Making and using word lists for language learning and testing. John Benjamins, Amsterdam. 
This text provides a comprehensive, practical, and very accessible discussion of important issues in frequency list design and evaluation. Nation provides useful recommendations for researchers through all steps in frequency list development, from designing (or choosing) a corpus to choosing an appropriate unit of analysis (including dealing with homoforms and multi-word units), to determining criteria for word selection and ordering. He also provides a helpful list of questions that can guide the analysis of pedagogically oriented frequency lists and walks readers through specifc examples of evaluations via refections on the merits and shortcomings of the BNC lists he designed. 
Baayen, H. 2001. Word frequency distributions. Kluwer Academic, New York. 
Whereas Nation’s (2016) text is geared more toward language teaching and learning specialists, Baayen’s book provides a much more challenging, theoretical discussion of properties and analysis of word frequency distributions. It provides a sophis­ticated discussion of statistical analyses of these distributions, particularly with regard to rare words, “Large Numbers of Rare Events” (LNRE), which comprise a considerable proportion of natural language. While this book aims to make word frequency-related statistical techniques “more accessible for non-specialists” (p. xxi), it does require that readers have a sound background in probability theory. 
References 
Adolphs, S., & Schmitt, N. (2003). Lexical coverage of spoken discourse. Applied Linguistics, 24(4), 425–438. 
Atkins, B. T. S., & Rundell, M. (2008). The Oxford guide to practical lexicography.New York: Oxford University Press. 
Baker, P. (2011). Times may change but we’ll always have money: A corpus driven examination of vocabulary change in four diachronic corpora. Journal of English Linguistics, 39, 65–88. 
Batista, R., & Horst, M. (2016). A new receptive vocabulary size test for French. The Canadian Modern Language Review, 72(2), 211–233. 
Bauer, L., & Nation, I. S. P. (1993). Word families. International Journal of Lexicography, 6, 253– 279. 
Biber, D. (1990). Methodological issues regarding corpus-based analyses of linguistic variation. Literary and Linguistic Computing, 5, 257–269. 
Biber, D. (1993). Representativeness in corpus design. Literary & Linguistic Computing, 8, 243– 257. 
Biber, D., & Barbieri, F. (2007). Lexical bundles in university spoken and written registers. English for Specifc Purposes, 26, 263–286. 
Biber, D., Conrad, S., & Cortes, V. (2004). If you look at ...: lexical bundles in university teaching and textbooks. Applied Linguistics, 25, 371–405. 
Biber, D., Johansson, S., Leech, G., Conrad, S., & Finegan, E. (1999). The Longman grammar of spoken and written English. London: Longman. 
Biber, D., & Reppen, R. (2002). What does frequency have to do with teaching grammar? Studies in Second Language Acquisition, 24(2), 199–208. 
Biber, D., Reppen, R., Schnur, E., & Ghanem, R. (2016). On the (non)utility of Juilland’s D to measure lexical dispersion in large corpora. International Journal of Corpus Linguistics, 21(4), 439–464. 
Brezina, V., & Gablasova, D. (2015). Is there a core general vocabulary? Introducing the new general service list. Applied Linguistics, 36(1), 1–22. 
Brown, D. W. (2017). Clinton-Trump corpus. http://www.thegrammarlab.com. Accessed 7 June 2019. 
Brysbaert, M., & New, B. (2009). Moving beyond Kucera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English. Behavior Research Methods, 41(4), 977–990. 
Cheng, W., Greaves, C., & Warren, M. (2006). From n-gram to skipgram to concgram. Interna­tional Journal of Corpus Linguistics, 11(4), 411–433. 
Cobb, T. (2013). Frequency 2.0: Incorporating homoforms and multiword units in pedagogical frequency lists. In L2 vocabulary acquisition, knowledge and use: New perspectives on assessment and corpus analysis. Eurosla Monographs Series 2. 
Conklin, K., & Schmitt, N. (2008). Formulaic sequences: Are they processed more quickly than nonformulaic language by native and nonnative speakers? Applied Linguistics, 29(1), 72–89. 
Conrad, D., & Biber, D. (2009). Real grammar: A corpus-based approach to English.New York: Pearson. 
Coxhead, A. (2000). A new academic word list. TESOL Quarterly, 34(2), 213–238. 
Davies, M., & Gardner, D. (2010). A frequency dictionary of contemporary American English. New York: Routledge. 
Doolan, S., & Miller, D. (2012). Generation 1.5 written error patterns: A comparative study. Journal of Second Language Writing, 21, 1–22. 
Durrant, P., & Doherty, A. (2010). Are high-frequency collocations psychologically real? Inves­tigating the thesis of collocational priming. Corpus Linguistics and Linguistic Theory, 6(2), 125–155. 
Erman, B., & Warren, B. (2000). The idiom principle and the open choice principle. Text, 20(1), 29–62. 
Francis, W., & Kucera, H. (1982). Frequency analysis of English usage: Lexicon and grammar. Boston: Houghton Miffin. 
Fletcher, W. H. (2011). Phrases in English. http://phrasesinenglish.org. Accessed 13 June 2019. 
Gardner, D. (2007). Validating the construct of word in applied corpus-based vocabulary research: A critical survey. Applied Linguistics, 28(2), 241–265. 
Gardner, D., & Davies, M. (2014). A new academic vocabulary list. Applied Linguistics, 35(3), 305–327. 
Grabowski, L. (2015). Keywords and lexical bundles within English pharmaceutical discourse: A corpus-driven description. English for Specifc Purposes, 38(2), 23–33. 
Gray, B., & Biber, D. (2013). Lexical frames in academic prose and conversation. International Journal of Corpus Linguistics, 18(1), 109–136. 
Gries, S. (2008). Dispersions and adjusted frequencies in corpora. International Journal of Corpus Linguistics, 13(4), 403–437. 
Gries, S. (2010). Dispersions and adjusted frequencies in corpora: Further explorations. In S. Gries, 
S. Wulff, & M. Davies (Eds.), Corpus linguistic applications: Current studies, new directions (pp. 197–212). Amsterdam: Rodopi. Heatley, A., & Nation, I. S. P. (1994). Range. [Computer Software]. Victoria University of Wellington, NZ. http://www.vuw.ac.nz/lals/. Accessed 7 June 2019. 
Houvardas, J., & Stamatatos, E. (2006). N-gram feature selection for authorship identifcation. In J. Euzenat & J. Domingue (Eds.), Artifcial intelligence: Methodology, systems, and applications (AIMSA 2006: Lecture notes in computer science) (Vol. 4183, pp. 77–86). Berlin: Springer. 
Hsu, W. (2011). A business word list for prospective EFL business postgraduates. Asian ESP Journal, 7(4), 63–99. 
Ikeo, R. (2016). An analysis of viewpoints by the use of frequent multi-word sequences in DH Lawrence’s Lady Chatterley’s Lover. Language and Literature, 25(2), 159–184. 
Kim, Y. (2009). Korean lexical bundles in conversation and academic texts. Corpora, 4, 135–165. 
Laufer, B., & Nation, I. S. P. (1995). Vocabulary size and use: Lexical richness in L2 written production. Applied Linguistics, 16(3), 307–322. 
Laufer, B., & Ravenhorst-Kalovski, G. (2010). Lexical threshold revisited: Lexical text coverage, learners’ vocabulary size and reading comprehension. Reading in a Foreign Language, 22, 15– 30. 
Leech, G., Rayson, P., & Wilson, A. (2001). Word frequencies in written and spoken English: Based on the British National Corpus. London: Longman. 
Lei, L., & Liu, D. (2016). A new medical academic word list: A corpus-based study with enhanced methodology. Journal of English for Academic Purposes, 22, 42–53. 
Liu, C., & Sloane, Z. (2006). Developing a core vocabulary for a Mandarin Chinese AAC system using word frequency data. International Journal of Computer Processing of Oriental Languages, 19(4), 285–300. 
Martinez, R., & Schmitt, N. (2012). A phrasal expressions list. Applied Linguistics, 33(3), 299– 320. 
Miller, D., & Biber, D. (2015). Evaluating reliability in quantitative vocabulary studies: The infuence of corpus design and composition. International Journal of Corpus Linguistics, 20(1), 30–54. 
Nation, I. S. P. (2004). A study of the most frequent word families in the British National Corpus. In P. Bogaards & B. Laufer (Eds.), Vocabulary in a second language (pp. 3–14). Amsterdam: John Benjamins. 
Nation, I. S. P. (2006). How large a vocabulary is needed for reading and listening? The Canadian Modern Language Review, 63(1), 59–82. 
Nation, I. S. P. (2016). Making and using word lists for language learning and testing. Amsterdam: John Benjamins. 
Nation, I. S. P., & Webb, S. (2010). Researching and analyzing vocabulary. Boston: Heinle. 
O’Donnell, M. B. (2011). The adjusted frequency list: A method to produce cluster-sensitive 
frequency lists. ICAME Journal, 35, 135–169. 
Park, S. (2015). Methodology for a reliable academic vocabulary list. Unpublished doctoral dissertation, Northern Arizona University, Flagstaff, Arizona. 
Paquot, M. (2007). Towards a productively oriented academic word list. In J. Walinski, K. Kredens, & S. Gozdz-Roszkowski (Eds.), Practical applications in language and computers 2005 (pp. 127–140). Frankfurt: Peter Lang. 
Paquot, M. (2010). Academic vocabulary in learner writing: From extraction to analysis.New York: Continuum. 
Rer, U. (2010). Establishing the phraseological profle of a text type: The construction of meaning in academic book reviews. English Text Construction, 3(1), 95–119. 
Savick P., & Hlavá.cová, J. (2002). Measures of word commonness. Journal of Quantitative Linguistics, 9(3), 215–231. 
Scott, M., & Tribble, C. (2006). Textual patterns: Keyword and corpus analysis in language education. Amsterdam: John Benjamins. 
Shin, D., & Nation, I. S. P. (2008). Beyond single words: The most frequent collocations in spoken English. ELT Journal, 62(4), 339–348. 
Simpson-Vlach, R., & Ellis, N. (2010). An academic formulas list: New methods in phraseology research. Applied Linguistics, 31(4), 487–512. 
Ward, J. (2007). Collocation and technicality in EAP engineering. Journal of English for Academic Purposes, 6, 18–35. 
West, M. (1953). A general service list of English words. London: Longman. 
Weisser, M. (2016a). Profling agents and callers: A dual comparison across speaker roles and British versus American English. In L. Pickering, E. Friginal, & S. Staples (Eds.), Talking at work: Corpus-based explorations of workplace discourse (pp. 99–126). London: Palgrave Macmillan. 
Weisser, M. (2016b). Practical corpus linguistics: An introduction to corpus-based language analysis. Oxford: Wiley-Blackwell. 
Zipf, G. (1936). The psychobiology of language. London: Routledge. 
Zipf, G. (1949). Human behavior and the principle of least effort. New York: Addison-Wesley. 
Chapter 5 Analyzing Dispersion 
Stefan Th. Gries 
Abstract This chapter provides an overview of one of the most crucial but at the same time most underused basic statistical measures in corpus linguistics, dispersion, i.e. the degree to which occurrences of a word are distributed throughout a corpus evenly or unevenly/clumpily. I frst survey a range of dispersion measures, their characteristics, and how they are computed manually; also, I discuss how different kinds of measures are related to each other in terms of their statistical behavior. Then, I address and exemplify the kinds of purposes to which dispersion measures are put in (i) lexicographic work and in (ii) some psycholinguistic explorations. The chapter then discusses a variety of reasons why, and ways in which, dispersion measures should be used more in corpus-linguistic work, in particular to augment simple frequency information that might be misleading; I conclude by discussing future directions in which dispersion research can go both in terms of how the logic of dispersion measures extends from frequencies of occurrence to co-occurrence and, potentially, even key words and in terms of how dispersion measures can be validated in future research on cognitive and psycholinguistic as well as applied-linguistics applications. 
5.1 Introduction 
Imagine a corpus linguist looking at a frequency list of the Brown corpus, a corpus aiming to be representative of written American English of the 1960s that consists of 500 samples, or parts, of approximately 2000 words each. Imagine further that corpus linguist is looking at that list to identify verbs and adjectives within a certain frequency range – maybe because he needs to (i) create stimuli for a psycholinguistic experiment that control for word frequency, (ii) identify words from a certain 
S. Th. Gries (•) University of California Santa Barbara, Santa Barbara, CA, USA 
Justus Liebig University Giessen, Giessen, Germany e-mail: stgries@linguistics.ucsb.edu 
© Springer Nature Switzerland AG 2020 99 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_5 
frequency range to test learners’ vocabulary, or (iii) compile a vocabulary list for learners, or some other application. Imagine, fnally, the frequency range he is currently interested in is between 35 and 40 words per million words and, as he browses the frequency list for good words to use, he comes across an adjective and averb– enormous and staining – that he thinks he can use because they both occur 37 times in the Brown corpus (and are even equally long) so he notes them down for later use and goes on. 
This is not an uncommon scenario and yet it is extremely problematic because, while that corpus linguist has indeed found words with the same frequency, he has probably not even come close to do what he actually wanted to do. The frequency range of the words he was interested in – 35-40 – or the actual frequency of the two words discussed – 37 – may have been an operationalization for things that might have to do with how fast people can identify the word in a psycholinguistic experiment (as in a lexical decision task) or with how likely a learner would be to have encountered, and thus hopefully know, a word of that kind of rarity. However, chances are that this choice of words is highly problematic: While both words are equally long and equally frequent in one and the same corpus, they could hardly be more different with regard to the topic of this chapter, their dispersion, which prob­ably makes them useless for the above-mentioned hypothetical purposes, controlled experimentation, vocabulary testing, or vocabulary lists. This is because 
•the 
word enormous occurs 37 times in the corpus, namely once in 35 corpus parts and twice in 1 corpus part; 

•the 
word staining occurs 37 times in the corpus, namely 37 times in 1 corpus part. 


In other words, given its (relatively low) frequency, enormous is pretty much as evenly dispersed as a word with that frequency can possibly be while, given its identical frequency, staining is as unevenly dispersed as a word with that frequency can possibly be: enormous is characterized by even dispersion, staining is characterized by a most uneven dispersion, clumpiness, or, to use Church and Gale’s (1995) terms, high burstiness or bunchiness. In the following section, I will discuss fundamental aspects of the notion of dispersion, including some of the very few previous applications as well as a variety of dispersion measures that have been proposed in the past. 

5.2 Fundamentals 


5.2.1 An Overview of Measures of Dispersion 
Corpus linguistics is an inherently distributional discipline: Virtually all corpus-linguistic studies with at least the slightest bit of a quantitative angle involve the frequency or frequencies with which 
• 
an element x occurs in a corpus or in a part of a corpus representing a register or variety or something else, ... or 

• 
an element x occurs in close proximity (however defned) to an element y in a corpus (or in a part of a corpus). 


Also, any kind of more advanced corpus statistic – for instance, association measures (see Chap. 7) or key words statistics (see Chap. 6) is ultimately based on the observation of, and computations based upon, such frequencies. However, just like trying to summarize the distribution of any numeric variable using only a mean can be treacherous (especially when the numeric variable is not normally distributed), so is trying to summarize the overall ‘behavior’ (or the co-occurrence preferences or the keyness) of a word x on the basis of just its frequency/frequencies because, as exemplifed above, words with identical frequencies can exhibit very different distributional behaviors. 
On some level, this fact has been known for a long time. Baron et al. (2009) mention Fries & Traver’s assessment that Thorndike was the frst scholar to augment frequency statistics with range values, i.e. the numbers of corpus parts or documents in which words were attested at least one. However, this measure of range is rather crude: it does not take into consideration how large the corpus parts are in which occurrences of a word are attested, nor does its computation include how many occurrences of a word are in one corpus part – to have an effect on the range statistic, all that counts is a single instance. Therefore, during the 1970s, a variety of measures were developed to provide a better way to quantify the distribution of words across corpus parts; the best-known measures include Juilland’s D (Juilland and Chang-Rodriguez 1964, Juilland et al. 1970), Carroll’s D2 (Carroll 1970), and Rosengren’s S (Rosengren 1971). 
To discuss how these statistics and some other competing ones are computed, I am following the expository strategy of Gries (2008), who surveyed all known dispersion measures on the basis of a small fctitious corpus; ours here consists of the following fve parts: 
b a mni beu p b a s a t bewqn bc a g a bes t a b a gh a be aa t b a h aa be a x a t 
This ‘corpus’ has several characteristics that make it useful for the discussion of dispersion: (i) it is small so all computations can easily be checked manually, (ii) the sizes of the corpus parts are not identical, which is more realistic than if they were, and (iii) multiple corpus-linguistically relevant situations are built into the data: 
•the 
words b and e are equally frequent in each corpus part (two times and one time per corpus part respectively), which means that their dispersion measures should refect those even distributions; 

•the 
words i, q, and x are attested in one corpus part each: i in the frst corpus part 

(which has 9 elements), q in the second corpus part (which has 10 elements), and x in the third corpus part (which has 11 elements), which means these words are extremely clumpily distributed, but slightly differently so (because the corpus parts they are in differ in size); 

•the 
word a, whose dispersion we will explore below and which is highlighted in bold, is attested in each corpus part, but with different frequencies. 


To compute the measures of dispersion to be discussed here, a few defnitions are in order; we will focus on the word a: 
(1) 
l = 50 (the length of the corpus in words) 

(2) 
n = 5 (the length of the corpus in parts) 

(3) 
s = (0.18, 0.2, 0.2, 0.2, 0.22) (the percentages of the n corpus part sizes) 

(4) 
f = 15 (the overall frequency of a in the corpus) 

(5) 
v = (1, 2, 3, 4, 5) (the frequencies of a in each corpus part 1-n) 

(6) 
p = (1/9, 2/10, 3/10, 4/10, 5/11) (the percentages a makes up of each corpus part 1-n) 


The most important dispersion measures – because of their historical value and evaluation studies discussed below – are computed as discussed in what follows; see Gries (2008) for a more comprehensive overview. The simplest measure is the range, i.e. the number of corpus parts in which the element in question, here a,is attested, which is computed as in (7): 
(7) range: number of parts containing a = 5 
Then, there are two traditional descriptive statistics, the standard deviation of the frequencies of the element in question in all corpus parts (sd, see (8)). This measure requires to take every value in v, subtract from it the mean of v (f /n, i.e. 3), square those differences, and sum them up; then one divides that sum by the number of corpus parts n and takes the square root of that quotient: 
 
2
 
n vi - f 
i=1 n
(8) sdpopulation: ˜ 1.414 (sdsample has n-1 in the denominator) 
n 
A maybe more useful variant of this measure is its ‘normalized version, the variation coeffcient (vc, see (9)); the normalization consists of dividing sdpopulation by the mean frequency of the element in the corpus parts f /n: 
sdpopulation(v) 
(9) vcpopulation: mean(v) ˜ 0.471 (vcsample would use sdsample) 
The version of Juilland’s D that can handle differently large corpus parts is then computed as shown in (10). In order to accommodate the different sizes of the corpus parts, however, the variation coeffcient is not computed using the observed frequencies v1-n (i.e. 1, 2, 3, 4, 5 in fles 1 to 5 respectively, see (5) above) but using the percentages in p1-n (i.e. how much of each corpus part is made up by the element in question, i.e. 1/9, 2/10, 3/10, 4/10, 5/11, see (6) above), which is what corrects for differently large corpus parts: 
1
(10) Juilland’s D:1 - sdpopulation(p) ×v ˜ 0.785
mean(p) (n-1) 
Carroll’s D2 is essentially a normalized version of entropy of the proportions of the element in each corpus part, as shown in (11) (see also Gries 2013: Sect. 3.1.3.1 for general applications of this measure). The numerator computes the entropy of the percentages in p1-n while dividing it by log2 n amounts to normalizing it against the maximally possible entropy given the number of corpus parts n. 
 
 
n pi pi
-×log2
i=1 pp
(11) Carroll’s D2:  ˜ 0.938
log2n 
The version of Rosengren’s S that can handle differently large corpus parts is shown in (12). Each corpus part size’s in percent (in s) is multiplied with the frequencies of the element in question in each corpus part (in v1-n); of each product, one takes the square root, and those are summed up, that sum is squared, and divided by the overall frequency of the element in question in the corpus (f ): 
 v 2
(12) Rosengren’s (1971) Sadj: n si · vi × 1 ˜ 0.95 (with min S=1n) 
i=1 f 
Finally, Gries (2008, 2010) and the follow-up by Lijffjt and Gries (2012) proposed a measure called DP (for deviation of proportions), which falls between 1-min s (for an extremely even distribution) and 1 (for an extremely clumpy distribution) as well as a normalized version of DP, DPnorm, which falls between 0 and 1, which are computed as shown in (13). For DP, one computes the differences between how much of the element in question is in each corpus fle in percent on the one hand and the sizes of the corpus parts in percent on the other – i.e. the differences between observed and expected percentages. Then, one adds up the absolute values of those and multiplies by 0.5; the normalization then consists of dividing this values by the theoretically maximum value of DP given the number of corpus parts (in a way reminiscent of (11)1: 
 
 
n 
vi DP
(13) DP:0.5 ×= 0.18 and ˜ 0.22
i=1  f - si  DPnorm:1-mins 
The fnal measure to be discussed here is one that, as far as I can tell, has never been proposed as a measure of dispersion, but seems to me to be ideally suited to be one, namely the Kullback-Leibler (or KL-) divergence, a non-symmetric measure that quantifes how different one probability distribution (e.g., the distribution of all the occurrences of a across all corpus parts, i.e. v/f ) is from another (e.g., the 
1As pointed out by Burch et al. (2017), DPnorm is equivalent to a measure called ADA (for average deviation analog) proposed by Wilcox (1973). 
Table 5.1 Dispersion measures for several ‘words’ in the above ‘corpus’ 
b  i  q  x  
Range  5  1  1  1  
Sd/vc  0/0  0.4/2  0.4/2  0.4/2  
Juilland’s D  0.968  0  0  0  
Carroll’s D2  0.999  0  0  0  
Rosengren’s S  0.999  0.18  0.2  0.22  
DP/DPnorm  0.02/0.024  0.82/1  0.8/0.976  0.78/0.951  
KL-divergence  0.003  2.474  2.322  2.184  

corpus part sizes s); the KL-divergence is computed as shown in (14) (with log2sof 0 defned as 0): 
 
 
n vi vi
(14) KL-divergence: ×log2 × 1 ˜0.137 with log20 :=0
i=1 f fsi 
Table 5.1 shows the corresponding results for several elements in the above ‘corpus’. The results show that, for instance, b is really distributed extremely evenly (since it occurs twice in each fle and all fles are nearly equally large). Note in particular how the values of Rosengren’s S, DP, and the KL-divergence for i, q, and x differ: all three occur only once in the corpus, only in one corpus part, but what differs is the size of the corpus part, and the larger the corpus part in which the single instance of i, q,or x is attested, the more even/expected that distribution is. 
In sum, corpus linguists have proposed quite a few different measures of dispersion, most of which are generally correlated with each other, but that also react differently to the kinds of distributions one fnds in corpus data, specifcally, 
• 
the (potentially large) number of corpus parts in which an element is not attested; 

• 
the (potentially large) number of corpus parts in which an element is attested much less often than the mean; 

• 
the range of distributions a corpus linguist would consider to be different but that would yield the same dispersion measure(s); 

• 
the number of different corpus parts a corpus linguist would assume and their (even or uneven sizes).2 


2Some dispersion measures do not require a division of the corpus into parts and/or also involve the differences between successive mentions in a corpus parts. These are theoretically interesting alternatives, but there seems to be virtually no research on them; see Gries (2008, 2010) for some review and discussion as well as the Further reading section for a brief presentation of one such study, Savick& Hlavá.cová (2002). 

5.2.2 Areas of Application and Validation 
There are at least a few areas where dispersion information is now considered at least occasionally, though much too infrequently. The area of research/application where dispersion has gained most ground is that of corpus-based dictionaries and vocabulary lists. Leech et al. (2001) discuss dispersion information of words in the British National Corpus (BNC) and remark that, as in the enormous/staining example above, for instance, the words HIV, lively, and keeper are approximately equally frequent in the corpus, but are very differently dispersed in the corpus and proceed to use Juilland’s D as their measure of choice. Similarly, Davies and Gardner (2010) and Gardner and Davies (2014) also use Juilland’s D in their frequency dictionary and academic vocabulary list, as does Paquot (2010) for her academic keyword list. 
It is worth pointing out in this connection that, especially in this domain of dictionaries/vocabulary lists, researchers have often also computed what is called an adjusted frequency, i.e. a frequency that is adjusted downwards depending on the clumpiness/unevenness of the distribution. In the mathematically simplest case, the adjusted frequency is the observed frequency of the word in the corpus times the dispersion value; for instance, Juilland’s usage coeffcient U is just that: the frequency of the word in the corpus f times Juilland’s D, a measure that, for instance, Davies and Gardner (2010) use. In the above case for the word a, U = 15 × 0.785 = 11.777 whereas for q, U = 1 × 0 = 0; similar adjusted frequencies exist for Carroll’s D2 (the so-called Carroll’s Um) and Rosengren’s S (the so-called Rosengren’s AF). 
Another area where dispersion information has at least occasionally been recognized as important is psycholinguistics, in particular the domain of lexical decision tasks. Consider, for instance, Schmid’s (2010:115) concise summary: “frequency is one major determinant of the ease and speed of lexical access and retrieval, alongside recency of mention in discourse.” And yes, for many decades now, (logged) frequency of occurrence has been known to correlate with reaction times to word/non-word stimuli. However, compared to frequency, the other major determinant, recency, has been considered much less in cognitive and psycholinguistic work. This is somewhat unexpected because there are general arguments that support the importance of dispersion as a cognitively relevant notion, as the following quote demonstrates: 
Given a certain number of exposures to a stimulus, or a certain amount of training, learning is always better when exposures or training trials are distributed over several sessions than when they are massed into one session. This fnding is extremely robust in many domains of human cognition. (Ambridge et al. 2006:175) 
Ambridge et al. do not mention dispersion directly, but what would be its direct corpus-linguistic operationalization. Similarly, Adelman et al. (2006:814) make the valid point that “the extent to which the number of repeated exposures to a particular item affects that item’s later retrieval depends on the separation of the exposures in time and context,” and of course the corpus-linguistic equivalent to this “separation of the exposures in time and context” is dispersion. 
More empirically, there are some studies providing supporting evidence for the role of dispersion when it comes to lexical decision tasks. One such study is in fact Adelman et al. (2006), who study dispersion. Their study has a variety of general problems: 
• 
they only use the crudest measure of dispersion possible (range) and do not relate to previous more psychological/psycholinguistic work that also studied the role of range (such as Ellis 2002a, b); 

• 
they do not establish any relation to the notion of dispersion in corpus linguistics and, somewhat worse even, refer to range with the misleading label contextual diversity, when in fact the use of a word in different corpus parts by no means implies that the actual contexts of the word are different: No matter in how many different corpus parts hermetically is used, it will probably nearly always be followed by sealed. 


Nonetheless, they do show that dispersion is a better and more unique predictor of word naming and lexical decision times than token frequency and they, like Ellis (2011), draw an explicit connection to Anderson’s rational analysis of memory. More evidence for the importance of dispersion is offered by Baayen (2010), who includes range in the BNC as a predictor in a multifactorial model that ultimately suggests that the effect of frequency when considered a mere repetition-counter as opposed to some other cognitive mechanism is in fact epiphenomenal and can partly be explained by dispersion, and Gries (2010), who shows that lexical decision times from Baayen (2008) are most highly correlated with vc and DP/DPnorm (see Box 2 for details). 
In spite of all the effort that has apparently gone into developing measures of dispersion and in spite of uneven dispersion posing a serious threat to the validity of virtually all corpus-based statistics, it is probably fair to say that dispersion is still far from being routinely included in both (more) theoretical research and (more) practical applications. One early attempt to study the behavior of these different measures is Lyne (1985), who compared D, D2, and S to each other using 30 words from the French Business Correspondence Corpus, which for that application was divided into 5 equally large parts; on the basis of testing all possible ways in which 10 words can be distributed over 5 corpus parts, Lyne concludes that Juilland’s D performs best; see also Lyne (1986), but there is little research that includes dispersion on a par with frequency or other corpus statistics and even less work that attempts to elucidate which measures are best (for what purpose); two studies that begin to work on this important issue are summarily discussed below. 
Representative Study 1 
Biber D., Reppen, R., Schnur, E., and Ghanem, R. 2016. On the (non)utility of Juilland’s D to measure lexical dispersion in large corpora. International Journal of Corpus Linguistics 21(4): 439–464. 
Starting out from observations in Gries (2008), Biber et al. (2016) is one of the most comprehensive tests, if not the most comprehensive one, of how the perceived default of Juilland’s D behaves in particular with contemporary cor­pora that are large and have many different corpus parts, i.e. high values of n. 
They begin by discussing the mathematical characteristics of Juilland’s D, in particular the fact that the formula shown above in (10) increases “degrees of uniformity” (i.e. evenness of distribution/dispersion across corpus parts) “as the number of corpus parts is increased” (Biber et al. 2016:443); thus, the larger the corpora one considers, the more likely one uses a relatively large number of corpus parts (for reasons of statistical sampling), and the more Juilland’s D is reduced, which “infat[es] the estimate of uniformity, and overall, greatly reduc[es] the effective range of values for D” (p. 444). 
Biber et al. then proceed with two case studies. The frst one explores D-values of a set of words in the British National Corpus, which, for the purpose of testing what effect the numbers of corpus parts n one assumes, was divided into n = 10, 1000, and 1000 equal-sized parts; crucially, the words explored were words for which theoretical considerations would lead an analysis to expect fairly different D-values, contrasting words such as at, all,or time (which should be distributed fairly evenly) with words such as erm, ah, and urgh (which, given their preponderance in spoken data, should be distributed fairly unevenly). Specifcally, they analyzed 153 words in 10 categories emerging from crossing (i) several different word frequency bands and (ii) expected distribution (uniform, writing-skewed, and speech-skewed). 
In this frst case study, they fnd the expected high D-values for higher-frequency words that would be uniformly-distributed or skewed towards writing (i.e. the 90% majority of the BNC) regardless of n. However, they also discover that the D-values for lower-frequency writing-skewed words are quite sensitive to variations of n. Their concern that these results are not due to the larger sampling sizes refecting the dispersions more accurately is supported by what they fnd for the speech-skewed words, namely “extremely large discrepancies even for the most frequent speech-skewed words” (p. 450). More precisely, D-values for high-frequency speech-skewed words can vary between very high (e.g. 0.885 for yeah with n = 1000) and very low (e.g. 
(continued) 
0.286 for yeah with n = 10). Even more worryingly, “[t]hese discrepancies become even more dramatic as [they] consider moderate and lower-frequency words” (p. 452), with differences in D-values frequently exceeding 0.5 just because of varying n, which on a scale from 0 to 1 of course corresponds to what seems to be an unduly large effect. Their main conclusion of the frst case study is that “D values based on 1,000 corpus parts completely fail to discriminate among words with uniform versus skewed distributions in naturalistic data” (p. 454). 
In their second case study, Biber et al. created different data sets with, therefore, known distributions of target words across different numbers of corpus parts, but the bottom line of this more controlled case study is in fact the same as that of the frst. Their maybe most extreme, and thus worrying, result is that 
the exact same distribution of a target word – a uniform distribution across 10% of a corpus – can result in a D value of 0.0 when the computation is based on a corpus split into 10 parts, versus a D value of 0.905 when the computation is based on a corpus split into 1000 parts. (p. 457) 
As a more useful alternative, they propose to use Gries’s (2008) DP.They recommend DP because it is conceptually simple, can easily handle unequally large corpus parts, and “it seems to be a much more reliable estimate of dispersion (and uniformity) in large corpora divided into many corpus parts” 
(p. 459). In a direct comparison with Juilland’s D, they show that DP not only returns values from a more useful wider range of values when given a diverse set of differently dispersed words, but it also reacts differently to larger numbers of n:(1-DP) values are consistently lower for corpus divisions into many parts, which Biber et al. interpret as being desirably compatible with the expected benefts of the fner-grained sampling that comes with increasing n: 
Theoretically, we would expect more conservative estimates of dispersion based on a large number of corpus parts. For example, it is more likely that a word will occur in 6 out of 10 corpus parts than for that same word to occur in 600 out of 1000 corpus parts. The values for 1-DP seem to refect this fact, resulting in consistently lower values when computations are based on a large number of corpus parts. In summary, DP is clearly more effective than D at discriminating between uniform versus skewed distributions in a corpus, especially when it is computed based on a large number of corpus-parts. (Biber et al. 2016:460) 
Biber et al. conclude with a plea for more validation and triangulation when it comes to developing corpus-linguistic statistics and/or more general methods. 
Representative Study 2 
Gries, S.T. 2010. Dispersions and adjusted frequencies in corpora: fur­ther explorations. In Corpus linguistic applications: current studies, new directions, eds. Gries S.T., Wulff S., and Davies, M., 197–212. Rodopi, Amsterdam. 
The second representative study to be discussed here is concerned with dispersion and its role in psycholinguistic contexts. Gries (2010) is an attempt to provide at least a frst glimpse at how different dispersion measures are behaving statistically and predictively when studied in conjunction with psycholinguistic (reaction time) data. To that end, he conducted two kinds of case studies: First, he explored the degree to which the many existing measures capture similar kinds of dispersion information by exploring their intercorrelations; second, he computed the correlations between raw fre­quency, all dispersion measures, and all adjusted frequencies on the one hand and experimentally-obtained reaction time data from lexical decision tasks in psycholinguistics; in what follows, I briefy discuss these two case studies. 
As for the frst case study, he extracted all word types from the spoken com­ponent of the BNC that occur 10 or more times – there are approx. 17,500 such types – and computed all 29 dispersion measures and adjusted frequencies cataloged in the most recent overview article of Gries (2008). All measures were z-standardized (to make their different scales more comparable) and then used as input to both hierarchical agglomerative cluster analyses (see Chap. 
18) and principal component analyses (see Chap. 19) separately for dispersion measures and adjusted frequencies. For the former, he used 1-Pearson’s r (see Chap. 17) as a similarity measures and Ward’s method as an amalgamation rule. 
The results from both analyses revealed several relatively clear groupings of measures. For instance, the following clusters/components were well established in both the cluster and the principal components analysis: 
• 
Rosengren’s S, range, and a measure called Distributional Consistency (Zhang et al. 2004); 

• 
Juilland’s D, Carroll’s D2, and a measure called D3 based on chi-squared (Lyne 1985); and, more heterogeneously, 

• 
DP, DPnorm, vc, and idf (inverse document frequency, see Spärck Jones 1972 and Robertson 2004); 

• 
frequency, the maxmin measure (the difference between max(v1-n) and min(v1-n)), and sd. 


In fact, the principal components analysis revealed that just two principal components capture more than 75% of the variance in the 16 dispersion 
(continued) 
measures explored: many measures behave quite similarly and fall into several smaller groups. Nevertheless, the results also show that the groups of measures also sometimes behave quite dissimilarly: “different measures of dispersion will yield very different (ranges of) values when applied to actual data” (Gries 2010:204, his emphasis). 
With regard to the adjusted frequencies, the results are less diverse and, thus, more reassuring. All measures but one behave relatively similarly, which is mostly interesting because it suggests that (i) the differences between the adjusted frequencies are less likely to yield very different results, but also that 
(ii) the computationally very intensive distance-based measures that have been proposed (see in particular Savickand Hlavá.
cová 2002 as well as Washtell 2007) do not appear to lead to fundamentally different results; given that these measures computing time can be 10 times as long or much much longer for large corpora, this suggests that the simpler-to-compute ‘classics’ might do the job well enough. 
The second case study in this paper involves correlating dispersion mea­sures and adjusted frequencies with response time latencies from several psycholinguistic studies, specifcally with (i) data from young and old speak­ers from Spieler and Balota (1997) and Balota and Spieler (1998), and (ii) data from Baayen (2008). All dispersion measures and adjusted frequencies were centered and then correlated with these reactions times (using Kendall’s t , see Chap. 17). For the Balota/Spieler data, the results indicate that some measures score best (including, for instance, AF, U, and DP), but that most measures’ correlations with the reaction times are very similar. However, for the reaction times of Baayen (2008), a very different picture emerges: While DP scores very well, only surpassed by vc, there is a distinct cline such that some measures really exhibit only very low and/or insignifcant correlations with the psycholinguistic comparison data. 
Gries concludes with some recommendations: Many dispersion measures are relatively similar, but if one is uncertain what measure to trust, it would be useful to compute measures that his cluster/principal component analyses considered relatively different to get a better picture of the diversity in the data; at present and until more data have been studied, it seems as if the computationally more demanding measures may not be worth the effort. Trivially, more analyses (than Lyne’s really small study) are needed, in particular of larger data sets and, along the lines of what Biber et al. (2016) did, of data sets with known distributional characteristics. 
5.3 Critical Assessment and Future Directions 
The previous sections already touched upon some recommendations for future work. It has hopefully become clear that dispersion is as important an issue as it is still neglected or even completely ignored. While every corpus linguist with only the slightest bit of statistical knowledge knows to never present a mean or median without a measure of dispersion, the exact same advice is hardly ever heeded when it comes to frequencies and dispersions in corpus data: There are really only very few studies that report frequency data and dispersion or, just as importantly, report frequencies and association measures and dispersion, although Gries (2008) has shown that the computation of association measures is just as much at risk as frequencies when dispersion information is not also considered. Thus, the frst desideratum is that more research takes the threat of underdispersion/clumpiness much more seriously; strictly speaking, reviewers should always request dispersion information so that readers can more reliably infer what reported frequencies or association measures really represent or whether they represent what they purport to represent. 
Second, we need more studies of the type discussed in the representative studies boxes so that we better understand the different measures’ behavior in actual but also controlled/designed data. One issue, for instance, has to do with how corpora are divided into how many parts and how this affects dispersion measures (see for example Biber et al.’s 2016 discussion of the role of the denominator in Juilland’s D, which features the number of corpus parts). Another is how dispersion measures relate to issues outside of corpus linguistics such as, again, psycholinguistically­or cognitively-informed approaches. This is particularly relevant for measures that are advertised as having certain characteristics. To discuss just one example, Kromer (2003:179) promotes his adjusted frequency measure by pointing to its interdisciplinary/psycholinguistic utility/validity: 
From our point of view, all usage measures considered above have one common disadvan­
tage: their introduction and application are not based psycholinguistically. A usage measure, 
free from the disadvantage mentioned, is offered below. 
However, the advantage is just asserted, not demonstrated, and in Gries (2010) at least, the only study I am aware of testing Kromer’s measure, his measure scored worse than most others when explicitly compared to psycholinguistic reference data. While that does of course not mean Kromer’s measures has been debunked, it shows what is needed: more and explicit validation. 
That being said, a certain frequent trend in corpus linguistic research should be resisted and this is best explained with a very short excursus on association measures (see Chap. 7), where the issue at hand has been recognized earlier than it has in the little existing dispersion research. For several decades now, corpus linguists have discussed dozens of association measures that are used to rank-order, for instance, collocations by the attraction of their constituent words. Some of these measures are effect sizes in the sense that they do not change if the co-occurrence tables from which they are computed are increased by some factor (e.g., the odds ratio), others are based on signifcance tests, which means they confate both sample size/actual observed frequencies and effect size (e.g., the probably most widely-used measure, the log-likelihood ratio). 

This is relevant in the present context of dispersion measures because we are now facing a similar issue in dispersion research, namely when researchers and lexicographers also take two dimensions of information – frequency and the effect size of dispersion – and confate them into one value such as an adjusted frequency (e.g., by multiplication, see above Juilland’s U). To say it quite bluntly, this is a mistake because, frequency and dispersion are two different pieces of information, which means confating them into a single measure loses a lot of information. This is true even though frequency and dispersion are correlated, as is shown in Fig. 5.1 and Fig. 5.2. Both have word frequency on the x-axis (logged to the base of 10) and a dispersion measure (DP in Fig. 5.1, range in Fig. 5.2)onthe y-axis, and have words represented by grey points. Also, in both plots, the words have been divided into 10 frequency bins, for each of which a blue whisker and the numbers above and below it represent the range of the dispersion values in that frequency bin. For example, in Fig. 5.1, the 6th frequency bin from the left includes words with frequencies between 2036 and 5838 and DP values between 0.23 and 0.86, i.e. a DP-range of 

0.63 also noted in blue at the bottom of the scatterplot. 
Obviously, there are the expected correlations between frequency and dispersion (R2 = 0.832 for logged frequency and DP), but just as obviously, especially in the middle range of frequencies – ‘normal content words’ with frequencies between 1000 and 10,000 – words can have extremely similar frequencies but still extremely different dispersions. This means several things: First, even though there is the above-mentioned overall correlation between frequency and dispersion, this correlation can be very much weakened in certain frequency bins. For example, in the 6th frequency bin, R2 for the correlation between frequency and dispersion is merely 0.086. 
Second, a relatively ‘specialized’ word like council is in the same (6th) frequency bin (freq = 4386, DP = 0.72, range = 292 out of 905) as intuitively more ‘common/widespread’ words like nothing, try, and whether (freqs = 4159, 4199, 4490; DPs = 0.28, 0.28, 0.32; ranges = 652, 664, 671 out of 905); in both plots, the positions of council and nothing are indicated with the c and the n respectively plotted into the graph. 
Also, even just in the sixth frequency band, the extreme range values that are observed are 85/905 = 9.4% vs. 733/905 = 81% of the corpus fles, i.e. huge differences between words that in a less careful study that ignores dispersion would simply be considered ‘similar in frequency’. 
Finally, these graphs also show that forcing frequency and dispersion into one value, e.g. an adjusted frequency, would lose a huge amount of information. This is obvious from the visual scatter in both plots, but also just from simple math: If a researcher reports an adjusted frequency of 35 for a word, one does not know whether that word occurs 35 perfectly evenly distributed times in the corpus (i.e., frequency = 35 and, say, Juilland’s D = 1) or whether it occurs 350 very unevenly distributed times in the corpus (i.e., frequency = 350 and, say, Juilland’s D = 0.1). And while this example is of course hypothetical, it is not as unrealistic as one might think. For instance, the products of observed frequency and 1-DP for the two words pull and chairman in the spoken BNC are very similar – 375 and 368.41 respectively – but they result from very different frequencies and DP-values: 750 and 0.5 for pull but 1939 and 0.81 for chairman. Not only is it the dispersion value, not the frequency one, that refects our intuition (that pull is more basic/widely-used than chairman) much better, but this also shows that we would probably not want to treat those two cases as ‘the same’ as we would if we simply computed and reported some confated adjusted frequency. Thus, keeping frequency and dispersion separate allows researchers to preserve important information and it is therefore important that we do not give in to the temptation of ‘a single rank-ordering scale’ and simplify beyond necessity/merit – what is needed is more awareness and sophistication of how words are distributed in corpora, not blunting our research tools. 
In all fairness, even if one decides to keep the two dimensions separate, as one defnitely should, there still is an additional unresolved question, namely what kind of threshold value(s) to choose for (frequency and) dispersion. It is unfortunately not clear, for instance, what dispersion threshold to adopt to classify a word as ‘evenly dispersed enough for it to be included in a dictionary’: DP = 0.4/D = 0.8? DP = 0.45/D = 0.85? In the absence of more rigorous comparisons of dispersion measures to other kinds of reference data, at this point any cut-off point is arbitrary (see Oakes and Farrow 2007:92 for an explicit admission of this fact). Future research will hopefully both explore which dispersion measures are best suited for which purpose and how their relation to frequency is best captured. In order to facilitate this necessary line of research, an R function computing dispersion measures and adjusted frequencies is provided at the companion website of this chapter, see Sect. 5.4; hopefully, this will inspire more research on this fundamental distributional feature of linguistic elements and its impact on other corpus statistics such as association measures, key (key) words, and others. 

5.4 Tools and Resources 
Dispersion is a corpus statistic that has not been implemented widely into existing corpus tools and arguably it is in fact a statistic that, unlike others, is less obvious to implement, which is why all implementations of dispersion in such general-purpose tools probably leave something to be desired. This is for two main reasons. First, most tools offer only a very small number of measures, if any, and no ways to implement new ones or tweak existing ones. Second, most existing dispersion measures require a division of the corpus into parts and the decision of how to do this is not trivial. While ready-made corpus tools such as WordSmith Tools or AntConc might assume for the user that the corpus parts to be used are the n (a user-defned number) equally-sized parts a corpus can be divided into or the separate fles of the corpus, this may actually not be what is required for a certain study if, for instance, sub-divisions in fles are to be considered as well (as might be useful for some fles in the BNC) or when groupings of fles into (sub-)registers are what is of interest. 
To mention a few concrete examples, WordSmith Tools offers a dispersion plot as well as range and Juilland’s D-values (without explicitly stating that that is in fact the statistic that is provided) while AntConc offers a version of a dispersion plot separately for each fle of a corpus, which is often not what one needs. The COCA-associated website https://www.wordfrequency.info/ (accessed 22 May 2019) provides data that went into Davies and Gardner (2010), which means they provide Juilland’s D for the corpus when split up into 100 equally-sized parts. As is obvious, the range of features is extremely limited and virtually non-customizable. 
By far the best – in the sense of most versatile and powerful – approach to exploring issues of dispersion is with programming languages such as R or Python (see Chap. 9), because then the user is not dependent on measures and settings enshrined in ready-made software but can customize an analysis in exactly the way that is needed, develop their own methods, and/or run such analysis on data/annotation formats that none of the above tools can handle. This chapter comes with some companion code for readers to explore as well as an R function to compute a large number of dispersion measures for data provided by a user. This function is an update of the function provided in Gries (2008), which adds the KL-divergence as a dispersion measure, updates the computation of some measures, cleans up the code, and drastically speeds up all computations; see the companion website for how to use it. 
Further Reading 
Burch, B., Egbert, J., and Biber, D. 2017. Measuring and interpreting lexical 
dispersion in corpus linguistics. Journal of Research Design and Statistics in 
Linguistics and Communication Science 3(2):189–216. 
Burch et al. (2017) is a study that introduces another dispersion measure DA (or MDA in Wilcox’s 1973 terminology) and compares it to the historically most widely-used dispersion measure of Juilland’s D and to the recently-proposed measure of Gries’s DP. They defne DA and test its performance by, for instance, a simulation study of three different scenarios by creating randomly sampled corpora and comparing the three different dispersion statistics. Also, they correlate the dispersion statistics for 150 words taken from the British National Corpus using scatterplots and pairwise differences of dispersion statistics. It is worth pointing out, as the authors also do, that (i) this study is based on the overall probably less realistic scenario that all corpus parts are equally large, which is not that likely when corpus parts are considered to be fles (e.g., in the BNC) or (sub-)registers (e.g. in the ICE-GB) and that (ii) computing DA can take literally thousands more time than D or DP even though its non-linear correlation R2 with DP exceeds 0.99. That being said, their study is nonetheless a good example of exactly the kind of study we need more of to further our understanding of (i) how different dispersion measures react to corpus-linguistic data and (ii) how they react to certain kinds of potentially extreme input data. 
Savick P., and Hlavá.
cová, J. 2002. Measures of word commonness. Journal of Quantitative Linguistics 9(3):15–31. 
Savickand Hlavá.
cová (2002) is another interesting reading. Their study starts out from the question of how to identify “common” words to be included in a universal dictionary. However, they propose to approach dispersion in ways that do not require a division of a corpus in parts – rather, the corpus is treated as a single sequence or vector of words and then dispersion is used to compute corrected frequencies that are close to the actual observed frequencies when a word is very evenly distributed and (much) small when it is not. They propose three different corrected frequencies – one based on Average Reduced Frequency (fARF), one based on Average Waiting Time (fAWT ), and one based on Average Logarithmic Distance (fALD) – and proceed to apply them to data from the Czech National Corpus to test the measures’ stability (how much do they vary when applied to different parts of the overall corpus?) and to exemplify the kinds of words that the measures return as highly unevenly distributed. While these dispersion measures can take much longer to compute than the parts-based measures reported on above and adjusted frequencies are problematic for the reasons discussed above, this paper is nonetheless noteworthy and interesting for the novel, non-parts-based approach to dispersion. 
References 
Adelman, J. S., Brown, G. D. A., & Quesada, J. F. (2006). Contextual diversity, not word frequency, determines word-naming and lexical decision times. Psychological Science, 19(9), 814–823. Ambridge, B., Theakston, A. L., Lieven, E. V. M., & Tomasello, M. (2006). The distributed 
learning effect for children’s acquisition of an abstract syntactic construction. Cognitive 
Development, 21(2), 174–193. 
Baayen, R. H. (2008). Analyzing linguistic data: A practical introduction to R. Cambridge: Cambridge University Press. 
Baayen, R. H. (2010). Demythologizing the word frequency effect: A discriminative learning perspective. The Mental Lexicon, 5(3), 436–461. 
Balota, D. A., & Spieler, D. H. (1998). The utility of item level analyses in model evaluation: A response to Seidenberg and Plaut. Psychological Science, 9(3), 238–240. 
Baron, A., Rayson, P., & Archer, D. (2009). Word frequency and keyword statistics in historical corpus linguistics. Anglistik: International Journal of English Studies, 20(1), 41–67. 
Biber, D., Reppen, R., Schnur, E., & Ghanem, R. (2016). On the (non)utility of Juilland’s D to measure lexical dispersion in large corpora. International Journal of Corpus Linguistics, 21(4), 439–464. 
Burch, B., Egbert, J., & Biber, D. (2017). Measuring and interpreting lexical dispersion in corpus linguistics. Journal of Research Design and Statistics in Linguistics and Communication Science, 3(2), 189–216. 
Carroll, J. B. (1970). An alternative to Juilland’s usage coeffcient for lexical frequencies and a proposal for a standard frequency index. Computer Studies in the Humanities and Verbal Behaviour, 3(2), 61–65. 
Church, K. W., & Gale, W. A. (1995). Poisson mixtures. Journal of Natural Language Engineering, 1(2), 163–190. 
Davies, M., & Gardner, D. (2010). A frequency dictionary of contemporary American English: Word sketches, collocates and thematic lists. London/New York: Routledge, Taylor and Francis. 
Ellis, N. C. (2002a). Frequency effects in language processing and acquisition: A review with implications for theories of implicit and explicit language acquisition. Studies in Second Language Acquisition, 24(2), 143–188. 
Ellis, N. C. (2002b). Refections on frequency effects in language acquisition: A response to commentaries. Studies in Second Language Acquisition, 24(2), 297–339. 
Ellis, N. C. (2011). Language acquisition as rational contingency learning. Applied Linguistics, 27(1), 1–24. 
Gardner, D., & Davies, M. (2014). A new academic vocabulary list. Applied Linguistics, 35(3), 305–327. 
Gries, S. T. (2008). Dispersions and adjusted frequencies in corpora. International Journal of Corpus Linguistics, 13(4), 403–437. 
Gries, S. T. (2010). Dispersions and adjusted frequencies in corpora: Further explorations. In S. 
T. Gries, S. Wulff, & M. Davies (Eds.), Corpus linguistic applications: Current studies, new directions (pp. 197–212). Amsterdam: Rodopi. Gries, S. T. (2013). Statistics for linguistics with R (2nd rev. and ext. ed, 359). Berlin/Boston: De Gruyter Mouton. Juilland, A. G., & Chang-Rodriguez, E. (1964). Frequency dictionary of Spanish words.The Hague: Mouton de Gruyter. Juilland, A. G., Brodin, D. R., & Davidovitch, C. (1970). Frequency dictionary of French words. The Hague: Mouton de Gruyter. Kromer, V. (2003). An usage measure based on psychophysical relations. Journal of Quantitative Linguistics, 10(2), 177–186. Leech, G. N., Rayson, P., & Wilson, A. (2001). Word frequencies in written and spoken English: Based on the British National Corpus. London: Longman. Lijffjt, J., & Gries, S. T. (2012). Correction to “Dispersions and adjusted frequencies in corpora”. International Journal of Corpus Linguistics, 17(1), 147–149. Lyne, A. A. (1985). Dispersion. In The vocabulary of French business correspondence (pp. 101– 124). Geneva/Paris: Slatkine-Champion. Lyne, A. A. (1986). In praise of Juilland’s D. In Méthodes quantitatives et informatiques dans l’Études des textes, vol. 2 (pp. 589–595). Geneva/Paris: Slatkine-Champion. 
Oakes, M., & Farrow, M. (2007). Use of the chi-squared test to examine vocabulary differences in English language corpora representing seven different countries. Literary and Linguistic Computing, 22(1), 85–99. 
Paquot, M. (2010). Academic vocabulary in learner writing: From extraction to analysis. London/New York: Continuum. 
Robertson, S. (2004). Understanding inverse document frequency: On theoretical arguments of IDF. Journal of Documentation, 60(5), 503–520. 
Rosengren, I. (1971). The quantitative concept of language and its relation to the structure of frequency dictionaries. Études de linguistique appliquée (Nouvelle Série), 1, 103–127. 
Savick P., & Hlavá.
cová, J. (2002). Measures of word commonness. Journal of Quantitative Linguistics, 9(3), 15–31. 
Schmid, H. J. (2010). Entrenchment, salience, and basic levels. In D. Geeraerts & H. Cuyckens (Eds.), The Oxford handbook of cognitive linguistics (pp. 117–138). Oxford: Oxford University Press. 
Spärck Jones, K. (1972). A statistical interpretation of term specifcity and its application in information retrieval. Journal of Documentation, 28(1), 11–21. 
Spieler, D. H., & Balota, D. A. (1997). Bringing computational models of word naming down to the item level. Psychological Science, 8(6), 411–416. 
Washtell, J. (2007). Co-dispersion by nearest-neighbour: Adapting a spatial statistic for the development of domain-independent language tools and metrics. Unpublished, M.Sc. thesis, School of Computing, Leeds University. 
Wilcox, A. R. (1973). Indices of qualitative variation and political measurement. The Western Political Quarterly, 26(2), 325–343. 
Zhang, H., Huang, C., & Yu, S. (2004). Distributional consistency: As a general method for de­fning a core lexicon. Paper presented at language resources and evaluation 2004, Lisbon, Portugal. 
Chapter 6 Analysing Keyword Lists 
Paul Rayson and Amanda Potts 
Abstract Frequency lists are useful in their own right for assisting a linguist, lexicographer, language teacher, or learner analyse or exploit a corpus. When employed comparatively through the keywords approach, signifcant changes in the relative ordering of words can fag points of interest. This conceptually simple approach of comparing one frequency list against another has been very widely exploited in corpus linguistics to help answer a vast number of research questions. In this chapter, we describe the method step-by-step to produce a keywords list, and then highlight two representative studies to illustrate the usefulness of the method. In our critical assessment of the keywords method, we highlight issues related to corpus design and comparability, the application of statistics, and clusters and n-grams to improve the method. We also describe important software tools and other resources, as well as providing further reading. 


6.1 Introduction 
As we have seen from Chap. 4, frequency lists are an essential part of the corpus linguistics methodology. They allow us to see what words appear (and do not appear) in a text, and give an indication of their prominence if we sort the list in frequency order. Beyond the world of the corpus linguistic researcher, frequency lists can be used directly or indirectly to support language learners by providing a way to focus on the more frequent words in a text, or suggesting priorities for language teachers when preparing lesson materials. Lexicographers use frequency lists indirectly when constructing traditional printed dictionaries. Frequency lists have also been allied with other kinds of grammatical information (such as major 
P. Rayson (•) Lancaster University, Lancaster, UK e-mail: p.rayson@lancaster.ac.uk 
A. Potts Cardiff University, Cardiff, UK e-mail: pottsa@cardiff.ac.uk 
© Springer Nature Switzerland AG 2020 119 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_6 
P. Rayson and A. Potts 
word class) and translational glosses (both for the words themselves and example sentences) and turned directly into frequency dictionaries for learners and teachers in a number of languages (e.g. Juilland et al. 1970; Tono et al. 2013). 
When used in simple form, frequency lists can sometimes be misleading, and care needs to be taken when making generalisations from them. For example, if frequency counts are derived from a large representative corpus such as the British National Corpus (BNC), we may reasonably claim that high frequency words in the corpus may also have a similarly high usage in the language. However, a word may have a high frequency count in the BNC not because it is widely used in all sections of the corpus, but because it has a very high frequency in only certain parts of the corpus and not in others, e.g. conversational speech rather than newspaper articles. Hence, as described in Chap. 4, and more particularly in Chap. 5, we need to pay careful attention to dispersion or range measures, which can help us estimate, for instance, how widely represented a word is across the various texts, domains, or genres within a written corpus, or across speakers within a spoken corpus. 
Using computer software to automate the creation of frequency lists from texts saves the researcher signifcant amounts of time, but the results can be overwhelming in terms of the amount of information to analyse. One option to reduce the wealth of information is to compare a frequency list from one corpus with another in order to highlight differences in word rank or frequency, since signifcant changes to the relative ordering of words can fag points of interest (Sinclair 1991: 31). Corpus linguistics is inherently comparative, so a method has evolved to support the comparison of corpora, which helps us study, for example, the differences between tabloid and broadsheet newspapers (Baker et al. 2013), vocabulary variation on the basis of age and gender (Murphy 2010), or grammatical, lexical, and semantic change over 100 years of British and American English (Baker 2017). The resulting widely-used method of keyword analysis is our focus in this chapter. 
Hofand and Johansson (1982) were early pioneers of this approach when they carried out a large (for the time) comparison of one million words of American English (represented by the Brown corpus) with one million words of British English (in the LOB corpus). Their study employed a difference coeffcient defned by Yule, which varies between +1 and -1 to calculate the difference between the relative frequencies of a word in the two corpora. In addition, Pearson’s statistical goodness-of-ft test, called the chi-squared test, was applied, enabling Hofand and Johansson to mark statistically signifcant differences at the 5%, 1% and 0.1% confdence levels (cf. Chap. 20). 
Another major milestone in the development of the keywords approach was the inclusion of the method by Mike Scott in his WordSmith Tools software. A number of authors had used signifcance tests to determine the importance of differences of specifc words or linguistic features between corpora, but Scott’s approach (1997) allowed for a systematic comparison of full word frequency lists. Scott demonstrated that the keyword results enable a researcher to understand the ‘aboutness’ of a text or corpus. 

6.2 Fundamentals 
The keywords method is conceptually simple, relying on the comparison of (normally) two word frequency lists. The complexity of the method lies in the choice of statistics and frequency cut-offs to appropriately flter the results (for further discussion of this, see Sect. 6.4). As part of the corpus linguist’s toolbox, the keywords method is most appropriate as a starting point to assist in the fltering of items for further investigations, rather than an end in and of itself. 
As a frst step, two frequency sorted word lists are prepared, one from the corpus being studied (the ‘target’) and one from a reference corpus. Each word list contains a list of tokens and associated frequencies. The reference dataset in corpus linguistics studies is usually a general corpus, representative of some language or variety of language. However, depending on the research question and aims, a suitable comparison set may also be used, e.g. from a corpus representing a different variety, time, or genre. Rather than using two different corpora entirely, some researchers use various subcorpora from the same (reference) corpus for the ‘target’ and ‘reference’ sets, and this approach is further exemplifed in the two representative studies summarised below. Next, the frequency of each word in the target corpus is compared to its frequency in the reference dataset in order to calculate a keyness value. Finally, the word list for the target corpus is reordered in terms of the keyness values of the words. The resulting sorted list contains two kinds of keyword: positive (those which are unusually frequent in the target corpus relative to the reference corpus) and negative (words which are unusually infrequent in the target corpus). It is also common to describe these two groups of words as overused (for positive) and underused (for negative), particularly in the learner corpus literature (cf. Chap. 13). 
In order to perform the keyness calculation for each word in the list, corpus tools set up a 2 by 2 contingency table, as shown in Table 6.1 (see also Chap. 7). The value ‘c’ is the total number of words in the target corpus and ‘d’ is the total number 
Table 6.1 Contingency table for each word in the list 
Target Corpus  Reference Corpus  Total  
Frequency of word  a  b  a + b  
Frequency of other words  c - a  d - b  c + d - a - b  
Total  c  d  c + d  

P. Rayson and A. Potts 
of words in the reference corpus. Numbers ‘a’ and ‘b’ are termed the ‘observed’ (O) 
values (i.e. the actual frequencies of a given word in each corpus). 
Two main signifcance test statistics have been used in the corpus linguistics liter­ature: chi-squared (as employed by Hofand and Johansson 1982) and log-likelihood 
(LL) (described by Rayson and Garside 2000 for use in keywords calculations and by Dunning 1993 for calculating collocations). Rayson et al. (2004a) compared the reliability of the two statistics when used for keyness calculations under varying conditions (corpus size, frequency of words) and showed that the log-likelihood test is preferred over the chi-squared test since it is a more accurate statistic for heavily skewed comparisons (e.g. a high ratio of target corpus to reference corpus sizes or low frequency words), so here we present only the log-likelihood formula. First, we need to calculate the expected values (E) corresponding to each observed value (O) in Table 6.1 and then insert these values into the second equation below. 
 
Oi
Ni i
Ei =  i Ni 
Here, N1 = c, and N2 = d. Hence, for E1 = c*(a + b)/(c + d) and E2 = d*(a + b)/(c + d). Note that the calculation of expected values takes account of the sizes of the corpora, so the raw frequency fgures should be used in the table. 
Oi
-2ln. = 2 Oiln 
i Ei 
The log-likelihood score in this case is LL = 2*((a*ln(a/E1)) + (b*ln(b/E2))).1,2 Once this calculation has been performed for each word and the resulting word list has been sorted on the LL value, we can see the words that are most indicative (or characteristic) of the target corpus relative to the reference corpus at the top of the list. Words which occur in the two corpora with roughly similar relative frequencies appear lower down the sorted list. At this point in the method, different researchers have applied different cut-offs relating to position in the list—signifcance value or p-value—and there is little agreement about a preferred approach. We will return to this discussion in our critical assessment in Sect. 6.3. 
1Online calculators and downloadable spreadsheets are available at http://corpora.lancs.ac.uk/ 
sigtest/ (accessed 25 June 2019) and http://ucrel.lancs.ac.uk/llwizard.html (accessed 25 June 2019). 2It should be noted that this formula represents the 2-cell calculation (Rayson and Garside 2000) which can be used since the contribution from the other two cells is fairly constant and does not affect the ranking order. Other tools, e.g. AntConc, and statistical calculators also support the 4-cell calculation incorporating contributions from frequencies of the other words into the Log-Likelihood value. 
Representative Study 1 
Seale, C., Ziebland, S., and Charteris-Black, J. 2006. Gender, cancer expe­rience and internet use: A comparative keyword analysis of interviews and online cancer support groups. Social Science & Medicine 62:2577– 2590. 
This work set out to achieve two aims: (1) to discuss comparative keyword analysis as a possible substitute for the sort of qualitative thematic analysis that a large number of previous illness studies found basis in; and (2) to apply this method in analysing gender differences in the online discourse of people with breast and prostate cancer. 
Previous studies of people with cancer found gender differences which align broadly with fndings in more wide-reaching sociolinguistic analyses, namely that women’s style is more expressive compared to men’s style, which is more instrumental (Boneva and Kraut 2002). Seale et al. (2006) expanded considerably on previous studies by incorporating tools from corpus linguistics, notably keyness analysis. 
The corpora utilised contained two types of data: research interviews and Internet-based support groups. Qualitative interviews were adopted for secondary analysis from the Database of Individual Patient Experiences project; these were conducted in the UK, 2000–2001, with 97 people with cancer (45 women with breast cancer; 52 men with prostate cancer), totalling 727,100 words. Posts were inspected to extract only those written by people with cancer (as opposed to family members, carers, or those experiencing symptoms that may be associated with cancer), which resulted in a fnal corpus comprising 12,757 posts and 1,629,370 words. 
Often, stylistic, grammatical, or syntactical features of the target corpus are highlighted through keyness comparison with a general reference corpus. However, the comparative keyword analysis reported here did not involve a general reference corpus (which may confate or obscure gender differences in the specifc data sets. Instead, the breast cancer texts were compared to the prostate cancer texts to facilitate analysis of meanings made by their female and male authors, respectively. 
Keywords were calculated using WordSmith Tools (Scott 2004). Measures of ‘keyness’ (expressed in positive and negative log-likelihood values with the prostate cancer corpus serving as the target corpus) were provided. Word-Smith Tools also provides their corresponding p-values, p < 0.00000001 for all items, rendering this an ineffective method of describing or differentiating results. The ‘top 300’ results from both the breast and prostate cancer corpora were analysed, with some exclusions. Concordances and clusters around keywords were analysed by hand, and keywords were manually placed into 
(continued) 
P. Rayson and A. Potts 
semantic categories. In the opinion of the researchers, “[t]his enabled impor­tant and meaningful comparative aspects of these large bodies of text to be identifed” in a “more economical and potentially replicable manner than con­ventional qualitative thematic analysis based on coding and retrieval” (ibid.). This answered the frst broad purpose of the paper, as introduced above. 
Analysis of the interviews and web fora led to fndings broadly aligned with previous research on gender differences in the experience of serious illness. Qualitative analysis of the interview corpus also supported fndings from previous studies: women were more likely to claim to seek social support on the Internet, whereas men said that they use the Internet to look for information. Quantitative analysis using keyness helped to identify further areas of interesting difference. Compared to women with breast cancer, men with prostate cancer had a much greater number of keywords pertaining to TREATMENT (i.e. catheter, brachytherapy, hormone, Zoladex, treatment), TESTS AND DIAGNOSIS (biopsy, MRI, screening), SYMPTOMS AND SIDE EFFECTS (incontinence, impotence), and DISEASE AND ITS PROGRESSION (PSA [prostate specifc antigen], staging, cancer, aggressive). By contrast, women with breast cancer had a greater number of keywords under categories such as SUPPORT (i.e. help, supportive), FEELINGS (scared, hope, depressed), PEOPLE (I, you, husband, ladies), CLOTHING AND APPEARANCE (wear, clothes), and SUPERLATIVES (lovely, defnitely, wonderful). 
Grouping keywords into a range of semantic categories helps to generalise and distil meanings. Viewed as a whole, fndings in Seale et al. suggested “that men’s experience of their disease appears to be more localised on particular areas of the body, while women’s experience is more holistic” (2006: 2588). 
Seale et al. (2006) conceded that the study had a number of limitations. The frst limitation was to do with sampling: as individual posts had not been linked to poster identity, there was the possibility that overuse of certain keywords by specifc individuals has resulted in patterns being interpreted as common across the corpus/sample as a whole. As with other keyness studies, the researchers acknowledged that this work focussed on difference rather than similarity, which has the effect of reifying gender differences. Finally, more complex syntactical or semantic patterns (such as tag questions or cognitive metaphors) were not highlighted and discussed with this methodology or in this study. 
Other limitations not acknowledged by the researchers were also present in this study. The use of WordSmith’s ‘keyness measure’ was used to rank results, with the ‘top 300’ skimmed from each corpus for further analysis. However, with all p-values nearing zero and no thresholds of log-likelihood included, it is unclear whether the most salient semantic categories were, in fact, included and populated. Finally, while we acknowledge that the creation of ad hoc semantic categories is useful for thematic content analysis, 
(continued) 
particularly when the researchers are very well-versed in the content of their corpora, we wonder about the accompanying detriment that this brings, particularly in relation to replicability. A number of other studies make use of further computational methods to undertake semantic annotation and categorisation. Below, we summarise one such work. 
Representative Study 2 
Culpeper, J. 2009. Words, parts-of-speech and semantic categories in the 
character-talk of Shakespeare’s Romeo and Juliet. International Jour­
nal of Corpus Linguistics 14(1):29–59. 
Culpeper (2009) moved beyond analysis of keywords in isolation to consider both key parts-of-speech and semantic categories in a corpus stylistic analysis of character-talk in Romeo and Juliet. 
In this study, the speech of the six characters with the highest frequency of speech in Romeo and Juliet was isolated to create subcorpora varying in length from 1293 to 5031 words. Culpeper posited that studying these subcorpora would expose the differing speech styles of characters (which, in turn, contribute to reader perception of characterisation). In this case, the very small sizes of data under analysis also allowed for full consideration of all results. 
Earlier literary studies have described (contextualised) frequency of fea­tures as indicators of an author’s or character’s style. Culpeper’s aim was to “produce key items that refect the distinctive styles of each character compared with the other characters in the same play, rather than ... stylistic features relating to differences of genre ... or aspects of the fctional world” (2009: 35). Therefore, in this study, the subcorpus of any individual character’s speech was compared to a reference corpus of all other character’s speech (exclusive). 
This study also made use of WordSmith Tools (Scott 2004) to calculate keywords. It found that keyword analysis did provide evidence for results that might be predictable (for instance, that “Romeo is all about love” (Culpeper 2009: 53)), but also exposed features that were less easily observable (Juliet’s subjunctive keywords, which may be linked to an anxious style), without relying on intuitions about which parts of the text or which features to focus on (ibid., p. 53). The study then went on to experiment with analysing key parts-of-speech and key semantic domains using a combination of software. 
Key parts-of-speech and key semantic categories are calculated by apply­ing the keywords procedure to part-of-speech and semantic tag frequency 
(continued) 
P. Rayson and A. Potts 
lists in the same way as described in Sect. 6.2 for word frequency lists. Keyword analysis can be illuminating, but can also be misleading, because semantic similarity is not explicitly taken into account during analysis. While many (or even most) researchers who use keyword analysis end up grouping or discussing words in semantic categories, this can be a somewhat fawed method, as these categories are subjective, and words which are too infrequent to appear as key on their own will be discounted. In small corpora (such as the Romeo and Juliet corpus), many content words of interest would necessarily be low-frequency. The incorporation of a computational annotation system allows for systematic, rigorous annotation followed by statistical analysis. 
In this study, semantic domains were annotated and analysed using UCREL’s (University Centre for Computer Corpus Research on Language, based at Lancaster University) Semantic Analysis System (USAS) in Wma­trix. The input is part-of-speech tagged text produced by CLAWS, which is then run through SEMTAG, which uses lexicons to assigns semantic tag(s) to each lexical item or multiword unit (for details, see Wilson and Rayson 1993, Rayson and Wilson 1996). The accuracy rate for SEMTAG is said to be approximately 91% for modern, general language (Rayson et al. 2004b), though the author acknowledged that semantic shift led to some ‘incorrect’ classifcations. Some of these were corrected using a historical lexicon, but Culpeper cautiously checked and interpreted the results to be more certain of their meaning and importance. 
In the analysis section of the paper, a number of key semantic categories and constituent items (words and set phrases) were presented for Romeo, Nurse, and Mercutio. An indicative selection of Romeo’s key semantic categories appears in Table 6.2. 
The frst two semantic categories (RELATIONSHIP:INTIMATE/SEXUAL and LIKING) are clearly linked semantically and metonymically. The keyness of these themes was predictable, but did provide empirical and statistical evidence for topicality of Romeo’s speech and a sense of his role as a lover. The third category in Table 6.2 is illustrative of key semantic domains which were much less predictable. Romeo describes literal light, but also makes use of conventional metaphors, such as light/dark for happiness/unhappiness. Exposure of metaphorical usage is an interesting and useful feature of the semantic tagger. 
Interestingly, very few items within key semantic domains were also iden­tifed as keywords in this study. In Table 6.2 above, only three of the 26 listed items were also keywords; in the full table in Culpeper (2009: 48), only fve out of 77 total items in key semantic categories were independently identifed keywords. This result highlighted a major advantage of key semantic domain analysis: lexical items which may not turn up in keyword analysis due to low frequency combine together to highlight larger felds of key meaning. 
(continued) 
Table 6.2 Romeo’s ‘top three’ semantic categories, as rank-ordered for positive keyness 
(i.e. relatively unusual overuse). Keywords—identifed independently earlier in the paper— are emboldened 
Semantic category, including the tag code and frequency  Items within the category (and their raw frequencies) up to a maximum of ten types if they are available  
RELATIONSHIP: INTIMATE/SEXUAL (S3.2) (48)  love (34), kiss (5), lovers (3), kisses (2), paramour (1), wantons (1), chastity (1), in love (1)  
LIKING (E2+) (38)  love (15), dear (13), loving (3), precious (2), like (1), doting (1), amorous (1), loves (1)  
COLOUR AND COLOUR PATTERNS (O4.3) (33)  light (6), bright (4), pale (3), dark (3), green (2), stained (2), black (2), golden (1), white (1), crimson (1)  

Adapted from Culpeper (2009: 48) 
As far as indicators of aboutness and style, all methods have merit. Key part-of-speech categories, keywords, and key semantic categories are usually dominated by a small number of very frequent items, allowing for overlap. Culpeper found that items identifed as keywords dominate 66.6% of the semantic categories, meaning that a keyword analysis would reveal most conclusions, but also leave out a not-insignifcant amount of fndings. The areas not overlapping (particularly the 33.4% between key semantic domains and keywords) are very salient but also very diffcult to predict. So, “[w]hen keywords are dominated by ideational keywords, capturing the ‘aboutness’ of the text, the part-of-speech and particularly the semantic keyness analyses have much more of a contribution to make, moving the analysis beyond what is revealed in the keywords” [ibid.]. 

6.3 Critical Assessment and Future Directions 
In this chapter so far, we have described the origins and motivations for the devel­opment of the keywords method in corpus linguistics, and shown two representative studies using the technique. In the next few sub-sections, we will undertake a critical assessment describing some of the pitfalls and misconceptions about the use of the technique, along with a summary of criticisms, concluding with future directions for the approach. 


6.3.1 Corpus Preparation 
Given that the input to the keywords method is word frequency lists, then the specifc details of the preparation of the lists are important—but often overlooked—in the corpus linguistics literature. What counts as a word in such lists (on the basis of tokenisation, punctuation, capitalisation, standardisation, etc.) can potentially make a large difference to the results. Usually, corpus software tools tokenise words by 
P. Rayson and A. Potts 
identifying boundaries with white space characters and removing any punctuation characters from the start and end of words. However, different texts and authors might use hyphenation differently; for instance, “todo”, “to-do” and “to do”, must be carefully cross-matched or standardised, or else the frequencies in contingency tables will not compare word types consistently. Most corpus tools avoid the capitalisation issue completely by forcing all characters to upper-or lowercase, but this can cause issues with different meanings, e.g. “Polish” versus “polish”. Wmatrix makes an attempt to preserve capitalisation if words are tagged as proper nouns but this, in turn, relies on the accuracy of the POS tagger software. 
Corpus methods are increasingly being applied to historical corpora, as we have seen in Culpeper (2009), described in Representative Study 2. Many authors rely on modernised or standardised editions to avoid spelling variation issues. Baron et al. (2009) carried out a detailed study to assess the degree to which keyword results are affected by spelling variants in original editions. First, they estimated the extent of spelling variation in various large Early Modern English corpora and found that, on average, in texts from 1500, the percentage of variant types is over 70% and variant tokens is around 40%. In terms of preparing frequency lists, this means, for example, that rather than counting all occurrences of the word “would”, corpus software needs to take account of frequencies for other potential variants: “wolde”, “woolde”, “wuld”, “wulde”, “wud”, “wald”, “vvould”, “vvold”, and so on. The amount of spelling variation drops down to less than 10% of types and tokens in corpus texts from around 1700 onwards. In terms of impact on the keyness method, spelling variation is a signifcant problem, since the rank ordering of words will be affected by the distribution of variant frequencies. Baron et al. (2009) estimated the difference with rank correlation coeffcients on keyword lists calculated before and after standardisation and found that Kendall’s Tau scores can drop as low as 0.6 (where a score of 1 indicates that the two lists are the same, 0 indicates that the rankings are independent and -1 indicates that one ranking is the reverse of the other; cf. Chap. 17). A similar effect will be observed when applying the keywords approach to computer-mediated communication (CMC) varieties e.g. online social media, emails, and SMS, so care must be taken with data preparation. 

6.3.2 Focus on Differences 
One of the central drawbacks to keyness analysis is the innate focus on difference (and obfuscation of similarity). Baker (2004) undertook a comparative study on online erotica, and explained that while large is a keyword in gay male erotic texts compared to lesbian erotic narratives from the same erotica website, other seman­tically related words (i.e. huge) may have occurred with comparable frequency in both corpora. This may lead analysts to (erroneously) over-generalise the keyness of ‘size’ in the gay male corpus, overlooking the central tenet of keyword analysis, which is allowance for fndings and discussion at the lexical level. Baker (2004) proposed one way to circumvent this focus on differences: to carry out comparisons on more than two sets of data. This is helpful when undertaking keyword analysis on two target corpora (rather than one target corpus compared to a reference corpus). By calculating keywords in two target corpora against one another and then, for instance, against a larger reference corpus, differences and similarities may be highlighted in the emerging results. 
Another issue in keyword analysis highlighted by Baker (2004) is that the ‘strongest’ words tend to reveal obvious patterns. While this does provide confr­matory evidence in new studies that the technique is working as expected, this bias can contribute to an unmanageable number of unsurprising keywords being thrown up for analysis. Possible proposed work-arounds have already been demonstrated in some of the studies discussed above: researchers may apply cut-off points related to relative dispersion across texts (see also Chap. 5), frequency in the entire corpus, or maximum p-values, or even switch the focus to use dispersion instead of frequency for keyness calculations (Egbert and Biber 2019). No robust guidelines as to ‘appropriate’ cut-offs for any of these measures have been recommended in the literature, which can be seen as a need for further development of the method. Similar issues with method settings and parameters can be observed in the area of collocation research (see Chap. 7). 

6.3.3 Applications of Statistics 
There have been a number of criticisms of the keywords approach in relation to the application and interpretation of the signifcance test statistics used in the procedure. The method described in Sect. 6.2 can be seen as a goodness-of-ft test, where the null hypothesis is that there is no difference between the observed frequencies of a word in the two corpora. If the resulting metric (log-likelihood in our case) exceeds a certain critical value, then the null hypothesis can be rejected. After choosing a degree of confdence, we can use chi-squared statistical tables to fnd the critical value, e.g. for the 5% level (p < 0.05) the critical value is 3.84, and for 1% (p < 0.01), it is 6.63 (cf. Chap. 20). For a comparison of two corpora, we use values with 1 degree of freedom, i.e. one less than the number of corpora. However, if the value calculated from the contingency table does not exceed the critical value, this only indicates that there is not enough evidence to reject the null hypothesis and we cannot conclude that the null hypothesis is true (i.e. which would indicate that there is no signifcant difference). 
It was Dunning (1993) who frst brought the attention of the community to the log-likelihood test, proposing it for collocation analysis rather than keywords. Dunning cautioned that we should not rely on the assumption of a normal distri­bution when carrying out statistical text analysis and recommended log-likelihood as parametric analysis based on the binomial or multinomial distributions instead. There is some disagreement in the literature here, with some authors stating that chi-squared assumes a multinomial distribution, making no special distributional assumptions of normality. Cressie and Read (1984) showed that Pearson’s X2 (chi-squared) and the likelihood ratio G2 (Dunning’s log-likelihood) are two statistics 
P. Rayson and A. Potts 
in a continuum defned by the power-divergence family of statistics and refer to the long running discussion (since 1900) of the statistics and their appropriateness for contingency table analysis. Kilgarriff (1996) considered the Brown versus LOB corpus comparison by Hofand and Johansson (1982) and highlighted that too many common words were marked as signifcant using the chi-squared test. In order to better discriminate interesting from non-interesting results, he suggested making use of the Mann-Whitney test instead, as this makes use of frequency ranks rather than frequency directly. However, with a joint LOB/Brown frequency above 30 where the test could be applied, 60% of the word types were still marked as signifcant. Results using Mann-Whitney also suffer towards the low end of the frequency spectrum, especially when words have a frequency of zero in one of the two corpora. This is because a large number of words occur with the same frequency (indeed, usually half of the types in a corpus occur with a frequency of one), so they cannot be satisfactorily ranked. For tables with small expected frequencies, many researchers have used Yates’ corrected chi-squared statistic (Y2), and some prefer Fisher’s exact test; for more discussion see Baron et al. (2009). 
More recent papers have also investigated similar issues of statistical validity and appropriateness of the keywords procedure as currently envisaged for comparing corpora with specifc designs. Brezina and Meyerhoff (2014: 1) showed that using a keywords approach to compare whole corpora “emphasises inter-group differences and ignores within group variation” in sociolinguistic studies. The problem is not the signifcance test itself, but rather the aggregation of frequency counts for a target linguistic variable, e.g. a word across speaker groupings. They recommend the Mann-Whitney U test instead, to take account of separate speaker frequency counts and variation within datasets. As Kilgarriff (2005) reminded us, language is not random, and the assumption of independence of words inherent in the chi-squared and log-likelihood tests “may lead to spurious conclusions when assessing the signifcance of differences in frequency counts between corpora” (Lijffjt et al. 2016: 395), particularly for poorly dispersed words. Paquot and Bestgen (2009) and Lijffjt et al. (2016) recommended representing the data differently in order to make the assumption about independence at the level of texts rather than the level of words. Lijffjt et al. (2016) recommended other tests that are appropriate for large corpora, such as Welch’s t-test, the Wilcoxon rank-sum test and their own bootstrap test (see also Chap. 24). In response to Kilgarriff (2005), Gries (2005) pointed out the importance of multiple corrections for post-hoc testing (e.g. Bonferroni, or the more recent Šidák correction), since, after applying those, the expected proportion of signifcant results are observed. Gries (2005) also directed readers to other methods such as effect sizes, Bayesian statistics (later picked up by Wilson 2013) and confdence intervals, and highlighted that null hypothesis signifcance testing has been criticised in other scientifc disciplines for many decades. Concerns over the reproducibility and replicability of scientifc results have led the editors of the Basic and Applied Social Psychology journal to ban p-values (null hypothesis signifcance testing) and the American Statistical Association produced a policy statement to discuss the issues (Wasserstein and Lazar 2016). 
Many misconceptions about statistical hypothesis testing methods are observable in the corpus linguistics literature and beyond; for further details, see Vasishth and Nicenboim (2016). One specifc example that we can demonstrate here illustrates the usefulness of including effect size measures alongside signifcance statistics to allow for comparability across different sample sizes. As with signifcance metrics, there are a number of different effect size formulae that could be used. Effect size measures show the relative difference in sizes between word frequencies in two corpora, rather than factoring in how much evidence we have in the corpus samples. This means that, unlike log-likelihood measures, they are not affected by sample size. Consider three hypothetical experiments for the frequencies of the words ‘blah’, ‘ping’ and ‘hoot’ in four corpora, as show in Table 6.3. Here, we are using log-likelihood (LL) as our signifcance measure and Log Ratio (LR) as the effect size measure (Hardie 2014). In experiment 1, LL tells us that there is enough evidence to reject the null hypothesis at p < 0.0001 (critical value 15.13) and the effect size shows the doubling of the frequency of the word in corpus 1 relative to corpus 2. Compare this with experiment 2, where the word frequencies and corpus sizes are all ten times larger than in experiment 1. As a result, the LL value is ten times larger, indicating more evidence for the difference, but the LR is still the same, given that the ratio of 1000 to 500 is the same as the ratio of 100 to 50. In experiment 3, we retain the same sized corpora as in experiment 2, but the frequencies of the word are closer together, and they illustrate that a smaller relative frequency difference is still shown to be signifcant at the same p-value as in experiment 1. Importantly, we should note the lack of comparability of the LL score between experiments 1 and 2 (as well as between 1 and 3) because they employ differently sized corpora. In contrast, effect size scores can be compared across all three experiments without the same concerns. 
Table 6.3 Three hypothetical keywords experiments 
Experiment  Word frequencies and corpus sizes  Signifcance and effect size results  
1  Corpus 1 and 2 contain 10,000 words each. Frequency of ‘blah’ in corpus 1 = 100 Frequency of ‘blah’ in corpus 2 = 50  Signifcance (LL) = 16.99 Effect size (LR) = 1.00  
2  Corpus 3 and 4 contain 100,000 words each. Frequency of ‘ping’ in corpus 3 = 1000 Frequency of ‘ping’ in corpus 4 = 500  Signifcance (LL) = 169.90 Effect size (LR) = 1.00  
3  Corpus 3 and 4 contain 100,000 words each. Frequency of ‘hoot’ in corpus 3 = 1000 Frequency of ‘hoot’ in corpus 4 = 824  Signifcance (LL) = 17.01 Effect size (LR) = 0.28  

P. Rayson and A. Potts 

6.3.4 Clusters and N-Grams 
Both Baker (2004) and Rayson (2008) have pointed out a serious limitation of the keywords procedure, which is that it can really only be used to highlight lexical differences and not semantic differences. This means that a word which has one signifcant meaning might not be correctly signalled as key when its various senses are counted together, thus masking something of interest. To some extent, the procedure implemented in Wmatrix and employed by Culpeper (2009) as described in Representative Study 2 will address this issue, because words are semantically tagged and disambiguated before the keyness procedure is applied. 
Researchers may also be interested in (semantic) meaning beyond the single word. The USAS semantic tagger can be used to identify semantically meaningful multiword expressions (MWEs) since these chunks need to be analysed as belonging to one semantic category or are syntactic units e.g. phrasal verbs, compounds, non-compositional idiomatic expressions. Wmatrix then treats these MWEs as single elements in word lists, allowing key MWEs to emerge alongside keywords. Consider an example MWE ‘send up’. If this were not identifed in advance as a semantically meaningful chunk meaning ‘to ridicule or parody’, then separate word counts for ‘send’ and ‘up’ would be observed and merged with the other occurrences of those words in the corpus, potentially incorrectly infating their frequencies. 
Without the beneft of a semantic tagger, Mahlberg (2008) combines, for the frst time, the keywords procedure with clusters or n-grams, i.e. repeated sequences of words counted in corpora. Once the clusters have been counted, then key clusters can be calculated using the same procedure as for keywords. Mahlberg then groups key clusters by function to draw conclusions about local textual functions in a corpus of Charles Dickens’ writing, which formed the basis of a corpus stylistic investigation. This key clusters (or key n-grams) approach can be seen as an extension of the keywords approach. The simple keywords approach is, in fact, a comparison of n-grams of length 1. It has proved to be a very fruitful line of investigation with a number of other studies employing this method. Paquot (2013, 2014, 2017) used key clusters to identify French learners’ lexical preferences and potential transfer effects from their native language. Additionally, others have used the key n-gram approach to support native language identifcation (Kyle et al. 2013) and automatically assessing essay quality (Crossley et al. 2013). 

6.3.5 Future Directions 
Many current studies using the keywords method are on English corpora. As this method is readily available in software such as WordSmith and AntConc— which work well in most languages—more thought should be given to how well keywords work in languages other than English, especially where much more complex infectional and derivational morphology occurs, e.g. Finnish. For these languages, it might be the case that comparing surface forms of words from the corpus works less well than comparing lemmas, because the frequencies are too widely dispersed across different word forms (in a similar way to historical spelling variants) to be comparable. 
In terms of future directions for keyness analysis, we recommend that more care is taken in the application of the technique. Rather than blindly applying a simple method to compare two relative frequencies, more thought is required to consider the criticisms and shortcomings that have been expressed in the preceding sections. Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specifc research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identifcation of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results. As a corpus community, we need to agree on better guidelines and expectations for fltering results in terms of minimum frequencies and signifcance and effect size values rather than relying on ad hoc solutions without proper justifcations. 
In the future, we recommend investigating the use of statistical power calcula­tions in corpus linguistics. Power calculations can be used alongside signifcance testing and effect size calculations and are increasingly employed in other dis­ciplines, e.g. psychology. Statistical power allows us to calculate the likelihood that an experiment will detect an effect (or difference in frequency in our case of comparing corpora) when there is an effect to be detected. We can use higher statistical power to reduce the probability of a Type-2 error, i.e. concluding that there is no difference in frequency of a word between two corpora, when there is in fact a difference. This might mean setting the effect size in advance and then calculating (a-priori) how big our corpora need to be, or at least being able to (post-hoc) calculate and compare the power of our corpus comparison experiments. This might help us answer the perennial question, ‘How big should my corpus be?’ and help researchers determine comparability and the relative sizes of sub-corpora defned by metadata such as socio-linguistic variables. Finally, related to the experimental design and interpretation of results, issues of corpus comparability, homogeneity and representativeness are highly important to consider alongside reliability of the statistical procedure (Rayson and Garside 2000). It should not be forgotten that interpretation of the results of any automatic procedure is the responsibility of the linguist, and the results of the keywords method are a starting point to help guide us rather than an end point of the research. 
P. Rayson and A. Potts 
6.4 Tools and Resources 


6.4.1 Tools 
Keyness was arguably one of the later additions to the quiver of corpus linguistic tools; many papers published between 2001–2008 (including the two representative studies summarised in this chapter) discussed the infancy of its adoption. Now, however, this is considered one of the standard fve methods of the feld, alongside frequency, concordance, n-gram and collocation. As a result, nearly all concor­dancers and corpus linguistic tools will offer some assistance in the calculation of keyness. Distinguishing features, then, are: 
1. 
the incorporation of more sophisticated taggers, allowing for calculation of key lemmas, parts-of-speech (POS), or semantic domains; 

2. 
the inclusion of built-in reference corpora, often general corpora or subsections thereof, allowing for immediate calculation against a known ‘benchmark’ with­out the necessity of sourcing or collecting a comparable corpus; 

3. 
the selection of measures of keyness available (see Sect. 6.3.3 for discussion). 


We have provided an overview of popular tools and these features in Table 6.4. 
If a user has both a reference and target corpus and is simply interested in straightforward calculation of keywords, we can recommend both AntConc and WordSmith as good beginner-level tools for this method. CQPweb has the greatest variety of measures available; with robust tagging systems and a range of reference corpora, it also allows for calculation of keyness across features and genres. However, key semantic domains are diffcult to access, and inability to upload target corpora may inhibit use for many users interested in exploring their own data. SketchEngine is extraordinarily powerful, with part-of-speech tagging and lemmatisation on a huge number of languages. However, semantic tagging is still under development, and some may disagree with the application of Simple Maths. The most powerful tool for semantic processing is inarguably Wmatrix. The main interface also offers easy access to key POS and keywords, and a small number of reference corpora are accessible. We recommend Wmatrix for keyness analysis, although with a caveat about size restrictions since it is currently suitable for corpora up to around fve million words. The keywords method can also be implemented directly in programming languages such as Python, R and Perl (see Chap. 9). 

6.4.2 Resources (Word Lists) 
Generation of keywords in a target corpus necessitates some point of comparison, usually either a second target corpus, a reference corpus, or a word list from a large, general corpus. Selection of a reference corpus will impact the results, and some care should be taken to select an appropriate ‘benchmark’ to highlight differences aligned with a given research question. Many research questions necessitate the 
Table 6.4 Overview of available tools. A tick mark indicates full usability of a given feature; a tilde indicates partial capacity (i.e. beta development or use restricted to special access) 
Tool  Keywords(*lemmatised)  Key clusters  Key POS  Key semanticdomains  Upload own corpora  Built-in referencecorpora  Measures of keynessavailable  
AntConcv3.4.4        Chi-square, LL  
CQPwebv3.2.25  *      ˜    LL, log ratio(unfltered, LLflter,confdence intervalflter)  
SketchEngine  *      ˜      Simple mathsa  
Wmatrix v3            LL, log ratio  
WordSmithTools v5        Chi-square (yatescorrection), LL 

aFor details, see https://www.sketchengine.eu/documentation/simple-maths/. Accessed 25 June 2019 
P. Rayson and A. Potts 
Table 6.5 Selection of available word lists, with descriptions and weblinks 
Source  Description  Link  
BNC  A number of word lists from the British National Corpus, including subcorpora divisions (e.g., written or spoken)  http://ucrel.lancs.ac.uk/bncfreq/fists. html. Accessed 25 June 2019.  
Brown family  Word lists from the 1961, 1991, and 2006 American and British Brown family corpora: Brown, LOB, Frown, FLOB, AmE06, and BE06  http://ucrel.lancs.ac.uk/wmatrix/ freqlists/. Accessed 25 June 2019.  
COCA  A range of word and phrase lists from the Corpus of Contemporary American English  http://corpus.byu.edu/resources.asp. Accessed 25 June 2019.  
Kelly  Multilingual word lists of the most frequent 9000 words in nine languages  https://www.hf.uio.no/iln/english/ about/organization/text-laboratory/ services/kelly.html. Accessed 25 June 2019.  
Moby  A range of word and phrase lists, including: Five languages; root words, synonyms, and related words; the complete works of Shakespeare; with some lists part-of-speech or IPA coded  https://en.wikipedia.org/wiki/Moby_ Project. Accessed 5 July 2019.  
Wiktionary  A huge range of word lists from a range of sources and domains (including, i.e., Project Gutenberg), from a large number of languages  https://en.wiktionary.org/wiki/ Wiktionary:Frequency_lists. Accessed 25 June 2019.  

collection of specialised reference corpora, or entail comparison of subcorpora. Those wishing to answer more general questions (e.g. ‘aboutness’) may choose to make use of a general reference corpus. Word lists of many of the largest general corpora are readily available online; we have provided a sample of some English-language resources in Table 6.5, although care should be taken when selecting these to ensure that tokenisation decisions are well documented and comparable. 
Further Reading 
Bondi, M. and Scott, M. (Eds.). 2010. Keyness in texts. Amsterdam: John Benjamins. 
This is quite a comprehensive guide for scholars with particular interest in keywords and phrases. The collection is divided into three sections: (1) Exploring keyness; 
(2) Keyness in specialised discourse; and (3) Critical and educational perspectives. Section one deals with a number of the issues that we have touched upon here in greater detail, with leading scholars such as Stubbs and Scott outlining the main concepts and problems in keyword analysis. Sections two and three function as interesting collections of case studies on corpora drawn from engineering, politics, media, and textbooks, from a range of time periods and places. 
Archer, D. (Ed.). 2009. What’s in a word-list? Investigating word frequency and keyword extraction. London: Routledge. 
This edited collection has a number of chapters of particular relevance for scholars interested in keyness. Mike Scott explores reference corpus selection and discusses the eventual impact on fndings. Tony McEnery and Paul Baker have chapters using keyness to critically examine the discourses in media and politics, respectively. Those interested in Culpeper’s (2009) paper above may like to read a wider study on Shakespeare’s comedies and tragedies, by Archer, Culpeper, and Rayson. Finally, Archer makes an argument for wider incorporation of frequency and keyword extraction techniques in the closing chapter. 
References 
Baker, P. (2004). Querying keywords: Questions of difference, frequency, and sense in keywords analysis. Journal of English Linguistics, 32(4). Baker, P. (2017). British and American English: Divided by a common language? Cambridge: Cambridge University Press. Baker, P., Gabrielatos, C., & McEnery, T. (2013). Discourse analysis and media attitudes: The representation of Islam in the British Press. Cambridge: Cambridge University Press. Baron, A., Rayson, P., & Archer, D. (2009). Word frequency and key word statistics in corpus linguistics. Anglistik, 20(1), 41–67. Boneva, B., & Kraut, R. (2002). Email, gender, and personal relations. In B. Wellman & C. Haythornthwaite (Eds.), The internet in everyday life (pp. 372–403). Oxford: Blackwell. 
Brezina, V., & Meyerhoff, M. (2014). Signifcant or random? A critical review of sociolinguistic generalisations based on large corpora. International Journal of Corpus Linguistics, 19(1), 1– 28. 
Cressie, N., & Read, T. (1984). Multinomial goodness-of-ft tests. Journal of the Royal Statistical Society: Series B: Methodological, 46(3), 440–464. 
Crossley, S. A., Defore, C., Kyle, K., Dai, J., & McNamara, D. S. (2013). Paragraph specifc n-gram approaches to automatically assessing essay quality. In S. K. D’Mello, R. A. Calvo, & A. Olney (Eds.), Proceedings of the 6th international conference on educational data mining (pp. 216–219). Heidelberg/Berlin: Springer. 
Culpeper, J. (2009). Words, parts-of-speech and semantic categories in the character-talk of Shakespeare’s Romeo and Juliet. International Journal of Corpus Linguistics, 14(1), 29–59. Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational 
Linguistics, 19(1), 61–74. Egbert, J., & Biber, D. (2019). Incorporating text dispersion into keyword analysis. Corpora, 14(1), 77–104. Gries, S. T. (2005). Null-hypothesis signifcance testing of word frequencies: A follow-up on Kilgarriff. Corpus Linguistics and Linguistic Theory, 1(2), 277–294. Hardie, A. (2014). Log Ratio – an informal introduction. CASS blog: http://cass.lancs.ac.uk/ ?p=1133. Accessed 25 June 2019. 
P. Rayson and A. Potts 
Hofand, K., & Johansson, S. (1982). Word frequencies in British and American English. Bergen, Norway: The Norwegian Computing Centre for the Humanities. 
Juilland, A., Brodin, D., & Davidovitch, C. (1970). Frequency dictionary of French words.Paris: Mouton &. 
Kilgarriff, A. (1996). Why chi-square doesn’t work, and an improved LOB-Brown comparison. In Proceedings of the ALLC-ACH conference (pp. 169–172). Bergen: Norway. 
Kilgarriff, A. (2005). Language is never ever ever random. Corpus Linguistics and Linguistic Theory, 1(2), 263–276. 
Kyle, K., Crossley, S., Daim J., & McNamara, D. (2013, June 13). Native language identifcation: A key N-gram category approach. In Proceedings of the eighth workshop on innovative use of NLP for building educational applications (pp. 242–250). Atlanta, Georgia. 
Lijffjt, J., Nevalainen, T., Säily, T., Papapetrou, P., Puolamäki, K., & Mannila, H. (2016). Signifcance testing of word frequencies in corpora. Literary and Linguistic Computing, 31(2), 374–397. 
Mahlberg, M. (2008). Clusters, key clusters and local textual functions in Dickens. Corpora, 2(1), 1–31. 
Murphy, B. (2010). Corpus and sociolinguistics: Investigating age and gender in female talk. Amsterdam: John Benjamins. 
Paquot, M. (2013). Lexical bundles and transfer effects. International Journal of Corpus Linguis­tics, 18(3), 391–417. 
Paquot, M. (2014). Cross-linguistic infuence and formulaic language: Recurrent word sequences in French learner writing. In L. Roberts, I. Vedder, & J. Hulstijn (Eds.), EUROSLA yearbook (pp. 216–237). Amsterdam: Benjamins. 
Paquot, M. (2017). L1 frequency in foreign language acquisition: Recurrent word combinations in French and Spanish EFL learner writing. Second Language Research, 33(1), 13–32. 
Paquot, M., & Bestgen, Y. (2009). Distinctive words in academic writing: A comparison of three statistical tests for keyword extraction. In A. Jucker, D. Schreier, & M. Hundt (Eds.), Corpora: Pragmatics and discourse (pp. 247–269). Amsterdam: Rodopi. 
Rayson, P. (2008). From key words to key semantic domains. International Journal of Corpus Linguistics, 13(4), 519–549. 
Rayson, P., & Garside, R. (2000). Comparing corpora using frequency profling. In Proceedings of the workshop on Comparing Corpora, held in conjunction with the 38th annual meeting of the Association for Computational Linguistics (ACL 2000), 1–8 October 2000, Hong Kong (pp. 1–6). 
Rayson, P., & Wilson, A. (1996). The ACAMRIT semantic tagging system: Progress report. In L. 
J. Evett & T. G. Rose (Eds.), Language engineering for document analysis and recognition, LEDAR, AISB96 workshop proceedings (pp. 13–20). Brighton: Faculty of Engineering and Computing, Nottingham Trent University, UK. 
Rayson, P., Berridge, D., & Francis, B. (2004a, March 10–12). Extending the Cochran rule for the comparison of word frequencies between corpora. In Purnelle, G., Fairon, C., & Dister, 
A. (Eds.) Le poids des mots: Proceedings of the 7th International Conference on Statistical analysis of textual data (JADT 2004) (Vol. II, pp. 926–936), Louvain-la-Neuve: Presses Universitaires de Louvain. 
Rayson, P., Archer, D., Piao, S. L., & McEnery, T. (2004b). The UCREL semantic analysis system. In Proceedings of the workshop on beyond named entity recognition semantic labelling for NLP tasks in association with 4th international conference on language resources and evaluation (LREC 2004), 7–12. 25th may 2004, Lisbon, Portugal. Paris: European Language Resources Association. 
Scott, M. (1997). PC analysis of key words – And key key words. System, 25(2), 233–245. 
Scott, M. (2004). WordSmith tools. Version 4.0. Oxford: Oxford University Press. ISBN: 0-19­459400-9. 
Seale, C., Ziebland, S., & Charteris-Black, J. (2006). Gender, cancer experience and internet use: A comparative keyword analysis of interviews and online cancer support groups. Social Science & Medicine, 62, 2577–2590. 
Sinclair, J. (1991). Corpus, concordance, collocation. Oxford: Oxford University Press. Tono, Y., Yamazaki, M., & Maekawa, K. (2013). A frequency dictionary of Japanese. Routledge. Vasishth, S., & Nicenboim, B. (2016). Statistical methods for linguistic research: Foundational 
ideas – Part I. Lang & Ling Compass, 10, 349–369. https://doi.org/10.1111/lnc3.12201. Wasserstein, R. L., & Lazar, N. A. (2016). The ASA’s statement on p-values: Context, process, and purpose. The American Statistician, 70(2), 129–133. Wilson, A. (2013). Embracing Bayes factors for key item analysis in corpus linguistics. In 
New approaches to the study of linguistic variability. Language competence and language awareness in Europe (pp. 3–11). Frankfurt: Peter Lang. Wilson, A., & Rayson, P. (1993). Automatic content analysis of spoken discourse. In C. Souter & 
E. Atwell (Eds.), Corpus based computational linguistics (pp. 215–226). Amsterdam: Rodopi. 
Chapter 7 Analyzing Co-occurrence Data 
Stefan Th. Gries 
and Philip Durrant 
Abstract In this chapter, we provide an overview of quantitative approaches to co-occurrence data. We begin with a brief terminological overview of different types of co-occurrence that are prominent in corpus-linguistic studies and then discuss the computation of some widely-used measures of association used to quantify co-occurrence. We present two representative case studies, one exploring lexical collocation and learner profciency, the other creative uses of verbs with argument structure constructions. In addition, we highlight how most widely-used measures actually all fall out from viewing corpus-linguistic association as an instance of regression modeling and discuss newer developments and potential improvements of association measure research such as utilizing directional measures of associa­tion, not uncritically confating frequency and association-strength information in association measures, type frequencies, and entropies. 
7.1 Introduction 


7.1.1 General Introduction 
One of the, if not the, most central assumptions underlying corpus-linguistic work is captured in the so-called distributional hypothesis, which holds that linguistic elements that are similar in terms of their distributional patterning in corpora also 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_7) contains supplementary material, which is available to authorized users. 
S. Th. Gries (•) University of California Santa Barbara, Santa Barbara, CA, USA 
Justus Liebig University Giessen, Giessen, Germany e-mail: stgries@linguistics.ucsb.edu 
P. Durrant University of Exeter, Exeter, UK 
© Springer Nature Switzerland AG 2020 141 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_7 
S. Th. Gries and P. Durrant 
exhibit some semantic or functional similarity. Typically, corpus linguists like to cite Firth’s (1957:11) famous dictum “[y]ou shall know a word by the company it keeps” but Harris’s (1970:785f.) following statement actually makes the same case much more explicitly, or much more operationalizably: 
[i]f we consider words or morphemes A and B to be more different in meaning than A and C, then we will often fnd that the distributions of A and B are more different than the distributions of A and C. In other words, difference of meaning correlates with difference of distribution. 
That is, a linguistic expression E – a morpheme, word, construction/pattern, ... – can be studied by exploring what is co-occurring with E and how often. Depending on what the elements of interest are whose co-occurrence is studied, different terms have been used for such co-occurrence phenomena: 
• 
lexical co-occurrence, i.e. the co-occurrence of words with other words such as the strong preference of hermetically to co-occur with, or more specifcally, be followed by, sealed, is referred to as collocation; for collocations, it is important to point out the locus of the co-occurrence and Evert (2009:1215) distinguishes between (i) surface co-occurrence (words that are not more than a span/window size of s words apart from each other; often s is 4 or 5), (ii) textual co-occurrence (words in the same clause, sentence, paragraph, ...), and (iii) syntactic co-occurrence (words in a syntactic relation); 

• 
lexico-grammatical co-occurrence, i.e. the co-occurrence of words with gram­matical patterns or constructions such as the strong preference of the verb regard to be used in the passive of the as-predicative (e.g., The Borg were regarded as the greatest threat to the Federation) is referred to as colligation or collostruction (see McEnery et al. 2006:11 or Stefanowitsch and Gries 2003).1 


Different studies have adopted different views on how collocation in particular, but also co-occurrence more broadly, should be approached – how many elements are considered (two or more)? Do we need a minimum observed frequency of occur­rence in some corpus? Is a certain degree of unpredictability/idiosyncrasy that the co-occurrence exhibits a necessary condition for collocation status? etc. Also, co-occurrence applications differ in their retrieval procedures: Studies that target a word or a construction may retrieve all instances of the word/construction in question and explore its co-occurring elements; other studies might approach a corpus with an eye to identify all (strong) collocations for lexicographic, didactic, contrastive, or other purposes. For the sake of generality, we will discuss here a somewhat atheoretical notion of co-occurrence that eschews commitments regarding all of the above questions and is based only on some mathematical relation between the observed co-occurrence and non-co-occurrence frequencies of l elements in a corpus; it goes without saying that different research questions or practical applications may require one or more commitments regarding the above questions (see Bartsch (2004), Gries 
1We are ignoring the lexico-textual co-occurrence sense of colligation here. 
7 Analyzing Co-occurrence Data 143 (2008b), and Evert (2009) for more discussion of the parameters underlying co-
occurrence and their historical development). 
The simplest possible way to explore a linguistic element (such as hermetically or regard) would be by raw co-occurrence frequency – how often do I fnd the collocation hermetically sealed in my corpus? – or, more likely, conditional probabilities such as p(contextual element(s)|E) – how likely is a verbal construction to be an as-predicative when the verb in the construction is regard? 
While obtaining sorted frequency lists that reveal which collocates or construc­tions occur most often or are most likely around an element E is straightforward, much corpus-linguistic research has gone a different route and used more complex measures to separate the wheat (linguistically revealing co-occurrence data) from the chaff (the fact that certain function words such as the, of,or in occur with everything a lot, qua their overall high frequency. Such measures are often referred to as association measures (AMs) simply because they, typically, quantify the strength of mutual association between two elements such as two words or a word and a construction. In the following section, we discuss fundamental aspects of the computation of some of the most widely-used AMs. 
7.2 Fundamentals 
For decades now, AMs have typically been explained on the basis of co-occurrence tables of the kind exemplifed in Table 7.1, which contain observed frequencies of (co-)occurrence of a linguistic expression E (for instance a particular word) and one of the l types of contextual elements X (e.g. other words or constructions X1-l can occur with/in); for instance, if the ditransitive construction is attested with l = 80 verb types in a corpus, one would generate 80 such co-occurrence tables. In each such table, cell a is the frequency with which E is observed with/in element X, cell b is the frequency with which E is observed without X, this means the overall frequency of E is a + b, etc. Often, such a table would also contain or at least refer to the corresponding expected frequencies in the same cells a to d, i.e. the frequencies with which X and E would be observed together and in isolation if their occurrences were completely randomized; these frequencies are computed from the row and column totals as indicated in Table 7.1 as they would be for, say, a chi-squared test. 
Table 7.1 Schematic co-occurrence frequency table 

Co-occurring element X  Other elements (not X)  Row totals  
Element E  Obs.: a Exp.: (a + b) × (a + c)/n  Obs.: b Exp.: (a + b) × (b + d)/n  a + b  
Other elements (not E)  Obs.: c Exp.: (c + d) × (a + c)/n  Obs.: d Exp.: (c + d) × (b + d)/n  c + d  
Column totals  a + c  b + d  a + b + c + d = n  

S. Th. Gries and P. Durrant 
Table 7.2 Co-occurrence frequencies of regard and the as-predicative in Gries et al. (2005) 
As-predicative  Other constructions  Row totals  
Regard  80 exp.: 99 × 687/138,664  19 exp.: 99 × 137,977/138,664  99  
Other verbs  607 exp.: 138,565 × 687/138,664  137,958 exp.: 138,565 × 137,977/138,664  138,565  
Column totals  687  137,977  138,664  

As mentioned above, such a co-occurrence table is generated for every element type X1-l ever occurring with E at least once or, if the element analyzed is X, then such a co-occurrence table is generated for every element type E1-l ever occurring with X at least once. For instance, if one studied the as-predicative construction, then X might be that construction and elements E1-l could be all verbs occurring in that construction at least once and one could use the values in each of the l tables to compute an AM for every one of the l verb types of E co-occurring in X to. These results could then be used to, for instance, rank-order them by strength of attraction and then study them, which is often interesting because of how expressions that co-occur with X reveal structural and/or functional characteristics of E (recall the Firth and Harris quotes from above). 
A large number of AMs has been proposed over the last few decades, including 
(i) 
measures that are based on asymptotic or exact signifcance tests, (ii) measures from, or related to, information theory, (iii) statistical effect sizes, various other measures or heuristics; Evert (2009) and Pecina (2010) discuss more than altogether 80 measures and since then even more measures have been proposed. However, the by far most widely-used measures are (i) the loglikelihood measure G2 (which is somewhat similar to the chi-squared test and, thus, the z-score, and which is highly correlated with the p-value of the Fisher-Yates exact test as well as the t-score), 

(ii) 
the pointwise Mutual Information (MI), (iii) the odds ratio (and/or its logged version), which are all exemplifed here on the basis of the frequencies in Table 7.2 of the co-occurrence of regard and the as-predicative in the British Component of the International Corpus of English reported in Gries et al. (2005). 


4
(1) G2 = 2 i=1obs × logobs ˜ 762.196
exp 
a-vaexp 80-0.49
(2) t = =v˜ 8.889 
a 80 
(3) pointwise Mutual Information = log2 a = log2 80 ˜ 7.349 
aexp 0.49 
a 80 607 a
(4) odds ratio= /c == /b ˜ 956.962 (log odds ratio ˜ 6.864)
bd 19 /137958 cd 
All four measures indicate that there is a strong mutual association between X (the as-predicative) and E (regard); if one computed the actual p-value following from this G2, one would obtain a result of p <10-167.2 However, this sentence also points to what has been argued to be a shortcoming of these measures: The fact that they quantify mutual attraction means that they do not distinguish between different kinds of attracted elements: 
• 
instances of collocations/collostructions where X attracts E but E does not attract X (or at least much less so); 

• 
instances where E attracts X but X does not attract E (or at least much less so); 

• 
instances where both elements attract each other (strongly). 

Based on initial discussion by Ellis (2007), Gries (2013a) has shown that each of these three kinds of collocations is common among the elements annotated as multi-word units in the British National Corpus: 

• 
according to or upside down are examples of the frst kind: If one picks any bigram that has to or down as its second word, it is nearly impossible to predict which words will precede it, but if one picks any bigram with according or upside as the frst word, one is quite likely to guess the second one correctly; 

• of course or for instance are examples of the second kind: If one picks any bigram with of or for as the frst word, it is nearly impossible to predict which word will follow, but if one picks any bigram with course or instance as the frst word, one is quite likely to guess that of or for are the frst word correctly; 

• 
Sinn Fein and bona fde are examples of the third kind: each word is very highly predictive of the other. 


Crucially, all of the above examples are highly signifcant – in the spoken part of the BNC, all have G2-values of >178 and p-values of <10–40 – but they are clearly different in the structure of the association between the two words, which none of the measures in (3) to (4) (can) reveal. This may in turn be the reason why it is not uncommon to fnd that bi-directional AMs are not as highly correlated with uni-directional psycholinguistic gold standard data such as reaction times or elicitation tasks (see, e.g., Mollin 2009). Therefore, one proposed ‘fx’ to research on co-occurrence phenomena has been to rely less on bi-directional, or symmetric, AMs, but rather use uni-directional, or asymmetric, ones such as simple conditional probabilities (see (5)) or the P measures (see (6)). 
a 80
(5) a. pE|X == 687 ˜ 0.116 
a+c a 80
b. pX|E = =˜ 0.808 
a+b 99 
2While AMs often agree fairly well in their assessment of the degree of attraction between two elements (or at least their overall ranking), their computation can lead to them having different ‘preferences’. For instance, pointwise MI is known to return low-frequency but perfectly predictive collocations (e.g. fxed expressions) whereas measures that are ultimately based on signifcance tests (such as G2 or t) often rank more frequent items higher; see Evert (2009) for more discussion. 
S. Th. Gries and P. Durrant 
ab 80 19
(6) a. PE|X =-= 687 -˜ 0.116 
a+cb+d 137977 ac 80 607
b. PX|E = a+b - c+d = 99 - 138565 ˜ 0.804 
As is obvious from the equations, P is essentially an adjusted conditional probability. In this case, and this is not atypical, the difference between the conditional probabilities and the corresponding P-values is quite small and may even seem to be negligible. However, P appears more useful for theoretical reasons (its exact form has proven useful in research on associative learning (Ellis 2007) and it seems reasonable that a co-occurrence percentage of an element (X) with another (E) gets normalized or adjusted by ‘what E does in general’ as most other AMs do anyway) as well as empirical reasons (it performed better than conditional probability in Schneider to appear). 
Another relevant issue is concerned with the fact that some AMs – in particular those ultimately related to signifcance tests such as G2, chi-squared, z, ... – confate frequency (how often are the elements X and E observed together and in isolation?) and effect size (how strong is the attraction between X and E?). From a seemingly moderate to high G2-value in isolation, it is not obvious whether that value refects medium frequencies of (co-)occurrence and high association or very high frequencies of (co-)occurrence and a medium degree of association. This also means that AM-values involving different overall frequencies (n in Table 7.1) such as from differently frequent elements and/or differently large corpora cannot be compared if the computation of the AM is sensitive to n: if multiplying all values of a table such as Table 7.1 or Table 7.2 changes the AM, differences between AMs cannot be interpreted meaningfully – one would need to use measures instead that keep frequency and association strength separate and only refect association strength (such as the odds ratio or P, see Chap. 5 for a similar recommendation regarding frequency and dispersion). 
As mentioned above, a fully-fedged application of AMs to co-occurrence can involve computing one or more AMs for each element E co-occurring with a fxed element X at least once and rank-ordering them. Often, studies focus on either the top t elements (with t taking on different number such as 20 or 100 depending on the application) or focus on all elements that meet a particular threshold value for the AM and/or also just the observed frequency value (e.g. a researcher might study only collocations whose MI-score is =3 and whose observed co-occurrence frequency a is =5; see e.g. Ackermann and Chen 2013). It is important to realize that such threshold values are hardly ever motivated by a robust theoretical or psycholinguistic perspective but are usually practical stop gaps; the goal of such a stop gap might be to trim down a list of co-occurrence elements by discarding collocates for which the evidence of the strength of attraction is more shaky (because the AM was computed on the basis of very few actual co-occurrences). In addition, some researchers focus on the actual numeric values of the AMs of X1-l whereas others might only focus on their ranks, as has been done in most collostructional research. Finally, some AMs have somewhat well-known characteristics: For instance, MI often returns very low-frequency but nearly deterministic co-occurrences (such as proper names) whereas t and G2 usually return higher-frequency co-occurrences, which has not only led some researchers to consider both the AM and the observed frequency a as mentioned above (to, for instance, avoid having to deal with many infrequent proper names 
returned by MI) but has also led them to use more than one AM with complementary 
characteristics (such as MI and t) at the same time. It is worth reiterating, however, 
that such decisions are typically pragmatically rather than theoretically motivated. We now discuss two representative studies in which many of the above method­
ological decisions are refected. 
Representative Study 1 
Durrant, P. 2014. Corpus frequency and second language learners’ knowledge of collocations. International Journal of Corpus Linguistics 19(4):443–477 
A prominent theme in recent second language research has been the learning of collocations. Historically, there has been a perception that second language learners fnd collocations diffcult to acquire. This led Wray (2002) to propose an infuential model which suggests that the mechanisms of L2 learning systematically focus learners on individual words and prevent them from acquiring collocations. Recent work appears to undermine this picture how­ever. There is evidence that second language learners do acquire collocations from exposure and that they make extensive use of such collocations in their language production (see Siyanova-Chanturia 2015 for a recent review). Within this work, there is evidence that different AMs offer different and complementary perspectives on collocation learning (Ellis et al. 2008; Durrant and Schmitt 2009; Bestgen and Granger 2014), which will need to be integrated to provide a rounded picture. 
Durrant (2014) is a recent example of such work, attempting to determine the relationships between second language learners’ knowledge of English collocations and various measures of collocation frequency and association. He re-analyzed the results of 19 different tests of collocation knowledge con­ducted in eight different countries, as identifed through a systematic review of the literature. Frequency and AMs for items on each test were retrieved from the British National Corpus (BNC) and the Corpus of Contemporary American (COCA) and correlated with the number of learners who answered the corresponding test items correctly. Correlations were summarized for the 19 tests through a meta-analysis (see Chap. 27). 
The study focuses in particular on how different measures of frequency and association differ in their ability to predict learner knowledge. The predictors assessed differed in fve key aspects: 
• the choice of corpus: frequency data were retrieved separately from the BNC and COCA, and from each of the main register-based sub-corpora of each, i.e. the fve written sub-corpora titled academic, fction, magazine, 
(continued) 
S. Th. Gries and P. Durrant 
newspaper and non-academic (this last appears in BNC only) and the spoken sub-corpus; 
• 
the choice of measure: collocations were quantifed in terms of raw frequency, t, MI, and conditional probability; 

• 
the span within which words had to appear to be counted towards an item’s frequency of collocation. Two spans were used: four words either side of the node and nine words either side of the node; 

• 
whether counts were based on lemmatized or non-lemmatized counts. In the former, for example, arguing strongly and argued strongly would both count as cases of the collocation argue strongly. In the latter, these counts would be kept separate; 

• 
to account for possible effects of the evenness of dispersion of a collocation within the corpus, each item was also quantifed with a DP value (see Chap. 5). 


A number of key fndings emerge from these data. First and overall, frequency and association data were found to be reliable predictors of learners’ knowledge of collocation. Frequency and t-values from COCA achieved correlations with knowledge of between . = 0.24 and . = 0.27. Second, there was a large difference in predictiveness between frequency and t, on the one hand, and MI and conditional probability on the other. For these foreign language learners, it seems that the number of times a collocation occurs is a far more important factor than the strength of association between components. This tallies with the psycholinguistic work of Ellis et al. (2008), who found that the accuracy and speed of processing of lexical bundles by ESL learners in the US was predicted by frequency but not by MI. In contrast, the accuracy and speed of processing of native speakers was predicted by MI but not by frequency. These fndings show with great clarity both the importance of the differences between different types of measures and the fact that no measure can, in any absolute sense, be regarded as ‘the best’. Different measures work suit different purposes and in some cases, using multiple, contrasting measures, can bring out important patterns that would be missed by the use of a single measure. 
Third, frequency data derived from COCA were substantially better predictors of knowledge than those from the BNC. Participants in the tests analyzed came from Denmark, France, Japan, Jordan, Saudi Arabia, Spain and Sweden. While it is possible that students in these settings are more infuenced by US than by British English, the impact of this is likely to have been marginal: Given the widespread view that learners’ overall knowledge of collocation is weak (see, e.g., Wray 2002), the idea that they have picked up a particular British or US ‘collocational accent’ (if such a thing exists) seems unlikely. A more plausible explanation is the more contemporary nature of 
(continued) 
COCA, which continues to be updated on a yearly basis. In contrast, the BNC includes mostly texts produced in the 1980s and 1990s. Since collocation is a highly context-sensitive phenomenon, it is likely that the 20–30 years which separate today’s students from the BNC texts will make it a less good guide to the sorts of language to which they are exposed. 
Fourth, within the two national corpora, there were also substantial differ­ences in the predictiveness of data from different registers. In both the BNC and COCA, fction showed the strongest correlation and academic writing the weakest. 
Finally, the dispersion of a collocation across a reference corpus had only a weak relationship with knowledge (more widely spread collocations were better recognized), and this relationship was signifcant only in the BNC. 
Representative Study 2 
Hampe, B. and Schefeld, D. 2006. Syntactic leaps or lexical varia­tion? – More on “Creative Syntax”. In Corpora in cognitive linguistics: corpus-based approaches to syntax and lexis, eds. Gries, S.T., and Stefanowitsch, A., 127–157. Berlin & New York: Mouton de Gruyter. 
Hampe and Schefeld (2006) use AMs to understand syntactic creativity. This refers to examples such as those in (7), in which a verb is used in an argument structure with which it is not usually associated. 
(7) a. Social media bore her stupid 
b. The boiler shuddered to a halt 
Hampe and Schefeld’s study asks how such instances should be accounted for in linguistic theory, comparing in particular two possible accounts. One, attributed to Goldberg (1995), holds that verbs maintain their usual meanings while inheriting syntactic slots and a generic meaning from the abstract argument-structure construction (ASC). This account is contrasted with Hampe and Schefeld’s own model, in which creative uses are described in terms of the syntactic blending of two verbal expressions. On this model, the unusual structure (e.g. Noun bored Adjective, as in (7a)) triggers the retrieval of another verbal concept which is more usually associated with the ASC (e.g. Noun makes Noun Adjective, as in Social media made her stupid). They argue that the intended meaning of the creative form is reached through conceptual integration or blending of the two concepts. 
Hampe and Schefeld evaluate the plausibility of these models through a detailed description, frst, of the typical verbal associates of a complex-transitive ASC and, second, of the syntactically creative uses of four verbs. 
(continued) 
S. Th. Gries and P. Durrant 
AMs – in particular, pFisher-Yates exact test (a measure very highly correlated with G2, see Sect. 7.2) – are central to both analyses. These analyses are particularly useful for illustrating the uses to which AMs can be put in that, though they rely on the same test, each makes use of a rather different type of association and for different purposes. The frst analysis looks at associations between an abstract construction and the verbs which instantiate it in order to understand the range of meanings which the construction can carry. The second looks at associations between verbs and their accompanying collocations in order to understand restrictions on the use of particular syntactically creative forms. 
The frst analysis focuses on the ASC illustrated above in (7a), i.e. constructions in which the verb is followed by a direct object and an adjectival phrase acting as object predicate (usually referred to in construction grammar as the resultative construction). Retrieving all cases of this construction from the syntactically parsed ICE-GB corpus, Hampe and Schefeld use the Fisher-Yates exact test to identify verbs which are associated with it. These verbs are taken to indicate the meanings in which the ASC is most characteristically used. The associated verbs are classifed into three semantic groups. The frst is most centrally characterized by make (the strongest associate of this ASC), which is used to indicate causation (as in He made John angry). Other associated verbs of this type are render, get, and set. Closely related to these are verbs most centrally represented by keep (as in She kept it safe), which indicate maintenance of a given state. Other examples include leave, hold, and have. The third group of verbs has fnd (after make, the second mostly strongly associated verb of the ASC) as its central example (as in He found her arrogant). The group is also represented by consider (as described above). This group is rather different from the frst two in that it cannot be classifed under a broad ‘resultative’ meaning by which the ASC has been characterized. These are cognition verbs which, Hampe and Schefeld observe, can be described as ‘attributive’, rather than ‘resultative’. They argue that these should be treated as distinct constructions, pointing out that the generic ASC described by Goldberg (1995) fails to provide the relevant semantics for the attributive uses. 
The second part of Hampe and Schefeld’s analysis explores syntactically creative uses of four verbs: encourage, support, bore and fear. In particular, they look at cases of these verbs in complex transitive patterns in which the direct object noun is followed by either a prepositional phrase (to create a ‘caused motion’ construction, e.g. encourage tourists into the area)oran adjective phrase (to create a ‘resultative’ or ‘attributive’ construction, e.g. the subject bores them stiff ). 
For three of the verbs – encourage, support, and fear – use in one of the searched constructions is rare (less than 1% of occurrences of the verb) 
(continued) 
and not listed as a possible form in the corpus-based Collins Cobuild English Language Dictionary. The resultative use of bore (as in He bore her stupid)is more common (accounting for around 7% of uses of the verb) and is listed in the dictionary. Importantly for our current focus on AMs, each verb’s syntactically creative use appears to come with collocational restrictions. That is to say, they are strongly associated with specifc accompanying words. As with the ASC-verb associations described above, strong collocates are identifed using the Fisher-Yates exact test. 
Hampe and Schefeld argue that the apparent restriction of these creative syntactic forms to particular lexical contexts cannot be accounted for in terms of the properties of the relevant ASCs alone. Taking as an example the case of fear and its strong association with dead and (to a lesser extent) with terms related to death (drowned, killed, murdered), they hypothesize that this use could be motivated by similar forms at different levels of abstraction. Specifcally, model verbs strongly associated with the attributive construction (most centrally, the verb fnd) provide a template for understanding events in which a feature or quality is attributed to the direct object. At a less abstract level, collocational restrictions can be explained by the collocates of the model verb. Hampe and Schefeld point out that the partially lexically-specifed sequence X (be) found dead appears to have served as a model for the feared dead pairing, which was shown to be the central instantiation of this form. The novel form feared dead, once instantiated, may in turn serve as a basis for creating similar forms (feared killed, feared murdered, etc.). 
Regardless of whether we ultimately accept Hampe and Schefeld’s conclusions, their paper demonstrates well how AMs can be used to identify two different types of patterning -attractions between abstract constructions and the verbs which instantiate them, and attractions between verbs and the collocates which accompany them. The former provides a way of understand­ing the meaning potential of a construction; the latter provides a way of understanding restrictions on use. As Hampe and Schefeld acknowledge, the purely textual nature of this analysis means that strong inferences about the nature of psycholinguistic representations and processes cannot be drawn. What these analyses do provide, however, is a clearer picture of the language use for which any linguistic model would need to account. This picture provides us with a basis both for forming linguistic hypotheses and for evaluating the prima facie plausibility of existing models. Most pertinently to our current topic, their use of AMs provides a granularity of description which cannot be convincingly provided through consideration of abstract syntactic forms or of vocabulary items alone, revealing additional levels of complexity in linguistic patterning and hence in the models that are required to explain it. 
S. Th. Gries and P. Durrant 

7.3 Critical Assessment and Future Directions 
Given its nature as a distributional discipline, the discussion of how to best approach the quantifcation and exploration of co-occurrence is likely to continue for the foreseeable future, in particular as corpus-linguistic methods are used in a wider range of theoretical frameworks and with a wider range of other kinds of data, be they observational, experimental, or simulation data. In this section, we are discussing a few areas that we feel should be on corpus linguists’ radar; they involve. 
• 
the recognition that much current discussion of AMs is more fragmented than it needs to be (Sect. 7.3.1); 

• 
candidates for measures that have so far not been explored but rather than just being yet even more different ways to crunch the same numbers, that offer additional advantages that current measures do not provide (Sect. 7.3.2); 

• 
additional pieces of information that virtually no current AM includes (Sect. 7.3.3). 




7.3.1 Unifying the Most Widely-Used AMs 
The above discussion presented AMs as they are typically discussed, namely based on seemingly unrelated mathematical formulae in turn based on 2 × 2 co-occurrence frequency tables such as Table 7.1 and Table 7.2. While this kind of presentation is nearly omnipresent and perhaps useful in particular for studies discussing very many AMs, it has one big disadvantage: The entirely differently-looking equations obfuscate the fact that the AMs that are used in probably 90% of all studies involving AMs – G2, MI, the odds ratio, P, and even t or z – can in fact all be unifed once a particular statistical perspective is adopted, namely that of (binary logistic) regression models. As discussed in Chap. 21, binary logistic regression is a statistical tool that allows the user to study the behavior of a dependent variable (e.g., the presence of a verb: any verb vs. regard) as a function of one or more predictors (e.g., the choice of a construction: any construction vs as-predicative). The results of binary logistic regression models are similar to those of the maybe more straightforward linear regression models and include the following: 
•an intercept: log odds of the predicted level of the dependent variable (the second, 
i.e. regard) when the predictor is the frst level (i.e. any construction); 
•a 
coeffcient: the change in log odds of the predicted level of the dependent vari­able (regard) when the predictor becomes the second level (i.e. as-predicative); 

• 
the intercept and the coeffcient can then be used to compute predicted probabil­ities of the two levels of the dependent variable; 

•a 
signifcance test of the overall regression model, which, in the case of a model 


with only one predictor, is also the signifcance test of that predictor (see also Gries 2013b: Section 5.3). 
Space does not permit a detailed discussion and exemplifcation here in prose; for detailed code, computations, and results in R, see the companion code fle. Suffce it to say here, that 
• 
G2 is the difference between a regression model that predicts the use of E (any verb vs. regard)given X (any construction vs. the as-predicative) and a null model that predicts the use of E (any verb vs. regard) given no other information; 

• 
the odds ratio is the exponentiated coeffcient in the regression model; 

• 
MI is log2 of the predicted probability of E being regard happening when X is the as-predicative divided by the probability of E being regard in general; etc. 


More interestingly, PConstruction . Verb, the adjusted conditional probability measure from (6), is simply the difference between the predicted probabilities of regard being used with and without the as-predicative being present. 
To reiterate, while corpus-linguistic research into the association between ele­ments has produced dozens of AMs, the frequencies of their use is as Zipfan­distributed as that of words: While there is still a lively discussion of which measure(s) is/are most useful for which specifc purpose, a mere handful of (symmetric) measures are used in the vast majority of studies. However, there is now more recognition that at least the symmetry-of-association assumption built into most AMs used is problematic and more uni-directional/asymmetric measures are being explored now. The still intense discussion of AMs notwithstanding, it is instructive to realize that all the most frequent measures – uni-and bi-directional ones – are really only different parts/facets of a simple binary logistic regression trying to predict the realization of one element based on another: Once that is realized, all the seemingly different AMs can be captured under one and the same approach (which is of course part of the reason why many AMs are very highly correlated with each other); not only does that facilitate their teaching, it also naturally bridges the gap between AMs on the one hand and hundreds of regression-based studies of alternation phenomena in sociolinguistics or usage-based linguistics or over-/underuse studies in learner corpus research (see Gries 2018). 
7.3.2 Additional (Different) Ways to Quantify Basic Co-occurrence 
As mentioned above, the number of AMs that have been proposed is vast and, ironically speaking, inversely proportional to the number of rigorous and compar­ative evaluations of many of AMs, which is why it may seem futile to add new measures to the mix. However, Baayen (2011) makes two suggestions regarding 
S. Th. Gries and P. Durrant 
how to quantify (directional) co-occurrence that nonetheless appear attractive and merit mention because of how they offer avenues of research or analysis that are as promising as they are underexplored. 
The frst of these is to use as an AM another general information-theoretic measure, namely the Kullback-Leibler (KL) divergence. The KL divergence is written as DKL (posterior/data || prior/theory), which refers to how much a posterior/data percentage distribution of an element (e.g., E) in the presence of another element (e.g., X) diverges from the overall/theoretical overall percentage distribution of E; it is computed as in (8). Eq. (9) shows the reverse perspective: how the percentage distribution of X in the presence of E diverges from X’s overall percentage distribution (in both equations, log20:=0)3: 
DKL (p (E|X)  p(E))
(8) 
aa×nc c×n
=× log2 +× log2 ˜ 0.699 
a+c (a+b)×(a+c) a+c (a+c)×(c+d) 
DKL (p (X|E)  p(X))
(9) 
aa×nb b×n
=× log2 +× log2 ˜ 5.483 
a+b (a+b)×(a+c) a+b (a+b)×(b+d) 
With a bit of simplifcation, this shows that the presence of regard says much more about the presence of the as-predicative than the presence of the as-predicative says about the presence of regard (because 5.483> > 0.699), which is more/different evidence that the distribution in Table 7.2 is better quantifed with uni-directional measures.4 The two versions of this measure are fairly highly correlated with P (r > 0.86 in as-predicative data, for instance, and > 0.8 in Baayen’s 2011 comparison of multiple AMs), but an attractive feature of DKL is that (i) it is a measure that has interdisciplinary appeal given the wide variety of uses that information-theoretical concepts have and (ii) it can also be used for other corpus-linguistically relevant phenomena such as dispersion (see Chap. 5), thus allowing the researcher to use one and the same metric for different facets of co-occurrence data. 
Baayen’s second proposal is to use the varying intercepts of the simplest kind of mixed-effects model (see Chap. 22). Essentially, for the as-predicative data from Table 7.2 used as an example above, this approach would require as input a data frame in the case-by-variable format, i.e. with 138,664 rows (one for each construction) and two columns (one with the constructional choices (as-predicative vs. other), one with all verb types (regard, see, know, consider, ..., other)in the data. Then, one can compute a generalized linear mixed-effects model in which one determines the basic log odds of the as-predicative (-3.4214) but, more crucially, also how each verb affects the log odds of the as-predicative differently, which refects its association to the as-predicative. These values are again positively correlated with, say, Ps, but the advantage they offer is that, because they too derive from the unifed perspective of the more powerful/general 
3The Kullback-Leibler divergence is also already mentioned in Pecina (2010). 4See Michelbacher et al. (2007, 2011)and Gries(2013a) for further explorations of uni­directional/asymmetric measures. 
approach of regression modeling, they allow researchers to effortlessly include other predictors in the exploration of co-occurrence. For instance, the as-predicative is not only strongly attracted to verbs (such as regard, hail, categorize, ...) but also to the passive voice. However, traditional AM analysis does usually not consider additional attractors of a word or a construction, but within a regression framework those are more straightforward to add to a regression model than just about any other method. 
In sum, AM research requires more exploration of measures that allow for elegant ways to include more information in the analysis of co-occurrence phenomena. 

7.3.3 Additional Information to Include 
Another kind of desiderata for future research involves the kind of input to analyses of co-occurrence data. So far, all of the above involved only token frequencies of (co-)occurrence, but co-occurrence is a more multi-faceted phenomenon and it seems as if the following three dimensions of information are worthy of much more attention than they have received so far (see Gries 2012, 2015, 2019 for some discussion): 
• 
type frequencies of co-occurrence: current analyses of co-occurrence based on tables such as Table 7.2 do not consider the number of different types that make up the frequencies in the cells b (19) and c (607) even though it is well-known that type frequency is correlated with many linguistic questions involving productivity, learnability, and language change. So far, the only AM that has ever been suggested to involve type frequencies is Daudaravi.cius and Marcinkevi.cien.e’s (2004) lexical gravity, but there are hardly any studies that explore this important issue in more detail (one case in point is Gries and Mukherjee 2010); 

• 
entropies of co-occurrence: similarly to the previous point, not only do studies not consider the frequencies of types with which elements co-occur, they therefore also do not consider the entropies of these types, i.e. the informativity of these frequencies/distributions. Arguably, distributions with a low(er) entropy would refect strong(er) associations whereas distributions with a high(er) entropy would refect weak(er) associations. Since entropies of type frequencies are relevant to many aspects of linguistic learning and processing (see Goldberg et al. 2004; Linzen and Jaeger 2015; or Lester and Moscoso del Prado 2016), this is a dimension of information that should ultimately be added to the corpus linguist’s toolbox. 

• 
dispersion of co-occurrence (see Gries 2008a, Chap. 5): given how any kind of AM is based on co-occurrence frequencies of elements in a corpus, it is obvious that the AMs are sensitive to underdispersion. Co-occurrence frequencies as entered into tables such as Table 7.2 may yield very unrepresentative results if 


S. Th. Gries and P. Durrant 
they are based on only very small parts of the corpus under investigation. For instance, Stefanowitsch and Gries (2003) fnd that the verbs fold and process are highly attracted to the imperative construction in the ICE-GB, but also note that fold and process really only occur with the imperative in just a single of the 500 fles of the ICE-GB – the high AM scores should therefore be taken with a grain of salt and dispersion should be considered whenever association is. 
To conclude, from our above general discussion and desiderata, one main take-home message should be that, while AMs have been playing a vital role for the corpus-linguistic analysis of co-occurrence, much remains to be done lest we continue to underestimate the complexity and multidimensionality of the notion of co-occurrence. Our advice to readers would be 
• 
to familiarize themselves with a small number of ‘standard’ measures such as G2, MI, and t;but 

• 
to also immediately begin to learn the very basics of logistic regression modeling to (i) be able to realize the connections between seemingly disparate measures as well as (ii) become able to easily implement directional measures when the task requires it; 

• 
to develop even the most basic knowledge of a programming language like R to avoid being boxed in into what currently available tools provide, which we will briefy discuss in the next section. 


7.4 Tools and Resources 
While co-occurrence is one of the most fundamental notions used in corpus linguistics, it is not nearly as widely implemented in corpus tools as it should be. This is for two main reasons. First, existing tools offer only a very small number of measures, if any, and no ways to implement new ones or tweak existing ones. For instance, WordSmith Tools offers MI and its derivative MI3, t, z, G2, and a few less widely-used ones (from WordSmith’s website) and AntConc offers MI, G2, and t (from AntConc’s website). While this is probably a representative section of the most frequent AMs, all of these are bidirectional, for instance, which limits their applicability for many questions. Second, these tools only provide AMs for what they ‘think’ are words, which means that colligations/collostructions and many other co-occurrence applications cannot readily be handled by them. As so often and as already mentioned in Chap. 5, the most versatile and powerful approach to exploring co-occurrence is with programming languages such as R or Python, because then the user is not restricted to lexical co-occurrence and dependent on measures/settings enshrined in ready-made software black boxes, but can customize an analysis in exactly the way that is needed; some very rudimentary exemplifcation can be found in the companion code fle to this chapter; also, see http://collocations. de for a comprehensive overview of many measures. 
Further Reading 
Pecina, P. 2010. Lexical association measures and collocation extraction. Lan­guage Resources and Evaluation 44(1):137–158. 
Pecina (2010) appears to be the most comprehensive overview of corpus-and computational-linguistic AMs focusing on automatic collocation extraction. In this highly technical paper, 82 different AMs are compared with regard to how well they identify true collocations in three scenarios (kinds of corpus data) and evaluated on the basis of precision-recall curves, i.e. curves that determine 
precision (true positives (correctly identifed collocations)/all positives (all identifed collocations)) and 
recall (true positives/all trues (collocations to be found)) values for every possible threshold value an AM would allow for. For two of the three kinds of corpus data, measures that can be assumed to be unknown to most corpus linguists score the highest mean average precision (cosine context similarity and the unigram subtuple measure); for the largest data set, the better-known pointwise MI scores second highest, and some other well-known measures (including z and the odds ratio) score well in at least one scenario. 
Wiechmann, D. 2008. On the computation of collostruction strength: testing measures of association as expression of lexical bias. Corpus Linguistics and Linguistic Theory 4(2):253–290. 
Wiechmann (2008) also provides a wide-ranging empirical comparison of associ­ation measures, specifcally those pertaining to collostruction. He focuses on how well various measures of collostruction strength predict the processing of sentences in which a noun phrase is temporarily ambiguous between being a direct object (The athlete revealed his problem because his parents worried) and the subject of a subordinate clause (The athlete revealed his problem worried his parents)using cluster and regression analyses. 
References 
Ackermann, K., & Chen, Y. H. (2013). Developing the academic collocation list (ACL) – A corpus-driven and expert-judged approach. Journal of English for Academic Purposes, 12(4), 235–247. Baayen, R. H. (2011). Corpus linguistics and naive discriminative learning. Brazilian Journal of Applied Linguistics, 11(2), 295–328. Bartsch, S. (2004). Structural and functional properties of collocations in English. Tingen: NARR. 
Bestgen, Y., & Granger, S. (2014). Quantifying the development of phraseological competence in L2 English writing: An automated approach. Journal of Second Language Writing, 26(4), 28–41. 
Daudaravi. cien.
cius, V., & Marcinkevi. e, R. (2004). Gravity counts for the boundaries of colloca­tions. International Journal of Corpus Linguistics, 9(2), 321–348. Durrant, P. (2014). Corpus frequency and second language learners’ knowledge of collocations. International Journal of Corpus Linguistics, 19(4), 443–477. 
S. Th. Gries and P. Durrant 
Durrant, P., & Schmitt, N. (2009). To what extent do native and non-native writers make use of collocations? International Review of Applied Linguistics, 47(2), 157–177. 
Ellis, N. C. (2007). Language acquisition as rational contingency learning. Applied Linguistics, 27(1), 1–24. 
Ellis, N. C., Simpson-Vlach, R., & Maynard, C. (2008). Formulaic language in native and second-language speakers: Psycholinguistics, corpus linguistics, and TESOL. TESOL Quarterly, 1(3), 375–396. 
Evert, S. (2009). Corpora and collocations. In A. Leling & M. Kyt(Eds.), Corpus linguistics: An international handbook (Vol. 2, pp. 1212–1248). Berlin/New York: Mouton De Gruyter. 
Firth, J. R. (1957). A synopsis of linguistic theory 1930–55. Reprinted in Palmer FR (Ed.), (1968) Selected papers of J.R. Firth, 1952–1959. Longman, London. 
Goldberg, A. E. (1995). Constructions: A construction grammar approach to argument structure. Chicago: University of Chicago Press. 
Goldberg, A. E., Casenhiser, D. M., & Sethuraman, N. (2004). Learning argument structure generalizations. Cognitive Linguistics, 15(3), 289–316. 
Gries, S. Th. (2008a). Dispersions and adjusted frequencies in corpora. International Journal of Corpus Linguistics, 13(4), 403–437. 
Gries, S. Th. (2008b). Phraseology and linguistic theory: A brief survey. In S. Granger & F. Meu­nier (Eds.), Phraseology: An interdisciplinary perspective (pp. 3–25). Amsterdam/Philadel­phia: John Benjamins. 
Gries, S. Th. (2012). Frequencies, probabilities, association measures in usage-/exemplar-based linguistics: Some necessary clarifcations. Studies in Language, 36(3), 477–510. 
Gries, S. Th. (2013a). 50-something years of work on collocations: What is or should be next .... International Journal of Corpus Linguistics, 18(1), 137–165. 
Gries, S. Th. (2013b). Statistics for linguistics with R (2nd rev. & ext. ed) De Gruyter Mouton: Boston/New York. 
Gries, S. Th. (2015). More (old and new) misunderstandings of collostructional analysis: On Schmid & Khenhoff (2013). Cognitive Linguistics, 26(3), 505–536. 
Gries, S. Th. (2015). 15 years of collostructions: some long overdue additions/corrections (to/of actually all sorts of corpus-linguistics measures). International Journal of Corpus Linguistics, 24(3), 385–412. 
Gries, S. Th. (2018). On over-and underuse in learner corpus research and multifactoriality in corpus linguistics more generally. Journal of Second Language Studies, 1(2), 276–308. 
Gries, S. T. (2019). 15 years of collostructions: Some long overdue additions/corrections (to/of actually all sorts of corpus-linguistics measures). International Journal of Corpus Linguistics, 24, 385. 
Gries, S. Th., & Mukherjee, J. (2010). Lexical gravity across varieties of English: An ICE-based study of n-grams in Asian Englishes. International Journal of Corpus Linguistics, 15(4), 520– 548. 
Gries, S. Th., Hampe, B., & Schefeld, D. (2005). Converging evidence: Bringing together experimental and corpus data on the association of verbs and constructions. Cognitive Linguistics, 16(4), 635–676. 
Hampe, B., & Schefeld, D. (2006). Syntactic leaps or lexical variation? – More on “Creative Syntax”. In S. T. Gries & A. Stefanowitsch (Eds.), Corpora in cognitive linguistics: Corpus-based approaches to syntax and lexis (pp. 127–157). Berlin/New York: Mouton de Gruyter. 
Harris, Z. S. (1970). Papers in structural and transformational linguistics. Dordrecht: Reidel. 
Lester, N. A., & Moscoso del Prado, M. F. (2016). Syntactic fexibility in the noun: Evidence from picture naming. In A. Papafragou, D. Grodner, D. Mirman, & J. C. Trueswell (Eds.), Proceedings of the 38th annual conference of the cognitive science society (pp. 2585–2590). Austin: Cognitive Science Society. 
Linzen, T., & Jaeger, T. F. (2015). Uncertainty and expectation in sentence processing: Evidence from subcategorization distributions. Cognitive Science, 40(6), 1382–1411. 
McEnery, T., Xiao, R., & Tono, Y. (2006). Corpus-based language studies: An advanced resource book. Oxon/New York: Routledge. 
Michelbacher, L., Evert, S., & Schze, H. (2007). Asymmetric association measures. International Conference on Recent Advances in Natural Language Processing. 
Michelbacher, L., Evert, S., & Schze, H. (2011). Asymmetry in corpus-derived and human word associations. Corpus Linguistics and Linguistic Theory, 7(2), 245–276. 
Mollin, S. (2009). Combining corpus linguistic and psychological data on word co-occurrences: Corpus collocates versus word associations. Corpus Linguistics and Linguistic Theory, 5(2), 175–200. 
Pecina, P. (2010). Lexical association measures and collocation extraction. Language Resources and Evaluation, 44(1), 137–158. 
Schneider, U. (to appear). Delta P as a measure of collocation strength. Corpus Linguistics and Linguistic Theory. 
Siyanova-Chanturia, A. (2015). Collocation in beginner learner writing: A longitudinal study. System, 53(4), 148–160. 
Stefanowitsch, A., & Gries, S. T. (2003). Collostructions: Investigating the interaction between words and constructions. International Journal of Corpus Linguistics, 8(2), 209–243. 
Wray, A. (2002). Formulaic language and the lexicon. Cambridge: Cambridge University Press. 
Chapter 8 Analyzing Concordances 
Stefanie Wulff and Paul Baker 
Abstract In its simplest form, a concordance is a list of all attestations (or hits) of a particular search word or phrase, presented with a user-defned amount of context to the left and right of the search word or phrase. In this chapter, we describe how to generate and manipulate concordances, and we discuss how they can be employed in research and teaching. We describe how to generate, sort, and prune concordances prior to further analysis or use. In a section devoted to qualitative analysis, we detail how a discourse-analytical approach, either on the basis of unannotated concordance lines or on the basis of output generated by a prior quantitative examination of the data, can help describe and, crucially, explain the observable patterns, for instance by recourse to concepts such as semantic prosody. In a section devoted to quantitative analysis, we discuss how concordance lines can be scrutinized for various properties of the search term and annotated accordingly. Annotated concordance data enable the researcher to perform statistical analyses over hundreds or thousands of data points, identifying distributional patterns that might otherwise escape the researcher’s attention. In a third section, we turn to pedagogical applications of concordances. We close with a critical assessment of contemporary use of concordances as well as some suggestions for the adequate use of concordances in both research and teaching contexts, and give pointers to tools and resources. 
S. Wulff (•) University of Florida, Gainesville, Florida, USA e-mail: swulff@uf.edu 
P. Baker Lancaster University, Lancaster, UK e-mail: j.p.baker@lancaster.ac.uk 
© Springer Nature Switzerland AG 2020 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_8 
S. Wulff and P. Baker 
8.1 Introduction 
In this chapter, we describe how to generate, manipulate, and analyze concordances. In its simplest form, a concordance is “a list of all the occurrences of a particular search term in a corpus, presented within the context that they occur in; usually a few words to the left and the right of the search term” (Baker 2006: 71). The specifed amount of context is often also referred to as the “(context) window/span”. Sometimes, concordances are called “key words in context”, or KWIC, displays, with “key word” referring to the search word or phrase; this is not to be confused with another use of the term “key word” in corpus linguistics as the words or phrases that are statistically distinctive for a certain corpus sample (see Chap. 6). The term concordance usually refers to the entire list of hits, although sometimes researchers refer to a single line from the list as a concordance. In this chapter, to avoid confusion, we refer to a concordance as the list of citations, distinguishing it from a concordance line, which is a single citation. We have used a range of concordancing tools to create this chapter, but for consistency, we have formatted the concordances in a standard way. 
Figure 8.1 is an example of a concordance for the search terms refugee and refugees in the British National Corpus (BNC), with a context window of 100 characters around the search terms. The BNC contains data from the late 20th century. Overall, refugee and refugees occur 2,733 times in the BNC, so displaying the entire concordance is not an option here–instead, Fig. 8.1 displays only a snippet of 15 concordance lines. 
The typical layout of concordances–the search term in the middle; some context around it; the left-hand context aligned fush right and the right-hand context aligned fush left–is meant to make it easier to inspect many examples at one glance in order 
L match R 
The paper also said CBS identified  refugees  walking on the Pakistani border as  
crossing into Czechoslovakia, from where most  refugees  are fleeing to the West. The  
last of eight special trains bringing 7,600  refugees  from the West German embassy in  
adjoining the embassy from which the  refugees  have clambered into the grounds. Some  
have clambered into the grounds. Some  refugees  in the embassy gardens said they  
but had not boarded the sealed  refugee  trains travelling through East Germany to  
numerous Czechoslovaks who befriended the  refugees  He said the debacle had been  
is cracking.’ The determination of the  refugees  despite the freezing and insanitary conditions  
business in cafes and restaurants because  refugees  were crammed like sardines on mattresses  
to Prague to convince the embassy  refugees  it was not a trap. Their  
Righting the Record By Brian Dooley 29  Refugees  Jumping the Hurdles By Jan Shaw  
urging them to ensure that Iraqi  refugees  then under the control of United  
hat it strongly feared that the  refugee  population would be at risk of  
and her sick child at Isikveren  refugee  camp on the Iraq/Turkish border  
Right of Asylum and help political  refugees  to find work.’ Adherence to this  
Fig. 8.1 Concordance snippet of 15 attestations of refugee|refugees in the BNC 


to identify patterns around the search term that might escape one’s notice if one were reading the examples as one running text. In Fig. 8.1, it is in fact diffcult to see recurring patterns, mainly because we are looking only at 15 examples. There are ways, however, to make patterns more easily visible by sorting the concordance, which we turn to next. For now, we can examine the snippet in Fig. 8.1 and already glean some information about how refugee(s) are talked about. For example, we learn about their location: the word embassy is mentioned four times; trains and camp also appear in the context window describing the location of refugee(s). Similarly, looking at the verb predicates following refugee(s), we get a glimpse of the actions associated with them: walking, feeing, clambered, and crammed are four examples. 

8.2 Fundamentals 



8.2.1 Sorting and Pruning Concordances 
While concordances are typically formatted slightly differently from regular run­ning text, it can be diffcult to see patterns because the attestations in a simple concordance as shown in Fig. 8.1 are in chronological order of their occurrence in the corpus. For that reason, most concordance tools have an option to widen the context window, and/or to sort the concordance display according to the words in the left and/or right-hand context. For example, we could choose to sort the concordance according to the frst word to the right of the search word so that we see the immediate right-hand collocates (also called “R1”) of refugee(s) in alphabetical order, which makes it easy to see if there are potential collocates prominently following. Alternatively, we could sort the concordance by the word immediately preceding refugee(s) (the L1 collocates), which would give us an idea of what words, if any, likely modify refugee(s). Below are two examples. Figure 8.2 shows another 15-line snippet from the same concordance for refugee(s) in the BNC that was sorted in a more useful way: according to the frst word to the right (R1) of refugee(s), then nested in a sort according to the second word to the right (R2) of refugee(s), and nested in a sort according to the third word to the right (R3) of refugee(s). This particular snippet shows that one of the words immediately to the right of refugee is camps, followed by the preposition in as the second word to the right, followed in the slot in third position to the right by a place name. Generalizing across these examples, we can say that one frequent pattern containing refugee(s)is [refugee camps in PLACENAME]. Other R1-collocates occurring at least 5 times or more often in the entire concordance include more specifc descriptions of refugees (children, communities, families, population), their movement between countries (exodus, migration), and societal evaluations of admitting refugees to one’s country (crisis, problem, situation). 
S. Wulff and P. Baker 
L match R 
1987, of whom nearly two-thirds were in principally refugee camps in Algeria. The two sides disagreed from Mozambiquan civil war, arriving at refugee camps in Bark and absolutely nothing smiles to the faces of children from refugee camps in Bosnia. (WES NEXT) They set the wife of the French President, to refugee camps in Diyarbakir in May 1989, but the have fled from their homes, some to refugee camps in Ethiopia, others to revert to and Breakfast at Tiffany's, recently toured refugee camps in famine -ravaged Somalia as a fears of a growing refugee problem. Five refugee camps in 1990 -91 handled 3,239 refugees Zagreb. But they were diverted instead to refugee camps in Hungary, where there were no There were similar jubilant scenes in refugee camps in Lebanon. Mr Khaled Al -Hassan before, Israeli troops pulled out of Palestinian refugee camps — in Leba non during the Israeli two young girls from one of the refugee camps in San Salvador were capt ured, we they can return home, either leaving the refugee camps in San Salvador or ending their Having seen the Shias in the refugee camps in south Iran and in south victims of attacks who are streaming into refugee camps in Thailand. According to H. B. were reported to have fled to temporary refugee camps in Thailand. As part of the 
L match R 
Fig. 8.2 Concordance snippet of 15 attestations of refugee|refugees in the BNC sorted by R1/R2/R3 

priority to reintroducinglegislation to curb bogus  refugees  The new Parliamentary session, to be  
The Asylum Bill, designed to curb bogus  refugees  was abandoned with the dissolution of  
; reintroducing the Asylum Bill curbing bogus  refugees  and asylum seekers; legislation to pave the  
TIDE LABOUR WOULD LET IN — Bogus  refugees  will grab state handouts’, complete with  
Kinnock ‘won't curb flood of bogus  refugees  ’.’ In other parts of Britain, the far  
than the so-called rights of bogus  refugees  Mr. Baker The purpose of the Bill  
Bill, designed to crack down on bogus  refugees  A new national lottery to aid sport,  
page lead article: ‘SCANDAL OF THE BOGUS  REFUGEES  --80% cheat their way into Britain and the  
a headline in The Times today, bogus  refugees  bleed Britain of £100 million through  
refugees along the Iraqi-Turkish border; 10,000  refugees  were reported already to have crossed into  
Refugee ban A CHARITY behind a Bosnian  refugee  rescue mission accused the Home Office of  
the newrecruit to Class 1 — a Bosnian  refugee  whose name no one could pronounce.  
Harrison said that we should accept Bosnian  refugees  because they have lost their jobs and  
bewildered mother. Azra Duric, another Bosnian  refugee  says Shnezhana and her husband Haris had  
SMITH/Appeal Organiser AZRA DURIC/Bosnian  Refugee  KEVIN BIRD/Aid Volunteer Voice over  

Fig. 8.3 Concordance snippet of 15 attestations of refugee(s) in the BNC sorted by L1/L2/L3 
Figure 8.3, in contrast, is a snippet from the same concordance sorted in a nested fashion frst by L1, then L2, and then L3 collocates. One pattern that emerges from this sorting is the phrase bogus refugee, frequently combined with the verb curb. Another pattern instantly visible is Bosnian refugee. If we had the space to display the entire concordance sorted to the left, many more such patterns would become visible, including phrases like Armenian refugee(s), Bosnian refugee(s), and Palestinian refugee(s), to give but three examples of L1-collocates telling us something about the home countries of refugees; phrases like civilian refugee(s), political refugee(s), and religious refugee(s), refecting how refugees are categorized by their (assumed) motivation to leave their home countries; and phrases like genuine refugee(s) and would-be refugee(s), which, alongside bogus refugee(s), are used frequently in the intense societal and political debate around the legitimacy of refugees. 
Depending on what your end goal is, you may want to not only sort (and sort in different ways to obtain different perspectives on your data), but also prune a concordance. By pruning, we here mean one or more of the following: deleting certain concordance lines and keeping others; narrowing down the context window; or blanking out the search term and/or collocates. Most typically, we delete concordance lines and/or clip the context window in the interest of saving space. Spatial restrictions apply to a handbook article like this one (hence the 15­line snippets as opposed to displaying the concordance in its entirety) as much as to the use of concordances for classroom use (not many students would want to inspect thousands of concordance lines). In a research context, in contrast, especially when researchers want to make a claim for exhaustive data retrieval and/or when the sequencing of attestations in the corpus matters for the research question at hand, one would only delete concordance lines that contain false hits. 
Another typical reason for deleting certain concordance lines is that depending on your search term (as well as the functionality of the software you are using and the make-up of the corpus data), the resulting concordance may contain a sizeable share of false hits. Imagine, for example, that you have a (untagged) corpus from which you want to retrieve all mentions of former and current Presidents of the United States. If you (can only) enter the simple search terms Bush, Clinton, Obama, and Trump, chances are that you will retrieve a number of hits that do not refer to the Presidents, but other people by the same name, or that do not refer to people at all, but instead, say, plants (Steff is hiding behind the bush) or card games (Paul played his trump card). 
Similarly, you may want to blank out either the search term or the collocates surrounding the search term to create a fll-in-the-blank exercise for teaching purposes. There are countless applications of this; to give but one example, imagine you wanted to teach non-native speakers of English the difference between near-synonymous words such as big, large, and great. You could create a worksheet in a matter of minutes by creating a concordance of these three adjectives, deleting any false hits or hits you deem too confusing or complex for your students, blanking out the search terms themselves, and printing out the formatted concordance (Stevens 1991). 
Concordance lines form the basis for both qualitative and quantitative analysis. We defne the term quantitative analysis here to refer to analyses that focus either exclusively, primarily, or in the initial stages of analysis, on the distributional patterns and global statistical trends underlying a given phenomenon, while we defne the term qualitative analysis to refer to analyses that focus either exclusively, primarily, or in the initial stages of analysis, on in-depth scrutiny of individual attestations of a phenomenon. As we see it, the distinction between the two kinds of analysis is more a matter of degree than a categorical choice, and which form of analysis dominates in a research project depends on the phenomenon 
S. Wulff and P. Baker 
under investigation and the researcher’s goals. Ideally, both forms of analysis are employed to provide complementary evidence: a qualitative analysis may be very thorough, yet leave open questions of generalizability and robustness that can be addressed in a more quantitative study. Conversely, even the most sophisticated quantitative analysis typically entails item-by-item annotation of each data point, which equates to a qualitative analysis of sorts; likewise, the results of a quantitative analysis demand interpretation, which typically requires a qualitative approach. In any case, concordance lines serve to provide the lexical and/or grammatical context of a search term and thus can be needed at all stages of an analysis, from data coding to interpretation. 

8.2.2 Qualitative Analysis of Concordance Lines 
In order to show the beneft of a qualitative analysis of concordance lines, we stick with the subject of constructions of refugees in the BNC. One way that concordance lines can be used fruitfully is to identify semantic prosody (Louw 1993; Sinclair 2004; Stubbs 1995, 2001; Partington 1998, 2004). The term has a range of overlapping but slightly different conceptualizations, but Louw (1993: 159) refers to it as “a consistent aura of meaning with which a form is imbued by its collocates”. One of the most famous examples of a semantic prosody is Sinclair’s (1991: 112) observation that “the verb happen is associated with unpleasant things– accidents and the like”. A semantic prosody can be identifed by simply classifying the collocates of a word, either by hand, as Sinclair did, or by using statistical measures to identify them (see Chap. 7), then noting whether their meanings are largely positive or negative. 
While an approach which considers collocates is useful in obtaining a general sense of a word’s semantic prosody, we would argue that such an approach works best when complemented by concordance analyses which may identify more nuanced uses. For example, Sinclair (2004: 33–34) argues that the term naked eye has a semantic prosody for ‘diffculty’, based upon the identifcation of concordance lines containing phrases like too faint to be seen with the naked eye. With many corpus tools, a concordance analysis can be used as a starting point for further forms of analysis, such as the identifcation of n-grams (see Chap. 4) which contain the search term or the collocates of the search term. For example, let us consider some of the collocates of the term refugee(s) in the British National Corpus. Figure 8.4 shows some concordance lines of refugee(s)–selected from the corpus to illustrate the word’s semantic prosody. First, using the Mutual Information statistic and a span of 5 words (see Chap. 7), we fnd strong noun collocates like infux, food and fow, as well as the verb collocate swollen (see the top half of Fig. 8.4). While the collocational analysis only identifed a small number of words relating to water, an examination of all the concordance lines of refugee(s) reveals less frequent 
L match R 
dispatched to reinforce dykes and assist flood refugees State grain reserves were opened in of 50,000 to 60,000 people. The new flood of refugees consequent upon the Russian withdrawal in 1960 alone) mirroring this year's flood of refugees . Watched by a demonstration of 5000 laws … Kinnock ‘won't curb flood of bogus refugees '.’ In other parts of Britain, the far right Pein the street are asked to prepare for an influx of refugees who will be looked after in the local church into Kenya at Moyale. AFRICA SUDAN refugee influx from Ethiopia — Sudanese Cutting off the flow of refugees did not solve the financial problem, The flow of East German refugees swelled during the 24 hours to Sunday reconstruction had only begun, large numbers of refugees had swollen the numbers of the be confident this tidal flow of disenchanted refugees the cities will not destroy the very sets in movement streams of developmental refugees , waves of environmentally displaced of attacks who are streaming into refugee camps in Thailand. contents of its many inns. Halfling refugees poured down the river Aver in a convoy hardships led to a continued outflow of refugees , particularly from the minorities explosions rolled closer — impelling a flux of refugees to eddy towards the troopers  
Fig. 8.4 Water metaphors used to represent refugees  

cases which are shown in the bottom half of Fig. 8.4 (waves, streaming, streams, poured, outfow, eddy). The examples of concordance lines in Fig. 8.4 indicate a negative semantic prosody of refugees as out-of-control natural disasters, using water metaphors. 
The qualitative concordance analysis is also helpful in indicating that not all of the co-occurrences of water-related words are used in metaphorical ways. For example, the frst concordance line in Fig. 8.4 uses food in a literal sense to refer to people who have become refugees due to a food. Such concordance lines are important in showing the danger of interpretative positivism (Simpson 1993: 113) where “a particular linguistic feature, irrespective of the context of its use, will always generate a particular meaning”. The food refugees case in line 1 was the only such example where a water-related collocate was not used in a metaphorical sense with refugees, so it does not negate the original fnding, but it does mean we should adjust our frequencies to take it into account. 
Let us consider another, even more important example of why a qualitative concordance analysis is important. Looking again at the collocates of refugee(s), the word bogus is the 10th strongest adjectival collocate of the term, occurring 14 times. Another collocate, genuine, appears with the search term 41 times. This entails a contrast between genuine and refugee(s) that calls into question the veracity of people who are identifed as refugees. Figure 8.3 gives some of the examples of bogus collocating with the search term. A qualitative analysis of these lines (and others like it) allows us to identify how bogus is used to construct refugees. In the BNC, it always occurs as a modifer of refugees, rather than say, refugees referring to someone or something else as bogus. Additionally, we can look at verbs and verb phrases to identify what bogus refugees are doing and what is being done to 
S. Wulff and P. Baker 
them. For example, fve concordance lines refer to curbing or cracking down on bogus refugees, particularly with reference to a new Act or Asylum Bill, which is designed to do this. Another example refers to a former Labour party leader (Neil) Kinnock who is claimed to say that he will not curb [the] food of bogus refugees. These concordances could be viewed as contributing towards a negative semantic prosody because they imply that someone or something wishes to curb/crack down on refugees. If we look at what bogus refugees are described as doing, we see phrases like 80% cheat their way into Britain and the good life, will grab state handouts and bleed Britain of £100 million. Again, this implies a negative semantic prosody as the examples describe bogus refugees as benefting fnancially from the British state. The last example contains a vivid metaphor that brings to mind bogus refugees as leeches or vampires, bleeding Britain. 
At this point it would be reasonable to conclude that one way that refugees are constructed in the BNC is negatively, as ‘bogus refugees’ who require regulation to stop them from illegally obtaining money from the British government. The fact that the terms bogus and genuine appear as collocates of refugee(s) in the corpus suggests that this occurs quite frequently. However, there is a danger of presenting an over-simplifed analysis if we stop here. It is often wise to look at expanded concordance lines before making a strong claim, in order to consider more context. Take for example the line “SCANDAL OF THE BOGUS REFUGEES–80% cheat their way into Britain and the good life”. The use of quotes at the start and end of this line perhaps indicates an intertextual use of language (where a text references another text in some way), and it is worth expanding the line to see whether this is occurring, and if so why. A fuller example of this is below: 
(1) Intending to increase sensitivity to the supposed threat, the right-wing tabloids have been regaling the public with anti-refugee stories during 1991 and 1992. For example, the Sunday Express of 20 October 1991 headlined its front-page lead article: ‘SCANDAL OF THE BOGUS REFUGEES — 80% cheat their way into Britain and the good life’. 
Consulting the header information from this particular fle, we see that it is from an article in the New Statesman. Importantly, this article references constructions of refugees in ‘right-wing tabloids’ like the Sunday Express by quoting from them. Reading the whole article, the New Statesman’s tone is critical of such constructions -the article is titled ‘Racist revival’. A similar case is found in the ninth line in Fig. 
8.3: a headline in The Times today, bogus refugees bleed Britain of £100 million through. This text is from a transcript of a political debate in parliament. More context is shown below: 
(2) Does my right hon. Friend agree that the opportunity for this country to help support genuine refugees abroad through various aid programmes is not helped by the fact that, according to a headline in The Times today, bogus refugees bleed Britain of £100 million through beneft fraud? Has he seen the comments of a DSS offcer in the same article that beneft fraud is now a national sport and that bogus asylum seekers think that the way in which this country hands out so much money is hilarious? 
Again, the example of bogus refugees is cited in this text in order to be critical of it, arguing that such representations do not help to support genuine refugees, although the speaker still makes a distinction between genuine and bogus refugees. However, the speaker does appear to be critical of the Times’s reference to bogus refugees, and so these two examples indicate that not every mention of a bogus refugee should be seen as uncritically contributing towards the negative semantic prosody. Had our analysis involved a close examination of a small number of full texts, this point would have quickly been obvious. However, due to the nature of a concordance analysis – the number of lines the analyst is presented with, along with the fact that they only contain a few words of context either side of a search term, it is possible that these more nuanced cases may be overlooked. Before making claims, it is important to consider a sample of expanded concordance lines and to maintain vigilance in terms of spotting lines that potentially may be functioning differently to a frst glance. In Fig. 8.3, it is use of quote marks along with mentions of other texts which indicates that something is being referenced e.g. a headline in the Times or front-page lead article. It is common for someone to voice an oppositional position by frst quoting the opinion they disagree with, and a good concordance analysis will take this into account. 

8.2.3 Quantitative Analysis of Concordance Lines 
Many quantitative corpus analyses are based on concordance data (though not necessarily all: one could think of, for example, a study that is based on frequency or collocation lists instead, see Chaps. 4–7). This is particularly true for multifactorial studies, that is, studies that try to explain a linguistic phenomenon with recourse to not one, but several variables (see Chaps. 21–26). These variables have to be identifed and coded for, and depending on the phenomenon in question, that may require the researcher to carefully examine the context around a given search term (one may consider that step a qualitative analysis). Let us consider one such example here, namely that-complementation in English. Speakers can choose to realize or omit the complementizer that in object, subject, and adjectival complement constructions as in (3): 
(3) 
a. I thought (that) Steff likes candy. 

b. 
The problem is (that) Steff doesn’t like candy. 

c. 
I’m glad (that) Paul likes candy. 


The variables that govern speakers’ choices to realize or omit the complementizer have been studied intensively (e.g. Jaeger 2010; Wulff et al. 2014). In a recent study, Wulff et al. (2018) explored if and to what extent non-native speakers of English make similar choices to native speakers of English. To that end, they compiled corpus data from native and non-native speaker corpora and ran concordances on 
S. Wulff and P. Baker 
each to retrieve all instances of complementation attested in these corpora. Since the goal was to present a unifed analysis that draws together the data from these different corpora and thus allows statistical evaluation of possible contrasts between native and non-native speakers, the frst step was to append all concordance lines into one big master spreadsheet (also called a “raw data sheet”). Most concordancers let you save concordances in a format that makes it easy to copy them into a spreadsheet with the left-hand context, the search term, and the right-hand context in separate columns. Not only is that more convenient for visual inspection, as we saw in the examples above; it also makes subsequent coding of the data much easier. We will discuss an example of that below. 
In a second step, after false hits had been pruned from the raw data sheet, the remaining true hits were coded for different variables known to impact native speakers’ choices. The result was a spreadsheet with one row for each concordance line and one column for each variable considered for analysis. A snippet of that spreadsheet is shown in Fig. 8.4 (we pruned the left-and right-hand context to ft the page). The variables coded for included, among others, the absence or presence of the complementizer (that: yes or no); the type of complementation construction (obj, subj, or adj); the frst language of the speaker who had produced the hit (English (GB), German (GER), or Spanish (SP)); the mode the hit came from (s(poken) or w(ritten)); the length of the main clause subject (number of letters); and many others. 
The conversion of the concordance lines into a spreadsheet like in Fig. 8.5 is helpful in various ways. For one, it facilitates the coding process because you can use the flter and sorting functions that all spreadsheet software includes. For example, you could sort the entire table by one construction type and look only at, say, object complementation, which will speed up the coding. Alternatively, you could apply a flter to the column containing the right-hand context and opt to see only instances that begin with that, which in turn allows you to reliably identify, and code accordingly, all hits that contain a complementizer in the that-column. A second advantage is that most statistics software requires you to input your data in a tabular format anyway, so you are working with the data in a format that makes loading them into, say, SPSS or R much more convenient (see Chap. 17). A third advantage is that once you are presented with the results of a statistical analysis, you can easily fnd relevant examples from your data by again applying flters. 
We do not have enough space to discuss the statistical evaluation and detailed results of this study here; suffce it to say that applying a multiple regression approach to the entire data sample of over 9,000 attestations, the authors found that intermediate-advanced German and Spanish learners rely on the same factors as native speakers do when they choose to realize or omit that. The main difference between the two speaker groups is that learners are overall more reluctant to omit the complementizer, especially if complexity-related variables increase the cognitive cost associated with processing the sentence (for instance, if the complement clause is quite long). For more information regarding regression-type approaches, see Chaps. 21–23. 
8  Analyzing Concordances  171  
L  match  R  that  Type  L1  Mode  Subj  …  
And do you  accept  that there is a crisis  yes  obj  GB  s  3  …  
Although I had to  admit  that she was very tiring  yes  obj  GER  w  1  …  
We can  argue  that television have  yes  obj  SP  w  2  …  
which  means  that the the pressures on us  yes  obj  GB  s  5  …  
Lucius  recognizes  that his life before  yes  obj  GER  w  6  …  
the distance is that I  remember  I was four weeks down there  no  obj  GER  s  1  …  
The ambassador  said  the allied bombardment  no  obj  GB  s  14  …  
They  think  that every person had the  yes  obj  SP  w  4  …  
Anyway the outcome  was  that I told him  yes  sub  ENG  w  11  …  
a rising number of people  wishes  that cars ought to be banned  yes  obj  GER  w  25  …  
....…  …  …  …  …  …  …  …  ...  
Fig. 8.5 Snippet of a raw data sheet in the case-by-variable format 



8.2.4 Pedagogical Applications of Concordance Lines 
There are two ways in which concordances (or any other corpus-based output) can be used in the classroom: either the students generate concordances themselves in class, or the instructor provides materials that include concordance lines. There is a growing strand of research that explores the effcacy of so-called data-driven learning (DDL) approaches, which give students access to large amounts of authentic language data (Frankenberg-Garcia et al. 2011), and corpus-based materials naturally lend themselves to use in such an approach. Space does not permit a comprehensive review of that literature here; for a good point of departure, see Rer (2008), who provides a general overview of the use of corpora in language teaching. To give but a few examples of prominently referenced studies that specifcally tested the usefulness of concordances, we refer the reader to Stevens (1991), who examined the effcacy of learners consulting corpus printouts. One group of students was asked to fll in a blank in single gapped sentences, while another group was instructed to fll in the missing words in a set of gapped concordances. Word retrieval seemed better in the latter condition. 
Another example is Cobb (1997), who tested the effcacy of concordance-based exercises for vocabulary acquisition compared to traditional vocabulary learning materials; the results of weekly quizzes of more than 100 students throughout one semester likewise suggested that concordance-based exercises were better for vocabulary retention than traditional materials. 
Boulton (2008, 2010) investigated the effcacy of paper-based concordance materials for vocabulary learning with 62 low-profciency English learners. After Boulton identifed a set of ffteen common language issues from students’ written productions, one group of students was taught the target language features either using concordance materials while another group received traditional materials retrieved from dictionary entries. The results of a post-test suggested that learners who had received the concordance-based materials outperformed those who had not on these target language features. 
S. Wulff and P. Baker 
Finally, for a recent study that explored the effect of concordance-based teaching not only in terms of learners’ performance on a vocabulary test, but its potential effect on learner production, see Huang (2014). More recently, meta-analyses of DDL in language learning testify to the effcacy of this teaching approach (Boulton and Cobb 2017; Lee et al. 2018). 
Representative Study 1 
Baker, P., and McEnery, A. 2014. Find the doctors of death: the UK press and the issue of foreign doctors working in the NHS, a corpus-based approach. In The Discourse Reader, eds. Jaworski, A., Coupland, N., 465–80. Routledge, London. 
This study examined the ways that foreign doctors were represented in a 500,000 word corpus of British national newspaper articles (The Times, The Mail, The Guardian, The Express, The Sun, The Star, The Mirror, The Telegraph and The Independent) containing the term foreign, followed by doctor, doc, medic, GP, locum,or physician or the plural of those words. The data was taken from 2000-2010 and collected using the online news database Nexis UK. Additionally, a 1.3 million word corpus of articles containing the word doctor, taken from the same newspapers and time period, was examined in order to determine if representations of foreign doctors were similar to representations of doctors more generally. Finally, a 1.2 million word corpus of articles containing the word foreign, foreigner or foreigners, taken from the same newspapers and time period, was examined in order to determine if representations of foreign doctors were similar to representations of foreigners more generally. The 1,180 concordance lines containing the search term foreign used to create the 500,000 million word corpus were examined qualitatively in order to identify representations of foreign doctors based on the qualities and actions that were attributed to them. Concordance lines with similar representations were grouped into categories. Additional concordance searches were carried out in order to fnd related examples, 
e.g. the word killer was examined through concordances as it had been noted in the headlines of texts from a number of other concordance lines. Concordance analyses of the two other newspaper corpora were carried out in similar ways in order to compare representations of doctors, foreigners and foreign doctors. The analysis found that 41% of the references to foreign doctors directly represented them as incompetent (particularly in terms of not being able to speak English), with a further 16% implying incompetence by calling for tighter regulation of them. There was frequent reference to foreign doctors who had accidentally killed patients, labelling them as killers. The concordance analysis also noted several contradictory 
(continued) 
representations, including the view that foreign doctors were desperate to work in the UK and were ‘fooding’ into the country (similar to the water metaphor used to describe refugees), appearing alongside other articles which claimed that foreign doctors ‘ignore’ vacancies in the UK. As well as being regularly described as incompetent and bungling, foreign doctors were also characterized as ‘sorely needed in their own countries’ and the UK was seen as amoral for ‘stripping poorer countries of professionals’. Foreign doctors were thus negatively represented, no matter what position they were seen to take. Representations of doctors were different to those of foreign doctors, with few mentions of the need for tighter regulation and only a small number of references to incompetent doctors. A common phrase in this corpus was ‘see your doctor’, which implied that journalists placed trust in doctors (as long as they were not foreign). Representations of foreigners were largely concerned with political institutions like the foreign offce, although a sample of 21 out of 100 concordance lines taken at random (using an online random number generator) showed negative constructions of foreigners involving stereotyping, implying they were taking up British resources or jobs, or controlling British interests. Overall, the analysis indicates that foreign doctors tend to be viewed negatively, as foreigners frst and doctors second, with individual stories of negligence being generalized and used as evidence of a wider problem. 
Representative Study 2 
Gries, S.T., and Wulff, S. 2013. The genitive alternation in Chinese and German ESL learners: towards a multifactorial notion of context in learner corpus research. International Journal of Corpus Linguistics 18(3):327–356. 
Gries and Wulff (2013) examined data obtained from the British sub-section of the International Corpus of English and the Chinese and German sub­sections of the International Corpus of Learner English in order to determine what factors govern learners’ choice of either the s-genitive (as in the squirrel’s nest)orthe of -genitive (the nest of the squirrel), and how learners’ choices align with those of native speakers. They annotated 2,986 attestations captured as concordance lines for 14 variables that were previously shown to impact native speakers’ choices, including the semantic relation encoded by the noun phrases, the morphological number marking on the noun phrases, their animacy, specifcity, complexity, and, crucially, the L1 background of the learners, among others. The data sample was analyzed with a binary 
(continued) 
S. Wulff and P. Baker 
logistic regression (Chap. 21) in which the dependent variable was the choice of genitive (-s vs. of ) and the 14 variables were the predictor variables. The fnal model suggested that learners generally heed to the same variables that native speakers consider in their choice of genitives. The most important variable across speakers’ groups was segment alternation: native and non­native speakers alike strongly preferred to opt for the genitive variant that represented the more rigid alternation of consonants and vowels. Overall, the Chinese learners were much better aligned with the native speaker’s choices than the German learners were, yet showed a stronger tendency to overuse the s-genitive across different contexts. 
8.3 Critical Assessment and Future Directions 
What are the limitations and drawbacks of concordance analysis? For one, it can be time consuming, particularly if we are using a large corpus or searching on a frequent item. This is a valid concern especially in the contexts of using concordances in the classroom, or for self-study. Secondly, human error and cognitive bias can creep in, meaning that we may over-focus on patterns that are easy to spot, such as a single word appearing frequently at the L1 position, while more variable patterns may go unnoticed. It can be mentally tiring to examine hundreds or thousands of lines, so there is a danger that what we notice frst may receive the most attention (which stresses the importance of trying different sorting patterns to yield different perspectives on the data). One option would be to use multiple analysts to carry out coding of concordance lines, with an attendant measure of inter-coder agreement (Hallgren 2012), a practice which is likely to help identify and resolve inconsistencies and coding errors. 
This chapter has also discussed the practice of pruning concordance lines, and some tools do allow for a concordance to be randomly ‘thinned’, giving a smaller sample to work with. However, there is no agreement on what an ideal sample size should be, and it is probably the case that different sample sizes are appropriate for different sized corpora, different types of corpora, and different search terms. For example, if we want to examine a word that has two main meanings, say a literal and metaphorical one (such as lamely), a sample of 50 lines would probably be adequate in helping us to see that the metaphorical use is much more typical. However, if we are interested in a word such as like, which has multiple functions, 50 lines may not be enough to ascertain the range of meanings and which ones are more typical. A good rule of thumb is to start with a reasonably low number (say between 20 and 100), carry out a concordance analysis of a sample of that size, noting the patterns and frequencies of different uses of an item, then take a second random set of concordance lines, of the same size, and redo the analysis. If the fndings from both sets are similar, then your sample size is most likely adequate. If not, double the sample size and repeat the exercise. 
Where is concordance analysis headed? To date, most concordancing research has been carried out on corpora of plain text. However, with moves towards multimodal corpora, it is possible to combine concordancing with analysis of sound or visuals (see Chaps. 11 and 16). For example, a sizeable proportion of the 10 million word spoken section of the British National Corpus has been aligned to the original speech fles, so when concordance searches are carried out using the online tool BNCweb, it is possible to select a line and hear the speech associated with it. WordSmith 7 also allows the analysis of corpora which contain tags which link to multimedia (audio or video) fles. Work which combines concordance analysis with image data is still in its early stages, and in the absence of adequate tools, can require painstaking hand-analysis. McGlashan (2016) carried out a concordance analysis of text from a corpus of children’s books, fnding that a common lexical item was the 3wordcluster love each other. By comparing instances of this item with the images that occurred alongside it, he found that it was often accompanied by pictures of family members hugging each other. Therefore, the meaning of love each other involved the representation of a tactile component which was only found in the images. McGlashan coined the term collustration to refer to the saliently frequent co-occurrence of features in multiple semiotic modes, and his concordance tables included numbered lines that referred to a grid of corresponding images that were shown underneath each table. 
In summary, concordance analysis is one aspect of corpus linguistics that sets it apart from other computational and statistical forms of linguistic analysis. It ensures that interpretations are grounded in a systematic appraisal of a linguistic item’s typical and atypical uses, and it guards against interpretative positivism. The inspection of dozens of alphabetically sorted concordance lines enables patterns to emerge from a corpus that an analyst would be less likely to fnd from simply reading whole texts or scanning word lists. By bridging quantitative and qualitative perspectives on language data, concordance analysis is and will remain a centerpiece of corpus-linguistic methodology. 

8.4 Tools and Resources 
Table 8.1 provides an overview of the most widely used concordance software, their platform restrictions (if any), pricing, and associated web links. Each concordance tool is slightly different: 
• 
some are tailored more towards research, others were designed primarily with classroom use in mind; 

• 
some can only query corpus fles that contain data in Latin alphabet format, while others are Unicode-compatible, i.e. can accommodate any writing system; 


S. Wulff and P. Baker 
Table 8.1 (Software including) concordance tools (information accurate at the time of writing; pricing for single user licenses) 
Concordance tool  Platform  Pricing  Web link  
aConCorde  macOS  Free  http://www.andy-roberts.net/coding/ aconcorde. Accessed 8 July 2019.  
AntConc  All  Free  http://www.laurenceanthony.net/ software/antconc/. Accessed 8 July 2019.  
CasualConc  macOS  Free  https://sites.google.com/site/ casualconc/Home. Accessed 8 July 2019.  
Conc  macOS  Free  http://software.sil.org/conc/. Accessed 8 July 2019.  
JConcorder  Java  Free  http://www.concorder.ca/index_en. html. Accessed 8 July 2019.  
LancsBox  All  Free  http://corpora.lancs.ac.uk/lancsbox/. Accessed 8 July 2019.  
MonoConc Pro  Windows  $85  http://www.athel.com/mono.html. Accessed 8 July 2019.  
Open Corpus Workbench  All  Free  http://cwb.sourceforge.net/index.php. Accessed 8 July 2019.  
Simple Concordance Program  Windows  Free  http://www.textworld.com/scp/. Accessed 8 July 2019.  
TextSTAT  All  Free  http://neon.niederlandistik.fu-berlin.de/ en/textstat/. Accessed 8 July 2019.  
WordSmith Tools  Windows  £50  http://www.lexically.net/wordsmith/. Accessed 8 July 2019.  

• 
some can handle regular expressions (cf. Chap. 9), while others only allow simple searches; 

• 
some tools are simple concordancers, others include many other functions such as generating frequency lists, collocate and n-gram lists, and visualization tools, to name but a few. 


The list below is not comprehensive in at least three ways: frstly, the tools listed below are all for offine use–there are web-based concordancers such as the Sketch Engine that either allow access to specifc corpora, or let the user upload data for online examination.1 Secondly, we only list monolingual concordancers, i.e. tools that let the user examine text from one corpus representing one language. There are also multilingual concordancers specifcally designed to query parallel corpora, i.e. corpora that contain data from multiple languages in their direct translations (see Chap. 12). Furthermore, it is worth pointing out that in research, there is a growing trend away from ready-made concordance tools and towards writing and adapting 
1For a list of web-based concordancers (and many other corpus-linguistic resources), see http:// martinweisser.org/corpora_site/CBLLinks.html. Accessed 31 May 2019. 
scripts written in programming environments like Python or R (e.g. Gries 2016). The rationale for many scholars is that this allows them to retrieve, annotate, statistically evaluate, and visually display their language data in one environment; it gives the user maximum control over each analytical step; and it facilitates the free sharing of data and scripts among the scientifc community (see Chap. 9). Ultimately, the choice for one concordancer is a matter of personal preference, and we encourage the reader to fnd their own favorite. 
Further Reading 
Hoffmann, S., Evert, S., Smith, N., Lee, D.Y.W., and Berglund, Y. 2008. Corpus linguistics with “BNCweb” – a practical guide. Peter Lang, Bern. 
In our chapter, we only provide examples of simple searches, that is, searches that involve a specifc whole word or phrase. Sometimes, however, we need not know in advance what specifc words or phrases we are looking for. For example, what if we want to create a concordance of all the adjectives in a corpus that is annotated for parts of speech? It would be tedious to try and enter each adjective individually (and we would likely miss out on a number of adjectives that do occur, yet we failed to think of them). If we can instead write a query that fnds all adjectives by their tags, we can fnd all of them with just one search. A second example of a more complex search could be: what if we are interested in fnding all words that end in the morpheme –licious without knowing what they are? In this case, we need a query that specifes only the fnal part of the word, but leaves open what the beginning of the word looks like. In all of these and many other cases, it can be useful to resort to more complex queries that involve what are called regular expressions. For more information on regular expressions, see Chap. 9 in this volume. A good introduction can also be found in Hoffmann et al. (2008), who provide many examples of how to combine regular expressions with corpus annotation such as part-of-speech tags, lemmatization, etc. While Hoffmann et al. (2008) focus specifcally on the query syntax associated with the Corpus Query Processor (CQP) of the IMS Open Corpus Workbench as it can be used for complex searches of the BNCweb corpus, it is a great way to get started with complex searches for several reasons: access to the BNCweb is free (for more information on how to access the BNCweb, go here: <http://corpora.lancs.ac.uk/BNCweb/>), and the Corpus Workbench can be used on many other corpora provided they meet certain requirements. That aside, once you understand the basic reasoning behind a corpus query syntax such as the one implemented in CQP, it is relatively easy to work with different implementations of it. 
Partington, A. 1998. Patterns and meanings: using corpora for English language research and teaching. John Benjamins, Amsterdam and New York. 
Partington presents a series of case studies that illustrate how corpus methods can shed light on diverse areas like synonymy, cohesion, and idioms; analysis of concordances plays a major role throughout. 
S. Wulff and P. Baker 
Sinclair, J. 1991. Corpus, concordance, collocation. Oxford University Press, Oxford. 
An early introduction to corpus linguistics written for students in language educa­tion. 
Stubbs, M. 2001. Words and phrases: corpus studies of lexical semantics. Blackwell, Malden MA. 
Stubbs outlines how the meanings of words depend on their contexts, and how the connotations of words arise from their recurring embedding in larger phrases. 
References 
Baker, P. (2006). Using corpora in discourse analysis. London/New York: Continuum. 
Baker, P., & McEnery, A. (2014). Find the doctors of death: the UK press and the issue of foreign doctors working in the NHS, a corpus-based approach. In A. Jaworski & N. Coupland (Eds.), The Discourse Reader (pp. 465–480). London: Routledge. 
Boulton, A. (2008). DDL: reaching the parts other teaching can’t reach? In A. Frankenberg-Garcia (Ed.), Proceedings of the 8th Teaching and Language Corpora conference (pp. 38–44). Lisbon: Associação de Estudos e de Investigação Cientifíca do ISLA-Lisboa. 
Boulton, A. (2010). Data-driven learning: taking the computer out of the equation. Language Learning, 60(3), 534–572. 
Boulton, A., & Cobb, T. (2017). Corpus use in language learning: a meta-analysis. Language Learning, 67(2), 348–393. 
Cobb, T. (1997). Is there any measurable learning from hands on concordancing? System, 25(3), 301–315. 
Frankenberg-Garcia, A., Aston, G., & Flowerdew, L. (Eds.). (2011). New trends in corpora and language learning. New York: Bloomsbury. 
Gries, S. T. (2016). Quantitative corpus linguistics with R: a practical introduction (2nd ed.). London/New York: Routledge. 
Gries, S. T., & Wulff, S. (2013). The genitive alternation in Chinese and German ESL learners: towards a multifactorial notion of context in learner corpus research. International Journal of Corpus Linguistics, 18(3), 327–356. 
Hallgren, K. A. (2012). Computing inter-rater reliability for observational data: an overview and tutorial. Tutorials in Quantitative Methods for Psychology, 8(1), 23–34. 
Huang, Z. (2014). The effects of paper-based DDL on the acquisition of lexico-grammatical patterns in L2 writing. ReCALL, 26(2), 163–183. 
Jaeger, T. F. (2010). Redundancy and reduction: speakers manage syntactic information density. Cognitive Psychology, 61, 23–62. 
Lee, H., Warschauer, M., & Lee, J. H. (2018). The effects of corpus use on second language vocabulary learning: a multilevel meta-analysis. Applied Linguistics. published online frst. 
Louw, W. E. (1993). Irony in the text or insincerity in the writer? The diagnostic potential of semantic prosodies. In M. Baker, G. Francis, & E. Tognini-Bonelli (Eds.), Text and technology: in honour of John Sinclair (pp. 157–176). Amsterdam: John Benjamins. 
McGlashan, M. (2016). The representation of same-sex parents in children’s picturebooks: A corpus-assisted multimodal critical discourse analysis. Dissertation, Lancaster University. 
Partington, A. (1998). Patterns and meaning: using corpora for English language research and teaching. Amsterdam/Philadelphia: John Benjamins. 
Partington, A. (2004). “Utterly content in each other’s company”: semantic prosody and semantic preference. International Journal of Corpus Linguistics, 9(1), 131–136. 
Rer, U. (2008). Corpora and language teaching. In A. Leling & M. Kyto (Eds.), Corpus linguistics: an international handbook (Vol. 1, pp. 112–130). Berlin: Mouton de Gruyter. 
Simpson, P. (1993). Language, ideology and point of view. London: Routledge. 
Sinclair, J. (1991). Corpus, concordance, collocation. Oxford: Oxford University Press. 
Sinclair, J. (2004). Trust the text: language, corpus and discourse. London: Routledge. 
Stevens, V. (1991). Concordance-based vocabulary exercises: a viable alternative to gap-fllers. English Language Research Journal, 4, 47–63. 
Stubbs, M. (1995). Collocations and semantic profles: on the cause of the trouble with quantitative studies. Functions of Language, 2(1), 23–55. 
Stubbs, M. (2001). Words and phrases: corpus studies of lexical semantics. Oxford: Blackwell. 
Wulff, S., Gries, S. T., & Lester, N. A. (2018). Optional that in complementation by German and Spanish learners. In A. Tyler & C. Moder (Eds.), What is applied cognitive linguistics? Answers from current SLA research (pp. 99–120). New York: De Gruyter Mouton. 
Wulff, S., Lester, N. A., & Martinez-Garcia, M. (2014). That-variation in German and Spanish L2 English. Language and Cognition, 6, 271–299. 
Chapter 9 Programming for Corpus Linguistics 
Laurence Anthony 
Abstract This chapter discusses the important role of programming in corpus linguistics. The chapter opens with a history of programming in the feld of corpus linguistics and presents various reasons why corpus linguists have tended to avoid programming. It then offers some strong counter arguments for why an understanding of the basic concepts of programming is essential to any corpus researcher hoping to do cutting-edge work. Next, the chapter explains the basic building blocks of all software programs, and then provides a number of criteria that can be used to assess the suitability of a programming language for a particular corpus linguistics project. To illustrate how the building blocks of programing are used in practice, two case studies are presented. In the frst case study, basic programming concepts are applied in the development of simple programs that can load, clean, and process large batches of corpus fles. In the second case study, these same concepts are applied in the development of a more advanced program that can replicate and, in some ways, improve on the tools and functions found in ready-built corpus tools. The chapter fnishes with a critical assessment and discussion of future developments in corpus linguistics programming. 
9.1 Introduction 
Computer programming has played a key role in corpus linguistics since its growth in the 1960s. Early researchers did not have access to any existing corpus-analysis software. As a result, they had to build simple tools from scratch in programming languages such as Fortran and COBOL. This work led to the creation 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_9) contains supplementary material, which is available to authorized users. 
L. Anthony (•) Faculty of Science and Engineering, Waseda University, Tokyo, Japan e-mail: anthony@waseda.jp 
© Springer Nature Switzerland AG 2020 181 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_9 
of concordancers, such as those of Clark (1966), Dearing (1966), Price (1966), and Smith (1966). Some researchers even created new programming languages for this work, such as ATOL, which was used to develop the CLOC collocation analysis tool for the COBUILD project (Reed 1978; Sinclair et al. 2004; Moon 2007). From the 1980s onwards, however, the importance of programming in corpus linguistics work diminished somewhat with the development of ready-built, user-friendly tools that could run on corpus linguists’ own personal computers. Examples of software from this time include Micro-concord (Johns 1986), the Oxford Concordance Program (Hockey and Martin 1987), the Longman Mini-Concordancer (Chandler 1989), and the Kaye concordancer (Kaye 1990). As these tools evolved and became even more powerful and easy-to-use, the need for programming within the corpus linguistics community became even less clear. 
Today, it is still the case that many corpus linguists see little need to develop their programming skills. Perhaps the main reason for this is the ready availability of freeware toolkits such as AntConc (Anthony 2019) and commercial tools, such as WordSmith Tools (Scott 2020). Ready-built tools allow corpus linguists to carry out much, if not all, they need to do with their own custom-designed corpora or directly accessible ready-built corpora, such as the British National Corpus (BNC). (Burnard 2000), the British Academic Spoken English (BASE) Corpus (Thompson and Nesi 2001), the British Academic Written English (BAWE) Corpus (Nesi et al. 2004), and the Michigan Corpus of Academic Spoken English (MICASE) (Simpson et al. 2002). Indeed, a review of the literature published in three mainstream corpus linguistics journals, i.e., Corpora, Corpus Linguistics and Linguistic Theory, and the International Journal of Corpus Linguistics, suggests that the majority of corpus linguists use corpus tools in relatively simple ways to complete a minimal number of analytical tasks. 
On the other hand, when corpus linguists are asked what they would want to do with corpora (ignoring what is possible with existing ready-built tools), it is clear that a much wider range of functions are desired (see Anthony 2014). Some of these include: 
• 
the automatic creation of clean, annotated corpora 

• 
the comparison of two or more texts across multiple lexical/grammatical/rhetor­ical dimensions 

• 
the batch counting of words, phrases, and n-grams (lexical bundles) to give per-text frequency and dispersion information 

• 
the calculation of lexical diversity scores for each fle in a learner corpus 

• 
the measurement of distance between priming words and target words 

• 
the identifcation and counting of collocates of certain target word types in target and reference corpora 

• 
the extraction of complex meaning units such as defnitions 

• 
the counting of interesting lexical phenomena, such as disfuencies, in a tagged corpus 

• 
the automated analysis of the rhetorical structure of texts (see Swales 1990) 

• 
the creation of novel and useful visualizations of text data 

• 
the look-up of pronunciations of words found in a corpus in a pronunciation database such as CELEX2 (Baayen et al. 1995) 


Another reason why many corpus linguists may see little need to learn program­ming is the growing preference to use web-based corpora. These corpora are often painstakingly created in teams that involve corpus linguists and software developers, and thus, circumvent the need for the user to program their own data collection and cleaning procedures. The corpora are also usually positioned behind a ‘wall’ and can only be accessed indirectly through either purpose-built web-browser interfaces 
(e.g. the Michigan Corpus of Upper-Level Student Papers (MICUSP) (http:// micusp.elicorpora.info/)) or online corpus management and query systems (e.g. corpus.byu.edu (corpus.byu.edu corpus site. corpus.byu.edu), CQPweb (https:// cqpweb.lancs.ac.uk/), Sketch Engine (https://www.sketchengine.co.uk/), and WMa­trix (http://ucrel.lancs.ac.uk/wmatrix/)). These systems not only remove the need for the user to develop their own query systems and/or analytical tools, but also greatly reduce (or completely prevent) a user from directly using custom programs with the data. 
Corpus linguists may also choose not to learn to program for other, more mun­dane, reasons. One is that corpus linguists, especially those working in academic institutions, are likely to be pressured for time (see Tribble 2015). MA and PhD students, for example, have a deadline to meet when it comes to analyzing their data and writing up their fndings in the form of a thesis. As a result, the advantages they gain by giving up some of this time to learn to program may not be immediately apparent. Another is that even if a corpus linguist has the time to learn to program, they may not know the best way to go about it or even know which programming language to attempt to learn. Working directly with computer programmers in the creation and analysis of data is not so straightforward, either. In an academic setting, computer programmers are likely to be working in a different department and/or faculty, introducing problems of physical distance, faculty-culture differences, and meeting-scheduling issues. Corpus linguists may also struggle to communicate to computer programmers exactly what they want to do with the new software, as they will lack the vocabulary commonly used by software engineers and programmers when discussing development issues. 
Although there are several reasons why a corpus linguist may not want to learn to program, there are also important reasons why knowledge of programming can beneft a corpus linguist greatly. Biber et al. (1998:254–256) explain that programming allows the corpus linguist to do analyses not possible with existing tools, do those analyses more quickly using a corpus of any size, and also tailor the output to meet their own needs. Similarly, Gries (2009:12) explains that the use of a programming language puts researchers “in the driving seat”, enabling them to circumvent the limitations of existing tools in terms of availability, function­ality, and user-control. Davies (2011) and Anthony (2009) also acknowledge the importance of programming in corpus linguistics. However, they are more cautious in their recommendations for who should learn to program. Davies suggests that “corpus users” (researchers who are not involved in creating a corpus) might be able to “get by” with standalone-tools and web-based corpora, whereas “corpus creators” (those who build new corpus resources) need at least some experience with programming. Anthony suggests that programming cannot be avoided in any cutting-edge corpus research, although this work may be carried out by a dedicated computer programmer who is part of the team. On the other hand, he recommends that corpus linguists in the team have some experience of programming as it will help when communicating design ideas and explaining potential problems to the programmer. 
Continuing from the ideas expressed in Anthony (2009), this chapter is posi­tioned on the side of the argument that corpus linguists do need some basic understanding of programming. Minimally, they need to understand the underlying concepts of programming, which will not only help them to recognize the limitations of the tools they use, but also allow them to work closer with expert programmers in the development of new and improved tools if the chance arises. Going further, if they hope to become “corpus creators”, as is the case with many MA and PhD students embarking on a corpus research project, they are likely to have to collect and process vast amounts of noisy, inconsistent, and poorly formatted data. In this case, a basic ability to program is clearly an advantage. To follow this path, however, they frst need to understand the different types of programming languages available and the inherent strengths and weaknesses of each. With this knowledge they can then make an informed decision about which language is most suitable for their needs and begin their journey to learn it. The same applies to “corpus users” who hope to do more complex, multi-leveled investigations than those possible with the features and functions of ready-built tools. Clearly, many of the leading corpus linguists in the feld can be included in this category. Fortunately, as this chapter explains, learning basic programming is becoming an ever-easier task as new learning resources become available and new features and functions are added to programming languages that simplify their use. 

9.2 Fundamentals 



9.2.1 The Basic Building Blocks of Software Programs 
Software programs are built using computer programming languages, i.e., simple, artifcial languages that are designed to enable humans to issue instructions to computers in a non-ambiguous way. The instructions written in a computer language consist of statements formed from a vocabulary of concepts recognized by the computer (e.g. variable names, variable values, mathematical operators, string operators, data types, and so on) arranged in a particular order as defned by the syntax of the language. If the statements are correctly formed, they can be parsed by a compiler or interpreter built into the computer language and converted into a machine language used internally by the computer. Complex instructions can be created by combining individual statements into larger blocks of code following structures that are also predefned by the computer language (e.g. fow structures, functions, methods, objects, and classes). 
Computer languages can be considered as members of particular language families if they share a similar vocabulary and syntax rules. The most popular languages in the world today are all part of a family of computer languages that evolved from the C language, which was developed towards the end of the 1960s. ‘C-like’ languages include C, C++, C#, Objective-C, Java, JavaScript, Perl, PHP, Python, and many more. Other well-known languages that have given rise to families of related languages include BASIC, Fortran, Pascal, Lisp, Prolog, Smalltalk, and S, the modern equivalent of which is R. 
As with human languages, learning one computer language in a family can be hugely benefcial when learning other languages in that same family. This is one reason why many degree courses in computer science frst focus on the C language (or a related ‘C-like’ language). However, all computer languages share many common vocabulary and syntax features. So, learning any computer language will be hugely benefcial to a corpus linguist if and when it becomes necessary to learn another computer language later. Some common features of computer languages are listed in Table 9.1, where it can be seen that computer languages usually work with a limited set of data types (e.g. Boolean, number, string, ...) and that manipulating these data types requires the use of special operator symbols. For example, in Python, the “+” operator is used to add numbers together, but it is also used to concatenate strings of letters together (“abc” + “def” => “abcdef”). 
Although all computer languages are similar in many ways, they are each designed with certain features and idiosyncrasies that offer advantages (as well as disadvantages) over other languages in particular settings. For example, C is a low-level language, i.e., one that closely resembles the computer’s core instruction set. This feature allows it to run very quickly and effciently, but also makes it quite diffcult to read and understand. In contrast, JavaScript, Perl, Python and R are high-level languages (ones that more closely resemble normal human languages), making them easier to read and understand, but also making them slower and less effcient than C. 
Table 9.1 Common features of computer languages 
Data types  Boolean, number, string, list, key/value pairs (often termed “dictionary” or “hash”)  
Operators  arithmetic (“+”, “-”, ...), comparison (“<”,“>”, ...), condition (“if”, “when”, ...), logical (e.g. “OR”, “NOT”, ...), string (“.”, “+”, “eq”, ...)  
Control and loop fow  if, while, for/foreach  
Modularization  functions or classes/objects or both  
Input/output  print, read, write  

Perhaps the most important feature that distinguishes one programming language from another is the way that it handles the grouping of statements into larger units. Here, there are two main approaches. One is a Functional Programming (FP) approach, where statements that are designed to instruct the computer to perform an action (e.g. sorting) are grouped into a self-contained “function” that does not rely on any external state (e.g. variable values). The other is an Object-Oriented Programming (OOP) approach, where the instructions to perform an action as well as the variables on which that operation are performed are all grouped into a single “object class”. FP programming generally allows for the easy addition of new functions to a program but keeping this growing set of functions organized and working well with existing and new variables can become confusing and error prone. OOP programming, on the other hand, requires more thought when deciding which variables and actions (methods) should be included in objects. However, creating and organizing new objects is relatively simple and because each object class is independent of the others, error checking is easier. 
Computer languages are designed from the ground-up to facilitate programming using one or both of these design approaches. C, JavaScript, Perl, and R are examples of languages designed for functional programming, whereas Java is an example of a language designed for object-oriented programming. Python is an example of a language that was designed to accommodate both programming approaches. In practice, current software engineering practices tend to favor an OOP approach as it allows programs to scale well and lends itself more easily to the modularization of code that is developed in a team. As a result, many functional programming languages, such as C, Perl, and R, have been adapted or extended to allow for some kind of object-oriented coding, although the designs that have been employed are not always elegant or effcient. 
9.2.2 Choosing a Suitable Language for Programming in Corpus Linguistics 
All modern programming languages can be used to develop programs that will fulfll the needs of corpus linguists, whether they are MA students, PhD students or seasoned experts in the feld. However, some languages are more suited to the tasks commonly carried out by corpus linguists, i.e., reading large text fles into memory, processing text strings, counting tokens, calculating statistical relation­ships, formatting data outputs, and storing results for use with other tools or for later use. In order to decide which language is most suitable for a particular corpus linguistics programming task, there are at least four important factors to consider at the outset. 
The frst consideration is the design of the language. High-level, functional languages are well suited for writing short, simple, “one-off” programs for quick cleaning and processing of a corpus, e.g. renaming fles, tokenizing a corpus, cleaning noise from a particular corpus, and calculating a particular statistic for corpus data. For this reason, languages such as Perl, Python (which can run as a functional language), and R have been commonly used for corpus linguistics applications (e.g. Desagulier 2017;Gries 2016; Johnson 2008; Winter 2019). Python and R, in particular, have grown to be especially popular among corpus linguists due to their strong statistical and data visualization features. High-level object-oriented languages, on the other hand, are more suited for writing longer, more complex, “reusable” programs, as they can be more easily maintained and extended. As a result, corpus toolkits that combine and integrate common corpus functions into a complete package are more likely to be built in a language such as Python or Java. Popular examples are the majority of AntLab tools (Anthony 2019) and the Natural Language Toolkit (NLTK) (https://www.nltk.org/), which are written in Python, and the suite of natural language processing tools offered by the Stanford Natural Language Processing Group (https://nlp.stanford.edu/), which are written in Java. If a corpus linguist hopes to use or contribute to the development of these tools, knowledge of Python or Java would certainly be useful. 
A second consideration is whether to use a language that is converted into machine language ‘on the fy’ during runtime (usually referred to as an ‘interpreted’ language) or one that is converted (or compiled) into machine language prior to runtime (usually referred to as a ‘compiled’ language). Interpreted languages, such Perl, Python, and R allow for programs to be “prototyped”, meaning that developers can create working programs with placeholders marking yet-to-be­written or unfnished code. They can also be used in a ‘live’, interactive way, with new lines of code written in response to the output generated by the interpreter. This makes them particularly suited to many MA and PhD student projects, where development speed and fexibility are important factors. They are also useful for carrying out some of the ‘quick and dirty’ programming tasks that a corpus linguist might need to do in order to get their data into a form that can be analyzed. Compiled languages, such as C and Java, on the other hand, require a slower “write-compile­run” design. This process can reduce the number of bugs in the fnal code (as they can be detected at compile time) and allow the code to run faster than equivalent code written in an interpreted language. For these reasons, compiled languages are often chosen for very large projects, where speed or accuracy are required, such as the engine of the IMS Open Corpus Workbench (CWB) (http://cwb.sourceforge. net/) toolkit, which is written in C, and the tools developed by the Stanford Natural Language Processing Group mentioned earlier, which are developed in Java. 
A third consideration is whether the programming language is designed primarily for creating web-based tools, traditional desktop tools, or mobile platform tools. Some languages, such as PHP and JavaScript, have many features designed specifcally for the web, with CQPweb using PHP extensively on the server side, and SketchEngine using JavaScript heavily for its interface. In contrast, languages such as Java, Perl, Python, Ruby, and R have features tailored for both web and desktop environments, making them a common choice for many corpus tools. As an example, AntConc (Anthony 2019) is written in Perl and TagAnt (Anthony 2019) is written in Python. For mobile applications, programs are usually written either natively for the operating system in Java (for Android systems) or Swift (for iOS system) or developed to run smoothly in a mobile web browser using JavaScript on the client side and PHP, Python, or Java or the server side. 
The fnal and perhaps most important consideration is the size and vibrancy of the community that supports a computer language. ‘Popular’ languages such as C, C++, PHP, Python, and R have large, active groups of core developers that continually develop and improve the code base. These languages also have a large group of active users who are willing to answer questions and offer advice on coding through forums such as StackOverfow (https://stackoverfow.com/). In fact, such is the popularity of the Python and R languages among the corpus linguistics community that there are dedicated groups serving this community and multiple sites that provide resources specifcally for corpus researchers (see Sect. 9.5 for more details). In contrast, very new and/or niche languages (e.g. LiveCode (https:// livecode.com/)), and some of the older languages with shrinking user groups (e.g. Perl) might have useful features for a particular project, but they may also lack the community support to maintain and update core packages and extension modules, or answer questions about how to use the language in practical contexts. One common problem with very new languages, niche languages, and older languages is that they do not integrate well with other languages and might fail to adapt to the evolving nature of different operating systems. It follows that choosing a ‘popular’ language in the corpus linguistics community, such as Python or R (or a popular language designed specifcally for web-development, such as JavaScript), is a safe route to programming unless there is a specifc reason to use one of the newer, older, or niche languages. 
Representative Study 1 
Edberg, J., and Biber, D. 2019. Incorporating text dispersion into key­word analyses. Corpora. 
In this paper, the authors aim to determine which keyness measure best identifes words that are distinctive to the target domain(s) present in a corpus (cf. Chap. 6). To achieve this goal, they compare various keyness measures that are primarily based on the frequency of words in a corpus together with their own “text dispersion keyness” measure, which is based on the number of texts in which a word appears. 
In order to carry out the analysis, the authors use Python scripts to calculate a traditional keyness measure based on log-likelihood, as well as two variations, each with different minimum dispersion cut-off values. They also use a Python script to calculate their own “text dispersion keyness” measure and compare all four of these measures against the “Key keyword analysis” measure available in WordSmith Tools (Scott 2020). 
(continued) 
In this project, the authors are working in a small team and are probably using one-off scripts that are unlikely to be extended further. Therefore, any scripting language would probably be suffcient for the purpose with Python being an excellent choice. Using Python scripts as part of their analysis, the authors are able to quickly and accurately compare a range of possibilities for calculating keyness, including their own “text dispersion keyness” measure, which is not yet available in ready-built tools. The results from the study not only show the relatively merits of using dispersion as part of a keyness measure, but also suggest an important extension that can be added to ready-built tools. 
9.3 First Steps in Programming 
The following case studies are designed to contextualize the previous discussion and show how programming languages can be used to carry out some of the most common and important tasks carried out by corpus linguists. Although the majority of examples presented in these case studies mimic the functionality of existing ready-built corpus tools, they will generally perform much faster because they are designed to carry out a specifc task and they remove the overhead of creating and updating an interface. It should also be noted that they can be easily extended or adapted to carry out tasks that are quite diffcult or impossible to achieve with the most popular tools used in corpus linguistics today. Script 4 in Case Study 1 illustrates this point. 
The programming language used in the case studies is Python. As described earlier, Python is a high-level, object-oriented, interpreted programming language that can be used to build web applications and also desktop applications for Windows, Macintosh, and Linux. It has a very large, vibrant community of core developers and users, and was ranked the second most popular programming language in the world in 2019 (StackOverfow 2019). In the survey, only JavaScript ranked higher, perhaps due to its common use for building web-based applications. Python also has a huge number of user-and company-developed extensions that add important features on top of its rich core functionality. For example, the Pandas extension allows Python to carry out advanced statistical measures that can be visualized using one of many visualization packages, such as Matplotlib or Seaborn. These features have resulted in Python becoming one of the most popular languages used in corpus linguistics work today. Interestingly, the importance of Python in corpus linguistics work might possibly grow as it is one of the most popular languages used to develop artifcial intelligence (AI) and deep learning applications due to its rich number of natural language processing and machine learning libraries. 
For reference, equivalent scripts written in the R language are also provided by the second editor, St. Th. Gries, at http://www.stgries.info/research/ 2020_STG_Scripts4Anthony_PHCL.zip. R is a high-level, functional, interpreted language that is mainly used for building desktop applications. However, a recent add-on package to the language called Shiny makes it much easier to build simple web-apps using the language. Although R is far less popular than Python among general programmers, it has a number of strong supporters in the corpus linguistics community. Also, it has perhaps the most vibrant community of developers and users focused on corpus linguistics work. One reason for this is that, as already mentioned above, R is particularly well suited to carrying out statistical analyses and visualizing the results. The recent trend to use more advanced quantitative methods in corpus linguistics work suggests that R will grow even more in popularity. Importantly, there are extensions in both Python and R to allow programs written in one language to interact smoothly with programs written in the other (see Sect. 9.5 for more information). 
In order to follow the case studies and implement the code samples presented, the following steps should frst be completed: 
1. 
Set up the target computer to run Python scripts This is a trivial task that simply involves downloading the latest version of the software and running the installer with default settings (https://www.python. org/downloads/). Numerous tutorials are available on the Internet, but the most detailed and comprehensive explanation is provided by the Python Software Foundation (see Sect. 9.5). During the installation process, it is useful to select the “Add Python to PATH” option so that you can run scripts directly from the command prompt later (see Step 4). 

2. 
Create a folder in a suitable place in the operating system from where Python scripts can be run (e.g. the Desktop) and give it the name “project” (or equivalent). 

3. 
Save all scripts described in the two case studies in the project folder as plain-text fles with a “.py” extension to signal that they are Python scripts (e.g. “script_1.py”, “script_2.py”, etc.) 

4. 
Run the scripts by launching the command prompt (on Windows) or Terminal (Macintosh/Linux) and then typing “python” followed by the name of the script (separated by a space), and then hitting the Enter/Return key. 


9.3.1 Case Study 1: Simple Scripts to Load, Clean, and Process Large Batches of Text Data 
We will start by assuming that a very simple, UTF-8-encoded, plain-text text fle called “text_1.txt” is needed to be processed. It contains a single line of text: “The cat sat on the mat.” 
9.3.1.1 Loading a Corpus File and Showing its Contents 
Script 1 is a short script that loads “text_1.txt” and shows its content in the console window (on Windows) or terminal (on Macintosh/Linux). 
Script 1: Load a File and Show Its Content 
from pathlib import Path
1 
corpus_file = Path('text_1.txt')
2 
file_contents = corpus_file.read_text() 
3 
print(file_contents) 
4 
Script 1 is just four lines long. However, it illustrates some interesting and useful features of a modern, object-oriented language. First, the script shows that the most important components of the language are stored in classes that are loaded when needed. Here, on line 1, the program imports the Python “Path” object class from the pathlib library. This is used to create “Path” objects that automatically adjust the string parameter to match the formatting rules of the operating system. The actual “Path” object is created on line 2 and given the name “corpus_fle” (for convenience). Second, the script shows that objects, such as “Path”, have associated methods that can be accessed using a dot notation. As an example, on line 3, the “read_text” method of the “Path” object is called using “corpus_fle.read_text()” in order to read the content of the fle into memory. Then, the content of the fle is printed to the screen in line 4 using a Python core “print” function. A third interesting feature is that the user-defned variables in the script (i.e., “corpus_fle” and “fle_content”) have long, meaningful names making the script easy to read and understand. The variables could just as easily be named “cf” and “fc” as long as they did not clash with reserved names used by the Python core. However, such names would be confusing and easily forgotten, especially if the script was not used for several weeks. Importantly, the Python core object classes and functions also have long, meaningful names, which is one reason why it is a commonly recommended frst language to learn. 
While Script 1 can be said to “work”, it includes some deliberate weaknesses that should be avoided when creating programs any longer than this. First, the code contains no comments (lines of code often prepended with a hash character that are ignored by the interpreter but can be read by humans). These are useful for “self-documenting” the code (i.e. adding documentation directly within the code) so that anyone reading the code later (including the original coder!) will understand its design choices. The code also contains no whitespace to divide up the different parts of the code, which again improves readability. Third, the code is written as a series of commands rather than grouped together in a well-formed function (or 
object class). As a result, the code cannot easily be recycled and used in other scripts. 
This writing style also leads to confusing code that is prone to include bugs. 
Script 2 shows a more useful version of Script 1 written as a function with comments and whitespace to improve readability and its likelihood of future use: 
Script 2: Load a File and Show Its Content (Written as a Function) 

Script 2 is comprised of three parts: First, the necessary object class is loaded (line 2). Second, the main function is defned (lines 5–11); Third, the function is launched with the path to the fle as a parameter (line 14), and the returned result is shown (line 15). Importantly, the function “get_fle_contents” is completely encapsulated (e.g. no outside parameter values are hard-coded into the function), so it can be used with any corpus fle in any program. As an example, to show the contents of a fle called “text_2.txt” only the one line of code that launches the function (line 14) needs to be changed: 
14 file_contents = get_file_contents('text_2.txt') 

9.3.1.2 Loading a Corpus File, Cleaning it, and Showing Its Contents 
For a more challenging example, imagine that a fle containing HTML code needs to be processed and the embedded text shown on screen: 
File Name: html_example.txt 
File Contents: 
<!DOCTYPE html> <html> <head> <title>The Cat</title> </head> <body> 
<h1>Chapter 1</h1> <p>Once upon a time, a cat sat on a mat.</p> 
</body> </html> 
This task would be quite troublesome to do manually as it would involve multiple copy/paste steps to create the new ‘cleaned’ fle that could then be shown on the screen. Script 2, however, can be very easily extended to load the HTML fle, remove all the HTML markup, and then show the text contents. The extended program is shown as Script 3 below: 
Script 3: Load an HTML File, Remove the Tags, and Show the Text Contents 

The following output produced by Script 3 is shown in the command window or terminal: 
The Cat 
Chapter 1 Once upon a time, a cat sat on a mat. 
Script 3 differs from Script 2 in only four places: 
• 
The “BeautifulSoup” object class from the bs4 Python extension library is loaded in line 3. This library is used to parse HTML fles and extract plain text. 

• 
An additional parameter, “fle_type”, is created for the function with a default value of “text“. This is introduced so that the script can handle both plain-text and HTML fles (line 6) 

• 
A conditional expression is added to the function to parse fles using a “Beauti­fulSoup” object if they are signaled to be HTML fles (lines 12–18) 

• 
The extended function is now called with two parameters: (1) the path to the fle, and (2) the fle type (line 21) 



9.3.1.3 Loading a Web Page, Cleaning it, and Showing Its Contents 
Script 3 can be easily adapted further to download an HTML webpage, clean it, and show its text contents. To do this, a “request” object from the urllib Python core library needs to be utilized. This object essentially serves as a mini Web browser, which can access servers and webpages, and download the content for later processing. The adapted script is shown as Script 4, with the statements in lines 9 and 10 serving to download and read the contents of a webpage into memory for processing. Line 19 is then used to call the new function with the specifc URL address of the desired webpage. 
Script 4: Download an HTML Page, Remove the Tags, and Show Its Text 
Content 

print(file_contents) 
20 

9.3.1.4 Loading an Entire Corpus and Showing its Contents 
We can extend Script 3 in a different way to process an entire corpus. To do this, we create a new function (“show corpus”) that fnds the paths of all the corpus fles in a folder (e.g. “target_corpus”), calls the “get_fle_content” function on each fle path, and then prints out the output of each fle (lines 21–28). Of course, this script is not very useful on its own, but it can be extended to form the foundation of a complete corpus toolkit, as described in Sect. 9.3.2. 
Script 5: Load an Entire Corpus and Show its Contents 

9.3.2 Case Study 2: Scripting the Core Functions of Corpus Analysis Toolkits 
For this case study, imagine that a toy corpus that comprises just three UTF-8­encoded, plain-text fles needs to be processed. Each corpus fle contains a single, short sentence and the whole corpus is stored in a folder called “target_corpus” in the project folder. The details of the “target_corpus” are given in Table 9.2. 
9  Programming for Corpus Linguistics  197  
Table 9.2 Description of target corpus for case study 2  Folder name Folder contents  Target_corpus text_1.txt, text_2.txt, text_3.txt  
Individual fle contents  
text_1.txt  
The cat sat on the mat.  
text_2.txt  
The cat chased the mouse.  
text_3.txt  
A dog barked at the cat.  

Analyzing such a small corpus can perhaps be done by hand or with a calculator. However, when developing computer programs that can analyze corpora of many thousands, millions, or even billions of words, it is often useful to test the scripts being developed with these simple examples that can be calculated exactly. This makes it possible to check if the code is running correctly and ensure that no bugs have been inadvertently introduced. 


9.3.2.1 Creating a Word-type Frequency List for an Entire Corpus 
To create a script that produces a word-type frequency list for a set of plain-text corpus fles, we can frst utilize the “get_fle_contents” function of Script 2 (or Script 3 if we want to process HTML fles). Then, we only need to adapt the “show_corpus” function from Script 5 to process each fle and count all the words in the corpus. Script 6 shows the complete program. 
Script 6: Create a Word-Type Frequency List for an Entire Corpus 

The following output produced by Script 6 is automatically saved in a fle named “word_type_frequency_list.txt”. 
Word  Frequency  Word  Frequency  
the  5  dog  1  
cat  3  mat  1  
a  1  mouse  1  
at  1  on  1  
barked  1  sat  1  
chased  1  

Script 6 introduces two new object class imports: “Counter” from the Python Core collections library, and “fndall” from the regex Python extension library. The “Counter” object is a very fast and memory effcient “key-value” data structure that is used to store the word types as keys and their growing frequencies as values as each corpus fle is processed. The “fndall” function is used to fnd all the tokens in each fle. This function uses a widely used and powerful search language called “regular expressions”, which is used to defne very precise search conditions based on four core concepts listed in Table 9.3. 
From Table 9.3, it can be seen that the r’[\p{L}]+’ regular expression used in line 47 is used to fnd one or more continuous strings of “letters” (A-Za-z for English), which is a simple but commonly used defnition of a “word” in corpus linguistics 
Table 9.3 Core concepts used in regular expressions 
Concept  Examples  
Matching with consumption (for searching and storing or replacing)  \p{L}\p{N}\p{P}·  any letter character any number character any punctuation character any character  
Matching with non-consumption (for positioning the start and end of searches)  \b  a boundary between ‘word’ characters (e.g. A-Z, a-z, and the underscrore_ for English)  
Quantifying the number of permissible results  ?or {0,1}+ or {1,}* or {0,}{m}{m,n}  zero or one occurrence one of more occurrences zero of more occurrences m occurrences between m and n occurrences  
Defning conditions  [] () |  single character alternatives (e.g. [abc] for ‘a’ OR ‘b’ or ‘c’ sequences of characters (e.g. (cat) for a string containing ‘cat’ OR separator for specifying alternatives (e.g. (cat)\(dog) for ‘cat’ OR ‘dog’  

work. The “r” prefx to the string defnition marks it as a regular expression (see: https://www.regular-expressions.info/ for more information). 
The main, “create_word_type_frequency_list” function is designed to accept four parameters: (1) the corpus location, (2) a defnition of word tokens to be counted, (3) an option to ignore case in the list (by converting all words to lowercase), and (4) a fle path for the results fle (lines 16–20). The function then performs fve main actions. First, it defnes a “corpus_folder” Path object as we saw in the scripts from Case Study 1 (line 22). Second, it defnes a “word_type_counter” Counter object which is used to store the word types and their frequencies (line 24). Third, it iterates through all the corpus fles in the directory processing each one (lines 26–35). Fourth, it ‘fattens’ the “word_type_counter” Counter object into a simple list in which the items are ordered frst by frequency (in reverse order from high to low) and then alphabetically (line 37). Fifth, it prints a header and the newly created list of word types and frequencies to the results fle (lines 39–42). 
At the end of the script, the “create_word_type_frequency_list” function is run with suitable parameter values (line 45–49). Here, the regular expression [\p{L}]+ is used for the token defnition, which translates as “a series of one or more characters in the Unicode “Letter” character category” (https://en.wikipedia.org/ wiki/Unicode_character_property). The “Letter” category is useful here as it leads to a defnition of tokens that includes “A-Z-a-z” for English, all the characters with accents for European languages, all the characters used as ‘letters’ in Asian languages such as Japanese, Korean, and Chinese, and also the ‘letters’ of all other languages defned in the Unicode standard (but not numbers or punctuation etc.). 
Script 6 is a very fast, effcient, and fully-featured program that can process and produce a word-type frequency list for the 1-million-word Brown corpus (Francis &Ku.
cera 1964) in just over 1 sec on a modest computer. This is much faster that most desktop corpus analysis tools. 

9.3.2.2 Creating a Key-Word-In-Context (KWIC) Concordancer 
To create a script that produces a classic Key-Word-In-Context (KWIC) concor­dancer, again, we can utilize the “get_fle_contents” function of Script 2 (or Script 3 if we want to process HTML fles). We then adapt the “show_corpus” function from Script 5 to process each fle and output a KWIC result for every search hit that is found. Script 7 shows the complete program. 
Script 7: Create a Key-Word-In-Context (KWIC) Concordancer 

# extract the kwic result 

The following output produced by Script 7 is automatically saved in a fle named “kwic_results.txt”. 
The  cat  sat  on  
The  cat  sat  on  the  
at  sat  on  the  mat.  
The  cat  chased  
ed  at  the  cat.  

Script 7 does not require the “Counter” object class from the Python Core collections library or the “fndall” from the regex Python extension library. However, it does use the “fnditer” and “sub” functions of the regex library (line 3). The “fnditer” function performs similarly to the “fndall” function, but produces results iteratively, one-by-one, allowing them to be saved to a fle immediately. The “sub” function, on the other hand, is used to fnd and replace (substitute) strings using a regular expression. This function is used to clean the KWIC results by removing unwanted line-break characters from the results. The call to the regex library also includes an “IGNORECASE” fag which tells the regex library to ignore case in searches. 
The main, “create_kwic_concordance” function is designed to accept fve param­eters: (1) the corpus location, (2) an option to ignore case when searching, (3) the search term, (4) a context size that defnes how many tokens to the left and right of the search term will be shown in the KWIC results, and (5) a fle path for the results fle (lines 15–20). The function then performs just three main actions. As in the previous scripts, it frst defnes a “target_corpus_reader” Path object (line 22). Next, it creates a fle handle that is used to output the results as they are generated and a variable to process the case option (lines 28–33). Finally, it performs the main action of the function, i.e., locating search term hits in the corpus fles (via “fnditer”) and generating KWIC results for each of them (lines 35–51). 
At the end of the script, the “create_kwic_concordance” function is run with suitable parameter values (lines 54–59). Here, the “search” parameter is given as r"\b[\p{L}]+at\b", which is a regular expression that translates to “any string of Unicode letter characters that is immediately followed by “at” and starts and ends with a word boundary”. This leads to results that include “cat”, “mat”, and “sat”. The “context_size” parameter is set to 10, which leads to KWIC results with 10 characters of context to the left and right of the search term. 
Script 7 is again a very fast, effcient, and fully-featured program. It can create the nearly 70,000 KWIC results for the word “the” in the 1-million-word Brown corpus (a notoriously slow search) in under 1 sec on a modest computer. This is very much faster than most desktop corpus analysis tools, which have to deal with color highlighting and other display issues. It can also search for words, phrases, or full regular expressions with case-sensitivity, and produces hits with any amount of surrounding context desired. Perhaps surprisingly, the script comprises just 59 lines of code, and again, most of these are in the form of whitespace and comments. One limitation of the program, however, is that it does not sort the results. Adding this functionality would require an additional sorting function. Or the sorting could be carried out later in a spreadsheet software tool, such as Excel. 

9.3.2.3 Creating a “MyConc” Object-Oriented Corpus Analysis Toolkit 
Script 8 recreates the functionality of Scripts 6 and 7 in a single object class called “MyConc”. As discussed earlier in this chapter, object-oriented programming offers numerous advantages over functional programming, especially for larger-scale projects. In this case, we could use the MyConc class as a foundation for a more complete corpus toolkit that could be released as an open source project allowing it to be used and extended by others. 

9.4 Critical Assessment and Future Directions 
Learning to program with a computer language is certainly not an easy task. As with learning to use a human language, it requires study, practice, and perhaps most importantly, a genuine need. There is also an aspect of creativity and beauty in computer language use that mimics that of human languages. Some programs may “work” but they are short, abrupt, and diffcult to understand. Others may be long and overly complex. This raises an aspect of programming that is often forgotten: Computer programs must be understood by humans. The frst human that needs to understand the code is the developer, especially when they return to the code months after the original project in order to fx a bug or add a new feature. Other humans are likely to see the code, too. If the program is written as part of a funded project, at some point, the developer might leave requiring others to take over the work. If the program is part of an open-source project, many people might want to contribute to the code. There is also a growing requirement by journals and funding agencies to make programs open access in order to facilitate the replicability and reproducibility of research results (Branco et al. 2017). Therefore, scripts should always be clear, clean, and readable. 
The scripts presented in this chapter are designed to illustrate good programming habits. However, they are limited in terms of scope (e.g., only two core functions of corpus analysis toolkits are presented) and functionality (e.g., the KWIC tool does not include a sorting function). Fortunately, corpus linguists who are interested in further developing their programming skills have an abundance of learning resources available to them. There are numerous MOOCs (Massive Open Online Courses) offered online, as well as specially prepared web-based courses and tutorial guides. One notable course for Python, for example, is the tutorial offered by the w3resource team (see Sect. 9.5). The main site for asking specifc questions about programming, as well as seeing code samples that have been posted in response to questions, is StackOverfow (again, see Sect. 9.5). This is a truly vital resource for anyone seriously considering to entering the world of programming. 
It is highly likely that at some point in a corpus linguist’s career, they will need to develop custom scripts to investigate their unique research questions. As discussed here, one strategy is to write these scripts directly. However, another possibility is to work with an expert programmer. In the latter case, it is important that the language of programming does not get in the way of communicating what the task should be. Corpus linguists should avoid trying to explain to the programmer how the task should be completed, e.g., saying that they want the programmer to create a program that opens each fle, tokenizes the content, and then counts the frequencies of each word. Rather, they should explain what they want, e.g. an ordered list of important words in the corpus. Through discussions, the precise meaning of “important” can be clarifed, as well as the best way to order the list. 
One danger when working with computer programmers is becoming over­whelmed by the amount of programming terminology that tends to appear in their conversations. To some extent, the discussions on different programming languages and the functional and object-oriented programming paradigms given in this chapter should help to demystify some of the terminology that may be used. Of course, most programmers are very willing to explain what they mean, so the corpus linguist should always ask for clarifcation where necessary. 

9.5 Tools and Resources 
Numerous tools and resources exist to help novice programmers download, install, setup, and use a programming language. The list that follows targets the Python and R programming languages, but a simple Internet search will produce resources that can fll the gaps for other languages 
Downloading, Installing, and Setting Up the Programming Language 
• 
Getting started with Python: https://docs.python.org/. Accessed 31 January 2020. 

• 
Getting started with R: https://www.r-project.org/. Accessed 31 January 2020. 


Online Tutorials and Resources for Python 
• 
Getting started with Python: https://docs.python.org/. Accessed 31 January 2020. 

• 
Interactive Python tutorial: https://www.learnpython.org/. Accessed 31 January 2020. 

• 
Python Tutorial: https://www.w3schools.com/python/. Accessed 31 January 2020. 

• 
Learn Python the hard way (a top-rated tutorial for beginners despite the name): https://learnpythonthehardway.org/. Accessed 31 January 2020. 

• 
Python Exercises, Practice, Solution: https://www.w3resource.com/python­
exercises/. Accessed 31 January 2020. 

• 
Natural Language Toolkit (NLTK documentation): https://www.nltk.org/. Accessed 31 January 2020. 


Online Tutorials and Resources for R 
• 
An Introduction to R: https://cran.r-project.org/doc/manuals/R-intro.pdf. Accessed 31 January 2020. 

• 
R manuals: https://cran.r-project.org/manuals.html. Accessed 31 January 2020. 

• 
Collostructional analysis with R: http://www.stgries.info/teaching/groningen/ index.html. Accessed 31 January 2020. 

• 
R Resources: https://www.ucl.ac.uk/ctqiax/PUBLG100/2015/resources.html. Accessed 31 January 2020. 

• 
R studio (with Shiny examples): https://www.rstudio.com/resources/. Accessed 31 January 2020. 


Online Communities for Programming (Including Corpus Linguistics) 
• 
StackOverfow: https://stackoverfow.com/. Accessed 31 January 2020. 

• 
Python Software Foundation: https://docs.python.org/. Accessed 31 January 2020. 

• 
Planet Python: https://planetpython.org/. Accessed 31 January 2020. 

• 
StatForLing with R: https://groups.google.com/forum/#!forum/statforling-with­
r. Accessed 31 January 2020. 

• 
CorpLing with R: https://groups.google.com/forum/#!forum/corpling-with-r. Accessed 31 January 2020. 


Packages to Allow Python and R to Interact with Each Other 
• 
The “rpy2” Python package to access R scripts from Python: https://rpy2. readthedocs.io/en/latest/. Accessed 31 January 2020. 

• 
The “reticulate” R package to access Python scripts from R: https://github.com/ rstudio/reticulate. Accessed 31 January 2020. 


Further Reading 
Bird, S., Klein, E., and Loper, E. 2009. Natural language processing with Python: Analyzing text with the natural language toolkit. Sebastopol: O’Reilly Media, Inc. https://www.nltk.org/book/. Accessed 31 January 2020. 
This book provides a comprehensive description of the Python NLTK framework, which allows even beginner programmers to easily download corpora, and analyze them through KWIC concordance views, word frequency lists, and a host of other commonly used corpus tools. 
Gries, S.T. 2016. Quantitative corpus linguistics with R: A practical introduction. Routledge (2nd edition). Abingdon and New York: Routledge. 
This book is a revised and updated edition of Gries’ 2009 introduction to R programming in corpus linguistics, which pioneered the use of R and advanced quantitative methods in corpus linguistics research. 
Desagulier, G. 2017. Corpus Linguistics and Statistics with R. Springer Interna­tional Publishing. 
This book provides another very useful introduction to the R programming language aimed at broad audience of applied linguists, including sociolinguists, historical linguists, computational linguists, and psycholinguists. 
References 
Anthony, L. (2009). Issues in the design and development of software tools for corpus studies: The case for collaboration. In P. Baker (Ed.), Contemporary corpus linguistics (pp. 87–104). London: Continuum Press. 
Anthony, L. (2014). Brainstorming the future of corpus tools. http://cass.lancs.ac.uk/?p=1432. Accessed 31 January 2020. Anthony, L. (2019). AntConc (Version 3.5.8) [Computer software]. Tokyo: Waseda University. https://www.antlab.sci.waseda.ac.jp/. Accessed 31 Jan 2020. Anthony, L. (2020). AntLab tools. Tokyo: Waseda University. https://www.antlab.sci.waseda.ac.jp/ software. Accessed 31 Jan 2020. Baayen, R. H., Piepenbrock, R., & Gulikers, L. (1995). CELEX2 LDC96L14. Philadelphia: Linguistic Data Consortium. https://catalog.ldc.upenn.edu/LDC96L14. Accessed 31 Jan 2020. Biber, D., Conrad, S., & Reppen, R. (1998). Corpus linguistics. Cambridge: Cambridge University Press. 
Branco, A., Cohen, K. B., Vossen, P., Ide, N., & Calzolari, N. (2017). Replicability and reproducibility of research results for human language technology: Introducing an LRE special section. Language Resources and Evaluation, 51, 1–5. 
Burnard, L. (2000). The British National corpus users reference guide. http:// 
www.natcorp.ox.ac.uk/archive/worldURG/index.xml. Accessed 31 Jan 2020. Chandler, B. (1989). Longman mini-concordancer [Computer Software]. Harlow: Longman. Clark, R. (1966). Computers and the humanities, 1(3), 39. 
Davies, M. (2011). Synchronic and diachronic uses of corpora. In V. Viana, S. Zyngier, & G. Barnbrook (Eds.), Perspectives on corpus linguistics: Connections & Controversies (pp. 63– 80). Philadelphia: John Benjamins. 
Dearing, V. A. (1966). Computers and the humanities, 1(3), 39–40. 
Desagulier, G. (2017). Corpus linguistics and statistics with R. Springer. 
Edberg, J., & Biber, D. (2019). Incorporating text dispersion into keyword analyses. Corpora, 14(1), 77–104. 
Francis, W. N., & Ku.
cera, H. (1964). Manual of information to accompany a standard corpus of present-day edited American English, for use with digital computers. Providence. Rhode Island: Department of Linguistics, Brown University. 
Gries, S. T. (2009). What is corpus linguistics? Language and Linguistics Compass, 3, 1–17. 
Gries, S. T. (2016). Quantitative corpus linguistics with R. (2nd rev & extend edition). London/New York: Routledge/Taylor & Francis. 
Hockey, S., & Martin, J. (1987). The Oxford concordance program version 2. Literary & Linguistic Computing, 2(2), 125–131. https://doi.org/10.1093/llc/2.2.125. 
Johns, T. (1986). Micro-concord: A language learner’s research tool. System, 14(2), 151–162. 
Johnson, K. (2008). Quantitative methods in linguistics. Hoboken: Wiley. 
Kaye, G. (1990). A corpus builder and real-time concordance browser for an IBM PC. In J. Aarts & 
W. Meijs (Eds.), Theory and practice in corpus linguistics (pp. 137–162). Amsterdam: Rodopi. Moon, R. (2007). Sinclair, lexicography, and the Cobuild Project: The application of theory. International Journal of Corpus Linguistics, 12(2), 159–181. 
Nesi, H., Sharpling, G., & Ganobcsik-Williams, L. (2004). Student papers across the curriculum: Designing and developing a corpus of British student writing. Computers and Composition, 21(4), 401–503. 
Price, K. (1966). Computers and the Humanities, 1(3), 39. 
Reed, A. (1978). CLOC [Computer Software]. Birmingham: University of Birmingham. 
Scott, M. (2020). WordSmith tools (Version 8.0) [Computer Software]. https://lexically.net/ wordsmith/. Accessed 31 Jan 2020. 
Simpson, R. C., Briggs, S. L., Ovens, J., & Swales, J. M. (2002). The Michigan corpus of academic spoken English. Ann Arbor: The Regents of the University of Michigan. 
Sinclair, J., Jones, S., & Daley, R. (2004). English collocation studies: The OSTI report. London: Continuum. 
Smith, P. H. (1966). Computers and the Humanities, 1(3): 39. 
StackOverfow. (2019). Developer survey results 2019. https://insights.stackoverfow.com/survey/ 
2019. Accessed 31 Jan 2020. Swales, J. (1990). Genre analysis: English in academic and research settings. Cambridge: Cambridge University Press. Thompson, P., & Nesi, H. (2001). The British Academic Spoken English (BASE) corpus project. Language Teaching Research, 5(3), 263–264. Tribble, C. (2015). Teaching and language corpora: Perspectives from a personal journey. In A. Le´ nska & A. Boulton (Eds.), Multiple affordances of language corpora for data­
nko-Szyma´ driven learning (pp. 37–62). Amsterdam: John Benjamins Publishing. Winter, B. (2019). Statistics for linguists: An introduction using R. Abingdon/New York: Rout-ledge. 



Part III Corpus types 
Chapter 10 Diachronic Corpora 
Kristin Davidse and Hendrik De Smet 
Abstract In this chapter, we frst consider the challenges specifc to diachronic corpus compilation. These result from the uneven (or non-)availability of historical records in respect of the varieties associated with users (temporal, regional, social and individual) and contexts of use. Various ways are discussed in which these biases can be redressed. Next, we discuss issues of diachronic corpus annotation and heuristic techniques that can be used to interrogate the syntagmatic-paradigmatic organization across the whole lexicogrammatical continuum, illustrating their rel­evance to diachronic corpus linguistics. Finally, we consider representative studies and corpora, tools and resources and key readings. These are informed by the future directions we advocate, viz. inductive, data-driven approaches to text classifcation, the identifcation of historical lexicogrammatical patterns and meaning change, and the sharing of rich data annotation and data analysis. 
10.1 Introduction 
Ever since its philological beginnings, diachronic linguistics has relied on corpus data. Past lexicogrammatical patterns are not accessible through speaker intuition or experimentation, but have to be reconstructed on the basis of the written historical record. Historical linguists therefore have always had to take recourse to collections of texts or collections of quotations. However, with the advent of electronic corpora, the speed and systematicity with which diachronic – like synchronic – records can be queried has increased tremendously, opening up new research possibilities by dramatically facilitating at least some aspects of data collection. This is, in fact, a continuing trend, as historical corpora continue to grow in number and size, and as the techniques for interrogating them become both more effcient and more sophisticated. At the same time, barring corpora of very recent history, 
K. Davidse · H. De Smet (•) KU Leuven (University of Leuven), Leuven, Belgium e-mail: Kristin.Davidse@kuleuven.be; Hendrik.Desmet@kuleuven.be 
© Springer Nature Switzerland AG 2020 211 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_10 
K. Davidse and H. De Smet 
diachronic data will always present us with important problems, failing to represent all the variation associated with the users and contexts of use, with especially spoken language as the perennial gap. Despite justifed enthusiasm about the recent advances in diachronic corpus linguistics, such concerns must be taken into account in corpus design as well as in the development of heuristic techniques to crack the code of past language systems and to explain variation and change. 
In the most general terms, our plea here is one for informed use of diachronic resources. For end users to make appropriate use of a corpus it is instrumental that they understand how it has been compiled. The corollary of this is that compilers should set out their compilation procedures accessibly and explicitly. At the same time, it would be benefcial for the research community as a whole to consider ways in which historical resources can be built and enriched in a more dynamic and bottom-up way, to ensure they are maximally adaptable to specifc research needs as well as being more responsive to newly gained insights (Diller et al. 2011; Nevalainen et al. 2016). The reason is that many of the problems of corpus compilation are not just theoretically uninteresting preliminaries to research, but actually refect issues that can seriously affect data interpretation, to the point of being research-worthy in their own right. 

10.2 Fundamentals 
10.2.1 Issues and Challenges of Diachronic Corpus Compilation 
There is one thing that all diachronic corpora have in common: the usage data they contain is at least organized along the temporal dimension, such that comparison across earlier and later manifestations of a language becomes possible. Apart from that, diachronic corpora differ widely in size, composition, scope, annotation, and the nature of their textual material. Because of the limitations of the historical data available, and because research goals are very diverse, there is no such thing as an ideal historical corpus. Specifc requirements of diachronic research simply need to be met in different ways. Nevertheless, there are recurrent challenges that both compilers and end users of diachronic corpora have to confront. 
As diachronic corpora are typically used to study language change, and language change is generally understood to arise from and give rise to language variation, it is something of a bitter irony that one of the greatest diffculties diachronic corpora face lies precisely in capturing historical variation. This holds for both major dimensions of variation, which – following Gregory (1967) and Romaine (2000)– we will refer to here as lectal and diatypic. Lectal variation refects “reasonably permanent characteristics of the user [italics ours]” (Gregory 1967:181), including variation that is structured temporally, regionally, socially and individually. Diatypic variation refects “recurrent characteristics of the user’s use of language in situations [italics ours]” (Gregory 1967:185) and depends on communicative goals, the mode of communication, and the speaker-addressee relationship. The typical challenges of compiling diachronic corpora, then, include (1) identifying the lectal and diatypic properties of texts, (2) handling the lectal and diatypic biases of the historical records and (3) circumventing impaired comparability across lectally and diatypically diverse datasets. These issues are the topics of the following sections. 
10.2.1.1 Identifying the Lectal and Diatypic Properties of Texts 
In addition to being organized along the temporal dimension, diachronic corpora often include information on other lectal and diatypic properties of the texts they contain. Identifying these properties, however, may be diffcult for historical texts. The challenges start with the identity of historical authors, which more often than not is something of a mystery. Added to that there are the complexities of textual transmission. Think, for instance, of scribal interference in mediaeval texts or editorial interference in more recently published materials. Tellingly, when creating the Helsinki Corpus of English texts, which was the frst diachronic electronic corpus of any language, its compilers found themselves forced to provide many of the older texts with multiple period labels, as the best way to indicate both a text’s (approximate) manuscript date and its creation date (Kyt1996). In general, the older the text material, the more reasonable it is to suspend overly precise attempts at dating and locating texts. But these problems are certainly not restricted to premodern texts. In fact, due to both the ease of digital reproduction and the anonymity of the world wide web, many recent web-based corpora, such as COW or GloWbE, run into the problems that only a decade ago were mainly associated with premodern texts, in that almost nothing is known about the language users represented, including some of the most basic sociolinguistic variables, such as age, gender and linguistic background (see also Chap. 15). 
Of course, not all historical texts pose these diffculties, and some offer special opportunities. Letters, for instance, have several advantages as a source of historical text material. They are often precisely located in time and space, can often be unambiguously assigned to a single author, have in their addressee a clearly identifable target audience, and may represent speakers who have otherwise left no written records. A corpus that has used this type of data to the best advantage is the Corpus of Early English Correspondence, compiled by Terttu Nevalainen, Helena Raumolin-Brunberg and a team of collaborators (Raumolin-Brunberg and Nevalainen 2007). Not only does the corpus contain letters from the sixteenth to eighteenth century, it also covers four major English regions, as well as giving information on the social rank of the letter writers and how they relate to their addressees. Providing this type of social background information of course poses new challenges, requiring a thorough knowledge of the social structure of past societies. The compilers eventually arrived at a very elaborate coding scheme to describe the letter writers in their corpus, using 27 different parameters, some with very open information. Parameters include, for instance, the letter writer’s gender, year of birth, occupation, rank, their father’s rank, their education, religion, place of 
K. Davidse and H. De Smet 
residence, any history of migration, their careers and social mobility, the general type of contents of their letters, and how well their letters can be authentifed. Additional information on a letter writer, if potentially relevant, is included in open comment boxes. In providing all this information, the compilers clearly chose to collect as much metadata as possible. As a result, it is to an important extent also up to the end user to interpret the complexities of historical reality. 
On the whole, however, detailed background information on historical texts is often impossible to come by directly. While in such cases healthy agnosticism remains a sensible default option, it is good to be aware that avenues towards possible solutions are currently being explored. Particularly in the domain of authorship, automated stylometric techniques now allow probabilistic identifcation. An example is the authentication of the writings of Julius Caesar by Kestemont et al. (2016). The Latin texts in question report on the military campaigns conducted by Julius Caesar. Some of the texts can be confdently attributed to Caesar himself, but there has long been controversy about others. The work by Kestemont et al. (2016) confrms that one of Caesar’s generals, Aulus Hirtius, has a good claim to the authorship of some of the writings. This type of work is of course potentially relevant to corpus compilers, who could rely on it to annotate the texts in their corpora. Moreover, while the probabilistic nature of identifcation may at frst sight appear to be a disadvantage, in the long term it may actually liberate compilers from overly rigid and reifed systems of classifcation. 
Another recent development promising new insights into lectal variation is the increasing access offered by very large corpora to individual variation. One example is the Hansard Corpus, compiled by Jean Anderson and Marc Alexander, which contains the proceedings of the British Houses of Parliament from 1803 to 2005 and represents nearly 40,000 individual speakers. Corpora built from Parliamentary proceedings may become immensely valuable to historical linguists and sociolinguists for a number of reasons (Marx 2009). First, comparable datasets are available for other languages than English, often free of copyright. Second, especially for the more recent decades, the lives of the language users who produced the texts are mostly well-documented, and even the social relations between them are to some extent known. And third, these datasets are already intrinsically structured along linguistically relevant dimensions (by date, speaker, speaker role, party affliation, house etc.), relieving the corpus compiler at least to some extent from the burden of imposing structure on the data. In the future, these very rich data sets may allow researchers to further disentangle the complexities of lectal variation. Because the usage of individuals can be compared directly, less recourse needs to be taken to what are potentially aprioristic and artifcial classifcations of language material based on authors’ or speakers’ regional provenance or social status. 

10.2.1.2 Redressing Historical Bias 
While classifying and contextualizing available text material will always pose diffculties, many of the problems diachronic corpora face do not come from the 
texts they have, but from the texts they do not have. Depending on the historical period at issue, the historical record is patchy to a greater or lesser degree, but in practically all cases it is severely biased. The voices of the less powerful and/or less literate strata of the population are as a rule unrecorded, and the texts that do reach us are – prior to the large-scale employment of modern recording equipment – biased to the written mode. Considering that direct spoken interaction and patterns of social stratifcation are believed to be crucial to the emergence and transmission of linguistic variants, this is of course a frustrating situation. Moreover, this is one area where, despite their merits, current big data projects may (for now) be exacerbating the problem by their tendency to go where the data is and harvest text materials indiscriminately. Typically, it is corpora making the most of unique but limited and less accessible resources that are best placed to redress the biases in the historical record. A good example is the Corpus of Early English Correspondence, already discussed above. Another example is the data set used by Blaxter (2015), who carefully extracted the direct reported speech passages from Old Norse sagas to assess the role of speaker gender in ongoing change in a mediaeval Scandinavian setting. 
It is instructive here to consider in some more detail another substantial effort to create a diachronic corpus that contains – at least by proxy – socially stratifed spoken interaction. The Old Bailey Corpus (OBC2.0), compiled by Magnus Huber and his team (Huber 2007; Huber et al. 2012), consists of trial proceedings from London’s Central Criminal Court, the Old Bailey, published between 1720 and 1913. Containing the published transcripts of the spoken interactions in court, it is about as close to a corpus of Late Modern spoken English as one can get, with the added bonus of a socially very diverse set of speakers. But, while the OBC2.0 is decidedly an exciting resource, the question remains to what extent it really represents spoken usage. In general, trial proceedings have been argued to give us some of the most reliable data on spoken usage before the advent of audio-recordings (Culpeper and Kyt2000). Regarding the material in OBC2.0 in particular, a balanced discussion is provided by Huber (2007) (but see also Archer 2014). The speech recorded in OBC2.0 obviously does not come down to us directly but has typically been transcribed in shorthand during the court sessions, reworked afterwards into standard text, typeset, perhaps proofread and printed. The most crucial step here is probably the transition from speech to shorthand transcript. Scribes themselves appear to have prided themselves on their ability to make verbatim records of the proceedings. At the same time, some of the tell-tale characteristics of spoken interaction (pauses, false starts, repetitions, etc.) are obviously missing from the printed records. What is most disturbing is that there may be very substantial lexical, grammatical and textual differences between records in the proceedings of the Old Bailey and records of the same trials published elsewhere. Then again, it is reassuring to fnd that the direct speech passages in OBC2.0 differ from the remainder of the corpus, for instance containing far more instances of contraction – much as one would expect if the direct speech passages refect actual speech. 
K. Davidse and H. De Smet 
Though limited to recent periods, another way to counter the biases in the historical record is of course the compilation of diachronic corpora containing actual audio-recorded speech (see Chap. 11 for more information about spoken corpora). There are several ways to pursue this goal. First, corpora of spoken usage began to be compiled in the second half of the twentieth century. Initially intended as representative of contemporary usage, these corpora are now gradually becoming historical corpora and it is to be hoped that current and future researchers will be willing to repeat the efforts of their predecessors to create new contemporary and comparable spoken corpora. One such effort is the recent creation of the Spoken BNC 2014, whose structure echoes the spoken component of the British National Corpus, originally released in 1994 (Love et al. 2017). Second, corpus compilers have also begun to dig into existing archives of old recordings, such as oral history projects. In some cases, the time depth that can be achieved is impressive. For example, Hay and Sudbury (2005) have been able to trace the emergence of English intrusive r (as in law-r-and-order) in New Zealand English thanks to the Origins of New Zealand English Project, which contains recordings from English-speaking New Zealanders born as early as 1850. Third, the exponentially increasing amounts of recorded speech posted online in combination with the advances in speech-to­text technology mean that a diachronic corpus compiled by automated harvesting and transcribing of large amounts of freely available spoken data is literally only a matter of time. 

10.2.1.3 Diachronic Comparability 
The problem of gaps and biases in the historical record is further complicated by the temporal dimension of diachronic corpora. Not only is it diffcult to approximate the full range of synchronic variability for a language at a given point in time, there is a further diffculty in doing so without compromising diachronic comparability. For example, given a rich historical record and a long tradition of high-quality text editions, it is perfectly possible to create a sizeable corpus of Old French, as shown by the FRANTEXT database. Yet it is impossible to create a corpus of Old French that is comparable in any straightforward way to a corpus of Present-day French. The reason is, put simply, that there is no Old French equivalent to a Present-day French newspaper, just as there is no Present-day French equivalent to an Old French epic poem. Similarly, it is possible to create a sizeable corpus of Latin, with a time-depth of over 2000 years, as shown by the 13-million-word LatinISE corpus, built by Barbara McGillivray. However, even despite efforts to preserve genre balance, any diachronic comparison across this timespan has to take into account that the language used in such a corpus changes from a relatively standardized written language with more or less direct roots in a contemporary vernacular to a functionally impoverished and much more writing-dependent language used mainly in religious contexts and as a lingua franca among European scholars and scientists. 
More generally, because the conditions under which language is produced are themselves subject to change, it is fundamentally impossible to compare linguistic material only along its temporal dimension. In this respect, historical linguists are always comparing apples and oranges. Arguably, all major diachronic reference corpora, though often striving to produce stratifed samples of language use across time, suffer from this problem. 
One possible response is to ignore the issue and simply include material as exhaustively as possible. This approach prioritizes coverage of synchronic variabil­ity, to the best level achievable and with no prior assumptions made. Especially where the body of historical data is fnite, disparate and severely biased, or where a corpus is to be used to study change over very long time periods, this is a perfectly defendable strategy. An example is the Dictionary of Old English Corpus (compiled by Antonette di Paolo Healey and colleagues), which exhausts all Old English texts available down even to the odd Runic inscription. Another example is the Oxford Corpus of Old Japanese (compiled by Bjarke Frellesvig and colleagues). 
A completely different response to the issue of comparability is to create single-genre diachronic corpora that cover relatively short time spans and draw their data from a single historical source. The OBC2.0 or the Hansard Corpus, both already discussed above, are good examples. The single-genre approach has the advantage of improving diachronic comparability, but comes at the expense of coverage, both with respect to synchronic variability and time-depth. Moreover, although diachronic comparability improves, it may still not be optimal, because genres themselves tend to be moving targets. Consider again trial proceedings, as represented in the OBC2.0. The recording of trial proceedings in the Old Bailey started outside the actual control of the court, as publishers were commercially interested in the more sensational cases and sent out their scribes to record them. But in the course of the eighteenth century the proceedings gradually developed into offcial true-to-fact records. Another example is found in De Smet and Vancayzeele (2014), who show that eighteenth-century English narrative fction contains far fewer action sequences and has longer descriptive passages than later narrative fction. In a diachronic corpus of narrative fction this inevitably affects the frequencies of specifc grammatical patterns associated with either descriptive or more action-driven narrative passages. Especially over longer time spans, it is virtually impossible to keep genre – or, for that matter, any other diatypic parameters – constant. 
Besides awareness of these diffculties, a more bottom-up and data-driven approach to describing diatypic text properties may, in the long term, provide the more satisfactory solutions. Dimensions such as ‘spokenness’ or ‘formality’ can be operationalized and measured on a text-by-text basis from linguistic properties (Biber 1988). For example, Hinrichs et al. (2015) apply a number of relatively simple measures to their English corpus data to assess individual texts’ adherence to prescriptivist dogma, which they then use as a predictor in a variationist study. One such measure is the relative rate of occurrence of shall and will as future auxiliaries. To apply such methods more systematically and across long time-spans will require further research, but it at least allows researchers to position texts relative to one another and to the contemporary norm on one or more dimensions of interest. The implication is again that responsibility for interpreting the structure of a corpus 
K. Davidse and H. De Smet 
moves from the corpus compiler to the researcher. It also means that it may be necessary for corpora themselves to become the object (rather than just a means) of study. 
10.2.2 Issues and Challenges of Text-Internal Annotation 
Turning from the level of the texts that make up a corpus to the internal properties of those texts, perhaps the most fundamental question compilers and users of diachronic corpora must ask is to what extent they can rely on methods devised for the annotation and analysis of contemporary data in handling data from older periods. Older texts are in principle somewhat alien. Their writing conventions differ from present-day practices and, obviously, the very language they represent is different from any present-day variety. Added to this is a layer of inadvertent ‘noise’ created along the way as a corpus text travels from historical manuscript or print to digital edition. All of this complicates even the most basic analyses, including the identifcation of lexical items, grammatical classes and grammatical structures. Nevertheless, corpora whose texts have been annotated with lexical and grammatical information can of course be extremely valuable tools for research. 
The most straightforward problems are the strictly technical issues. The devel­opment of new techniques of corpus analysis is often spearheaded by research on contemporary performance data. To extend such techniques to older data requires both circumspection and additional technical know-how. For example, Schneider et al. (2016) describe the application of part-of-speech taggers created for contemporary English to older text material, comparing their performance against results for present-day data. They found that, although (surprisingly) tagging accuracy improved for nineteenth-century texts, it became progressively worse for older material. They further describe ways to improve results, including spelling normalization using VARD (Baron and Rayson 2008), combining different taggers, and modest manual intervention. An excellent discussion of the technical challenges of applying Natural Language Processing to historical texts (including Optical Character Recognition and spelling normalization) is offered by Piotrowski (2012), who draws examples from a variety of languages and historical corpora. 
Solving technical problems of course pertains to only one side of the issue. More fundamental are matters of linguistic analysis proper, which present all the problems associated with lemmatization, tagging and annotation of synchronic data to a much higher degree (see Chap. 12 for a general introduction to corpus annotation; see again Piotrowski 2012 for discussion of the various techniques, e.g. for part-of­speech tagging and syntactic parsing). For this reason, rich annotation is best seen only as a means to facilitate querying a corpus. 
Consider, for example, the York-Toronto-Helsinki Parsed Corpus of Old English Prose (YCOE), developed by Ann Taylor and collaborators (Taylor et al. 2003; Taylor 2003), who used a combination of automated and manual parsing to create a 1.5-million-word syntactically annotated corpus representing Old English prose. 
While exciting and impressive, it is nevertheless good to bear in mind that the syntactic annotation in YCOE is indeed a tool. That is, it is not meant as a fnal analysis of all the sentences in the corpus but as a means to retrieve with greater ease data that is of potential research interest and that is otherwise nearly impossible to collect. The syntactic annotation scheme is a simplifed and in some ways deliberately agnostic version of generative X-bar theory. Optimal precision and recall are certainly not guaranteed (see Chap. 2 on precision and recall). As regards precision, results collected automatically from the corpus should be manually checked to make sure they actually include what the researcher is looking for. As Taylor herself points out, there may be “a strong temptation to skip this relatively time-consuming step”, but it “must be manfully resisted” (2003:200). As regards recall, solutions might be either to go through part of the corpus manually (if the search target is reasonably frequent), or to compare corpus fndings to fndings reported in earlier work where data has been collected manually (if the target is infrequent) (see D’hoedt 2017, who applies both methods). 
Finally, any effort at creating richly annotated corpora runs the risk of obscuring existing patterns in the data. It is not obvious, for instance, that Old English authors had a concept of sentences that is exactly comparable to the notion of sentence assumed by the formal theory underlying the parsing in YCOE – Old English punctuation, in any case, suggests otherwise (Fischer et al. 2017:163). In other words, in the end it is again the researcher who should always be wary of any prior assumptions and who should try to fnd ways to let historical data speak for themselves. At the same time, the ideal practice for corpus compilers is to strive to give end users access to original spelling, punctuation and even text layout. As is the case now for many online text archives, users can only beneft from being able to consult high-quality images of the original manuscripts or prints in the corpus. 
10.2.3 Issues and Challenges Specifc to the Analysis of Diachronic Corpora 
From the design properties of corpora and their texts, we move to the actual use of diachronic corpora for research. Methods of corpus interrogation will be affected by how linguistic organization is conceived. This holds a fortiori for the complex interrogation of diachronic corpora. The tenet that lexicon and grammar form an integrated continuum of form-meaning pairing has gained ground to the point of probably forming the majority position in current corpus linguistics. In this section, we will discuss a number of heuristic techniques that can be used to interrogate the syntagmatic-paradigmatic organization across the whole lexicogrammatical continuum, and we will illustrate their potential relevance to diachronic corpus linguistics. 
It is generally recognized that, at the lexical end of this continuum, major methodological progress has come from Firth’s (1957) insight that the meaning 
K. Davidse and H. De Smet 
of a word and the words it frequently co-occurs with mutually infuence each other. This co-occurrence manifests itself syntagmatically in the relation between a node and its collocates irrespective of their grammatical classes or relations. The larger paradigmatic organization is formed by the relations between the node and all its collocates (see also Chap. 7). Collocation-based methodology measures the degrees of attraction or repulsion between a lexical node and other individual lexical items, which are revealing of the lexicosemantics of the node, and its semantic prosody (Sinclair 1991). The drawing up of “behavioural profles” of lexical items (Gries and Divjak 2009) can objectively identify different polysemous senses of one word, and relations of synonymy and antonymy between different words. A good diachronic illustration is Kossmann’s (2007) study of the lexicosemantic variation and change of poor and rich and related adjectives in Old and Middle English on the basis of detailed contextual and collocational analysis of large historical databases. Kossmann (2007:70ff) nuances earlier, less data-driven claims that rich meant ‘powerful’ in Old English and acquired the sense ‘wealthy’ in Middle English. She shows that rich had a polysemous structure from Old English on, which persisted beyond Middle English, all the while refecting sociocultural changes. She shows that similar culturally evolving polysemies typify the main antonyms of rich, such as Old English arf (‘needy’) and the Middle English loanword poor. Kung’s (2005) collocational study of the noun melancholy in British historical novels shows that its semantic prosody changed from negative in the pre-romantic period to positive in the romantic period, as refected in a shift from mainly negative to predominantly positive collocates. Study of changing collocational patterns can also reveal essential dimensions of delexicalization and grammaticalization processes. In his study of the development of intensifers, Lorenz (2002) noted that advanced grammaticalization, as manifested by very in English, correlates with the lifting of preferences for specifc sets of collocates and semantic prosodies. In her study of the grammaticalization of expressions like heaps/piles of into quantifers, Brems (2003:291) operationalizes delexicalization in corpus data in terms of “a gradual broadening of collocational scatter or a loosening of the collocational requirements of the MN [measure noun] via such semantico-pragmatic processes as metaphorization, metonymization, analogy, etc.”, which typically precede the actual grammatical re-analysis into a quantifer marked by changing agreement behaviour. 
With collostructional analysis (Stefanowitsch and Gries 2003), the focus of description moves somewhat more towards the grammatical end of the lexicogram­mar: this method studies which lexemes are strongly attracted or repelled by a particular slot in a construction, i.e. occur more frequently or less frequently than expected. The earliest case study of diachronic distinctive collexeme analysis was, according to (Hilpert 2008:41), Kemmer and Hilpert (2005), which reconstructs the shifting preferences for types of lexical collocates attracted to the verbal complement of the English causative construction with make. In the earliest stages, the complement slot attracted mainly verbs referring to mechanical action such as grow. In later stages, emotional and cognitive verbs such as cry were preferred and fnally verbs depicting epistemic states such as seem. This semantic development is interpreted as an instance of progressive subjectifcation. 
Further towards the grammatical end is colligational analysis, which is now mostly implemented in Sinclair’s (1991) defnition as the relation between a lexical node and grammatical categories. Changes in the co-occurrence of grammatical categories with lexical nodes may refect grammaticalization. For instance, the development of progressive aspect meanings of be in the middle/midst of is refected by colligational extension from nouns designating spatial extensions or temporal periods to, frst, nominalizations and deverbal nouns, e.g. (1), and, secondly, verbal gerunds, e.g. (2) (Van Rompaey and Davidse 2014). 
(1) 
While you were in the middest of your sport [ ...] (OED, a1548) 

(2) 
[ ...] when you are in the middle of loving me. (CLMETEV, 1873) 


Firth’s (1957) original notion of colligation was more purely grammatical. It is concerned with interrelations between “elements of structure [that] [ ...] share a mutual expectancy in an order which is not merely a sequence”(Firth 1957:17). Colligations form the basis of “the statement of meaning at the grammatical level” (Firth 1957:13). Davidse and Van linden (2020) discuss changing colligations (in this sense) in extraposition constructions with matrix ‘it is a/no little/etc. wonder’ and related constructions with ‘there is no/little/etc. doubt’. They redefne these constructions as one macro-construction subsuming two distinct subtypes, the generally recognized instances with predicative matrices as well as ones with existential matrices, on the basis of two colligational relations. 
Firstly, predicative and existential matrices are distinguished from each other by the different diachronic realization of the position syntactically enclitic with the fnite matrix verb. In predicative matrices, this paradigmatic distribution is zero (3), that (4), it (5). In existential matrices it is zero, it, there, but never * that, which according to Larsson (2014) is the characteristic distribution of existential clauses in Germanic languages. In view of this distribution and the existential meaning of be (nan) tweo in the whole historical dataset, Davidse and Van linden (2020) reject the predicative analysis of examples with it like (7) in YCOE, where nan tweo is annotated as NP-NOM-PRD.1 
(3) 
Micele mare wundor is t he wolde beon mann on sum life ‘Much greater wonder it is (lit: is) that he wanted to be a human in this life’ (YCOE, 950–1050) 

(4) 
t is wundor, t  swa ræ forhæfdnisse & swa hearde habban wilt. ‘that is wonder, that you want to have ferce and harsh abstinence’ (YCOE, 850–950) 

(5) 
Full mycel wundor hit wæs t t mæden gebær cild. ‘Full great wonder it was that that maiden bore a child’ (YCOE, 1050–1150) 

K. Davidse and H. De Smet 

(6) 
Nis s nne nan tweo, gif suelc eaodnes bimid oum godum awum begyrded, t t bibeforan Godes eagum soeaodness, [...] ‘There is about that then no doubt (lit: not-is no doubt), if such humility is encompassed with other good virtues, that that is true humility before God’s eyes’ (YCOE, 850–950) 

(7) 
Form hit is nan tweo t  goodan beosymle waldende [ ...] ‘Therefore there (lit: it) is no doubt that the good ones are always powerful’ (YCOE, 850–950) 

(8) 
[ ...]anæs r nænig tweo, t hit nealæhte ra forore,  r gecigde wæron. ‘there was no doubt then that it drew near to the death of them who were named there’ (YCOE, 1050–1150) 

Secondly, predicative and existential matrices historically used the same pronouns to refer to the complement clause. In Old English, the most commonly used pronoun was demonstrative that, which occurred as subject (nominative) in predicative matrices like (4) and as adjunct (genitive) in existential matrices like (6). In a further stage, which has persisted into Present-day English, non-salient pronoun it became predominant, functioning as subject (5) in predicative matrices and as complement of a preposition like about (9) in existential matrices. 

(9) 
There is no doubt about it that he is in discomfort all the time (WB) 


1Of the 12 matrices in YCOE containing be + NP with tweo only, 10 are tagged as NP-NOM-PRD and2asNP-NOM. 
The fact that this colligation with its changing realization (that, it) occurs in both predicative and existential matrices suggests that the so-called it-extraposition construction is part of a larger class of evolving complementation constructions, even though, because of the different matrix syntax, reference to the complement is obligatory in predicative and optional in existential matrices. 
Finally, at what is arguably the most distinctively grammatical end of the lexi­cogrammar, we fnd syntactic paradigms based on relations between constructions. Relations between constructions have been studied mainly from a variationist perspective, in which examples of variants are annotated in terms of various pre­dictors (language internal, language-external, information theoretic), and processed statistically (Gries 2017.). Typical language-internal parameters, often referred to as “discourse functional” (Gries 2017:9), are animacy, humanness, defniteness, givenness, etc. A representative diachronic case study is Szmrecsanyi et al.’s (2016) study of genitive variation in Late Modern English, which considers the ‘s and of genitive, as well as noun-noun realization of the possessor-possessum relation. The study (2016:1) establishes “an overall drift towards the N-N genitive, which is preferred over other variants, when constituent noun phrases are short, possessor constituents are inanimate, and possessum constituents are thematic”. 
As pointed out by McGregor (1994:305), syntactic paradigms have been studied less in terms of what they can reveal about meaning and semantic change. Alternations associated with lexical verb senses allow the analyst to interrogate usage data for all the aspects associated with verb-argument semantics. Firstly, verb-specifc alternations “can be used effectively to probe for linguistically pertinent aspects of verb meaning” (Levin 1993:1), i.e. to draw up linguistically-based, rather than intuition-based, classifcations of verb senses. Secondly, as argued by Laffut and Davidse (2002), the study of verb-specifc alternations in corpus data can also reveal semantic selection restrictions on the arguments. They investigated the lexical sets realizing the arguments of the two types of locative verbs, spray-verbs (e.g. spray, smear, spread) and load-verbs (e.g. load, pack). They found that alternating spray-verbs have locatums designating dispersive entities (e.g. water, butter, herbs, sheet), which because of this semantic feature can be construed either as patient or oblique argument with preposition with. Alternating load-verbs have locations that are containers (e.g. box, suitcase, car booth), which semantic feature likewise motivates their being codable as either patient or oblique with in(to).In a sorting experiment involving the in/on(to)-with-alternation, Perek (2012:628) found evidence that users mentally store “constructional meaning abstracted from the meanings of the variants of the alternation”. We propose that the generalizations stored by users involve precisely such features as the ‘dispersiveness’ shared by spray-verbs and their locatums across alternations, as well as more general semantic features such as (non-)intentionality of the relation between agent and action, etc. 
Lemmens (1998) exploits the heuristic potential of verb-specifc alternations in his diachronic study of abort. On the basis of OED-data, he reconstructs changes in its alternation paradigms, which he correlates with meaning changes of the verb and changing selection restrictions on the arguments. The verb abort came into English from Latin in the sixteenth century as an intransitive verb with the meaning “intr. Of a pregnant woman or animal: to expel an embryo or fetus from the uterus, esp. before it is viable; to suffer a spontaneous abortion or miscarriage” (OED, abort v intr 1a), as in (10). Abort then developed metaphorical meanings, which could be construed both intransitively (11) and transitively (12). That is, these new meanings enabled the causative-inchoative alternation (Levin 1993:27f), in which the intransitive construes the ‘coming to a premature end’, while the semantic scope of the transitive also includes the cause of the premature end (which is implied by the passive in 12). 
(10) The pregnant woman which hath tenasmum, for the moste parte aborteth 
[L. abortit] (OED, 1540) 
(11) 
Hee wrote a large Discourse..which he intended to send to her Maiestie..but that death preuented him; and (he dying) that worke aborted with him. (OED, 1620) 

(12) 
It [sc. the Parliament] is aborted before it was born. (OED, 1614) 

In Modern English, abort acquired the meaning of ‘deliberately terminating a pregnancy’, as in (13). In this meaning, abort is a purely transitive verb that does not participate in the causative-inchoative alternation because it does not designate (the causation of) a quasi-spontaneous event. Rather, this meaning of abort expresses the intentional targeting of the action of ‘aborting’ onto the unborn child. 

(13) 
I don’t think I would abort a baby. (WB) 


Alternations that are not dependent on the lexical verb differ from the verb-specifc ones in fundamental ways. They are not selectively but generally available to all clauses with internal constituent structure. Examples are subject-fnite inver­
K. Davidse and H. De Smet 
sion, anteposition of non-subjects in the clause, etc. With these alternations, each syntagmatic variant is meaningful in its own right. From the perspective of functional frameworks such as Halliday (1994), these variants appear as members of mood paradigms and information structure systems. Formation of moods (e.g. declarative, interrogative) and information variants (realized by linear order and prosody) are by and large not dependent on the verbs used in clauses. A classic corpus study of this type of variation is Breivik’s (1989) reconstruction of the transition from the ‘expletiveless’ existential clauses of early Old English to existentials with it and there in Middle English. This study encapsulates the challenge of identifying and interpreting changing paradigms, in relation to changes both in the coding and positioning of subjects in declaratives and interrogatives, and in the marking of information structure. 
To sum up, in this section, we have discussed and illustrated a number of heuristic techniques that can be used to interrogate lexicogrammatical patterning in diachronic data from various perspectives: collocational, collostructional, colli­gational and variational. 
Representative Study 1 
Perek, F., and Hilpert, M. 2017. A distributional semantic approach to the periodization of change in the productivity of constructions. International Journal of Corpus Linguistics 22:490–520. 
In their study, Perek and Hilpert (2017) seek to identify patterns inherent in the data, rather than applying pre-existing classifcations. They do so in two areas of diachronic study: qualitative semantic change and periodization of change. 
Their case studies focus on changes in the semantic range of verbs found in the ‘V the hell out of ’ construction and the ‘V one’s way’ construction through the various decades represented in the Corpus of Historical American English (COHA), for which they develop an alternative to the collostructional approach (see Sect. 10.2.3). They argue that the meaning of lexical items can best be revealed by their association with mid-to high-frequency content words, which are semantically specifc and co-occur with a wide range of target words in non-random ways, and therefore “yield robust measurements of meaningful lexical associations” (Perek and Hilpert 2017:496). As the two case studies focus on verbs, they frst constructed a distributional matrix for all verbs from COHA with a corpus frequency of at least 1000 to guarantee suffcient distributional data to make meaningful comparisons with other verbs. The 2532 verbs extracted were then related in a matrix to the 10,000 most frequent nouns, verbs, adjectives and adverbs in COHA, frst recording their raw frequencies of co-occurrence and then converting these into positive 
(continued) 
measures of strength of association. This matrix was then transformed and reduced into a matrix where each verb receives 300 numerical values, which constitute the verb’s high-dimensional vector, the values being understood as co-ordinates in a multidimensional space. Such a model is referred to as a vector space model, which allows the authors to precisely quantify semantic similarity between words “by similarity in their semantic vectors, whose correlation (as opposed to sheer closeness) can be quantifed by standard measures such as cosine distance” (Perek and Hilpert 2017:500). For the specifc case studies at hand, they then built representations of the semantic range of the verbal slot-fllers in each successive period by summing and averaging their vector values, i.e. calculating period vectors. These represent the semantic average of the lexical types, making abstraction of both token and type frequency in that period. Comparison of the period vectors shows whether the semantic range of the verbs in the constructions expanded or contracted in the various periods – to what degree and how slowly or quickly. 
Perek and Hilpert fnd that only the ‘V one’s way’ construction has undergone qualitative semantic changes. Initially it was associated mainly with verbs expressing the creation of a material path, e.g. carve, break, rip, fght, but from the 1880s it also accommodated verbs of perception, cognition and communication, e.g. smell, guess, joke, talk, expressing the creation of metaphorical paths. Perek and Hilpert then compare these results with the fndings obtained by collostructional analysis, which does not flter out highly frequent and semantically neutral collocates that do not contribute much to the meaning of the construction. The collexemes that in the early stages of the ‘V one’s way’ construction score highest for being more frequent than expected are take one’s way and fnd one’s way, which arguably are barely instances of the ‘V one’s way’ construction. 
Perek and Hilpert then take on the intrinsic periodization of changes as opposed to relating them to language-external historical landmarks. For this, they use variability-based neighbour clustering (VNC), a variant of an agglomerative hierarchical clustering algorithm which allows only periods that are temporally adjacent to be merged. VNC was proposed as a method for inductive periodization by Gries and Hilpert (2008) in combination with collostructional analysis. Perek and Hilpert (2017) combine VNC with a distributional semantic approach to periodize on the basis of qualitative semantic change. For the ‘V the hell out of ’ case study, this yields a radically different periodization than that on the basis of quantitative change. If VNC is combined with frequency-based data, i.e. type frequency, token-frequency and hapax legomena, then a sharp divide emerges for this construction between the period from the 1930s to 1970s and the period from the 1980s to 2000s, in which productivity sharply increased according to all the frequency indicators. By contrast, if VNC is combined with distributional 
(continued) 
K. Davidse and H. De Smet 
semantic representations, then each decade witnesses consistently gradual and relatively minor qualitative change, suggesting that no discrete periods of qualitative change should be distinguished for this construction. 
Representative Study 2 
Buyle, A., and De Smet, H. 2018. Meaning in a changing paradigm: 
The semantics of you and the pragmatics of thou. Language Sciences 
68:42–55. 
Buyle and De Smet’s (2018) study is based on a small but richly annotated corpus of seventeenth and eighteenth century comedies. The advantage of comedies is that they contain dialogic interactions within complex social settings. While the settings themselves are of course fctitious and often unrealistic, they can nevertheless reveal how the symbolic resources of the language at the time would have been used to respond to and shape social relations. To exploit this property of drama texts, Buyle and De Smet annotated all speaker-hearer dyads in their corpus for three interactional variables, describing whether the speaker has any authority over the hearer or vice versa, whether speaker and hearer are socially intimate or distant, and whether (at any one point) their relationship is conficted or not. 
Using this information, Buyle and De Smet reassess the use of the Modern English address pronouns thou and you. While earlier literature suggests that you in particular had become a semantically neutral pronoun by the seventeenth century, the analysis shows that it in fact continued to associate with deferential and formal usage as long as thou was still a systemic option. As for thou, which earlier literature analyzes as a marker of negative speaker emotion, the analysis shows that the association with expressions of anger and contempt is in fact partly an artefact of the data – in that negative emotions are simply more often expressed in intimate relations – and that it is partly a result of thou’s increasing pragmatic salience, following from its dwindling frequency. This brings the semantics of Modern English thou and you closer in line with the classical analysis of a pronominal T/V system by Brown and Gilman (1960) and departs from alternative analyses that tended to assign an exceptional status to the English thou/you contrast. 
For present purposes, Buyle and De Smet’s study shows that there is insight to be gained from small and closely annotated purpose-built corpora, particularly when it comes to some of the more elusive domains of grammat­ical analysis such as paradigmatic meaning and interactional pragmatics in earlier stages of a language. 
Representative Corpus 1 
Base textuelle FRANTEXT is in between a text archive and a typical reference corpus that strives to represent the history of a language – in this case, French. It contains some of the oldest French texts from 950 up to the present-day and, with currently about 300 million words of text, it decidedly qualifes as a large corpus. In its present version, it intends to cater to a great variety of researchers, including literary scholars and historians, as well as linguists. It started life, however, as a corpus for lexicographic research which explains its great time-depth and wide coverage in terms of genres. Part of the corpus has been part-of-speech tagged. One striking feature is that the corpus comes with various predefned subcorpora, varying in size or in the period that is represented, so as to meet different research needs. Indeed, the corpus has such a fexible online interface that it allows the user to dynamically select a working corpus precisely tailored to their specifc objectives. Another distinguishing feature is that it is open to contributions by third parties, who can submit new material for inclusion in the corpus. 
Representative Corpus 2 The York-Toronto-Helsinki Parsed Corpus of Old English Prose (YCOE) 
is a 1.5-million-word syntactically annotated corpus representing Old English prose, with texts dating from before 850 up to 1150. YCOE is a member of a bigger family of corpora. With the Helsinki Corpus of English Texts (Kyt1996), YCOE shares its text categorization scheme, including periodization and text genres, as well as fle naming conventions. The linguistic annotation of YCOE, including its part-of-speech tagging and syntactic annotation, is shared with other members of the family, such as the Penn-parsed Corpus of Middle English (Kroch and Taylor 2000) – albeit with some adjustments to deal with the specifcities of Old English. Finally, YCOE has drawn on electronic editions of Old English texts that had originally been created by the Toronto Dictionary of Old English Project. As such, the corpus illustrates how corpus compilation benefts from broad collaboration networks, which has the further advantage of having led to a high degree of standardization across a substantial set of historical corpora of English. Remarkably, YCOE’s selection of texts has been primarily guided by syntactic interest, favouring longer texts of running prose. The reason is that such texts provide the richest, most varied and best contextualized evidence of the syntactic patterns of Old English. For discussion of the syntactic annotation of the corpus, see Sect. 

10.2.2 above. 
K. Davidse and H. De Smet 
Representative Corpus 3 
The Old Bailey Corpus (OBC2.0) consists of trial proceedings from Lon­don’s Central Criminal Court, the Old Bailey, published between 1720 and 1913. With its 24.4 million words it is a sizeable corpus, but what really sets it apart is the kind of text material it contains, with published transcripts of the spoken interactions in court. It was social historians Tim Hitchcock and Robert Shoemaker who started the process of digitizing the proceedings of the Old Bailey, annotating the texts and making them available online with a dedicated search engine (Hitchcock et al. 2012). These digitized data formed the input to the work done by Magnus Huber and his team. They took the necessary steps to prepare the data for linguistic research. A balanced subset was selected from the material, divided over roughly equally-sized 10-year subperiods. Direct speech passages were automatically identifed and annotated. From the data information was retrieved and systematically annotated about the speakers (age, gender, social status), their role in the trial (defendant, witness, etc.), and information about the text (scribe, printer, publisher). Finally, the corpus has been part-of-speech tagged. For discussion of how well the corpus captures actual speech, see Sect. 10.2.1.2 above. 
10.3 Critical Assessment and Future Directions 
In this chapter, we have seen how the increase in the variety and overall size of corpus data allows researchers to broach new horizons in diachronic linguistics. Looking ahead, at least two ongoing trends can be expected to continue. 
The frst trend pertains to corpora themselves. The quantitative turn is strong, and with it the idea that bigger data are better data. Sound generalizations are increasingly expected to be based on large datasets, while at the same time, large data sets are bringing within reach the possibility of studying change in the language system of individual users over their own lifetime. Even so, there are some risks involved that critics of big data will not hesitate to point out. First, using large data sets may make it impossible for analysts to familiarize themselves in any detail with the texts that make up the empirical basis of their research. Second, for research to remain feasible it must be increasingly automated, again increasing the risk that researchers lose touch with their data. Third, a bigger data set is not necessarily more balanced or representative of historical usage. 
As we have seen, however, strategies are emerging to avoid some of the potential pitfalls. More bottom-up text-based approaches to text classifcation, for instance, can reduce the need to rely on more aprioristic classifcations. Computational techniques may even begin to supply information – such as author identity – that traditional philological work could not defnitively determine. Another strategy lies not in automation but in team work. Complex questions, involving large data sets and requiring specialized knowledge of diverse domains such as language history, text traditions, and computational techniques, can be handled if information passes between specialists effciently. Therefore, it is to be hoped that the future will see corpora that can support and incorporate end user input, as well as researchers making richly annotated data sets available to colleagues. 
The second trend pertains to the kind of research corpora are used for. As we have seen, particularly strong advances have been made with bottom-up and data-driven approaches to historical patterns and changes in these patterns, that “mak[e] the study of grammar more similar to the study of the lexicon” (Stefanowitsch and Gries 2003:210). At the same time, we have argued that other areas are in need of development such as the corpus implementation of paradigmatic phenomena such as distributions and alternations. Change cannot be studied as affecting an isolated syntagm, which moves through time unconnected to the grammatical systems surrounding it (Fischer 1994). As paradigmatic patterns operate in absentia in relation to the specifc syntagms that concordances naturally extract, they have to be reconstructed by querying corpora for the different environments an element occurs in (its distribution), or for its different related patterns (alternations). Qualitative and quantitative description are as complementary here as in other domains of corpus linguistics. There is, in conclusion, still ample room for developing further creative methods to identify and interpret lexicogrammatical change in the empirical detail that only diachronic corpora can provide. 
10.4 Tools and Resources 
What is useful to diachronic corpus linguists depends obviously on the languages they intend to study. Indeed, there are few resources that are truly language-independent. There are several types of resources, however, that it is good for any diachronic corpus linguist to be on the lookout for. 
First, there are several websites dedicated to cataloguing or collecting corpora, such as the Linguistic Data Consortium (https://www.ldc.upenn.edu/) (accessed 29 May 2019). A remarkable tool, though specifc to diachronic corpora for English, is the Corpus Resource Database, which has a specialized interface to allow visitors to search for the corpora that best meet their needs (http://www.helsinki.f/varieng/ CoRD/) (accessed 29 May 2019). For corpora that have no online search interface of their own, a great variety of concordancing tools exist: see Chap. 8 for more info. 
Second, anyone wanting (or needing) to compile their own corpora can beneft from digitized texts in online repositories, such as Project Gutenberg (http://www. gutenberg.org/) (accessed 29 May 2019) – even though it is good to be aware that freely available online editions may not always meet scholarly editorial standards. To draw texts from repositories a web crawler may be useful, or programming one using Python or Perl (cf. Chap. 9). The material in online repositories may not yet 
K. Davidse and H. De Smet 
have been converted into machine-readable text. To this end, it may be necessary to acquire OCR software. ABBYY FineReader is one such package, whose OCR editor can be equipped with tailor-made dictionaries and can be trained on special character sets (though the work tends to be time-consuming) (https://www.abbyy. com/en-au/) (accessed 29 May 2019). 
Third, corpus compilers may choose to optimally adapt their corpus to various further research needs. It is best to be aware of the Text Encoding Initiative, which seeks to standardize XML annotation for digital text editions (https://tei-c.org/) (accessed 3 June 2019). A further interesting possibility is to adapt a corpus to use in Sketch Engine, which supports querying and offers facilities for text analysis or text mining applications (https://www.sketchengine.eu/) (accessed 29 May 2019). For many purposes, spelling normalization and lemmatization may be required. For this, language-specifc tools are needed, such as (again for English) VARD2 (http:// ucrel.lancs.ac.uk/vard/about/) (accessed 29 May 2019). For corpus annotation tools, see Chap. 2. 
Further Reading 
Jenset, G., and McGillivray, B. 2017. Quantitative historical linguistics. A corpus framework. Oxford: Oxford University Press. 
This book reconceptualizes the newest quantitative methods of corpus linguistics for diachronic linguistics. It argues for richer annotation of historical corpora and open, reproducible research. 
Meurman-Solin, A., and Tyrkk J. 2013. Principles and Practices for the Digital 
Editing and Annotation of Diachronic Data. Helsinki: VARIENG. http://www. 
helsinki.f/varieng/series/volumes/index.html. Accessed 29 May 2019. 
This edited volume offers an overview of current principles and practices of digital editing, from a corpus linguistic and philological perspective. 
Piotrowski, M. 2012. Natural Language Processing for Historical Texts. San 
Rafael, CA: Morgan & Claypool. doi:10.2200/S00436ED1V01Y201207 
HLT017. 
This book addresses the challenges posed by historical texts to the application of Natural Language Processing techniques, ranging from digitization to syntactic annotation. 
References 
Archer, D. (2014). Historical pragmatics: Evidence from the old Bailey. Transactions of the Philological Society, 112, 259–277. 
Baron, A., & Rayson, P. (2008). VARD 2: A tool for dealing with spelling variation in historical corpora. In: Proceedings of the postgraduate conference in corpus linguistics, Birmingham: Aston University. 
Biber, D. (1988). Variation across speech and writing. Cambridge: Cambridge University Press. 
Blaxter, T. (2015). Gender and language change in Old Norse sentential negatives. Language Variation and Change, 27, 349–375. 
Brems, L. (2003). Measure noun constructions: An instance of semantically-driven grammatical­ization. International Journal of Corpus Linguistics, 8, 283–312. 
Breivik, L. (1989). On the causes of syntactic change in English. In L. Breivik (Ed.), Language change: Contributions to the study of its causes (pp. 29–70). Berlin: Walter de Gruyter. 
Brown, R., & Gilman, A. (1960). The pronouns of power and solidarity. In T. A. Sebeok (Ed.), Style in language (pp. 253–276). Cambridge: MIT Press. 
Buyle, A., & De Smet, H. (2018). Meaning in a changing paradigm: The semantics of you and the pragmatics of thou. Language Sciences, 68, 42–55. 
Culpeper, J., & Kyt M. (2000). Data in historical pragmatics. Spoken interaction (re)cast as writing. Journal of Historical Pragmatics, 1, 175–199. 
D’hoedt, F. (2017). Language change in constructional networks: The development of the English secondary predicate construction. Doctoral dissertation. KU Leuven: Department of Linguistics. 
Davidse, K., & Van Linden, A. (2020). Revisiting ‘it-extraposition’: The historical development of constructions with matrices (it)/(there) be + NP followed by a complement clause. In P. Nez-Pertejo et al. (Eds.), Crossing linguistic boundaries (pp. 81–103). London: Bloomsbury. 
De Smet, H., & Vancayzeele, E. (2014). Like a rolling stone: The changing use of English premodifying present participles. English Language and Linguistics, 19, 131–156. 
Diller, H.-J., De Smet, H., & Tyrkk J. (2011). A European database of descriptors of English electronic texts. The European English Messenger, 19, 21–35. 
Firth, J. R. (1957). A synopsis of linguistic theory, 1930-1955. In J. R. Firth (Ed.), Studies in linguistic analysis (pp. 1–32). Oxford: Blackwell. 
Fischer, O. (1994). The development of quasi-auxiliaries in English and changes in word order. Neophilologus, 78, 137–162. 
Fischer, O., De Smet, H., & van der Wurff, W. (2017). A brief history of English syntax. Cambridge: Cambridge University Press. 
Gregory, M. (1967). Aspects of varieties differentiation. Journal of Linguistics, 3, 177–274. 
Gries, S. (2017). Syntactic alternation research: Taking stock and some suggestions for the future. Belgian Journal of Linguistics, 31, 8–29. 
Gries, S., & Divjak, D. (2009). Behavioral profles: A corpus-based approach to cognitive semantic analysis. In V. Evans & S. Pourcel (Eds.), New directions in cognitive linguistics (pp. 57–75). Amsterdam: Benjamins. 
Gries, S., & Hilpert, M. (2008). The identifcation of stages in diachronic data: Variability-based neighbor clustering. Corpora, 3, 59–81. 
Halliday, M. A. K. (1994). An introduction to functional grammar (2nd ed.). London: Arnold. 
Hay, J., & Sudbury, A. (2005). How Rhoticity Became /r/-Sandhi. Language, 81, 799–823. 
Hilpert, M. (2008). Germanic future constructions: A usage-based approach to language change. Amsterdam: Benjamins. 
Hinrichs, L., Szmrecsanyi, B., & Bohmann, A. (2015). Which-hunting and the Standard English relative clause. Language, 91, 806–836. 
Hitchcock, T., Shoemaker, R., Emsley, C., Howard, S., & McLaughlin, J., et al., (2012). The Old Bailey proceedings online, 1674-1913. www.oldbaileyonline.org, version 7.0, 24 March 2012. Accessed 3 June 2019. 
K. Davidse and H. De Smet 
Huber, M. (2007). The old bailey proceedings, 1674-1834: Evaluating and annotating a corpus of 18th-and 19th-century spoken English. In A. Meurman-Solin & A. Nurmi (Eds.), Studies in variation, contacts and change in English, Vol. 1: Annotating variation and change. Research Unit for Variation, Contacts and Change in English (VARIENG), University of Helsinki. http:/ /www.helsinki.f/varieng/journal/volumes/01/huber/. Accessed 29 May 2019. 
Huber, M., Nissel, M., Maiwald, P., & Widlitzki, B. (2012). The Old Bailey Corpus. Spoken English in the 18th and 19th centuries. www.uni-giessen.de/oldbaileycorpus. Accessed 29 May 2019. 
Kemmer, S., & Hilpert, M. (2005). Constructional grammaticaliation in the make-causative. Paper presented at ICHL 17, Madison, WI. 
Kestemont, M., Stover, J., Koppel, M., Karsdorp, F., & Daelemans, W. (2016). Authenticating the writings of Julius Caesar. Expert Systems with Applications, 63, 86–96. 
Kossmann, B. (2007). Rich and poor in the history of English: corpus-based analyses of lexico­semantic variation and change in Old and Middle English. PhD dissertation. Albert-Ludwigs-Universität Freiburg i. Br. 
Kroch, A., & Taylor, A. (2000). The Penn-Helsinki Parsed Corpus of Middle English (PPCME2). Department of Linguistics, University of Pennsylvania. CD-ROM, second edition, release 4 https://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-4/index.html. Accessed 3 June 2019. 
Kung, S. (2005). A diachronic study of melancholy in a British novel corpus. Manuscript. University of Birmingham. https://www.birmingham.ac.uk/Documents/college-artslaw/corpus/ Intro/Unit52Melancholy.pdf. Accessed 29 May 2019). 
Kyt M. (1996). Manual to the diachronic part of the Helsinki Corpus of English texts. Coding conventions and lists of source texts. Department of English, University of Helsinki. 
Laffut, A., & Davidse, K. (2002). English locative constructions: An exercise in Neo-Firthian description and dialogue with other schools. Functions of Language, 9, 169–207. 
Larsson, I. (2014). Choice of non-referential subject in existential constructions and with weather-verbs. Nordic Atlas of Language Structures, 1, 55–71. 
Lemmens, M. (1998). Lexical perspectives on transitivity and ergativity. Causative constructions in English. Amsterdam: Benjamins. 
Levin, B. (1993). English verb classes and alternations: A preliminary investigation. Chicago: The University of Chicago Press. 
Lorenz, G. (2002). Really worthwhile or not really signifcant? A corpus-based approach to the delexicalization and grammaticalization of intensifers in modern English. In I. Wischer & G. Diewald (Eds.), New refections on grammaticalization (pp. 143–161). Amsterdam: Benjamins. 
Love, R., Dembry, A., Hardie, C., Brezina, V., & McEnery, T. (2017). The spoken BNC2014: Designing and building a spoken corpus of everyday conversations. International Journal of Corpus Linguistics, 22, 319–344. 
Marx, M. (2009). Advanced information access to parliamentary debates. Journal of Digital Information, 10(6). https://journals.tdl.org/jodi/index.php/jodi/article/view/668. Accessed 30 May 2019. 
McGregor, W. (1994). Review of P. Hopper and E.C. Traugott (1993), Grammaticalization. Functions of Language, 1, 304–307. 
Nevalainen, T., Vartiainen, T., Säily, T., Kesäniemi, J., Dominowska, A., & Öhman, E. (2016). Language change database: A new online resource. ICAME Journal, 40, 77–94. 
Perek, F. (2012). Alternation-based generalizations are stored in mental grammar: Evidence from a sorting task experiment. Cognitive Linguistics, 23, 601–635. 
Perek, F., & Hilpert, M. (2017). A distributional semantic approach to the periodization of change in the productivity of constructions. International Journal of Corpus Linguistics, 22, 490–520. 
Piotrowski, M. (2012). Natural language processing for historical texts. San Rafael: Morgan & Claypool. https://doi.org/10.2200/S00436ED1V01Y201207HLT017. 
Raumolin-Brunberg, H., & Nevalainen, T. (2007). Historical sociolinguistics: The corpus of early English correspondence. In J. Beal, K. Corrigan, & H. Moisl (Eds.), Creating and digitizing language Corpora. Volume 2: Diachronic databases (pp. 148–171). Houndsmills: Palgrave-Macmillan. 
Romaine, S. (2000). Language in society: An introduction to sociolinguistics. Oxford: Oxford University Press. 
Schneider, G., Hundt, M., & Oppliger, R. (2016). Part-of-speech in historical Corpora: Tagger evaluation and ensemble systems on ARCHER. In Proceedings of the Conference on Natural Language Processing (KONVENS) (Vol. 13, pp 256–264). 
Sinclair, J. (1991). Corpus, concordance, collocation. Oxford: Oxford University Press. 
Stefanowitsch, A., & Gries, S. (2003). Collostructions: Investigating the interaction of words and constructions. International Journal of Corpus Linguistics, 8, 209–243. 
Szmrecsanyi, B., Biber, D., Egbert, J., & Franco, K. (2016). Toward more accountability: Modeling ternary genitive variation in Late Modern English. Language Variation and Change, 28, 1–29. 
Taylor, A. 2003. The York-Toronto-Helsinki Parsed corpus of Old English prose. YCOE Lite: A beginner’s guide. http://www-users.york.ac.uk/˜lang22/YCOE/doc/annotation/YcoeLite.htm. Accessed 29 May 2019. 
Taylor, A., Warner, A., Pintzuk, S., & Beths, F. (2003).The York-Toronto-Helsinki parsed Corpus of Old English prose. Electronic texts and manuals available from the Oxford Text Archive. 
Van Rompaey, T., & Davidse, K. (2014). The different developments of progressive aspect markers be in the middle/midst of and be in the process of V-ing.InS.Hancil&E. Knig(Eds.), Grammaticalization: Theory and data (pp. 181–202). Amsterdam: Benjamins. 
Chapter 11 Spoken Corpora 
Ulrike Gut 
Abstract This chapter provides a detailed introduction to the central aspects of the compilation and use of spoken corpora, which have become increasingly popular in linguistic research. It discusses the challenges associated with collecting raw data and creating annotations for spoken corpora and shows how these are determined by the specifc research aims and traditions in the various felds of linguistics. A wide range of tools are presented and evaluated that can be used to annotate and search spoken corpora and examples of different spoken corpora and their use are given, representing the myriad ways in which the analysis of spoken corpora has contributed to the description of aspects of human language use. With a view to the increasing technological advances that will meet many of the current challenges of constructing and analysing spoken corpora, the chapter discusses future challenges and desired developments with respect to the linguistic use of spoken corpora. 
11.1 Introduction 
Compared to written corpora, spoken corpora are still few in number and are typically much smaller. This is chiefy due to the greater costs and challenges in terms of technology and time that are connected with the compilation and annotation of spoken corpora. However, interest in spoken corpora has been on the increase in the past two decades (e.g. Kirk and Andersen 2016; Durand et al. 2014;Raso and Mello 2014; Ruhi et al. 2014), based on the growing conviction that with a corpus-based method a wide range of the properties of spoken human language and communication can be analysed in exciting new ways. 
Some of the earliest spoken corpora were developed for the study of the vocabulary of Australian workers (Schonell et al. 1956) and the acquisition of vocabulary in a frst language (Beier et al. 1967). Nowadays, a large variety of 
U. Gut (•) Department of English, University of Mster, Mster, Germany e-mail: gut@uni-muenster.de 
© Springer Nature Switzerland AG 2020 235 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_11 
corpora containing spoken language exist that have been compiled for a myriad of uses: they often form part of national reference corpora such as the BNC (British National Corpus)1 and the Czech National Corpus (Benešová et al. 2014) and they serve as a data basis for linguistic research, for example in dialectology (see Szmrecsanyi and Wolk 2011 for an overview), conversation analysis (e.g. O’Keeffe and Walsh 2012), the study of the grammar of speech (e.g. Leech 2000), pragmatics (e.g. Schauer and Adolphs 2006), phonetics and phonology (see Delais-Roussarie and Yoo 2014 for an overview), language acquisition (e.g. Gut 2009) and forensic linguistics (e.g. Cotterill 2004). Moreover, spoken corpora have been applied in language pedagogy (e.g. Carter 1998;Gut 2005) and are used for the development of dictionaries and grammars (e.g. The Longman grammar of spoken and written English; Biber et al. 1999), for the development of new technologies and commercial tools in speech engineering (see Gibbon et al. 1997) and translation (cf. Chap. 12) and for the documentation of endangered languages.2 
The different research aims and applications that the corpus compilers pursued with the construction of the various spoken corpora has resulted in numerous different types of corpora that differ drastically in the type of raw data they contain, the annotations that were carried out, the data format the corpora have as well as the possibilities corpus users have to search them. 
There is no doubt that the advent of spoken corpora has opened up new avenues for studying spoken language properties and use that have resulted in some fundamental reshapings of linguistic theories. For example, decisive advances have been made in the corpus-based description of the grammatical features of spoken language (e.g. Biber 1988; Leech 2000), phonological processes such as apicalization in Norwegian (e.g. Kristoffersen and Simonsen 2014), the intonation of spontaneous speech (e.g. Martin 2014), the use of discourse particles in con­versations (e.g. Aijmer 2002) as well as grammatical and phonological aspects of frst and second language acquisition (e.g. Dimroth 2008;Rose 2014). Yet, a number of challenges remain, especially concerning the (re-)usability of spoken corpora, the further development of standard practices in spoken corpus annotation, technological requirements for corpus archiving as well as ethical considerations in corpus compilation and dissemination. 
11.2 Fundamentals 
There are two largely distinct but not exclusive types of corpora that contain spoken language, usually referred to as speech corpora (or speech databases) and spoken corpora respectively. Speech corpora or databases such as the Multi-Language Conversational Telephone Speech 2011-Slavic Group database (Jones 
1http://www.natcorp.ox.ac.uk/. Accessed 22 May 2019. 2see DOBES project http://dobes.mpi.nl/. Accessed 22 May 2019. 
et al. 2016) typically contain large amounts of spoken language, often recorded under experimental conditions, and are used for industrial and technological applications such as assessing automatic speech recognition systems and developing human-machine communication or text-to-speech systems (see e.g. Gibbon et al. 1997). Spoken corpora, by contrast, are compiled for a linguistic purpose such as hypothesis testing in the study of language use and human communication, as well as for linguistic applications such as language teaching and the development of grammars and dictionaries. They typically include the corresponding sound fles, but some ‘mute’ corpora that only provide access to the transcriptions also exist 
(e.g. ICE Philippines, a corpus of written and spoken Philippine English http://ice­corpora.net/ice/icephi.htm). This chapter is concerned with typical spoken corpora rather than databases or mute corpora. 
11.2.1 Raw Data and Different Types of Spoken Corpora 
The types of raw data that can be found in spoken corpora differ in terms of the corpus compilers’ infuence and control over the communicative context, ranging from ‘no control at all’ to ‘highly controlled data elicitation methods’. Spoken language that was produced without any involvement of the corpus compiler is often referred to as ‘authentic’ or ‘natural’ language, as it avoids the observer’s paradox, 
i.e. the fact that the presence of a researcher and the speakers’ awareness that they are being recorded infuence the nature of the language produced (Labov 1970). Ideally thus, the corpus raw data was produced in real communicative situations and was recorded on video or audio for other purposes than including it in a linguistic corpus. The corpus compiler’s role is simply to select this already recorded and archived data to be included in the corpus. This type of raw data is typically found in so-called reference corpora that were designed to be representative of a language or language variety. For example, the various components of the International Corpus of English (ICE; Greenbaum and Nelson 1996) that constitute reference corpora for the different varieties of English spoken around the world, include broadcast discussions, broadcast interviews, broadcast talks, cross-examinations and news readings, which were not recorded specifcally for the compilation of these corpora. 
Attempting to avoid the observer’s paradox, the compilers of the London-Lund Corpus of English (Svartvik 1990) surreptitiously recorded conversations, a practice that violates current ethical research standards but is still occasionally in use (e.g. Guirao et al. 2006). Typically, nowadays when recording raw data for a corpus, speakers are aware of it. In addition, all speakers should have given their formal consent for the recordings to be included in the corpus (cf. Chap. 1). 
Most spoken corpora contain language productions that were purposefully elicited by the corpus compilers. By controlling the situation in which language is produced corpus compilers increase the probability that the phenomena they are interested in are actually included in the corpus. Especially rare linguistic phenomena might not occur in suffcient numbers in a corpus that contains data exclusively produced in uncontrolled situations. A wide range of speaking styles with varying degrees of corpus compilers’ infuence can be elicited, including unplanned and pre-planned as well as scripted and unscripted speech: In interviews carried out with speakers of different British dialects such as those recorded for the Freiburg English Dialect (FRED) corpus (Anderwald and Wagner 2007), the researchers only control the topic of the conversation. In games and story retellings, specifc vocabulary can be elicited: For the HCRC Map Task corpus3 (Anderson et al. 1991), for instance, speakers played a game in which one speaker had to explain the route on a map to another speaker who could not see it on his or her map. The names of the locations on the map thus give the corpus compilers the chance to elicit certain sounds and prosodic patterns. For some corpora such as the LeaP corpus (see Sect. 11.4), speakers are asked to retell a story they have previously read or seen as a flm, which again allows corpus compilers to elicit targeted lexical items or possibly even syntactic structures. 
The degree of control that is exerted over the communicative situation in which the raw data is produced determines the characteristics of the language that is being produced. For example, under highly controlled conditions speakers cannot choose their own words but have to produce language that is visually or aurally presented to them by the researcher. Thus, it is typically monologues rather than dialogues that are recorded in very controlled situations. In contrast, data produced in uncontrolled conditions comprises many types of spontaneously produced, unplanned, preplanned, scripted and unscripted language in monologues and dialogues. The researcher’s control over the raw data production moreover infuences the degree of variation that is represented in the corpus. While in uncontrolled data social, situational and genre-related variation in language use is typically present, it is increasingly restricted in the different types of elicited language data. 
As the selection of the corpus data is determined by the intended uses of the corpus, the spoken corpora that are collected in the various linguistic subdisciplines differ sharply in terms of the raw data they contain. Spoken corpora that comprise ‘authentic’ raw data are typically compiled for the study of spoken language morphosyntax, pragmatics, discourse, conversations and sociolinguistics as they contain a breadth of different types of language (registers) and a suffcient amount of language variation. Spoken corpora that were assembled for the study of phonological and phonetic phenomena, on the other hand, tend to contain highly controlled raw data in the form of scripted monologues that ensure the occurrence of suffcient tokens of the features under investigation (they have thus been classifed as peripheral corpora, e.g. by Nesselhauf 2004:128). This in turn means that the potential use of spoken corpora very much depends on the type of raw data they contain. Anderwald and Wagner (2007:47) for example note that in the interviews the FRED corpus contains speakers mainly talk about the past, which causes an overrepresentation of past tense verb forms and constitutes a drawback for the 
3http://www.hcrc.ed.ac.uk/maptask/. Accessed 22 May 2019. 
investigation of the present tense. By the same token, a corpus that consists of recordings of text passages and word lists is unsuitable for the study of grammatical phenomena and a corpus that consists of audio recordings only cannot be used for the study of the interplay of prosody and gestures in human communication (cf. Chap. 16). 
Many spoken corpora contain several types of raw data, thus combining more and less controlled recording scenarios. This is true for all reference corpora, which aim to constitute representative samples of a language or language variety. Apart from written language, they contain a wide range of types of spoken language to guarantee the representation of variation across registers in this language (see 
e.g. the BNC). Other spoken corpora that combine raw data types include the IvIE corpus (Nolan and Post 2014), which was compiled for the study of intonational variation on the British Isles and which contains read speech, retellings, Map Tasks and free conversations, and the LeaP corpus of non-native German and English, which contains word lists, read speech, retellings and interviews (see Sect. 11.4). 
11.2.2 Corpus Annotation 
A collection of spoken language recordings does not constitute a linguistic corpus unless linguistic annotations are added to it. The one type of annotation that all spoken corpora share is an orthographic transcription (see also Chap. 14). Whether any further annotations are added to the corpus and what type is again largely determined by the intended use of the corpus (see also Sect. 11.3). Some types of annotation such as orthographic, phonemic and prosodic annotations are unique to spoken corpora, while others such as part-of-speech (POS)-tagging and parsing have been adapted from written corpora, which in some cases has led to specifc challenges as discussed below. The process of corpus annotation is interpretative and often theory-dependent. With each annotation, corpus compilers have to take various decisions that can constrain the future use of their corpus. This is true even for a seemingly simple annotation as an orthographic transcription. 
11.2.2.1 Orthographic Transcription 
All spoken corpora contain orthographic transcriptions. However, they can vary considerably in terms of the orthographic conventions chosen (some languages like English and German have different standard spellings in the different countries in which they are spoken, e.g. British English colour vs. American English color), spelling conventions concerning capitalisation rules (e.g. Ladies and Gentlemen vs. ladies and gentlemen) and hyphenisation (ice-cream vs. icecream). Likewise, they differ in the transcription of communicative units such as sounds of hesitation (erm) or affrmative sounds (mhm), for which no agreed spelling standard exists. Moreover, some corpora contain transcriptions for word forms typical of spoken but not written language, for instance contractions like wanna, while others do not and corpora can differ in whether and how mispronunciations of words by individual speakers and non-speech events such as laughter and background noises are transcribed. It is important for corpus users to be aware of such potential differences in the orthographic transcriptions as they may lead to the missing of tokens in a word-based corpus search: in a search for ‘because’, for instance, transcribed forms such as ‘coz’ and ‘cos’ will not be found. Options to avoid the time-consuming manual orthographic transcription are beginning to materialise: for example, it is possible to train speech-to-text software such as the one included in GoogleDocs on one’s voice and then record oneself repeating the content of the raw data fle. Moreover, youtube offers a free automatic transcription service for videos. While these systems do not work very reliably yet, especially on recordings with background noise or overlapping speech, they will no doubt improve dramatically over the next few years. 
11.2.2.2 POS-Tagging and Lemmatisation 
Some spoken corpora have POS-tagging, which indicates the grammatical category of each transcribed word and which is essential for the quantitative analysis of the grammatical properties of spoken language (see Sect. 11.2.4; Chap. 2). A number of different taggers with different tag sets are in use that were originally developed for automatically annotating written corpora: these include the freely available CLAWS tagger,4 which was used for tagging the BNC, and the PiTagger system (Panunzi et al. 2004) that was used for the tagging of the Italian component of the C-ORAL corpus. POS taggers tend to be less reliable for spoken data with error rates of up to 5% (e.g. Moreno-Sandoval and Guirao 2006) because, in comparison to written data, repetitions, repairs, false starts and hesitations can occur (see Oostdijk 2003). Due to this and to systematic errors in the automatic tagging process itself, manual corrections have been carried out for some spoken corpora such as part of the BNC (Garside 1995). During lemmatisation, the various word forms of an infectional paradigm are assigned to a common lemma by automatic lemmatisers such as the one used for the VOICE corpus5 or the CGN tagger6 for Dutch. This type of annotation allows corpus users to compile lists and frequencies of different lexical types rather than of individual word forms only. 
4http://ucrel.lancs.ac.uk/claws/trial.html. Accessed 22 May 2019. 5http://www.univie.ac.at/voice/. Accessed 22 May 2019. 6https://ilk.uvt.nl/cgntagger/. Accessed 22 May 2019. 
11.2.2.3 Parsing 
Parsing, i.e. the automatic annotation of the syntactic structure of a language (cf. Chap. 2), is still tremendously challenging for spoken corpora. Due to the char­acteristics of spoken language such as constructions with word order patterns not found in written language and incomplete utterances, parsers that were developed for written language usually yield poorer output when applied to spoken corpora. Yet, frst advances have been made with the Constraint Grammar parser PALAVRAS that was successfully adapted to handle the structures of spoken language and which was used for parsing the C-ORAL Brazil corpus with 95% accuracy for syntactic function assignment (Bick 2014). 
11.2.2.4 Phonemic and Phonetic Transcription 
Some spoken corpora that were compiled for the study of phonological and phonetic phenomena (so-called ‘phonological corpora’; see Gut and Voormann 2014) further contain phonemic or phonetic transcriptions. In a phonemic transcription, the phonological form of a word is transcribed while a phonetic transcription represents the actual pronunciation by a speaker. For a phonemic transcription, each phoneme is transcribed using either the symbols of the International Phonetic Alphabet7 or the corresponding UTF-8 encoding or, more commonly, its machine-readable equivalents SAMPA (Wells et al. 1992) or CELEX.8 Phonemic transcriptions of spoken corpora can be carried out manually or automatically: manual transcriptions often suffer from inconsistencies across and within transcribers (e.g. Gut and Bayerl 2004) and are very time-consuming, while automatic transcriptions, which can be generated by tools such as WebMAUS (Schiel 2004; see Strik and Cucchiarini 2014 for an overview) typically contain systematic errors that have to be corrected manually. Phonetic annotations can be carried out with different degrees of detail ranging from coarticulatory processes such as labialisation to articulatory details such as tongue position (see e.g. Delais-Roussarie and Post 2014 for an overview). In addition, pronunciation errors can be transcribed by indicating both the phonetic target form and its actual realisation (see Neri et al. 2006 for a methodology of pronunciation error annotation and measuring annotator agreement). Pronunciation errors are often transcribed in spoken corpora compiled for the analysis of frst language acquisition (Chap. 14). 
7https://www.internationalphoneticassociation.org/content/ipa-chart. Accessed 22 May 2019. 8https://catalog.ldc.upenn.edu/docs/LDC96L14/celex.readme.html. Accessed 22 May 2019. 
11.2.2.5 Prosodic Transcription 
Spoken corpora that were compiled in order to study prosodic phenomena in speech can contain various types of prosodic transcriptions. Unlike for phonemic transcriptions, several transcription conventions exist side by side for prosody: the most commonly used are the transcription system ToBI (Silverman et al. 1992; for example in the LeaP corpus) and the transcription conventions of the British School of intonation analysis (e.g. O’Connor and Arnold 1961; Halliday 1967) that were for example used for the Lancaster/IBM corpus (Williams 1996). Attempts to automatically transcribe intonation exist, e.g. the INTSINT system (e.g. Hirst 2005) or Prosograms (Mertens 2004), which remove the micro-prosodic perturbations from the fundamental frequency curve and give as output a smoothed intonation curve that is assumed to be perceptually equivalent. For fne-grained analyses of intonational patterns or for the transcription of ‘challenging’ speech such as non­native or emotional speech manual transcriptions of prosody are still more reliable. 
11.2.2.6 Multi-layered and Time-Aligned Annotation 
Spoken corpora do not only differ in terms of the number and type of annotations they contain but also in the fundamental question of how these annotations are represented. Annotations of existing spoken corpora differ in two ways: annotation layering and time-alignment. The term ‘annotation layer’ refers to the issue of whether different types of annotation are integrated together in one linear transcrip­tion or whether they are represented individually on separate layers. Many spoken corpora compiled by researchers working in the feld of conversation analysis and interactional linguistics use transcription systems that integrate various linguistic aspects in one linear transcription. The GAT-2 system (Selting et al. 2009), for example, combines an orthographic, a literal and a prosodic transcription with the transcription of other events such as speaker overlap, pauses and non-verbal events. Figure 11.1 shows an example of the beginning of a dialogic conversation that has been transcribed using GAT-2. Symbols such as ‘.’ and ‘;’ represent the intonation, stress is transcribed by using capital letters, speaker overlap is indicated by ‘[’ and phonetically reduced forms such as ‘ham’ (line 02, for ‘haben’ have) are used. 
Most modern spoken corpora, however, use multi-layered annotations, where only one type of linguistic annotation is contained per layer (or tier) (see also Chaps. 3 and 14). This has many advantages such as the possibility to represent 
01 S1: ja:; (.) die 
`VIERziger genera`tiOn so;= 
02 =das_s: `!WA:HN!sinnig viele die sich da ham 
[ `SCHEI]den lasse[n.= 
03 S2: [ ja; ] 
Fig. 11.1 Example of a GAT-2 transcription (from Selting et al. 2009:396). [Rough translation: S1: Yes the forties generation so that incredibly many who then fled for divorce S2: yes] 

different overlapping annotations for one speech event (see Fig. 11.2, tiers 3 and 4, for example). Figure 11.2 shows the multi-layered annotation of the LeaP corpus (Gut 2012) that was carried out with Praat. The different types of annotation are each assigned to one tier: tier 1 contains the transcription of pitch levels, on tier 2, the intonation is transcribed, tier 3 contains the transcriptions of all vocalic and consonantal intervals, tier 4 the phonemic transcriptions, tier 5 the orthographic transcription and tier 6 the intonation phrases. In addition, on tiers 7 and 8, the lemmas and POS are marked. 
The multi-layered annotation of the LeaP corpus in Fig. 11.2 is also time-aligned. The term time-alignment refers to the technical linking of an annotation with the corresponding part of the audio or video fle. On tier 5, for example, the beginning and end of each transcribed word is marked by vertical lines, as are the beginning and end of each syllable on tier 4 and the beginning and end of each intonation phrase on tier 6. The TextGrid fle that is produced by Praat carries this information as time stamps that give the exact point in time in the sound fle for each such annotation. Thus, it is possible to listen directly to each individual annotated word or syllable in the recording. Time-aligned spoken corpora are essential for any research on phonology or phonetics where the exact duration of linguistic units (e.g. vowels, pauses) or their further characteristics (e.g. formants for vowels, pitch height) is important. Most modern tools for spoken corpus compilation support multi-layered time-aligned annotations (see Sect. 11.4 below). 
11.2.3 Data Format and Metadata 
The large number of corpus annotation tools that are in use determines that spoken corpora also differ widely in their data format, ranging from txt fles to XML formats. While nearly every tool produces its proprietary output format, many of them have import/export functions that allow data exchange across tools. However, the interoperability between tools is still one of the major challenges for spoken corpus use and re-use across linguistic subdisciplines as discussed in Sect. 11.3. One major problem, for example, is that linear annotations cannot be converted to multi-layered ones. The data format of the corpus also restricts the range of tools that can be used for corpus searches (see Sect. 11.2.4). 
Further heterogeneity across spoken corpora exists in terms of metadata. The term metadata refers to any additional information about the corpus compilers, the data collection (e.g. the procedure and date and place of recordings), the speakers that were recorded (e.g. their age, gender, regional background, further languages) and the data annotation process (e.g. the transcription conventions, tagset used, see Chap. 1). While several international initiatives have attempted to standardise metadata formats (see Broeder and van Uytvanck 2014 for an overview), its divergent use across existing spoken corpora is pronounced. As is true for any corpora, when the metadata is provided in a separate fle without using standoff techniques (e.g. IDs pointing from annotations to corresponding speaker metadata), it requires great manual effort to make use of it for automatic corpus searches (see also Chap. 3). 
11.2.4 Corpus Search 
As existing spoken corpora vary greatly in the type and number of annotations as well as their data format, possibilities for corpus search diverge signifcantly. In general, corpora with rich multi-layered annotations lend themselves to large-scale automated quantitative analyses and statistical exploitation (e.g. Moisl 2014), while others that contain only orthographic transcriptions can be used mainly for manual corpus inspection (see also Sect. 11.3). For KWIC (keyword-in-context) searches that are used to fnd specifc words, phrases or tags in a corpus and that are usually followed by a qualitative analysis of the displayed hits (cf. Chap. 8), some corpus compilers offer specially designed query interfaces on their website: On the Scottish Corpus of Texts and Speech (Anderson and Corbett 2008) website,9 for example, it is possible to search for the occurrence of individual words in all or selected fles of the corpus. The hits are displayed in their context and a link to the corresponding audio fle is provided. Thus, it is possible to analyse how often and where speakers use Scots grammatical forms such as didnae (didn’t)asshown in Fig. 11.3. Similarly, the Nordic Dialect Corpus (Johannessen et al. 2014) can also be searched on the corpus website.10 Words searched for are displayed as concordances with links to the audio fles, and it provides facilities for frequency counts, collocation analyses and statistical measures as well as visualisations of the search results as pie charts or maps. 
9http://www.scottishcorpus.ac.uk/search/. Accessed 22 May 2019. 10http://www.tekstlab.uio.no/scandiasyn/. Accessed 22 May 2019. 

Large-scale quantitative analyses of spoken corpora often require programming skills: for example, in order to run automatic calculations of annotations in a corpus that has an XML data format a script needs to be written (see also Chap. 9). For example, Gut and Fuchs (2017) used Praat scripts in order to calculate the fuency (mean number of words per utterance and mean number of phonemes in total articulation time) in ICE Nigeria and ICE Scotland. 
Representative Study 1 
Aijmer, K. 2002. English discourse particles – evidence from a corpus. Amsterdam: John Benjamins. 
This is an example of an innovative study using the corpus-based method to gain new insights into the linguistic subdiscipline of pragmatics. She analysed the use of the English discourse particles now, oh/ah, just, sort of, actually, and some phrases such as and that sort of thing in the London-Lund corpus and compared this to the Lancaster-Oslo/Bergen Corpus of written English and the COLT Corpus. Since these corpora are not pragmatically annotated, she used a classic KWIC search method to fnd these words in the corpora and carried out subsequent qualitative analyses of the use of these discourse particles and phrases. Her fndings show that these discourse particles have multiple functions and differ in their use with respect to the level of formality. 
Representative Study 2 
Biber, D., and Staples, S. 2014. Variation in the realization of stance adverbials. In Spoken Corpora and Linguistic Studies, eds. Raso, T., and Mello, H, 271–294. Amsterdam/Philadelphia: John Benjamins. 
This is an example of a study based on a corpus with integrated linear annotations. For investigating the interplay between grammatical stance expressions and prosody they carried out a KWIC search (using AntConc)11 of a subset of the Hong Kong Corpus of Spoken English for selected stance adverbials and subsequently manually determined their prosodic prominence and their syntactic distribution. Their fndings show that the use of stance adverbials varies with both the speaker’s language background and register; less grammaticalised stance adverbials and those in utterance-initial position receive greater prosodic prominence than more grammaticalised ones and those in medial or fnal position. 
Representative Study 3 
Kohler, K. 2001. Articulatory dynamics of vowels and consonants in speech communication. Journal of the International Phonetic Asso­ciation 31:1–16. 
This study exemplifes the use of a large time-aligned phonetically annotated corpus (Kiel Corpus of Read and Spontaneous Speech) that was automatically searched and statistically evaluated. For this, the sound fles were annotated with phonological and phonetic transcriptions using a modifed SAMPA and markers for secondary articulation. Subsequently, the annotations were fed into a databank (Kiel data bank; Pätzold 1997), which was searched. Kohler analysed articulatory movements of German speakers in real speech communication compared to read speech and found great variability in their schwa elision as well as nasalization and deletion of plosives, thus showing how corpora can be used to challenge existing theories and models and how they can open up new avenues for research. 
11http://www.laurenceanthony.net/software/antconc/. Accessed 22 May 2019. 
Representative Corpus 1 
The Spoken Dutch Corpus (Oostdijk 2000, 2002; van Eynde et al. 2000) is a multi-purpose reference corpus for Dutch and with nearly 9 million words (800 h of recordings) represents the largest spoken corpora that are currently available. It was compiled between 2000 and 2003 as a resource for linguistic research on the syntactic, lexical and phonological properties of contemporary spoken Dutch. It also functions as a database for technological applications such as the development of speech recognizers and as a tool for language teaching. The raw data comprises private informal speech such as spontaneous face-to-face conversations and telephone dialogues, dialogic and monologic broadcast speech such as broadcast interviews, news readings and discussions, unscripted monologues as in classroom lessons, lectures, speeches and sermons as well as scripted read speech. The corpus contains time-aligned orthographic transcriptions, automatic lemmatization and part-of-speech tagging. For one million words automatically created and manually verifed broad phonemic transcriptions, orthographic transcripts aligned at the word level and semi-automatic syntactic annotation exist. For approximately 250,000 words of the corpus, manual prosodic annotations of prominent syl­lables, pauses and segmental lengthenings were carried out. The orthographic and phonemic/prosodic transcriptions are available as Praat TextGrid and XML fles. POS tags, lemmatization and the syntactically annotated portion of the corpus exist as ASCII and XML fles. The corpus can be searched with COREX (CORpus Exploitation) that was developed for it. The corpus is avail­able on 33 DVDs, distributed by the Dutch Language Institute (https://ivdnt. org/downloads/taalmaterialen/tstc-cgn-annotaties) and the documentation is available at http://lands.let.ru.nl/cgn/doc_English/topics/project/pro_info.htm 
Representative Corpus 2 
The LeaP corpus (Gut 2005, 2012; Milde and Gut 2004), which was compiled between 2001 and 2003, is a multilingual learner corpus of non-native German and English. It was designed for the study of the phonological, lexical and syntactic properties of non-native speech and as a tool for language teaching. With extensive multi-layered and time-aligned manual annotations of intonation phrases and non-speech events, orthographic transcriptions at the word level, phonemic transcriptions using SAMPA at syllable level, transcriptions of vocalic and consonantal intervals, of intonation and pitch range as well as automatic part-of-speech annotation and lemmatization (see Fig. 11.2) but a size of only 73,841 words (12 h of recordings, including 
(continued) 
interviews, reading passages, story retellings and word lists), it is a typical example of a small richly annotated spoken corpus The corpus is available as both Praat TextGrid and XML fles with metadata in the IMDI format https:// tla.mpi.nl/imdi-metadata/ and can be searched, for example, with XSLT-scripts. The corpus and corpus manual are available for free at https:// sourceforge.net/projects/leapcorpus/ 
Representative Corpus 3 
The Michigan Corpus of Academic Spoken English (MICASE; Simpson et al. 2002; Simpson-Vlach and Leicher 2006) represents a specialised corpus, designed as a resource for linguistic research on lexical and syntactic properties of academic English and as a tool for language teaching. It was compiled between 1997 and 2002 and comprises 1.8 million words (>200 h of recordings) of lectures, classroom discussions, laboratory sections, student presentations in seminars, defences, meetings and advising sessions. It has time-aligned orthographic transcriptions that are available in an XML format. A website with a searchable interface http://quod.lib.umich.edu/m/ micase/ allows KWIC searches. There is a handbook for sale at http://micase. elicorpora.info/ 
11.3 Critical Assessment and Future Directions 
Section 11.2 has shown that the term ‘spoken corpus’ covers a wide range of fairly heterogeneous corpora of spoken language, whose types of raw data, annotations, data formats and search procedures can differ enormously depending on the intended linguistic use of the corpus as well as on the research traditions of the respective disciplines. With more linguistic subdisciplines adopting a corpus-based approach into their methodological repertoire (e.g. the newly established branch of corpus phonology [Durand et al. 2014] and the recently founded Journal of Corpus Pragmatics) and the increasing interest in the corpus-based exploration of multimodal aspects of human communication (see Chap. 16), there is a strong need for more spoken corpora to be compiled. In particular, the construction of corpora with video raw data seems especially desirable as only they allow researchers to study all aspects of human communication. 
However, the compilation of spoken corpora is still very time-consuming and cost-intensive. Despite many efforts to automatize more types of annotation such as phonemic transcription (see Strik and Cucchiarini 2014 for an overview) and prosodic transcription (e.g. Hirst 2005; Mertens 2004), there is another avenue to creating new opportunities for corpus-based linguistic studies that should be explored to a much greater extent in the future: the re-use of existing corpora. The re-use of spoken corpora seems to be still much rarer than that of written corpora, which is probably due to four factors: insuffcient documentation, lack of standardisation in terms of annotations and data format, lack of standardised corpus search tools and lack of access. 
The frst major obstacle for the reusability of spoken corpora is the often insuffcient documentation of the corpus creation process, the type of raw data and metadata in the corpus and the annotation schemes applied. If a potential spoken corpus user interested in the grammatical variation between older and younger speakers cannot fnd information on the age of the speakers represented in the corpus, and if a researcher interested in the interplay between prosody and syntax in a language cannot interpret the transcription symbols used for prosody, re-use of corpora is impossible. It is therefore essential for corpus compilers to make available to future corpus users ample metadata and a corpus manual detailing the corpus compilation and annotation process. Equally, it would be of great beneft to the research community if the metadata for already existing spoken corpora could still be made available, for example for some of the older ICE corpora. 
Yet, even well documented spoken corpora are often not immediately (re-)usable to the wider research community, especially if the intended linguistic use is not the original one of the corpus compilers. Thus, the syntactic or prosodic annotation of a corpus might be based on a different theoretical tradition than the one preferred by the researcher or one type of annotation that is necessary for the current study might be missing altogether. Adding new annotations to a spoken corpus, however, can still constitute a major challenge for the simple lack of suitable tools although great advances have been made in the last decade in terms of the interoperability of the major tools in use for spoken corpus construction: ELAN, Praat, EXMARaLDA and ANVIL (see Sect. 11.4 below) now all have import and export functions for their respective fle formats so that it is possible to add new annotations with one of these tools to a spoken corpus that was compiled with another tool (for this Transformer by Oliver Ehmer can also be used). Yet, the conversion of linear corpus annotations into multi-layered ones still constitutes an unsolved challenge. Moreover, many older spoken corpora could be opened up for entirely new directions of research if their annotations were time-aligned. Auran et al. (2004) were pioneers in making the Spoken English Corpus reusable for new research purposes: they time-aligned the original orthographic transcriptions with the sound fles and in addition supplied an automatically generated phonemic and prosodic transcription. The now renamed Aix-Marsec corpus thus constitutes a new resource for phonological and phonetic research, like the AudioBNC.12 It is hoped that similar efforts will be undertaken in the future for the many as yet not time-aligned spoken corpora. 
Apart from re-using and enriching existing spoken corpora, future corpus-based research might also increasingly make use of combining existing corpora for 
12http://www.phon.ox.ac.uk/AudioBNC. Accessed 22 May 2019. 
linguistic hypothesis testing in order to overcome size limits of individual corpora or in order to allow diachronic studies of phenomena (see also the SPADE project https://spade.glasgow.ac.uk/ and Wittenburg et al. 2014). For example, for a study of the variation and change in the use of intensifers such as awfully and massive in spoken language, it would be worthwhile to search together the BNC and BNC2014, which represent British English of the 1990s and current use respectively. 
The third impediment for the (re-)use of some spoken corpora sometimes is the lack of suitable automatized corpus search tools. Corpora that can only be searched with specialised tools might prove inaccessible to some linguists as discussed in Sect. 11.2.4. Some international initiatives such as CLARIN in Europe have the aim of overcoming this challenge by providing an infrastructure for corpus users that includes easy-to-use standardised tools. Many more similar efforts are necessary to make more spoken corpora accessible to researchers from all linguistic subdisciplines. 
The last challenge for the future of spoken corpora is their continued availability and accessibility. While an increasing number of corpus compilers are eager to make their spoken corpora available to the research community, technological and ethical diffculties have to be met as discussed below. For corpus data stored in non-digital form such as analogue tapes (there is still a lot of historical data that has not been digitised yet) every access means loss of quality. Moreover, many older data formats will not be accessible anymore in the near future. The archiving and dissemination of spoken corpora, even in digital form, thus implies the constant pressure of keeping up with technological advances. For instance, raw video data encoded in one format such as MPEG1 will have to be regularly updated to new encoding schemes. Furthermore, new strategies for corpus dissemination are being proposed to ensure long-term access to spoken corpora. Some corpora that were made available and searchable via websites are ‘lost’ due to lack of maintenance. As an alternative, corpora can be stored in external data services such as clouds with corpus access for example by compressed media streaming. Yet another option is the storage of spoken corpora at large data centres such as the MPI Archive (Wittenburg et al. 2014), which cover the costs for data archiving and dissemination. In any case, for the long-term preservation of spoken corpora mirroring them at different sites seems to be a good option. 
Further challenges for research based on spoken corpora are ethical issues such as privacy rights and copyrights. Typically, privacy laws require corpus compilers to ask for the formal written authorisation by each speaker to be recorded for the corpus and to allow the transcription, sharing and re-use of his or her data. Only when corpus compilers have received the written consent of the speakers recorded for the corpus that their data can be used for research and be disseminated can corpora be used and shared. Moreover, privacy rights require corpus compilers to anonymise the disseminated data (cf. Chap. 1): while this is easily achieved in the transcriptions, where references to people and places can be removed, complete anonymization in audio fles, i.e. the changing of the voice quality, would run counter and make impossible many research purposes of the corpus. Legislation on copyright and privacy issues often changes and can differ widely across nations. In some national laws the speaker can withdraw his or her consent at any later point 
in time, which poses serious challenges for corpus dissemination. The European 
General Data Protection Regulation, which became enforceable in May 2018, for 
example, states that personal data may only be processed when the data subjects 
have given their consent for specifc purposes. Changing legislation in these areas 
might pose further diffculties for corpus-based research in the future. In conclusion, both the compilation of new spoken corpora and the reuse of older 
ones remain exciting and challenging tasks for the future. I have no doubts, however, 
that they will help to provide many more important insights into human language 
use. 
11.4 Tools and Resources 
Tools for multi-layered time-aligned annotation and search of spoken corpora 
CLAN http://dali.talkbank.org/clan/ (accessed 22 May 2019): allows complex queries such as built-in metrics of syntactic complexity (e.g. mean length of utterance); very popular in language acquisition research 
FOLKER http://agd.ids-mannheim.de/folker.shtml (accessed 22 May 2019): Tool for time-aligned transcription of spoken corpora using the GAT-2 transcription system; popular in conversation analysis 
Pacx http://pacx.sourceforge.net/ (accessed 22 May 2019) (Gut 2011): Platform for the multi-layered time-aligned annotation and search of spoken corpora in XML, based on Eclipse and using ELAN 
for multi-layered annotation of videos 
ANVIL http://www.anvil-software.org/ (accessed 22 May 2019) (Kipp 2014): popular in gesture research ELAN https://tla.mpi.nl/tools/tla-tools/elan/ (accessed 22 May 2019) (Sloetjes 2014): popular in language documentation EXMARaLDA http://exmaralda.org/de/ (accessed 22 May 2019) (Schmidt & Wner 2014): popular in conversation and discourse analysis 
especially suited for phonetic research 
EMU http://emu.sourceforge.net/ (accessed 22 May 2019) (John and Bombien 2014): tools for the creation, manipulation and analysis of speech databases, includes an R interface 
Praat http://praat.org (accessed 22 May 2019) (Boersma 2014): provides an envi­ronment for running perception experiments and speech synthesis, facility for running scripts (see Brinckmann 2014 for a detailed introduction) for automatic acoustic analyses 
Phon https://www.phon.ca/phon-manual/misc/Welcome.html (accessed 22 May 2019) (Rose and MacWhinney 2014): developed for child language corpora; allows specifcation of target pronunciation and actual realisation; in-built query system; popular in frst language acquisition research 
Further Reading 
Durand, J., Gut, U., and Kristoffersen, G. (eds.) 2014. Oxford Handbook of Corpus Phonology. Oxford: Oxford University Press. 
The most comprehensive volume on spoken corpora to date that covers all aspects of the construction, use and archiving of spoken corpora with a focus on phonological corpora. It contains chapters on innovative approaches to phonological corpus compilation, corpus annotation, corpus searching and archiving and exemplifes the use of phonological corpora in various linguistic felds ranging from phonology to dialectology and language acquisition. Furthermore, it contains descriptions of existing phonological corpora and presents a wide range of popular tools for spoken corpus compilation, annotation, searches and archiving. 
Raso, T., and Mello, H. (eds.) 2014. Spoken Corpora and Linguistic Studies. Amsterdam/Philadelphia: John Benjamins. 
This volume contains a comprehensive collection of chapters discussing cutting-edge issues in spoken corpus compilation, spoken corpus annotation and presents examples of research based on spoken corpora. The individual articles show how the exploitation of richly annotated and text-to-tone aligned spoken corpora can render new insights into the syntax of speech and the use of prosody in human interactions. 
Ruhi, ¸
S., Haugh, M., Schmidt, T., and Wner, K. (eds.) 2014. Best Practices for Spoken Corpora in Linguistic Research. Newcastle upon Tyne: Cambridge Scholars Publishing. 
This collection of papers focuses on questions of standards for the construction, annotation, searching, archiving and sharing of spoken corpora used in conversation analysis, sociolinguistics, discourse analysis and pragmatics. The individual contri­butions discuss these issues and illustrate current practices in corpus design, data collection and annotation, as well as strategies for corpus dissemination and for increasing the interoperability between tools. 
References 
Aijmer, K. (2002). English discourse particles – Evidence from a corpus. Amsterdam: John Benjamins. 
Anderson, A., Bader, M., Bard, E., Boyle, E., Doherty, G. M., Garrod, S., Isard, S., Kowtko, J., 
McAllister, J., Miller, J., Sotillo, C., Thompson, H. S., & Weinert, R. (1991). The HCRC map 
task corpus. Language & Speech, 34, 351–366. Anderson, W., & Corbett, J. (2008). The Scottish corpus of texts and speech – A user’s guide. 
Scottish Language, 27, 19–41. Anderwald, L., & Wagner, S. (2007). FRED – The Freiburg English dialect corpus. In J. Beal, K. 
Corrigan, & H. Moisl (Eds.), Creating and digitizing language corpora. Volume 1: Synchronic 
corpora (pp. 35–53). London: Palgrave Macmillan. 
Auran, C., Bouzon, C., & Hirst, D. (2004). The Aix-Marsec project: An evaluative database of spoken British English. In Proceedings of speech prosody 2004. Nara. 
Beier, E., Starkweather, J., & Miller, D. (1967). Analysis of word frequencies in spoken language of children. Language and Speech, 10, 217–227. 
Benešová, L., Waclawi. ren, M. (2014). Building a data repository of spontaneous 
cová, M., & K. spoken Czech. In ¸
S. Ruhi, M. Haugh, T. Schmidt, & K. Wner (Eds.), Best practices for spoken corpora in linguistic research (pp. 128–141). Cambridge Scholars Publishing: Newcastle upon Tyne. 
Biber, D. (1988). Variation across speech and writing. Cambridge: Cambridge University Press. 
Biber, D., Johansson, S., Leech, G., Conrad, S., & Finegan, E. (1999). The Longman grammar of spoken and written English. London: Longman. 
Biber, D., & Staples, S. (2014). Variation in the realization of stance adverbials. In T. Raso & H. Mello (Eds.), Spoken corpora and linguistic studies (pp. 271–294). Amsterdam/Philadelphia: John Benjamins. 
Bick, E. (2014). The grammatical annotation of speech corpora. In T. Raso & H. Mello (Eds.), Spoken corpora and linguistic studies (pp. 105–128). Amsterdam/Philadelphia: John Benjamins. 
Boersma, P. (2014). The use of Praat in corpus research. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 342–360). Oxford: Oxford University Press. 
Brinckmann, C. (2014). Praat scripting. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 361–379). Oxford: Oxford University Press. 
Broeder, D., & van Uytvanck, D. (2014). Metadata formats. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 150–165). Oxford: Oxford University Press. 
Carter, R. (1998). Orders of reality: CANCODE, communication and culture. English Language Teaching Journal, 52, 43–56. 
Cotterill, J. (2004). Collocation, connotation and courtroom semantics: Lawyer’s control of witness testimony through lexical negotiation. Applied Linguistics, 25(4), 513–537. 
Delais-Roussarie, E., & Yoo, H. (2014). Corpus research in phonetics and phonology: Method­ological and formal considerations. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 193–213). Oxford: Oxford University Press. 
Delais-Roussarie, E., & Post, B. (2014). Corpus annotation: Methodology and transcription systems. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 46–88). Oxford: Oxford University Press. 
Dimroth, C. (2008). Age effects on the process of L2 acquisition? Evidence from the acquisition of negation and fniteness in L2 German. Language Learning, 58, 117–150. 
Durand, J., Gut, U., & Kristoffersen, G. (Eds.). (2014). Oxford handbook of corpus phonology. Oxford: Oxford University Press. 
Garside, R. (1995). Grammatical tagging of the spoken part of the British National Corpus: A progress report. In G. Leech, G. Myers, & J. Thomas (Eds.), Spoken English on computer: Transcription, mark-up and application (pp. 161–167). London: Longman. 
Gibbon, D., Moore, R., & Winski, R. (1997). Handbook of standards and resources for spoken language systems. Berlin: Mouton de Gruyter. 
Greenbaum, S., & Nelson, G. (1996). The International Corpus of English (ICE) project. World Englishes, 15(1), 3–15. 
Guirao, J., Moreno-Sandoval, A., González Ledesma, A., de la Madrid, G., & Alcántara, M. (2006). Relating linguistic units to socio-contextual information in a spontaneous speech corpus of Spanish. In A. Wilson, D. Archer, & P. Rayson (Eds.), Corpus linguistic around the world (pp. 101–113). Amsterdam/New York: Rodopi. 
Gut, U. (2005). Corpus-based pronunciation training. In: Proceedings of phonetics teaching and learning conference, London. Gut, U. (2009). Non-native Speech. A corpus-based analysis of the phonetic and phonological properties of L2 English and L2 German. Frankfurt: Peter Lang. 
Gut, U. (2011). Language documentation and archiving with Pacx, an XML-based tool for corpus creation and management. In N. David (Ed.), Workshop on language documentation and archiving (pp. 21–25). London. 
Gut, U. (2012). The LeaP corpus. A multilingual corpus of spoken learner German and learner English. In T. Schmidt & K. Wner (Eds.), Multilingual corpora and multilingual corpus analysis (pp. 3–23). Amsterdam: John Benjamins. 
Gut, U., & Bayerl, P. (2004). Measuring the reliability of manual annotations of speech corpora. Proceedings of Speech Prosody, 2004, 565–568. 
Gut, U., & Voormann, H. (2014). Corpus design. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 13–26). Oxford: Oxford University Press. 
Gut, U., & Fuchs, R. (2017). Exploring speaker fuency with phonologically annotated ICE corpora. World Englishes. https://doi.org/10.1111/weng.12278. 
Halliday, M. (1967). Intonation and grammar in British English. The Hague: Mouton. 
Hirst, D. J. (2005). Form and function in the representation of speech prosody. Speech Communi­cation, 46, 334–347. 
Johannessen, J., Vangsnes, O., Priestley, J., & Hagen, K. (2014). A multilingual speech corpus of North-Germanic languages. In T. Raso & H. Mello (Eds.), Spoken corpora and linguistic studies (pp. 69–83). Amsterdam/Philadelphia: John Benjamins. 
John, T., & Bombien, L. (2014). EMU. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 321–341). Oxford: Oxford University Press. 
Jones, K., Graff, D., Walker, K., & Strassel, S. (2016). Multi-language conversational telephone speech 2011-Slavic Group LDC2016S11. Web Download. Philadelphia: Linguistic Data Con­sortium. https://catalog.ldc.upenn.edu/LDC2016S11. Accessed 22 May 2019. 
Kipp, M. (2014). ANVIL: The video annotation research tool. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford Handbook of corpus phonology (pp. 420–436). Oxford: Oxford University Press. 
Kirk, J., & Andersen, G. (2016). Compilation, transcription, markup and annotation of spoken corpora. Special issue of International Journal of Corpus Linguistics, 21,3. 
Kohler, K. (2001). Articulatory dynamics of vowels and consonants in speech communication. Journal of the International Phonetic Association, 31, 1–16. 
Kristoffersen, G., & Simonsen, H. (2014). A corpus-based study of apicalization of /s/ before /l/ in Oslo Norwegian. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 214–239). Oxford: Oxford University Press. 
Labov, W. (1970). The study of language in its social context. Studium Generale, 23, 66–84. 
Leech, G. (2000). Grammars of spoken English: New outcomes of corpus-oriented research. Language Learning, 50(4), 675–724. 
Martin, P. (2014). Speech and corpora: How spontaneous speech analysis changed our point of view on some linguistic facts. The case of sentence intonation in French. In T. Raso & H. Mello (Eds.), Spoken corpora and linguistic studies (pp. 191–209). Amsterdam/Philadelphia: John Benjamins. 
Mertens, P. (2004). The prosogram: Semi-automatic transcription of prosody based on a tonal perception model. In Proceedings of speech prosody 2004 (pp. 549–552). Nara, Japan. 
Milde, J.-T., & Gut, U. (2004). TASX – eine XML-basierte Umgebung f die Erstellung und Auswertung sprachlicher Korpora. In A. Mehler & H. Lobin (Eds.), Automatische Textanalyse: Systeme und Methoden zur Annotation und Analyse natlichsprachlicher Texte (pp. 249–264). Wiesbaden: Verlag f Sozialwissenschaften. 
Moisl, H. (2014). Statistical corpus exploitation. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 110–132). Oxford: Oxford University Press. 
Moreno-Sandoval, A., & Guirao, M. (2006). Morpho-syntactic tagging of the Spanish C-ORAL­ROM corpus: Methodology, tools and evaluation. In Y. Kawaguchi, S. Zaima, & T. Takagaki (Eds.), Spoken language corpus and linguistic informatics (pp. 199–218). Amsterdam: John Benjamins. 
Neri, A., Cucchiarini, C., & Strik, H. (2006). Selecting segmental errors in non-native Dutch for optimal pronunciation training. International Review of Applied Linguistics in Language Teaching, 44, 354–404. 
Nesselhauf, N. (2004). Learner corpora and their potential in language teaching. In J. Sinclair (Ed.), How to use corpora in language teaching (pp. 125–152). Amsterdam: Benjamins. 
Nolan, F., & Post, B. (2014). The IViE corpus. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 475–485). Oxford: Oxford University Press. 
O’Connor, J. D., & Arnold, G. (1961). Intonation of colloquial English. London: Longman. 
O’Keeffe, A., & Walsh, S. (2012). Applying corpus linguistics and conversation analysis in the investigation of small group teaching in higher education. Corpus Linguistics and Linguistic Theory, 8–1, 159–181. 
Oostdijk, N. (2000). The spoken Dutch corpus. Overview and frst evaluation. In M. Gravilidou, 
G. Carayannis, S. Markantonatou, S. Piperidis, & G. Stainhaouer (Eds.), Proceedings of the second international conference on Language Resources and Evaluation (LREC) (Vol. 2, pp. 887–894). 
Oostdijk, N. (2002). The design of the spoken Dutch corpus. In P. Peters, P. Collins, & A. Smith (Eds.), New frontiers of corpus research (pp. 105–112). Amsterdam: Rodopi. Oostdijk, N. (2003). Normalization and disfuencies in spoken language data. In S. Granger & 
S. Petch-Tyson (Eds.), Extending the scope of corpus-based research. New applications, new challenges (pp. 59–70). Amsterdam: Rodopi. Pätzold, M. (1997). KielDat ± Data bank utilities for the Kiel Corpus. Arbeitsberichte des Instituts f Phonetik der Universität Kiel, 32, 117–126. 
Panunzi, A., Picchi, E., & Moneglia, M. (2004). Using PiTagger for lemmatization and PoS tagging of a spontaneous speech corpus: C-Oral-Rom Italian.InM.T.Lino, M. F. Xavier,F.Ferreira, 
R. Costa, & R. Silva (Eds.), Proceedings of the 4th LREC conference (pp. 563–566). Raso, T., & Mello, H. (Eds.). (2014). Spoken corpora and linguistic studies. Amsterdam/Philadel­
phia: John Benjamins. Rose, Y. (2014). Corpus-based investigations of child phonological development. In J. Durand, 
U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 265–285). Oxford: Oxford University Press. 
Rose, Y., & MacWhinney, B. (2014). The Phonbank project. Data and software-assisted methods for the study of phonology and phonological development. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The oxford handbook of corpus phonology (pp. 380–401). Oxford: Oxford University Press. 
Ruhi, ¸
S., Haugh, M., Schmidt, T., & Wner, K. (Eds.). (2014). Best practices for spoken corpora in linguistic research. Newcastle upon Tyne: Cambridge Scholars Publishing. Schauer, G., & Adolphs, S. (2006). Expressions of gratitude in corpus and DCT data: Vocabulary, formulaic sequences, and pedagogy. System, 34, 119–134. Schiel, F. (2004). MAUS goes iterative. In Proceedings of the IV. International conference on language resources and evaluation (pp. 1015–1018). University of Lisbon. 
Schmidt, T., & Wner, K. (2014). EXMARaLDA. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 402–419). Oxford: Oxford University Press. Schonell, F., Meddleton, I., Shaw, B., Routh, M., Popham, D., Gill, G., Mackrell, G., & Stephens, 
C. (1956). A study of the oral vocabulary of adults. Brisbane/London: University of Queensland Press/University of London Press. 
Selting, M., Auer, P., Barth-Weingarten, D., Bergmann, J., Bergmann, P., Birkner, K., Couper-Kuhlen, E., Deppermann, A., Gilles, P., Gthner, S., Hartung, M., Kern, F., Mertzlufft, C., Meyer, C., Morek, M., Oberzaucher, F., Peters, J., Quasthoff, U., Schte, W., Stukenbrock, A., & Uhmann, S. (2009). Gesprächsanalytisches Transkriptionssystem 2 (GAT 2). Gesprächs­forschung, 10, 353–402. 
Silverman, K., Beckman, M., Pitrelli, J., Ostendorf, M., Wightman, C., Pierrehumbert, J., & Hirschberg, J. (1992). ToBI: A standard for labeling English prosody. In Proceedings of second International conference on spoken language processing (Vol. 2, pp. 867–870). Banff, Canada. 
Simpson, R. C., Briggs, S. L., Ovens, J., & Swales, J. M. (2002). The Michigan corpus of academic spoken English. Ann Arbor: The Regents of the University of Michigan. 
Simpson-Vlach, R., & Leicher, S. (2006). The MICASE handbook: A resource for users of the Michigan corpus of academic spoken English. University of Michigan Press/ELT. 
Sloetjes, H. (2014). ELAN: Multimedia annotation application. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 305–320). Oxford: Oxford University Press. 
Strik, H., & Cucchiarini, C. (2014). On automatic phonological transcription of speech corpora. In 
J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 89–109). Oxford: Oxford University Press. Svartvik, J. (Ed.). (1990). The London corpus of spoken English: Description and research (Lund studies in English) (Vol. 82). Lund: Lund University Press. 
Szmrecsanyi, B., & Wolk, C. (2011). Holistic corpus-based dialectology. RevistaBrasileirade Linguística Aplicada, 11(2). https://doi.org/10.1590/S1984-63982011000200011. Accessed 22 May 2019. 
Van Eynde, F., Zavrel, J., & Daelemans, W. (2000). Lemmatisation and morphosyntactic anno­tation for the spoken Dutch corpus. In P. Monachesi (Ed.), Computational linguistics in the Netherlands 1999. Selected papers from the tenth CLIN meeting (pp. 53–62). Utrecht Institute of Linguistics OTS. 
Wells, J., Barry, W., Grice, M., Fourcin, A., & Gibbon, D. (1992). Standard computer-compatible transcription. Technical report. No. SAM Stage Report Sen.3 SAM UCL-037. Williams, B. (1996). The formulation of an intonation transcription system for British English. In 
A. Wichmann, P. Alderson, & G. Knowles (Eds.), Working with speech (pp. 38–57). London: Longmans. 
Wittenburg, P., Trilsbeek, P., & Wittenburg, F. (2014). Corpus archiving and dissemination. In J. Durand, U. Gut, & G. Kristoffersen (Eds.), The Oxford handbook of corpus phonology (pp. 133–149). Oxford: Oxford University Press. 
Chapter 12 Parallel Corpora 
Marie-Aude Lefer 
Abstract This chapter gives an overview of parallel corpora, i.e. corpora containing source texts in a given language, aligned with their translations in another language. More specifcally, it focuses on directional corpora, i.e. parallel corpora where the source and target languages are clearly identifed. These types of corpora are widely used in contrastive linguistics and translation studies. The chapter frst outlines the key features of parallel corpora (they typically contain written texts translated by expert translators working into their native language) and describes the main methods of parallel corpus analysis, including the combined use of parallel and comparable corpora. It then examines the major challenges that are linked with the design and analysis of parallel corpora, such as text availability, metadata collection, bitext alignment, and multilingual linguistic annotation, on the one hand, and data scarcity, interpretation of the results and infelicitous translations, on the other. Finally, the chapter shows how these challenges can be overcome, most notably by compiling balanced, richly-documented parallel corpora and by cross-fertilizing insights from cross-linguistic research and natural language processing. 
12.1 Introduction 
This chapter gives an overview of parallel corpora, which are widely used in corpus-based cross-linguistic research (here understood as an umbrella term for contrastive linguistics and translation studies) and natural language processing. Parallel corpora (also called translation corpora) contain source texts in a given language (the source language, henceforth SL), aligned with their translations in another language (the target language, henceforth TL). It is important to point out from the outset that the term parallel corpus is to some extent ambiguous, because it is sometimes used to refer to comparable original texts in two or more languages, especially texts that 
M.-A. Lefer (•) Université catholique de Louvain, Centre for English Corpus Linguistics, Louvain-la-Neuve, Belgium e-mail: marie-aude.lefer@uclouvain.be 
© Springer Nature Switzerland AG 2020 257 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_12 
belong to comparable genres or text types and deal with similar topics (e.g. Italian and German newspaper articles about migration or English and Portuguese medical research articles). Here, the term will only be used to refer to collections of source texts and their translations. 
The compilation of parallel corpora started in the 1990s. Progress has been rather slow, compared with monolingual corpus collection initiatives, but in recent years we have witnessed a boom in the collection of parallel corpora, which are increasingly larger and multilingual. Parallel corpora are highly valuable resources to investigate cross-linguistic contrasts (differences between linguistic systems) and translation-related phenomena, such as translation properties (features of translated language). They can also be used for a wide range of applications, such as bilingual lexicography, foreign language teaching, translator training, terminology extraction, computer-aided translation, machine translation and other natural language process­ing tasks (e.g. word sense disambiguation and cross-lingual information retrieval). 
This chapter is mainly concerned with the design and analysis of parallel corpora in the two felds of corpus-based contrastive linguistics and corpus-based translation studies. Contrastive linguistics (or contrastive analysis) is a linguistic discipline that is concerned with the systematic comparison of two or more languages, so as to describe their similarities and differences. Corpus-based contrastive linguistics was frst pioneered by Stig Johansson in the 1990s and has been thriving ever since. Corpus-based translation studies is one of the leading paradigms in Descriptive Translation Studies (Toury 2012). This feld also emerged in the 1990s, under the impetus of Mona Baker and relies on corpus linguistic tools and methods to elucidate translated text (in particular, the linguistic features that set translated language apart from other forms of language production) (cf. Kruger et al. 2011;De Sutter et al. 2017). Contrastive linguistics and translation studies, which both make intensive use of parallel corpora, are quite close, as demonstrated by edited volumes such as Granger et al. (2003) and the biennial Using Corpora in Contrastive and Translation Studies conference series (e.g. Xiao 2010). 
12.2 Fundamentals 
12.2.1 Types of Parallel Corpora 
Parallel corpora can be of many different types. They can be bilingual (one SL and one TL), such as the English-Norwegian Parallel Corpus (ENPC; Johansson 2007), or multilingual (more than one SL and/or TL), such as the Oslo Multilingual Corpus, which is fully trilingual (English, Norwegian and German), with some texts available in Dutch, French and Portuguese as well (ibid., 18–19). Other multilingual parallel corpora include the Slavic parallel corpus ParaSol (Waldenfels 2011) and InterCorp ( .
Cermák and Rosen 2012). A further distinction is made between monodirectional corpora, when only one translation direction is repre­
sented (SLX >TLY, e.g. English > Chinese), and bidirectional (or reciprocal) corpora, when both translation directions are included (SLX >TLY and SLY >TLX, 
e.g. English > Chinese and Chinese > English). The ENPC, for example, is bidirectional (from English to Norwegian, and vice versa). Most parallel corpora contain published translations (with some exceptions, e.g. when translations are specifcally commissioned for a particular corpus compilation project). In most cases, only one translation of each source text is included. However, there are also multiple translation corpora, which include several translations of the same source text in a given TL. Such corpora make it possible to compare the translation solutions used by various translators rendering the same source text. 
12.2.2 Main Characteristics of Parallel Corpora 
The majority of parallel corpora used in contrastive linguistics and translation studies are characterized by two key features. First, the source and target languages are clearly identifed. In other words, the translation direction is known (from LanguageX to LanguageY or from LanguageY to LanguageX). In cross-linguistic research, it is of paramount importance to know, for instance, whether a given text was translated from Spanish into German or vice versa, because corpus studies have shown that translation direction infuences translation choices, and hence the linguistic make-up of translated text (e.g. Dupont and Zufferey 2017). Second, only direct translation is included, i.e. no pivot (intermediary, mediating) language is used between the source and target languages. In texts produced by the European Union (EU), for example, English has been systematically used as a pivot language since the early 2000s. In practical terms, this means that a text originally written in, say, Slovenian or Dutch is frst translated into English. The English version is then translated into the other offcial languages of the EU. In other words, English acts as a pivot language and most target texts originating from EU institutions are in fact translations of translations (see Assis Rosa et al. 2017 on the issue of indirect translation). Parallel corpora that display these two features (known as translation direction and translation directness) will be referred to as directional parallel corpora in this chapter (a term borrowed from Cartoni and Meyer 2012). Parallel corpora whose translation direction is unknown and/or where a pivot language has been used will be called non-directional. Examples of the latter type include the Europarl corpus (Koehn 2005), the Eur-Lex corpus (Baisa et al. 2016) and the United Nations Parallel Corpus (Ziemski et al. 2016). It is important to bear in mind, however, that the distinction between directional and non-directional parallel corpora is not always clear-cut. In some parallel corpora, both types of parallel texts are included. For example, the Dutch Parallel Corpus (DPC; Macken et al. 2011), which is largely directional, contains some indirect, EU translations. 
Directional parallel corpora typically (i) contain written texts (ii) translated by expert translators (iii) working into their native language (L1), and (iv) cover a rather limited number of text types or genres. Each of these typical features will be discussed in turn: 
(i) 
Directional parallel corpora mainly cover written translation (e.g. the ENPC), to the detriment of other translation modalities, such as interpreting and audio­visual translation. In recent years, however, efforts have been made to include other forms of translation. A case in point is the compilation of several parallel corpora of simultaneous interpreting (see Russo et al. 2018 for an overview of corpus-based interpreting studies). In these corpora, the main source of data (speeches and their interpreted versions) is the European Parliament (Bernar­dini et al. 2018). An example of one such European Parliament interpreting corpus is the fully trilingual English-Italian-Spanish European Parliament Interpreting Corpus (Russo et al. 2006). Recent developments also include the compilation of intermodal parallel corpora, i.e. corpora representing several translation modalities (e.g. written translation and simultaneous interpreting), such as the European Parliament Translation and Interpreting Corpus (EPTIC; Ferraresi and Bernardini 2019). EPTIC features two main components: (i) simultaneous interpreting: transcripts of speeches delivered at the European Parliament plenary sittings and transcripts of the simultaneous interpretations of these speeches, and (ii) written translation: the verbatim reports of the plenary sittings, as offcially published on the European Parliament website, alongside the offcial translations of these verbatim reports (the Europarl corpus is also based on this written material, see Representative Corpus 2 below). Parallel corpora of sign interpreting (e.g. Meurant et al. 2016) and audiovisual translation modalities (such as subtitling, dubbing, and flm audio description; cf. Bas et al. 2013) have also been collected recently. Some of these parallel corpora are multimodal, in the sense that they contain different modes, such as language, image, sound and music (e.g. Jimenez Hurtado and Soler Gallego 2013; Chap. 16). 

(ii) 
In general, parallel corpora include target text translated (or assumed to have been translated) by professional and/or expert translators (it must be stressed, however, that limited metadata on translators’ status have been collected to date; see Sect. 12.2.4.2). In some cases, the translators’ status is rather unclear 


(e.g. in translated news items, found in several parallel corpora, from Le Monde Diplomatique, a French monthly newspaper with more than 30 international editions, in 20+ languages1). Other translators’ profles are also represented, albeit less frequently, such as non-professional, volunteer translators, as in the TED Talks WIT3 corpus (Web Inventory of Transcribed and Translated Talks; Cettolo et al. 2012). Aside from professional and volunteer translators, some parallel corpora, called learner translation corpora (LTC), contain translations produced by foreign language learners or trainee translators, i.e. novices (see also Chap. 13). The frst LTC emerged in the early 2000s (Uzar 2002;Bowker 
1https://www.monde-diplomatique.fr/diplo/int/. Accessed 22 May 2019. 
and Bennison 2003) and have been followed by several similar initiatives, such as the MeLLANGE corpus (Castagnoli et al. 2011), the English-Catalan UPF LTC (Espunya 2014), the Russian Learner Translator Corpus (Kutuzov and Kunilovskaya 2014), and the Multilingual Student Translation corpus (Granger and Lefer 2020). The vast majority of directional parallel corpora contain translations produced by human translators (in some cases, with the help of computer-aided translation tools). Recently, however, translation scholars have started to include machine-translated texts alongside human-translated texts, with a view to uncovering the linguistic traits that differentiate machine translation from human translation (computer-aided or otherwise) (e.g. Lapshinova-Koltunski 2017). 
(iii) Directional parallel corpora tend to be restricted to L1 translation (i.e. when the translation is carried out into the translator’s native language), except in the case of some LTC, which contain L2 (inverse, reverse) translation as well, or corpora representing language pairs for which L2 translation is common practice (e.g. Finnish to English) (see Beeby Lonsdale 2009 on directionality practices). 
(iv) Most directional, balanced parallel corpora used in contrastive linguistics and translation studies are restricted to a couple of genres or text types, mainly fctional prose (e.g. the English-Portuguese COMPARA; Frankenberg-Garcia and Santos 2003; the “core” part of InterCorp), news (news items and opinion articles published in newspapers and magazines) and/or non-fction, such as popular science texts (e.g. the ENPC; the English-French Poitiers-Louvain Échange de Corpus Informatisés PLECI2; the French-Slovenian FraSloK par­allel corpus, Mezeg 2010; and the English-Spanish ACTRES parallel corpus, Izquierdo et al. 2008). A handful of directional parallel corpora cover a wider range of text types. Examples include the DPC for the language pairs Dutch-English and Dutch-French (Macken et al. 2011) and the CroCo corpus for German-English (Hansen-Schirra et al. 2012), with fve and ten text types represented, respectively. 
The directional parallel corpora featuring the four characteristics outlined above are relatively modest in size compared with monolingual reference corpora com­monly used in corpus linguistics (they usually contain a few million words). This is even more striking for parallel corpora of interpreted language, in view of the many hurdles inherent in transcribing spoken data (Bernardini et al. 2018; Chap. 11). If more parallel data are needed, and provided translation direction and directness are not considered to be of particular relevance, researchers can turn to several non-directional parallel corpora (mainly of legislative and administrative texts) that are much larger than the parallel corpora discussed so far. These mega corpora are used widely in natural language processing, for example for data-driven machine translation. However, it is important to bear in mind that (i) in these corpora, 
2https://uclouvain.be/en/research-institutes/ilc/cecl/pleci.html. Accessed 22 May 2019. 
translation direction is often unknown (i.e. the source and target languages are not clearly identifed), and (ii) in many instances, the translation relationship between the parallel texts for a given language pair is indirect (either the translation is done through an intermediary, pivot language, or the parallel texts in a given pair are both translations from another, third language). Generally speaking, non-directional parallel corpus data should be treated with caution. While their use makes sense in natural language processing research, it remains to be seen whether they can yield reliable insights into cross-linguistic differences. 
12.2.3 Methods of Analysis in Cross-Linguistic Research 
Parallel corpora are widely used in corpus-based contrastive linguistics and transla­tion studies and they are starting to emerge as a useful source of data in typology as well (Levshina 2016). As pointed out by Johansson (2007:3), most contrastive scholars “have either explicitly or implicitly made use of translation as a means of establishing cross-linguistic relationships. [ ...] As translation shows what elements may be associated across languages, it is fruitful to base a contrastive study on a comparison of original texts and their translations”. In other words, parallel corpora can be used to study cross-linguistic correspondences (e.g. between lexical items, lexico-syntactic patterns or grammatical structures). The corpus methods used to achieve that goal are similar to those applied in monolingual corpus linguistics, such as concordances (Chap. 8) and co-occurrence data (Chap. 7). 
Figure 12.1 provides a sample of bilingual concordances for the English phrase no kidding and its Italian equivalents in a corpus of subtitled flms and series (OpenSubtitles2011, available in Sketch Engine; see Sect. 12.4). A cursory glance at Fig. 12.1 shows that Italian equivalents include non scherzo (‘I am not kidding’), sul serio (‘seriously’) and davvero (‘really’). The detailed analysis of English-Italian equivalences found in the corpus can act as a springboard for an in-depth contrastive analysis (e.g. what are the discursive and pragmatic functions of no kidding in scripted spoken English and which equivalent expressions are used in Italian to fulfll these functions?). Bilingual concordances are also widely used in translation studies to investigate the translation procedures used to render specifc items (e.g. lexical innovations, proper names, culture-specifc elements). For instance, on the basis of an Italian-to-German parallel corpus of tourist brochures, it is possible to determine whether translators adapt SL culture-bound items (e.g. macchiato, caffè latte) or whether they keep them in their translation (perhaps with an explanatory note), which refects more general translation strategies towards domestication and foreignization. 

Figure 12.2 shows a sample of a bilingual Word Sketch, i.e. a summary of the grammatical and collocational behaviors of equivalent words, for English sustainability and its French equivalent durabilité in parliamentary proceedings (Europarl). The bilingual Word Sketch makes it possible, among other things, to detect equivalent verbal collocates of the English and French nouns under scrutiny, such as jeopardize/menacer and ensure/assurer. This kind of co-occurrence analysis is particularly helpful for contrastive phraseology, applied translation studies (e.g. to raise trainee translators’ awareness of phraseological equivalence) and bilingual lexicography. 
In the two examples mentioned above, we started with a given SL item (no kidding, sustainability) and examined its translation equivalents in the TL (Italian and French, respectively), i.e. going from source to target. Interestingly, this source-to-target approach is also used in monolingual corpus linguistics to examine the semantic, discursive and pragmatic features of source-language items (Noël 2003). For example, Aijmer and Simon-Vandenbergen (2003) examine the meanings and functions of the English discourse particle well on the basis of its Swedish and Dutch translation equivalents in a parallel corpus of fctional texts. 

An alternative method is to start off from a given item or structure in translated texts and examine its corresponding source-text items or structures, i.e. from target to source. Taking the same example as above, this would entail analyzing all occurrences of sul serio in Italian subtitles and identifying the English source items that have triggered their use. This target-to-source approach is quite common in translation studies. Delaere and De Sutter (2017), for example, rely on an English-to-Dutch parallel corpus to fnd out whether the English loanwords found in translated Dutch stem from their corresponding trigger words in the English source texts. 
Naturally, these two approaches (source to target and target to source) can be combined if a more comprehensive picture of cross-linguistic correspondences is required. Indeed, many new insights can be gained by investigating a given item or structure in both source and target texts, so as to fnd out how it is commonly translated and which items in the other language have triggered its use in translation 
(e.g. Zufferey and Cartoni 2012). 
It is also possible, on the basis of parallel corpora, to work out what Altenberg has termed mutual correspondence (or mutual translatability), i.e. “the frequency with which different (grammatical, semantic and lexical) expressions are translated into each other” (Altenberg 1999:254). Mutual correspondence is calculated as follows, with At and Bt corresponding to the frequencies of the compared items A and B in the target texts (t), and As and Bs to their frequencies in the source texts (s): 
(At + Bt) × 100 
mutual correspondence = 
As + Bs 
If, say, a lexical item A is always translated with an item B, and vice versa, then items A and B have a mutual correspondence of 100%. If, on the contrary, A and B are never translated with each other, they display a mutual correspondence of 0%. In other words, this index makes it possible to assess the extent to which items are equivalent across languages: “the higher the mutual correspondence value is, the greater the equivalence between the compared items is likely to be” (Altenberg and Granger 2002:18). For example, Dupont and Zufferey (2017) fnd that in samples of 200 occurrences extracted from Europarl, the adverb pair however/ cependant displays a mutual correspondence of 57% (however > cependant: 87/200, cependant > however: 140/200), while the however/toutefois pair has a lower 114/200): (87 + 140) × 100 
however/cependant = 
200 + 200 
(80 + 114) × 100 
however/toutef ois = 
200 + 200 
The scores tend to indicate that in parliamentary proceedings, the however/ cependant cross-linguistic equivalence is somewhat stronger than for however and toutefois. 
So far, we have outlined different methods of parallel corpus analysis (from source to target, from target to source, mutual correspondence). However, it should be stressed that several types of corpora can be combined to reveal and disentangle cross-linguistic contrasts and translation-related phenomena (Bernardini 2011; Johansson 2007; Halverson 2015). Two types of corpora are commonly used in cross-linguistic research in combination with parallel corpora: (i) bilingual/multilin­gual comparable corpora and (ii) monolingual comparable corpora. Their combined use with parallel corpora will be discussed in turn. 
Bilingual (or multilingual) comparable corpora are “collections of original 
[i.e. non-translated] texts in the languages compared” (Johansson 2007:5). The texts are strictly matched by criteria such as register, genre, text type, domain, subject matter, intended audience, time of publication, and size. Examples include KIAP, a comparable corpus of research articles in Norwegian, English, and French (Fltum et al. 2013) and the Multilingual Editorial Corpus, a comparable corpus of newspaper editorials in English, Dutch, French, and Swedish.3 Bilingual and multilingual comparable corpora usefully complement parallel corpora in that a given phenomenon can be studied cross-linguistically on the basis of comparable original texts, i.e. texts displaying no trace of source-language or source-text infu­ence, unlike translations in parallel corpora. Corpus studies combining both types of corpora can start either with the bilingual/multilingual comparable analysis, before turning to the parallel corpus analysis, or the other way around, depending on the research questions to be tackled (see Johansson 2007 for more details). Interestingly, bilingual comparable and parallel corpora can be combined in the same corpus framework, namely bidirectional parallel corpora whose two translation directions are truly comparable in terms of size, text types, etc. As shown in Fig. 12.3,for example, the ENPC can function both as a bidirectional parallel corpus (English originals > Norwegian translations and Norwegian originals > English translations; see black arrows) and as a bilingual comparable corpus (English originals and Norwegian originals; see white double arrow). Numerous parallel corpora are 
3https://uclouvain.be/en/research-institutes/ilc/cecl/mult-ed.html. Accessed 22 May 2019. 


based on the ENPC model, such as the English-Swedish Parallel Corpus (ESPC), COMPARA and PLECI. 
However, the main problem of the bidirectional ENPC model is that the selection of texts to be included in the corpus is limited to genres that are commonly translated in both directions (see Johansson, 2007:12 on this issue). In other words, the number of genres and texts that can be included in the corpus is often limited 
(e.g. only fction and non-fction texts in the ENPC). As a result, to improve representativeness, the comparable, original components of bidirectional parallel corpora need to be supplemented with larger, multi-genre (reference) monolingual corpora of the languages investigated (see Fig. 12.4). 

Parallel corpora can also be combined with monolingual comparable corpora, which include comparable translated and non-translated (i.e. original) texts in a given language (e.g. novels originally written in English alongside novels translated into English from a variety of source languages; see, for example, the ten-million-word Translational English Corpus4). Monolingual comparable corpora of translated and original texts are widely used in translation studies, with a view to identifying the major distinguishing features of translated language, when compared with original language production (the so-called translation universals,or translation features/properties, such as simplifcation, normalization and increased explicitness; cf. Baker 1993, 1995). Parallel corpora, when combined with monolin­
gual comparable corpora, are used to check for source-text and/or source-language infuence. Cappelle and Loock (2013), for example, use parallel corpus data to fnd out whether the under-representation of existential there in English translated from French (as compared with non-translated English) stems from SL (French) interference. Parallel and monolingual comparable corpora can be integrated within the same overall corpus framework, as shown in Fig. 12.5. 
12.2.4 Issues and Methodological Challenges 
12.2.4.1 Issues and Challenges Specifc to the Design of Parallel Corpora 
This section presents an overview of some of the main challenges specifc to the design of parallel corpora (for a detailed discussion of more general issues, such 
4https://www.alc.manchester.ac.uk/translation-and-intercultural-studies/research/projects/ translational-english-corpus-tec/. Accessed 22 May 2019. 
as representativeness and balance, copyright clearance,5 and text encoding, see 
Chap. 1). 
The frst issue is text availability. As mentioned above, parallel corpora, espe­cially bidirectional ones, tend to be modest in size and are often restricted to a small number of text types. One of the reasons for this is that for any given language pair (LX and LY), there is often some kind of asymmetry or imbalance between the two translation directions (LX >LY and LY >LX). This imbalance can take several forms: either there are simply fewer texts translated in one direction than in the other (especially when the language pair involves a less “central”, or more “peripheral”, language), or certain text types are only (or more frequently) translated in one of the two directions. For example, as noted by Frankenberg-Garcia & Santos (2003:75) in relation to the translation of tourist brochures for the English-Portuguese pair: 
[t]ourist brochures in Portuguese translation are practically non-existent: Portuguese-speaking tourists abroad are expected to get by in other, more widely known languages. In contrast, almost all material destined to be read by tourists in Portuguese-speaking countries comes with an English translation. 
To sum up, “translations are heavily biased towards certain genres, but these biases are rarely symmetrical for any language pair” (Mauranen 2005:74). In addition, some widely translated text types may be hard to obtain, for obvious confdentiality reasons specifc to translation projects carried out by translation agencies and freelance translators (e.g. legal texts or texts translated for internal use only). Finally, there are language pairs for which there are very few parallel texts available (cf. for example, Singh et al. 2000 on building an English-Punjabi parallel corpus). To compensate for data scarcity, there have been a number of initiatives since the early 2000s (Resnik and Smith 2003) aiming to create mainly non-directional parallel corpora by crawling sites across the web (Chap. 15). 
Obtaining detailed metadata is another challenge facing anyone wishing to compile a parallel corpus. In this respect, parallel corpora are clearly lagging behind compared with other corpus types, such as learner corpora, which are more richly documented (Chap. 13). Ideally, the following metadata should be collected (this list is non-exhaustive): 
• 
Source text and target text: author(s)/translator(s), publisher, register, genre, text type, domain, format, mode, intended audience, communicative purpose, publication status, publication date, etc. 

• 
Translation direction, including SL and TL (and their varieties) 

• 
Translation directness: use of a pivot language or not 


5Unsurprisingly, it is far from easy to obtain copyright clearance for texts to be included in parallel corpora. For this reason, many parallel corpora are not publicly available (e.g. ENPC, PLECI, Raf Salkie’s INTERSECT, P-ACTRES, CroCo). 
• 
Translation directionality: L2 > L1 translation, L1 > L2 translation, L2 > L2 translation, etc. 

• 
Translator: translator’s status (professional, volunteer/amateur, student, etc.), translator’s occupation, gender, nationality, country of residence, translation expertise (expert vs. novice), translation experience (which can be measured in many different ways, e.g. number of years’ experience), language background (native and foreign languages), etc. 

• 
Translation task: use of computer-aided translation tools (translation memories, terminological databases) and other tools and resources (dictionaries, forums, corpora, etc.), use of a translation brief (set of translation instructions, including, for instance, use of a specifc style guide or in-house terminology), fee per word/line/hour, deadline/time constraints, etc. 

• 
Revision/editorial intervention: self-and other-revision, types of revision (e.g. copyediting, monolingual vs. bilingual revision), etc. 


It is also important to stress here that the concepts of source language and source text are becoming increasingly blurred. In today’s world, some “source” documents are simultaneously drafted in several languages. In multilingual translation projects, there are also cases where there is no single “source” text, as translators translate a given text while accessing some of its already available translations (e.g. when confronted with an ambiguous passage). 
Third, there is the issue of alignment, i.e. the process of matching corresponding segments in source and target texts (see Tiedemann 2011). Software tools can be used to align parallel texts automatically at paragraph, sentence and word level (see, for instance, Hunalign, Varga et al. 2007;GIZA++, Och and Ney 2003; fast_align, Dyer et al. 2013). Most directional corpora are aligned at sentence level. Different sources of information can be used to match sentences across parallel texts, such as sentence length (in words or characters, normalized by text length), word length, punctuation (e.g. quotation marks), and lexical anchors (e.g. cognates). Some aligners also rely on bilingual dictionaries. Sentence alignment is not a straightforward task, as translators often merge or split sentences when producing the target text. This is referred to as 2:1 and 1:2 alignment links, respectively (see examples in Table 12.1). 
As pointed out by Macken et al. (2011:380), “[t]he performance of the individual alignment tools varies for different types of texts and language pairs and in order to guarantee high quality alignments, a manual verifcation step is needed”. A good option is to use a tool that combines automatic sentence alignment and manual post-alignment correction options, such as the open-source desktop application InterText editor (Vond.ri.
cka 2014) or the Hypal interface (Obrusnik 2014). One way of reducing this manual editing step is to combine the output of several aligners, as done for the DPC, where the corpus compilers combined the output of three 
Table 12.1 Splitting and merging source-text sentences in translation 
1:1 alignment  Hachez les feuilles de coriandre et mélangez au gingembre.  Chop the coriander leaves and mix with the ginger.  
Splitting (1:2 alignment)  Hachez les feuilles de coriandre et mélangez au gingembre.  Chop the coriander leaves. Mix with the ginger.  
Merging (2:1 alignment)  Râpez le gingembre. Coupez les feuilles de coriandre et mélangez au gingembre.  Grate the ginger, then chop the coriander leaves and mix with the ginger.  

aligners. The alignment links that were present in the output of at least two aligners were considered as reliable alignment links. All the other links were then checked manually (this shows that manual editing of automatically aligned texts is essential, even when the output of several aligners is combined). Aligners typically generate the following types of XML output: (i) one source-text fle, one target-text fle and one link fle (linking up the source-and target-text segments), (ii) one source-text fle and one target-text fle, containing the same number of segments, or (iii) a TMX (Translation Memory eXchange)fle. 
Finally, yet another major challenge relating to the compilation of parallel corpora (or any other type of multilingual corpus) is multilingual linguistic anno­tation (e.g. lemmatization, morphosyntactic annotation, syntactic parsing, semantic tagging; Chap. 2). Johansson (2007:306) rightly argues that “[t]o go beyond surface forms, we need linguistically annotated corpora that allow more sophisticated studies”. However, multilingual annotation raises the following key questions, which echo the more general “universality vs. diversity” debate in linguistics (see, for example, Evans and Levinson 2009): 
If corpora are annotated independently for each language, to what extent is the analysis comparable? If they are provided with some kind of language-neutral annotation (for parts of speech, syntax, etc.), to what extent do we miss language-specifc characteristics? (Johansson 2007:306). 
At present, no defnite answers have been found to these questions. As a matter of fact, issues related to multilingual annotation (e.g. whether it should be language-specifc or language-neutral, or, more generally, how cross-linguistic comparability can be achieved) have received relatively little attention in contrastive linguistics and translation studies (one notable exception is Neumann 2013). The language-specifc and language-neutral approaches are both used in parallel corpora, the former being more common. In the language-specifc approach, researchers rely either on separate annotation tools (one per language involved) or on one single tool that is available for several languages, such as the TreeTagger (Schmid 1994) or FreeLing (Padrand Stanilovsky 2012) POS taggers. However, it is important to bear in mind that in these multilingual annotation tools, (i) the annotation systems are not designed to be cross-linguistically comparable: some tags are language-specifc (e.g. the RP tag used for English adverbial particles) while, unsurprisingly, “shared” tags display language-specifc features (e.g. the TreeTagger JJ tag used for English adjectives does not correspond fully to what the French ADJ tag covers), and (ii) precision and recall ratios (Chap. 2) differ across languages (e.g. for the TreeTagger, they tend to be higher for English than for French). These two factors can potentially jeopardize the contrastive comparability of annotated multilingual data. Great care should therefore be taken when analyzing annotated data in cross-linguistic research (see, for example, Neumann 2013 and Evert and Neumann 2017 on the English-German language pair). An interesting language-neutral approach, suggested in Rosen (2010), consists in using an abstract, interlingual hierarchy of linguistic categories mapped to language-specifc tags. In the same vein, some researchers have proposed “universal” tagsets, which include tags that accommodate language-specifc parts-of-speech (see, for example, Benko 2016;the MULTEXT-East project,6 with its harmonized morphosyntactic annotation system for 16 languages; Erjavec’s SPOOK specifcations,7 with harmonized tagsets for English, French, German, Italian, and Slovenian). 
The multilingual annotation of existing parallel corpora is still very basic, being mostly limited to lemmatization and POS tagging. Syntactic annotation will probably become more standard in years to come, given recent advances in multilingual parsing (e.g. Bojar et al. 2012;Volketal. 2015; Augustinus et al. 2016 on parallel treebanks; see also the Universal Dependencies project8). 
12.2.4.2 Issues and Challenges Specifc to the Analysis of Parallel Corpora 
Clearly, compared with monolingual corpora, parallel corpora are lagging behind in terms of size (representativeness is also an issue, as small corpora tend to repre­sent relatively few authors and translators/interpreters). Low-frequency linguistic phenomena may be hard to analyze on the basis of parallel corpora, for sheer lack of suffcient data that would allow reliable generalizations. Researchers in contrastive linguistics and translation studies are therefore often forced to combine several parallel corpora to extract a reasonable amount of data, but this approach raises a number of problems. One is that several confounding variables may be intertwined in the various corpora used, which in turn hinders the interpretabil­ity of the results. In Lefer and Grabar (2015), for instance, we relied on two parallel corpora, i.e. verbatim reports of parliamentary debates (Europarl) and interlingual subtitles of oral presentations (TED Talks), so as to investigate the translation of rather infrequent lexical items, namely evaluative prefxes (e.g. over-and super-). We found marked and seemingly insightful differences between the translation procedures used in Europarl and TED Talks but were forced to recognize that it was impossible to assess to what extent the observed differences were due to source-text genre (parliamentary debates vs. oral presentations), translation 
6http://nl.ijs.si/ME/V4/msd/html/index.html. Accessed 22 May 2019. 7http://nl.ijs.si/spook/msd/html-en/. Accessed 22 May 2019. 8http://universaldependencies.org/. Accessed 22 May 2019. 
modality (written translation vs. subtitling) or translator expertise (professional translators vs. non-professional volunteers) or, for that matter, a combination of some or all of these factors. 
Another issue, also directly related to the interpretability of the results, is the cross-linguistic comparability (or lack thereof) of genres and text types in bidirectional parallel corpora (such as the ENPC, the DPC and CroCo) (see Neumann 2013). Matching genres or text types cross-linguistically is “by no means straightforward” (Johansson 2007:12). We may indeed wonder whether the observed differences refect genuine cross-linguistic contrasts and/or translation-specifc features or whether they are due to fundamental cross-linguistic differences between supposedly similar genres or text types (e.g. research articles or newspaper opinion articles) (cf. Fltum et al. 2013 on medical research articles in Norwegian). This question cannot be overlooked. 
It is also worth pointing out that most parallel corpora are poorly meta-documented (source and target texts and languages, translator, translation task, editorial intervention, etc.), which, unfortunately, can lead researchers to jump to hasty conclusions as regards both cross-linguistic contrasts (“this pattern is due to differences between the two language systems under scrutiny”) and features of translated language (“this is inherent in the translation process”). 
One fnal point to be made in this section is that parallel corpora (even those whose texts have all been translated by highly-skilled professionals) contain infelicities and even translation errors (to err is human, after all). Researchers may therefore feel uncomfortable with some of the data extracted from parallel corpora. Rather than sweeping erroneous items under the carpet, when in doubt it is probably safer to acknowledge these seemingly infelicitous or erroneous data explicitly. Moreover, looking on the bright side, these infelicities and errors can prove to be highly valuable in applied felds such as bilingual lexicography, foreign language teaching or translator training. In Granger and Lefer (2016), we suggest using them to devise corpus-based exercises, such as the detection and correction of erroneous translations or the translation of sentences containing error-prone items. 
Representative Study 1 
Dupont, M., and Zufferey, S. 2017. Methodological issues in the use of directional parallel corpora. A case study of English and French concessive connectives. International Journal of Corpus Linguistics 22(2):270–297. 
In their study, Dupont & Zufferey make an important methodological con­tribution to the feld of corpus-based contrastive linguistics by examining three factors that can potentially affect the nature of the cross-linguistic correspondences found in parallel corpora: register, translation direction 
(continued) 
and translator expertise. More specifcally, they compare three registers (news, parliamentary proceedings, and TED Talks) in two translation direc­tions (from English into French, and vice versa), examining three types of translator expertise (they compare professional, semi-professional and amateur translators). Their study is particularly innovative in that relatively few contrastive corpus studies to date have taken into consideration these infuencing factors (especially translation direction and translator expertise), focusing almost exclusively on the source and target linguistic systems under scrutiny. By assuming that the correspondences extracted from parallel corpora are mainly (or solely) due to similarities and differences between the source and target languages, researchers fail to acknowledge the inherently multidimensional nature of translation. In this study, Dupont & Zufferey investigate the translation equivalences between English and French adverbial connectives expressing concession (e.g. yet, however, nonetheless) across three parallel corpora (PLECI news, Europarl Direct and TED Talk Corpus). Their results indicate that translation choices (and hence, observed cross-linguistic correspondences) depend on the three factors investigated. 
Representative Study 2 
Delaere, I., and De Sutter, G. 2017. Variability of English Loanword Use in Belgian Dutch Translations: Measuring the Effect of Source Lan­guage, Register, and Editorial Intervention. In Empirical Translation Studies: New Methodological and Theoretical Traditions, eds. De Sutter, G., Lefer, M.-A., and Delaere, I., 81–112. Berlin/Boston: De Gruyter Mouton. 
Delaere & De Sutter’s study is situated in the feld of corpus-based translation studies. The authors explore three factors that can impact on the linguistic traits of translated language, namely source-language infuence, register, and editorial intervention (i.e. revision). They do so through an analysis of English loanwords (vs. their endogenous variants) in translated and original Belgian Dutch (e.g. research & development vs. onderzoek en ontwikkeling). Loanword use is related to a widely investigated topic in translation studies, viz. the normalization hypothesis, which states that translated text is more standard than non-translated text. The starting-point hypothesis of Delaere & De Sutter’s study is that overall, translators make more use of endogenous lexemes (a conservative option compared with the use of loanwords), than 
(continued) 
do non-translators (writers). Relying on the Dutch Parallel Corpus,the authors combine two approaches in their study: monolingual comparable (Dutch translated from English and French, alongside original Dutch) and parallel (English to Dutch). As is often the case in corpus-based translation studies, parallel data are used with a view to identifying the source-text items/structures that have triggered the use of a given item/structure in the translations (in this case, the presence of a trigger term in the English source texts, such as unit, job,or team). The authors apply multivariate statistics (profle-based correspondence analysis and logistic regression analysis) to measure the effect of the three factors investigated on the variability of English loanword use. The logistic regression analysis reveals that the effect of register is so strong that it cancels out the effect of source language. Their study convincingly illustrates the need to adopt multifactorial research designs in corpus-based translation studies, as these make it possible to go beyond the monofactorial designs where, typically, only the “translation status” variable is considered (translated vs. non-translated). 
Representative Corpus 1 
The Dutch Parallel Corpus (Macken et al. 2011) is a ten-million-word bidirectional Dutch-French and Dutch-English parallel corpus (Dutch being the central language). The DPC includes fve text types: administrative texts (e.g. proceedings of parliamentary debates, minutes of meetings, and annual reports), instructive texts (e.g. manuals), literature (e.g. novels, essays, and biographies), journalistic texts (news reporting articles and comment articles) and texts for external communication purposes (e.g. press releases and scientifc texts). The DPC also features rich metadata, such as publisher, translation direction, author or translator of the text, domain, keywords and intended audience. The corpus is fully aligned at sentence level and is lemmatized and part-of-speech tagged. Unlike many similar corpora, the DPC is available to the research community, thanks to its full copyright clearance. 
Representative Corpus 2 
To date, Europarl (Koehn 2005) is one of the few parallel corpora to have been used widely in both corpus-based contrastive/translation studies and natural language processing. It contains the proceedings (verbatim reports) of the European Parliament sessions in 21 languages. Its seventh version, 
(continued) 
released in 2012 by Koehn, includes data from 1996 to 20119 and amounts to 600+ million words. Europarl contains two types of European Parliament offcial reports, viz. written-up versions of spontaneous, impromptu speeches and edited versions of prepared (written-to-be-spoken) speeches. Europarl fles contain some metadata tags, such as the speaker’s name and the language in which the speech was originally delivered.10 The main problem, however, is that in part of the corpus, LANGUAGE tags are either missing or inconsistent across corpus fles. To solve this problem, Cartoni and Meyer (2012) have homogenized LANGUAGE tags across all corpus fles. Thanks to this approach, they have been able to extract directional Europarl subcorpora, 
i.e. subcorpora where the source and target languages are clearly identifed (see https://www.idiap.ch/dataset/europarl-direct). 
12.3 Critical Assessment and Future Directions 
As shown above, anyone wishing to design and compile a directional parallel corpus faces a number of key issues, such as parallel text availability (especially in terms of text-type variety), access to source text-, translator-and translation task-related metadata, automatic sentence alignment, and linguistic annotation. Relying on existing parallel corpus resources poses its own challenges as well, as present-day parallel corpora tend to be quite small and/or poorly meta-documented and typically cover relatively few text types. Notwithstanding these issues and challenges, parallel corpus research to date has yielded invaluable empirical insights into cross-linguistic contrasts and translation. 
There are many hopes and expectations for tomorrow’s parallel corpora. There are three ways in which headway can be made in the not too distant future. The frst 
9The practice of translating the European Parliament proceedings into all EU languages was ceased in the second half of 2011. The verbatim reports of the plenary sittings are still made available on the European Parliament website, but the written-up versions of the speeches are only published in the languages in which the speeches were delivered. 
10In this respect, it is important to stress that English is increasingly used as a lingua franca at the European Parliament. In other words, some of the speeches originally delivered in English are in fact given by non-native speakers of English (the same holds, albeit to a lesser extent, for other languages, such as French). This is not a trivial issue, as recent research indicates that the use of English as a Lingua Franca can have a considerable impact on translators’ (and interpreters’) outputs (see Albl-Mikasa (2017) for an overview of English as a Lingua Franca in translation and interpreting). 
two are related to the design of new parallel corpora, while the third is concerned with a rapprochement between natural language processing and cross-linguistic studies. 
First, it is high time we started collecting richer metadata, notably in terms of SL/TL, source and target texts, translator, translation task, and editorial intervention. This will make it possible to adopt multifactorial research designs and use advanced quantitative methods in contrastive linguistics and translation studies much more systematically, thereby furthering our understanding of cross-linguistic contrasts and of the translation product in general. 
Second, whenever possible, we should go beyond the inclusion of translated novels, news, and international organizations’ legal and administrative texts, and strive for the inclusion of more genres and text types, especially those that are dominant in today’s translation market, to which corpus compilers have had limited access to date, for obvious reasons of confdentiality and/or copyright clearance. This also entails compiling corpora representing different translation modalities (e.g. audiovisual translation, interpreting) and translation methods, such as computer-aided translation and post-editing of machine-translated output, as translation from scratch is increasingly rarer today (one notable exception is literary translation). Including different versions of the same translation would also prove to be rewarding (e.g. draft, unedited, and edited versions of the translated text). 
Finally, we need to cross-fertilize insights from natural language processing and corpus-based cross-linguistic studies. This “bridging the gap” can go both ways. On the one hand, cross-linguistic research should take full stock of recent advances in natural language processing, for tasks such as automatic alignment and multilingual annotation. Signifcant progress has been made in recent years in these areas, but parallel corpora, especially those compiled by research teams of corpus linguists, have not yet fully benefted from these new developments. At present, for instance, very few parallel corpora are syntactically parsed or semantically annotated. On the other hand, natural language processing researchers involved in parallel corpus compilation projects could try to document, whenever possible, meta-information that is of paramount importance to contrastive linguists and translation scholars, such as translation direction (from LX to LY, or vice versa) and directness (use of a pivot language or not). In turn, taking this meta-information into account may very well help signifcantly improve the overall performance of data-driven machine translation systems and other tools relying on data extracted from parallel corpora. 
Even though it is quite diffcult to predict future developments with any certainty, especially in view of the fact that translation practices are changing dramatically 
(e.g. human post-editing of machine-translated texts is increasingly common in the translation industry), it is safe to say that compiling and analyzing parallel corpora will prove to be an exciting and rewarding enterprise for many years to come. 
12.4 Tools and Resources 
12.4.1 Query Tools 
Sketch Engine by Lexical Computing Ltd. is undoubtedly the most powerful tool available to linguists, translation scholars, and lexicographers to analyze bilingual and multilingual parallel corpora. The Sketch Engine interface offers powerful functionality, such as bilingual Word Sketches and automatic bilingual terminology extraction. It contains several ready-to-use sentence-aligned, lemmatized, and POS-tagged parallel corpora, such as DGT-Translation Memory, Eur-Lex, Europarl7 and OPUS2. It is also possible to upload your own parallel corpora in various formats (including XML-based formats used in the translation industry, such as TMX Translation Memory eXchange and XLIFF XML Localization Interchange File Format), and exploit them in Sketch Engine. A free, simpler version of the tool, NoSketchEngine, is freely available to the research community (https://nlp.f. muni.cz/trac/noske) (accessed 22 May 2019). 
There are also a number of multilingual parallel concordancers specifcally designed for the extraction of data from parallel corpora, such as: 
• 
Anthony’s AntPConc (available from: http://www.laurenceanthony.net/software/ antpconc/) (accessed 22 May 2019), a freely available parallel corpus analysis toolkit for concordancing and text analysis using line-break aligned, UTF-8 encoded text fles. 

•Barlow’s 
ParaConc (http://www.athel.com/para.html) (accessed 22 May 2019), a multilingual concordancer with the following functionality: semi-automatic alignment of parallel texts, parallel searches, automatic identifcation of trans­lation candidates (called Hot Words) and collocate extraction. 


12.4.2 Resources 
• OPUS project (Tiedemann 2012; 2016), a large collection of freely available parallel corpora: its current version covers 200 languages and language variants and contains over 28 billion tokens, and the collection is constantly growing, in terms of both coverage and size. Compared with other non-directional parallel corpora, OPUS has two major advantages: (i) rather than being restricted to administrative and legal texts (mainly EU and UN), it covers a relatively wide range of other genres and text types, such as user-contributed movie and TV show subtitles, software localization, and multilingual wikis; (ii) a number of poorly-resourced and non-EU language pairs are well represented (albeit often through an indirect translation relationship; e.g. in the LX-LY language pair, the two languages LX and LY are both translations from the source language LZ). http://opus.nlpl.eu/ (accessed 22 May 2019). 
• ParaCrawl (Web-Scale Parallel Corpora for Offcial European Languages): 
parallel corpora for various languages paired with English, created by crawling websites https://paracrawl.eu/index.html (accessed 22 May 2019). 
• CLARIN’s Key Resource Families – parallel corpora (Fišer et al. 2018): many parallel corpora can be downloaded from the CLARIN webpage. https://www. clarin.eu/resource-families/parallel-corpora (accessed 22 May 2019). 
12.4.3 Surveys of Available Parallel Corpora 
A large number of parallel corpora have been mentioned or discussed in this chapter, but it was outside the scope of the present overview to list all avail­able parallel corpora. As a matter of fact, there is as yet no up-to-date digital database documenting all existing parallel corpora (be they bilingual or mul­tilingual, directional or non-directional, developed for cross-linguistic research and/or natural language processing). However, there are some promising ini­tiatives in this direction, such as Mikhailov and Cooper’s (2016) survey, the “Universal Catalogue” of the European Language Resources Association (ELRA) (http://www.elra.info/en/catalogues/universal-catalogue/) (accessed 22 May 2019), CLARIN’s overview of parallel corpora (https://www.clarin.eu/resource-families/ parallel-corpora) (accessed 22 May 2019), and the TransBank project (https:// transbank.info/) (accessed 22 May 2019). 
Further Reading 
Johansson, S. 2007. Seeing through Multilingual Corpora. On the use of corpora in contrastive studies. Amsterdam/Philadelphia: John Benjamins. 
Johansson’s monograph is a must-read for anyone interested in corpus-based contrastive linguistics. The book provides a highly readable introduction to corpus design and use in contrastive linguistics. It also offers a range of exemplary case studies contrasting lexis, syntax, and discourse on the basis of parallel corpus data. 
Mikhailov, M., and Cooper, R. 2016. Corpus Linguistics for Translation and Contrastive Studies. A guide for research. London/New York: Routledge. 
In this accessible guide for research, Mikhailov & Cooper provide detailed informa­tion on parallel corpus compilation and describe a wide range of search procedures that are commonly used in corpus-based contrastive and translation studies. The book also offers a useful survey of some of the available parallel corpora. 
Zanettin, F. 2012. Translation-Driven Corpora. Corpus Resources for Descriptive and Applied Translation Studies. London/New York: Routledge. 
Zanettin’s coursebook is a practical introduction to descriptive and applied corpus-based translation studies. In addition to providing clear background information on the study of translation features in the feld, it offers a wealth of useful information on translation-driven (including parallel) corpus design, encoding, annotation, and analysis. Each chapter is enriched with insightful case studies and hands-on tasks. 
References 
Aijmer, K., & Simon-Vandenbergen, A.-M. (2003). The discourse particle well and its equivalents in Swedish and Dutch. Linguistics, 41(6), 1123–1161. 
Albl-Mikasa, M. (2017). ELF and translation/interpreting. In J. Jenkins, W. Baker, & M. Dewey (Eds.), The Routledge handbook of English as a Lingua Franca (pp. 369–384). London/New York: Routledge. 
Altenberg, B. (1999). Adverbial connectors in English and Swedish: Semantic and lexical correspondences. In H. Hasselgård & S. Oksefjell (Eds.), Out of corpora. Studies in honour of Stig Johansson (pp. 249–268). Amsterdam: Rodopi. 
Altenberg, B., & Granger, S. (2002). Recent trends in cross-linguistic lexical studies. In B. Altenberg & S. Granger (Eds.), Lexis in contrast. Corpus-based approaches (pp. 3–48). Amsterdam/Philadelphia: John Benjamins. 
Assis Rosa, A., Pi.eta, H., & Bueno Maia, R. (2017). Theoretical, methodological and terminolog­ical issues regarding indirect translation: An overview. Translation Studies, 10(2), 113–132. 
Augustinus, L., Vandeghinste, V., & Vanallemeersch, T. (2016). Poly-GrETEL: Cross-lingual example-based querying of syntactic constructions. In Proceedings of the tenth international conference on language resources and evaluation (LREC 2016) (pp. 3549–3554). European Language Resources Association (ELRA). 
Baisa, V., Michelfeit, J., Medved, M., & Jakubí.
cek, M. (2016). European Union language resources in sketch engine. In Proceedings of tenth international conference on language resources and evaluation (LREC’16). European Language Resources Association (ELRA). 
Baker, M. (1993). Corpus linguistics and translation studies. Implications and applications. In M. Baker, G. Francis, & E. Tognini-Bonelli (Eds.), Text and technology. In honour of John Sinclair (pp. 233–250). Amsterdam: John Benjamins. 
Baker, M. (1995). Corpora in translation studies: An overview and some suggestions for future research. Targets, 7(2), 223–243. Bas, R., Bruti, S., & Zanotti, S., (Eds.). (2013). Corpus linguistics and audiovisual translation: In search of an integrated approach. Special issue of Perspectives, 21(4). Beeby Lonsdale, A. (2009). Directionality. In M. Baker & G. Saldanha (Eds.), Routledge encyclopedia of translation studies (pp. 84–88). Abingdon: Routledge. 
Benko, V. (2016). Two years of Aranea: Increasing counts and tuning the pipeline. In Proceedings of 10th international conference on language resources and evaluation (LREC’16) (pp. 4245– 4248). European Language Resources Association (ELRA). 
Bernardini, S. (2011). Monolingual comparable corpora and parallel corpora in the search for features of translated language. SYNAPS, 26, 2–13. 
Bernardini, S., Ferraresi, A., Russo, M., Collard, C., & Defrancq, B. (2018). Building interpreting and intermodal corpora: A How-to for a formidable task. In M. Russo, C. Bendazzoli, & B. Defrancq (Eds.), Making way in corpus-based interpreting studies (pp. 21–42). Springer. 
Bojar, O., Žabokrtsk Z., Dušek, O., Galušcáková, P., Majliš, M., Marecek, D., Maršík, J., Novák, M., Popel, M., & Tamchyna, A. (2012). The joy of parallelism with CzEng 1.0. In Proceedings 
of the 8th international conference on language resources and evaluation (LREC-2012) (pp. 3921–3928). European Language Resources Association (ELRA). 
Bowker, L., & Bennison, P. (2003). Student translation archive and student translation tracking system. Design, development and application. In F. Zanettin, S. Bernardini, & D. Stewart (Eds.), Corpora in translator education (pp. 103–117). Manchester: St. Jerome Publishing. 
Cappelle, B., & Loock, R. (2013). Is there interference of usage constraints? A frequency study of existential there is and its French equivalent il y a in translated vs. non-translated texts. Target, 25(2), 252–275. 
Cartoni, B., & Meyer, T. (2012). Extracting directional and comparable corpora from a multilingual corpus for translation studies. In Proceedings of the 8th international conference on language resources and evaluation (LREC-2012) (pp. 2132–2137). European Language Resources Association (ELRA). 
Castagnoli, S., Ciobanu, D., Kler, N., Kunz, K., & Volanschi, A. (2011). Designing a learner translator Corpus for training purposes. In N. Kler (Ed.), Corpora, language, teaching, and resources: From theory to practice (pp. 221–248). Bern: Peter Lang. 
.
Cermák, F., & Rosen, A. (2012). The case of InterCorp, a multilingual parallel corpus. Interna­tional Journal of Corpus Linguistics, 13(3), 411–427. 
Cettolo, M., Girardi, C., & Federico, M. (2012). WIT3: Web inventory of transcribed and translated talks. Proceedings of EAMT, 261–268. 
De Sutter, G., Lefer, M.-A., & Delaere, I. (Eds.). (2017). Empirical translation studies: New methodological and theoretical traditions. Berlin/Boston: De Gruyter Mouton. 
Delaere, I., & De Sutter, G. (2017). Variability of English loanword use in Belgian Dutch translations: Measuring the effect of source language, register, and editorial intervention. In G. De Sutter, M.-A. Lefer, & I. Delaere (Eds.), Empirical translation studies: New methodological and theoretical traditions (pp. 81–112). Berlin/Boston: De Gruyter Mouton. 
Dupont, M., & Zufferey, S. (2017). Methodological issues in the use of directional parallel corpora. A case study of English and French concessive connectives. International Journal of Corpus Linguistics, 22(2), 270–297. 
Dyer, C., Chahuneau, V., & Smith, N. A. (2013). A simple, fast, and effective reparameterization of IBM model 2. Proceedings of NAACL-HLT, 2013, 644–648. 
Espunya, A. (2014). The UPF learner translation corpus as a resource for translator training. Language Resources and Evaluation, 48(1), 33–43. 
Evans, N., & Levinson, S. C. (2009). The myth of language universals: Language diversity and its importance for cognitive science. Behavioral and Brain Sciences, 32, 429–492. 
Evert, S., & Neumann, S. (2017). The impact of translation direction on characteristics of translated texts. A multivariate analysis for English and German. In G. De Sutter, M.-A. Lefer, & I. Delaere (Eds.), Empirical translation studies: New methodological and theoretical traditions (pp. 47–80). Berlin/Boston: De Gruyter Mouton. 
Ferraresi, A., & Bernardini, S. (2019). Building EPTIC: A many-sided, multi-purpose corpus of EU parliament proceedings. In M. T. S. Nieto & I. Doval (Eds.), Parallel corpora: Creation and application. Amsterdam/Philadelphia: John Benjamins. 
Fišer, D., Lenardi.c, J., & Erjavec, T. (2018). CLARIN’s key resource families. In Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018) (pp. 1320–1325). 
Fltum, K., Dahl, T., Didriksen, A. A., & Gjesdal, A. M. (2013). KIAP – Refections on a complex corpus. Bergen Language and Linguistics Studies, 3(1), 137–150. 
Frankenberg-Garcia, A., & Santos, D. (2003). Introducing COMPARA: The Portuguese-English parallel Corpus. In F. Zanettin, S. Bernardini, & D. Stewart (Eds.), Corpora in translator education (pp. 71–87). Manchester: St. Jerome Publishing. 
Granger, S., & Lefer, M.-A. (2016). From general to learners’ bilingual dictionaries: Towards a more effective fulfllment of advanced learners’ phraseological needs. International Journal of Lexicography, 29(3), 279–295. 
Granger, S., & Lefer, M.-A. (2020). The multilingual student translation corpus: A resource for translation teaching and research. Language Resources and Evaluation: Online First. 
Granger, S., Lerot, J., & Petch-Tyson, S. (Eds.). (2003). Corpus-based approaches to contrastive linguistics and translation studies. Amsterdam/New York: Rodopi. 
Halverson, S. L. (2015). The status of contrastive data in translation studies. Across Languages and Cultures, 16(2), 163–185. 
Hansen-Schirra, S., Neumann, S., & Steiner, E. (2012). Cross-linguistic corpora for the study of translations. Insights from the language pair English-German. Berlin: De Gruyter. 
Izquierdo, M., Hofand, K., & Reigem, Ø. (2008). The ACTRES parallel corpus: An English– Spanish translation corpus. Corpora, 3(1), 31–41. 
Jimenez Hurtado, C., & Soler Gallego, S. (2013). Multimodality, translation and accessibility: A corpus-based study of audio description. Perspectives, 21(4), 577–594. 
Johansson, S. (2007). Seeing through multilingual corpora. On the use of corpora in contrastive studies. Amsterdam/Philadelphia: John Benjamins. 
Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. Proceedings of MT Summit X, 79–86. 
Kruger, A., Wallmach, K., & Munday, J. (Eds.). (2011). Corpus-based translation studies. Research and applications. London/New York: Bloomsbury. 
Kutuzov, A., & Kunilovskaya, M. (2014). Russian learner translator corpus. Design, research potential and applications. In P. Sojka, A. Horák, I. Kope.cek, & K. Pala (Eds.), Text, speech and dialogue. TSD 2014 (pp. 315–323). Springer. 
Lapshinova-Koltunski, E. (2017). Exploratory analysis of dimensions infuencing variation in translation. The case of text register and translation method. In G. De Sutter, M.-A. Lefer, & I. Delaere (Eds.), Empirical translation studies: New methodological and theoretical traditions (pp. 207–234). Berlin/Boston: De Gruyter Mouton. 
Lefer, M.-A., & Grabar, N. (2015). Super-creative and over-bureaucratic: A cross-genre corpus-based study on the use and translation of evaluative prefxation in TED talks and EU parliamentary debates. Across Languages and Cultures, 16(2), 187–208. 
Levshina, N. (2016). Verbs of letting in Germanic and Romance languages: A quantitative investigation based on a parallel corpus of flm subtitles. Languages in Contrast, 16(1), 84– 117. 
Macken, L., De Clercq, O., & Paulussen, H. (2011). Dutch parallel corpus: A balanced copyright-cleared parallel corpus. Meta, 56(2), 374–390. 
Mauranen, A. (2005). Contrasting languages and varieties with translational corpora. Languages in Contrast, 5(1), 73–92. 
Meurant, L., Gobert, M., & Cleve, A. (2016). Modelling a parallel corpus of French and French Belgian sign language. In Proceedings of the 10th edition of the language resources and evaluation conference (LREC 2016) (pp. 4236–4240). 
Mezeg, A. (2010). Compiling and using a French-Slovenian parallel corpus. In R. Xiao (Ed.), Proceedings of the international symposium on using corpora in contrastive and translation studies (UCCTS 2010) (pp. 1–27). Ormskirk: Edge Hill University. 
Mikhailov, M., & Cooper, R. (2016). Corpus linguistics for translation and contrastive studies. A guide for research. London/New York: Routledge. 
Neumann, S. (2013). Contrastive register variation. A quantitative approach to the comparison of English and German. Berlin/Boston: De Gruyter Mouton. 
Noël, D. (2003). Translations as evidence for semantics: An illustration. Linguistics, 41(4), 757– 785. 
Obrusnik, A. (2014). Hypal: A user-friendly tool for automatic parallel Text alignment and error tagging. In Eleventh international conference teaching and language corpora, Lancaster, 20– 23 July 2014 (pp. 67–69). 
Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1), 19–51. 
Padr L., & Stanilovsky, E. (2012). FreeLing 3.0: Towards wider multilinguality. In Proceedings of the language resources and evaluation conference (LREC-2012). European Language Resources Association (ELRA). 
Resnik, P., & Smith, N. A. (2003). The web as a parallel corpus. Computational Linguistics, 29(3), 349–380. 
Rosen, A. (2010). Mediating between incompatible Tagsets. NEALT Proceedings Series, 10, 53– 62. 
Russo, M., Bendazzoli, C., & Sandrelli, A. (2006). Looking for lexical patterns in a trilingual corpus of source and interpreted speeches: Extended analysis of EPIC. Forum, 4(1), 221–254. 
Russo, M., Bendazzoli, C., & Defrancq, B. (Eds.). (2018). Making way in corpus-based interpret­ing studies. Springer. 
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of international conference on new methods in language processing. 
Singh, S., McEnery, T., & Baker, P. (2000). Building a parallel corpus of English/Panjabi. In J. Véronis (Ed.), Parallel Text processing. Alignment and use of translation corpora (pp. 335– 346). Kluwer Academic Publishers. 
Tiedemann, J. (2011). Bitext alignment. Morgan & Claypool Publishers. 
Tiedemann, J. (2012). Parallel data, tools and interfaces in OPUS. In Proceedings of the 8th international conference on language resources and evaluation (LREC’2012) (pp. 2214–2218). 
Tiedemann, J. (2016). OPUS – Parallel corpora for everyone. Baltic Journal of Modern Computing, 4(2), 384. 
Toury, G. (2012). Descriptive translation studies – And beyond. Amsterdam/Philadelphia: John Benjamins. 
Uzar, R. S. (2002). A corpus methodology for analysing translation. Cadernos de Tradução, 9(1), 235–263. 
Varga, D., Halácsy, P., Kornai, A., Nagy, V., Németh, L., & Tr, V. (2007). Parallel corpora for medium density languages. In N. Nicolov, K. Bontcheva, G. Angelova, & R. Mitkov (Eds.), Recent advances in natural language processing IV: Selected papers from RANLP 2005 (pp. 247–258). Amsterdam & Philadelphia: John Benjamins. 
Volk, M., Ghring, A., Rios, A., Marek, T., & Samuelsson, Y. (2015). SMULTRON (version 
4.0) – The Stockholm MULtilingual parallel TReebank. An English-French-German-Quechua-Spanish-Swedish parallel treebank with sub-sentential alignments. Institute of Computational Linguistics, University of Zurich. 
Vond.ri.cka, P. (2014). Aligning parallel texts with InterText. In Proceedings of the ninth interna­tional conference on language resources and evaluation (LREC 2014) (pp. 1875–1879). 
Waldenfels, R. V. (2011). Recent developments in ParaSol: Breadth for depth and XSLT based web concordancing with CWB. In D. Majchráková & R. Garabík (Eds.), Natural Language Processing, Multilinguality. Proceedings of Slovko 2011, Modra, Slovakia, 20–21 October 2011 (pp. 156–162). Bratislava: Tribun EU. 
Xiao, R. (Ed.). (2010). Using corpora in contrastive and translation studies. Newcastle upon Tyne: Cambridge Scholars Publishing. 
Ziemski, M., Junczys-Dowmunt, M., & Pouliquen, B. (2016). The United Nations parallel corpus v1.0. Language Resources and Evaluation (LREC’16). 
Zufferey, S., & Cartoni, B. (2012). English and French causal connectives in contrast. Languages in Contrast, 12(2), 232–250. 
Chapter 13 Learner Corpora 
Gaëtanelle Gilquin 
Abstract This chapter deals with learner corpora, that is, collections of (spoken and/or written) texts produced by learners of a language. It describes their main characteristics, with particular emphasis on those that are distinctive of learner cor­pora. Special types of corpora are introduced, such as longitudinal learner corpora or local learner corpora. The issues of the metadata accompanying learner corpora and the annotation of learner corpora are also discussed, and the challenges they involve are highlighted. Several methods of analysis designed to deal with learner corpora are presented, including Contrastive Interlanguage Analysis, Computer-aided Error Analysis and the Integrated Contrastive Model. The development of the feld of learner corpus research is sketched, and possible future directions are examined, in terms of the size of learner corpora, their diversity, or the techniques of compilation and analysis. The chapter also features representative corpus-based studies of learner language, representative learner corpora, tools and resources related to learner corpora, and annotated references for further reading. 
13.1 Introduction 
Learner corpora are corpora representing written and/or spoken ‘interlanguage’, that is, language produced by learners of that language. Typically, the term covers both foreign language and second language situations, that is, respectively, situations in which the target language has no offcial function in the country and is essentially confned to the classroom (and, possibly, international communication), and situ­ations in which the target language is learned by immigrants in a country where it is the dominant native language. It is normally not used to refer to corpora of child language, which are made up of data produced by children acquiring their frst language (see Chap. 14), nor corpora of institutionalized second-language varieties, 
G. Gilquin (•) Université catholique de Louvain, Centre for English Corpus Linguistics, Louvain-la-Neuve, Belgium e-mail: gaetanelle.gilquin@uclouvain.be 
© Springer Nature Switzerland AG 2020 283 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_13 
which are collected in countries that have the target language as an offcial, though not native, language (cf. ‘New Englishes’ like those represented in the International Corpus of English), although their data may also refect a process of learning or acquisition. 
While the frst corpora were compiled in the 1960s, it took some 30 years before the frst learner corpora started to be collected, both in the academic world (International Corpus of Learner English (ICLE)) and in the publishing world (Longman Learners’ Corpus). Initially, they were corpora of written learner English, keyboarded from handwritten texts. Gradually, however, learner corpora represent­ing other languages as well as spoken learner corpora made their appearance, while written learner corpora were increasingly compiled directly from electronic sources, which facilitated the compilation process. The nature of learner language made it necessary to rethink and adapt some of the general principles of corpus data collection and analysis. This led, among other things, to the creation of new types of corpora, like longitudinal corpora representing different stages in the language learning process, to the collection of new types of metadata, such as information about the learner’s mother tongue and exposure to the target language, and to the use of new methods to annotate or query the corpus, for example to deal with the errors found in learner corpora. These specifcities, and others, will be considered in Sect. 13.2. 
13.2 Fundamentals 
13.2.1 Types of Learner Corpora 
Like other corpora, learner corpora can include written, spoken and/or multimodal data; they can be small or large; and they can represent any (combination of) languages. The ‘Learner Corpora around the World’ resource (see Sect. 13.4) reveals that the majority of learner corpora are made up of written data, and that these data often correspond to learner English. Other types of corpora, however, including spoken learner corpora and corpora representing other target languages, are becoming more widely available. As for size, many of the learner corpora listed in the ‘Learner Corpora around the World’ resource are under one million words, with some of them not even reaching 100,000 words and a couple just containing some 10,000 words. It is likely that among those learner corpora that are not listed but exist ‘out there’, most can be counted in tens of thousands rather than in millions of words. Yet, there are also learner corpora that are much larger, especially those that have continued to grow over the years (like the Longman Learners’ Corpus, which now comprises ten million words) and those that come out of the testing/assessment world, such as EFCAMDAT (Geertzen et al. 2014)or TOEFL 11 (Blanchard et al. 2013). 
One of the defning features of corpora is that they should be made up of authentic texts. This concept of authenticity, however, tends to be problematic in the case of learner corpora. Learner language, most of the time, is not produced purely for communicative purposes, but as part of some pedagogical activity, to practise one’s language skills. Writing an argumentative essay or role-playing with a classmate, for example, may be natural tasks in the classroom, but they are not authentic in the sense of being “gathered from the genuine communications of people going about their normal business” (Sinclair 1996). Our understanding of the concept of authenticity must therefore be adapted to the context of learner corpora and encompass tasks that would not be described as natural in other contexts. It must also be acknowledged that some learner corpora will be more “peripheral” (Nesselhauf 2004:128), as is the case of spoken learner corpora like the Giessen-Long Beach Chaplin Corpus (Jucker et al. 2003) which are elicited on the basis of a picture or a movie and thus include data of a more constrained nature. Another, related feature of learner language is that it usually does not cover the whole spectrum of genres that is characteristic of native varieties. Because its use tends to be associated with educational settings, there are certain genres that are diffcult to capture or simply do not exist in the target language. Having a spontaneous conversation with a friend, for example, is more likely to occur in the mother tongue (L1) than in the target language (L2). As a result, most learner corpora represent one of a limited number of genres, including argumentative essays, academic writing, narratives and interviews. 
One type of learner corpus that is worth singling out, because it is specifc to varieties that are in the process of being learned or acquired, is the longitudinal learner corpus. In such a corpus, data are collected from the same subjects at different time intervals, so as to refect the development of their language skills over time. Belz and Vyatkina (2005), for example, use longitudinal data from the Telecollaborative Learner Corpus of English and German (Telekorp) to study the development of German modal particles over a period of 9 weeks, with one data collection point every week. Most longitudinal learner corpora, however, are less ‘dense’, in that they include data collected at longer intervals, sometimes only once or twice a year (cf. LONGDALE, the Longitudinal Database of Learner English; Meunier 2016). Note that non-longitudinal learner corpora can sometimes also be used to investigate the development of learner language. Thus, if a learner corpus contains data produced by distinct learners from different profciency levels, like the National Institute of Information and Communications Technology Japanese Learner English (NICT JLE) Corpus (Izumi et al. 2004), it is possible to identify developmental patterns by comparing subcorpora representing different levels, even if all the data were collected at a single point in time. Such learner corpora are called ‘quasi-longitudinal’ corpora and, because they are easier to collect than longitudinal corpora, they have often been used to study interlanguage development. 
13.2.2 Metadata 
Given the “inherent heterogeneity of learner output” (Granger 1998:177), it is crucial that information about the data included in a learner corpus should be available. Learner corpora tend to be characterized by a large amount of such metadata. These metadata can have to do with the text itself (genre, length, conditions in which the task took place, etc.), but they can also concern the learners: what is their mother tongue? how old are they? how long have they been learning the target language? what kind of exposure to the target language have they received? do they know any other languages? etc. Usually, some of these variables are controlled for in the very design of the corpus, in the sense that the corpus only includes data corresponding to a certain value, e.g. only written essays (in ICLE) or only native speakers of English learning Spanish (in the Spanish Learner Language Oral Corpora (SPLLOC)). For the variables that are not controlled for during the compilation of the corpus, it is often possible for users to fnd information that enables them either to use a subset of the data meeting specifc criteria (e.g. only texts written in exam conditions) or to examine the distribution of the results according to these variables (e.g. percentage of a given linguistic phenomenon among male vs female learners). Using the Multilingual Platform for European Reference Levels: Interlanguage Exploration in Context (MERLIN),1 for example, one can select a number of criteria, like the task (essay, email, picture description, etc.), the learner’s mother tongue, his/her age, gender or profciency level according to the Common European Framework of Reference for Languages (CEFR), in order to defne a subcorpus and then restrict the search to this subcorpus. Figure 13.1 is a screenshot from the MERLIN website that shows the selection of a subcorpus made up of data produced by French-speaking learners of Italian with an A2 CEFR level (test and overall rating) and aged between 30 and 59. The ICLE interface (Granger et al. 2009) also allows users to defne a subcorpus according to certain criteria. In addition, it makes it possible to visualize, in the form of tables and graphs, the distribution of the results according to all the other variables encoded in the metadata. Figure 13.2 is a screenshot from the ICLE interface that represents the output of a search for the word informations in the ICLE data produced by learners with Chinese (or Chinese-Cantonese/Chinese-Mandarin) as their mother tongue (ICLE-CH). More particularly, the graph shows the distribution of the results according to the time available to write the essay and indicates that the incorrect pluralization of information is more frequent in timed than in untimed essays. 
Despite the wealth of metadata that accompany most learner corpora and despite the facilities that some of these corpora provide to access them, it must be recog­nized that metadata are not used to their full potential in learner corpus research. One variable that is regularly taken into account is that of the learner’s L1 background 
(e.g. Golden et al. 2017, based on ASK, the Norsk andrespråkskorpus), which makes 
1http://merlin-platform.eu/. Accessed 22 May 2019. 

Conditions : Occurrences distribution for selected profiles (Relative frequencies per 100,000 words)  
3 2.8  2.8  0.1 No Timing  
2.6  2.8 Timed  
2.4  0.6 Unknown  
2.2  
2  
1.8  
1.6  
1.4  
1.2  
1  
0.8  0.6  
0.6  
0.4  
0.2  0.1  
0  
No Timing  Timed  Unknown  
Fig. 13.2 Relative frequency of informations in ICLE-CH according to time available. (Source: Granger et al. 2009) 


it possible to identify probable cases of transfer from the L1. Sometimes it is another variable that is investigated, for example exposure to the target language through a stay abroad (Gilquin 2016) or presence of a native or non-native interlocutor (Crossley and McNamara 2012). Studies that examine the possible impact of several variables, on the other hand, are relatively rare, although such studies can offer important insights into the factors that are likely to affect learner language. The problem with this type of approach is that, because of the relatively limited size of most learner corpora, selecting many variables may result in a very small subset of 
data (see Callies 2015:52), which, in effect, may make any kind of generalization 
impossible. 
13.2.3 Annotation 
Learner corpora can be enriched by means of the same types of annotation as all other corpora, including part-of-speech (POS) tagging, parsing, semantic annotation, pragmatic annotation and, for spoken learner corpora, phonetic and prosodic annotation (see Chaps. 2 and 11). One issue to bear in mind, however, is that, with very few exceptions, the tools that one has to rely on to annotate learner corpora automatically are tools that have been designed to deal with native data. Applying them to non-native data may therefore cause certain diffculties. For POS tagging, for example, the many spelling errors found in written learner corpora have been shown to lower the accuracy of POS taggers (de Haan 2000; Van Rooy and Schäfer 2002). As for parsing, punctuation and spelling errors in written learner corpora have the highest impact according to Huang et al. (2018), and in spoken learner corpora Caines and Buttery (2014) have demonstrated that disfuencies and (formal and idiomatic) errors can lead to a 25% decrease in the success rate of the parser. However, while tools and formats of annotation specifcally designed for learner data would of course be desirable (as suggested by Díaz-Negrillo et al. (2010) for POS tagging), it must be underlined that some attempts to automatically annotate learner corpora with off-the-shelf tools have been quite successful. Granger et al. (2009:16), for example, report accuracy rates between 95% and 99.1% for the POS tagging of ICLE. A frst attempt at POS tagging the Louvain International Database of Spoken English Interlanguage (LINDSEI; Gilquin et al. 2010) revealed an accuracy rate of 92% (Gilquin 2017). As for parsing, it seems to be more affected by the nature of learner language than POS tagging (see Huang et al. 2018). However, Geertzen et al. (2014:247) note that the parser they used actually scored slightly better on EFCAMDAT, a written learner corpus, than on the Wall Street Journal corpus (89–92% for EFCAMDAT, to be compared with 84–87% for the Wall Street Journal). These reasonably good accuracy rates – given the non-native nature of the corpora – may be explained by the fact that the errors and disfuencies found in learner language are compensated by the relatively simple structure of the sentences which learners tend to produce (see Meunier (1998) on POS tagging and Huang et al. (2018) on parsing). Another possible explanation is that most learner corpora represent university-level interlanguage (like ICLE and LINDSEI) and that such data are arguably easier to deal with for a POS tagger or parser than data produced at a lower profciency level. Geertzen et al. (2014:248) point out that the accuracy rate of the parser was higher on the more advanced EFCAMDAT data, although “the effect seem[ed] small”. Next to these automated methods of annotation, learner corpora can also be annotated manually. While a full annotation of the corpus may not be feasible (nor, in fact, desirable), one type of annotation that may be particularly useful is problem-oriented tagging (de Haan 1984). This tagging is geared towards a specifc research question and consists in annotating only those items that are of direct relevance to the research question. Spoelman’s (2013) study of partitive case-marked noun phrases in learner Finnish, for instance, involved tagging instances of this phenomenon, depending on the category they represented. Such tagging then opens the way to automatic treatment of the annotated corpus. 

Besides these types of annotation that are common to all corpora, there is one that is typical of learner corpora (and also child-language corpora, see Chap. 14), namely error tagging, which consists in the annotation of the errors found in a corpus (syntactic errors, unusual collocations, mispronunciations, etc.). The Fehler­annotiertes Lernerkorpus (‘error annotated learner corpus’, Falko), for instance, is an error-tagged corpus of learner German. The annotation of errors is usually accompanied by a correction (the ‘target hypothesis’) as well as a tag indicating the category of the error (e.g. spelling error, error in verb morphology, complementation error). Figure 13.3 shows an error-tagged sentence from Falko, as retrieved from the ANNIS platform.2 Falko uses a multi-layer standoff architecture, in which each layer represents an independent level of annotation (see also Chap. 3). The ‘tok’ (= token) layer shows the original sentence as produced by the learner. ‘ZH1’ provides a corrected version of the sentence (ZH = Zielhypothese ‘target hypothesis’), with ‘ZH1Diff’ highlighting the differences between the original and corrected versions, and ‘ZH1lemma’ and ‘ZH1pos’ corresponding, respectively, to a lemmatized and POS-tagged version of the sentence. In this case, the learner has mistakenly used the article (‘ART’) der instead of the correct form die,an error which involves a changed token (‘CHA’) in the target hypothesis. Note that the multi-layer architecture of the corpus allows for enough fexibility to encode competing target hypotheses (Reznicek et al. 2013). In Falko, the step of attributing an ‘edit tag’ to the error (change, insertion, deletion, etc.) can 
2https://korpling.german.hu-berlin.de/falko-suche/. Accessed 22 May 2019. 
be automated by comparing the learner text and the (manually encoded) target hypothesis/hypotheses. In learner corpus research, attempts have also been made to automate the process of error detection itself, although this is usually restricted to specifc phenomena, for example preposition errors (De Felice and Pulman 2009), article errors (Rozovskaya and Roth 2010) or spelling errors (Rayson and Baron 2011). Most of the time, however, the whole error tagging procedure is done manually, a time-consuming task that can be facilitated by the use of an error editor like the Université Catholique de Louvain Error Editor (UCLEE; see Dagneaux et al. 1998 and Sect. 13.4). Once a learner corpus has been error tagged, it becomes possible to automatically extract instances of erroneous usage, which, as will be described in the next section, lies at the basis of one of the methods of analysis that have been developed to deal with learner corpora. 
13.2.4 Methods of Analysis 
In addition to the application of well-established corpus linguistic methods, like the use of concordances (Chap. 8), frequency lists (Chap. 4) or collocations (Chap. 7), a number of techniques have been developed to deal specifcally with learner corpora. Among these, we can mention Computer-aided Error Analysis (Dagneaux et al. 1998), Contrastive Interlanguage Analysis (Granger 1996) and the Integrated Contrastive Model (Granger 1996; Gilquin 2000/2001). Computer-aided Error Analysis (or CEA) relies on the use of an error-tagged learner corpus (cf. Sect. 13.2.3). Through error tagging, errors are identifed and categorized according to a taxonomy, such as that developed by Dagneaux et al. (2008) to error tag ICLE. These error tagging systems are usually hierarchical, distinguishing for example between grammar, lexis, lexico-grammar and style at a high level of annotation, and then making further distinctions within each of these categories, for example grammatical errors related to nouns, pronouns or verbs, and within grammatical verb errors, those having to do with number, tense, voice, etc. This hierarchy is refected in Dagneaux et al.’s (2008) tagset: grammatical errors are indicated by the letter ‘G’, grammatical verb errors by ‘GV’, and grammatical errors in verb tense by ‘GVT’. Such tags make it very easy to automatically retrieve all the annotated errors in a certain category (e.g. all the complementation errors) or all the occurrences of a word representing a certain type of error (e.g. all the cases where the verb enjoy is used with an erroneous complement). These errors are the focus of analysis of CEA, as was the case in traditional error analysis (see James 1998). Unlike traditional error analysis, however, CEA allows the linguist to examine the errors in context, to consider correct uses along with incorrect uses, and to easily quantify the results (percentage of incorrect uses out of all uses or relative frequency of the error per 10,000 words, for instance). 
Contrastive Interlanguage Analysis (CIA) consists of two types of comparison: a comparison of learner language with native language and a comparison between different learner varieties (Granger 2009:18). These two types of comparison should preferably be combined with each other, but they can also be drawn separately. The comparison between native and learner language lies at the basis of a majority of the studies in learner corpus research (Flowerdew 2015:469). Such a comparison helps identify non-standard forms (cf. CEA), but also, importantly, instances of ‘overuse’ and ‘underuse’ (or ‘overrepresentation’ and ‘underrepresentation’, see Granger 2015:19). These terms, which are not meant as being evaluative but purely descriptive, refer to cases in which a given linguistic phenomenon (word, construction, function, etc.) is used signifcantly more or signifcantly less in the learner corpus than in a comparable native corpus, as indicated by a measure of statistical signifcance. The study of over-and underuse has been a real eye-opener in learner corpus research, because it has shown that the foreign-soundingness of learner language, especially at advanced levels of profciency, is to be attributed as much (or perhaps even more) to differences in the frequency of use as to downright errors (Granger 2004:132). The second type of comparison in CIA involves comparing different learner varieties, most notably varieties produced by learners from different L1 backgrounds. Such a comparison helps detect possible traces of transfer from the mother tongue: if a feature is only found among learners from a specifc L1 population, say Italian learners of French, it might be a sign that it is the result of crosslinguistic infuence, that is, interference from the L1 (Italian) on the L2 (French) (see Jarvis and Pavlenko 2008, on crosslinguistic infuence, and Osborne 2015, on its link with learner corpus research). The learner varieties that are compared with each other could however differ along another dimension, which could be any of the variables encoded in the corpus metadata (comparison of foreign and second language learners, of male and female learners, of learners who have spent some or no time in a target language country, etc.). Recently, a revised version of CIA, called CIA2, has been proposed by Granger (2015). Among its major developments, we can mention the fact that this revised model no longer advocates the exclusive use of native language as a reference point against which to compare learner varieties. Instead, it promotes the comparison of “interlanguage varieties” against one or several “reference language varieties” which, in the case of English, could include, in addition to native English, New Englishes (like Hong Kong English or Singapore English) and English as a Lingua Franca (i.e. English as used by competent L2 users). CIA2 also includes an explicit reference to a number of variables (diatypic, dialectal, task and learner variables), thus encouraging researchers to take these into account in the application of the model. 
The Integrated Contrastive Model (ICM) is partly based on CIA, but it also integrates a contrastive analysis (CA), comparing the target language and the mother tongue thanks to comparable or parallel corpora (cf. Chap. 12). The model aims to predict possible cases of negative transfer (when the CA shows the target language and the mother tongue to differ in a certain respect) and seeks to explain problematic uses – misuse, overuse, underuse – in the learner corpus (by checking whether they could be due to discrepancies between the target language and the mother tongue). It thus has both predictive and diagnostic power. By combining careful analyses of learner, native and bilingual corpora, the model avoids the trap of misattributing certain phenomena to transfer simply because intuition seems to suggest that this is a plausible interpretation. Liu and Shaw (2001:179), for example, claim that the frequent use of the causative constructions make sb/sth feel and make sb/sth become by Chinese learners of English “may be attributable to L1 interference” because such sequences “have word for word translational equivalents in Chinese”. However, such a claim would require a thorough contrastive analysis of English and Chinese to confrm the equivalence between the English and the Chinese constructions. Moreover, a study of causative constructions in different varieties of learner English has demonstrated that the overuse of make sb/sth feel and make sb/sth become is in fact characteristic of several other L1 populations of learners (Gilquin 2012), which suggests that Liu and Shaw’s (2001) results do not point to a case of transfer (or at least not only), but a more general tendency. 
The last few years have witnessed a general refnement of the methods of analysis in learner corpus research. One major change is the increasingly prominent role of statistics in the feld. While statistical signifcance testing has almost always been part of learner corpus studies, through the notions of over-and underuse, criticism has recently been voiced against this type of monofactorial statistics. Gries and Deshors (2014), for example, argue that, instead of comparing overall frequencies in learner and native corpora, researchers should look at the linguistic contexts in which an item is used – or not – by learners and native speakers, as determined by a multifactorial analysis involving a variety of morpho-syntactic and semantic features. Statistics also help researchers go beyond the typical global approach of corpus linguistics (studying corpora as wholes), by taking corpus/learner variation into account through statistical techniques such as Wilcoxon tests (e.g. Paquot 2014) or linear modelling (e.g. Meunier and Littré 2013). By adopting this more individual type of approach, learner corpus research is following the general quantitative trend in corpus linguistics as well as theories in second language acquisition (SLA) research like the Dynamic Systems Theory, which focuses on “individual developmental paths” (de Bot et al. 2007:14). The link with theoretical frameworks, incidentally, is another way in which learner corpus research has evolved over the last few years. More and more learner corpus studies nowadays are grounded in SLA theories (see Myles 2015) or usage-based theories like cognitive linguistics (see De Knop and Meunier 2015), which gives such studies a more solid background and helps improve their explanatory power. Finally, methodological refnement in learner corpus research also comes from its rapprochement with the feld of natural language processing, which has provided powerful tools and techniques for the automated analysis of large datasets (see Meurers 2015). 
Representative Study 1 
Altenberg, B., and Granger, S. 2001. The grammatical and lexical pat­
terning of MAKE in native and non-native student writing. Applied 
linguistics 22(2):173–194. 
This study of the grammatical and lexical patterning of the high-frequency verb make among French-and Swedish-speaking learners of English seeks to test a number of hypotheses from the literature, e.g. the idea that a core verb like make is safe to use (hence not error-prone) or the contradictory claims that high-frequency verbs tend to be underused/overused by learn­ers. It uses the French and Swedish components of ICLE, as well as a comparable native English corpus, the Louvain Corpus of Native English Essays (LOCNESS). The article provides a good overview of some of the techniques that can be applied to learner corpus data, including a comparison of the overall frequency of make in the three (sub)corpora, an examination of the distribution of its main semantic uses, a phraseological analysis of the collocates of the verb, and a syntactic and error analysis of its causative uses. In addition, the potential role of the mother tongue is examined, and some possible cases of transfer are highlighted, as well as strategies that appear to be common to the two groups of learners (e.g. a “decompositional” strategy which results in constructions like make the family live instead of support the family). Interestingly, the article also discusses the methodological issue of how accurate and useful an automatic extraction of collocates is. More generally, it demonstrates the benefts of combining an automatic and manual analysis, as well as a quantitative and qualitative approach. 
Representative Study 2 
Leling, A., Hirschmann, H., and Shadrova, A. 2017. Linguistic models, acquisition theories, and learner corpora: Morphological productivity in SLA research exemplifed by complex verbs in German. Language Learning 67(S1):96–129. 
This study focuses on German as a foreign language, and how advanced learners acquire morphological productivity for German complex verbs, that is, prefx verbs (like verstehen ‘to understand’) and particle verbs (like aufstehen ‘to get up’). Looking at the treatment of morphological productivity in different acquisition models, including generative and usage-based models, the authors put forward a number of hypotheses, which are then tested against a learner corpus. The corpus is Falko (see Sect. 13.2.3) and its 
(continued) 
L1 equivalent. The study combines Contrastive Interlanguage Analysis and Computer-aided Error Analysis. First, it compares the frequency and uses of complex verbs in learner and native German. Second, it relies on the error tagging of Falko to identify grammatical and ungrammatical uses of complex verbs and to determine error types. The results show that learners tend to underuse prefx verbs and, especially, particle verbs, and that the variance between individual learners is greater than that between individual native speakers. Learners also appear to use complex verbs productively, although the new forms they produce sometimes result in errors. The paper illustrates some of the latest developments in learner corpus research, such as a solid grounding in theories and a combined aggregate and individual approach. It also makes the interesting methodological point that, through corpus annotation, categorization of the data can be made explicit and available to other researchers. 
Representative Study 3 
Alexopoulou, T., Geertzen, J., Korhonen, A., and Meurers, D. 2015. Exploring big educational learner corpora for SLA research: Per­spectives on relative clauses. International Journal of Learner Corpus Research 1(1):96–129. 
This study is based on one of the large learner corpora coming out of the testing/assessment world (see Sect. 13.2.1), namely EFCAMDAT, the EF Cambridge Open Language Database. EFCAMDAT is made up of 33 million words, representing 85,000 learners and spanning 16 profciency levels. Although the corpus includes longitudinal data for certain individual learners, this study adopts an aggregate approach, considering each profciency level as a ‘section’, but with the acknowledgment that “combining the cross-sectional perspective with an analysis of individual learner variation is a necessary next step” (p. 126). The paper investigates the development of learners’ use of relative clauses. Like Leling et al. (2017), it is grounded in theories of (second language) acquisition. In addition, it illustrates the rapprochement between learner corpus research and natural language processing (NLP), since it makes use of NLP tools and techniques to automatically extract relative clauses from a “big data” resource and to analyze their uses. The study reveals that very few relative clauses are found before Level 4, that their frequency increases until Level 6 and that it then remains more or less stable, with a peak at Level 11. The results show some limited effect of learners’ nationalities (in terms of the types of relative clauses that are used) and a strong task effect. 
(continued) 
This focus on tasks echoes Granger’s (2015) recommendation to take this kind of variable into account (see Sect. 13.2.4). However, other variables that are equally important in learner corpus research cannot be investigated in EFCAMDAT because of the relative lack of metadata about learners (information about their L1, for example, is so far not available but has to be approximated through nationality and country of residence). This shows that, at the moment, learner corpus size may still come at the expense of rich metadata. 
Representative Corpus 1 
International Corpus of Learner English (ICLE; Granger et al. 2009) 
One of the frst learner corpora to have been compiled, ICLE is a mono-L2 and multi-L1 corpus, in that it contains data from a single target language, English, produced by (high-intermediate to advanced) learners from different L1 backgrounds. It is a written learner corpus made up of argumentative (and some literary) essays written by university students under different conditions (exam or not, timed or untimed, access to reference tools or not). It is accompanied by rich metadata which can be queried through the interface that comes with the released version of the corpus. In its current version, it contains 3.7 million words, representing 16 L1 backgrounds. The whole corpus has been POS tagged. 
Representative Corpus 2 
Corpus Escrito del Espal L2 (CEDEL2; http://cedel2.learnercorpora. com/) 
CEDEL2, directed by Cristobal Lozano, is a mono-L2 and multi-L1 learner corpus, made up of Spanish learner data produced by speakers of various L1s. It includes texts written by learners of all profciency levels, from beginners to advanced learners. The texts were collected via a web application, together with detailed metadata. Unlike many learner corpora which fail to include pre­cise information about learners’ profciency levels (see Sect. 13.3), CEDEL2 provides, for each learner, the result of an independent and standardized placement test which the participants also took online. The corpus currently includes over one million words. It comes with native Spanish corpora built according to the same design criteria, which can be used for L1-L2 comparisons. 
Representative Corpus 3 
Parallèle Oral en Langue Étrangère (PAROLE; Hilton et al. 2008) 
PAROLE is a multi-L2 and multi-L1 spoken learner corpus, which represents L2 Italian, French and English speech produced by learners from various L1 backgrounds and profciency levels. It also contains some data produced by L1 speakers. The data were collected through fve oral production tasks, which correspond to varying degrees of naturalness. Next to the usual type of information (learner’s L1, knowledge of other languages, etc.), the metadata include, for each learner, measures of L2 profciency, phonological memory, grammatical inferencing and motivation. PAROLE is a speech (or speaking) learner corpus, which means that, unlike so-called mute spoken learner corpora, it comes with sound fles. The data have been transcribed according to the CHILDES system (see Sect. 13.3) and the transcriptions have been time-aligned with the sound fles (see Chap. 11 on time-alignment). 
13.3 Critical Assessment and Future Directions 
Over the last few years, learner corpora have grown in number, size and diversity. Written learner corpora are already quite numerous and large. In the near future, we should see the release of more and bigger spoken learner corpora, like the (still growing) Trinity Lancaster Corpus (Gablasova et al. 2017). In this respect, it is to be hoped that the developments in speech recognition will one day make it possible to automatically create reliable transcriptions based on recordings of learner language. In Zechner et al. (2009), the authors tested the reliability of a speech recognizer that they had trained on non-native spoken English produced by learners from a wide range of L1 backgrounds and profciency levels. The result was that about one word in two was (wholly or partly) incorrectly transcribed. Although progress has been made in the meantime, Higgins et al. (2015:593) still acknowledge that the performance of speech recognizers “can degrade substantially when they are presented with non-native speech”. 
Another possible development is that the learner corpora of the future will be mega databases (rather than corpora in the strict sense), bringing together data produced by the same learners in different contexts, with different degrees of monitoring (thus including some constrained data, even perhaps of an experimental nature, in addition to the more naturalistic data), at different stages in their learning process and in different languages, including their mother tongue. The last-mentioned type of data, L1 data to be compared with L2 data from the same subjects, can help distinguish between linguistic behaviours that are typical of a person, regardless of whether s/he is using his/her mother tongue or a non-native language (e.g. a slow speech rate), and those that the person only displays when using the L2. García Lecumberri et al. (2017), for instance, have compiled a bi-directional corpus made up of speech produced by English and Spanish native speakers in both their L1 and their L2, and they show how such a corpus can open up new possibilities for the study of learner language. 
More and more learner corpora nowadays come with an equivalent L1 corpus representing the target language (cf. CEDEL2 and PAROLE). This is a welcome development, as it makes it possible to carry out contrastive interlanguage analyses on the basis of fully comparable data. Such target language data are likely to be included in the mega databases of the future. What would also be desirable is input data, which should strive to represent the language that learners get exposed to, so that correlations between input and output can be measured. While in the past learners’ input has been approximated by means of textbook corpora (cf. Rer 2004), it is clear that, especially in the case of an international language like English, learners’ input is no longer limited to textbooks, even in foreign language situations, and that additional sources of exposure to the target language should therefore be taken into account. 
At the same time as we should witness an exponential growth in the size of learner corpora/databases, we should also observe the creation of new types of learner corpora, some of which have already started to be collected. The PROCEED corpus (Process Corpus of English in Education),3 for example, is a ‘process learner corpus’ which aims to refect the whole of the writing process among language learners. It does so by combining screencast and keystroke logging and by examining at a micro-level the different steps leading to the fnal product (see Gilquin Forthcoming). Multimodal learner corpora (see Chap. 16) like the Multimedia Adult ESL Learner Corpus (MAELC; Reder et al. 2003) are likely to become more common, as well as translation learner corpora (corpora of texts translated by non-native students/translator trainees) like the MeLLANGE Learner Translator Corpus (Castagnoli et al. 2011) or the Multilingual Student Translation (MUST) corpus (see Chap. 12). More generally, it seems as if the new generation of learner corpora will be characterized by a higher degree of diversifcation than is currently the case: more (target and frst) languages will be represented, more profciency levels (including young learners, as in the International Corpus of Crosslinguistic Interlanguage; Tono 2012), more tasks, etc. The use of web applications to collect learner corpus data (cf. CEDEL2) will also make it possible to include the production of a wider range of non-native populations, and in particular learners outside universities, where, for reasons of convenience, many participants so far have been recruited. 
In addition to an expansion and diversifcation of learner corpora, we can also expect these corpora to come with more additional information than ever before, in the form of metadata and annotation. Starting with metadata, although learner corpora have included a large variety of them from the very beginning, there is 
3https://uclouvain.be/en/research-institutes/ilc/cecl/proceed.html. Accessed 22 May 2019. 
also a growing recognition that these may not be enough to refect the complexity of the second language acquisition process. Limiting target language exposure to the ‘time abroad’ factor, for example, means neglecting other possible sources of exposure like the Internet, TV series or songs, all of which have become omnipresent in the lives of many young people. Profciency is another case in point. While typically it has been evaluated on the basis of external criteria such as age or number of years of English instruction, scholars like Pendar and Chapelle (2008) have demonstrated that these may only give a very rough approximation of a learner’s actual profciency, which speaks in favour of having the participants take a placement test as part of the data collection procedure (cf. CEDEL2) and/or having the corpus data rated according to a scale like the CEFR. More cognitive measures are also likely to be added in the future, as is the case in PAROLE or in the Secondary-level COrpus Of Learner English (SCooLE), which relies on a whole battery of psychometric tests measuring verbal comprehension, reasoning, perseverance, anxiety and many others (see Mler 2017). In terms of annotation, we can expect learner corpora to more systematically be POS tagged, parsed and/or error tagged (to cite only the main types of annotation mentioned in Sect. 13.2.3), which should be easier once adequate tools have been designed or adapted to deal with learner language more accurately. As with other types of corpora (see, e.g., spoken corpora in Chap. 11), standardization will become even more important as metadata and annotation keep being added. A project like the Child Language Data Exchange System (CHILDES)4 has contributed to the standardization of child-language corpora by proposing a common format for transcription, POS tagging, etc. Although some learner corpora have adopted this system too, like PAROLE or the French Learner Language Oral Corpora (FLLOC),5 they are relatively rare, and there is currently no corresponding system for learner corpora which could ensure the same degree of standardization. 
The availability of more, more diverse, bigger and more richly annotated learner corpora will have an impact on the way we conduct learner corpus research. Ellis et al. (2015), for example, call for “more longitudinal studies based on dense data”. This will involve, frst, the compilation of bigger and denser longitudinal learner corpora. Once these corpora have been collected, appropriate techniques will have to be developed to automate the analysis of individual developmental trajectories in large datasets (see Hokamura 2018 for a step in this direction, based on a set of 20 data collection points but limited to two learners). With such techniques, it will become possible to investigate much larger populations of learners than is currently the case and thus achieve a higher degree of reliability. It can also be hoped that new and better resources will attract more users. In particular, teachers should be encouraged not only to use learner corpora, but also to collect data produced by their own students, in the form of ‘local learner corpora’ (Seidlhofer 2002). With more and more teachers receiving some training in corpus linguistics, we can expect 
4https://childes.talkbank.org/. Accessed 22 May 2019. 5www.floc.soton.ac.uk/. Accessed 22 May 2019. 
that an increasingly large number of them will want to apply the methods of learner corpus research in their classrooms, thus bringing learner corpora closer to those who, ultimately, should beneft from their exploitation, namely learners. 
13.4 Tools and Resources 
Learner Corpus Bibliography: this bibliography is made up of references in the feld of learner corpus research. The bibliography can be found on the CECL website (https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpus-bibliography. html) (accessed 22 May 2019). A searchable version is accessible to members of the Learner Corpus Association in the form of a Zotero collection. 
Learner Corpora around the World (https://uclouvain.be/en/research-institutes/ ilc/cecl/learner-corpora-around-the-world.html) (accessed 22 May 2019): this web­
site, which is regularly updated, contains a list of learner corpora, together with their main characteristics (target language, frst language, medium, text/task type, profciency level, size) as well as information about whether (and how) they can be accessed. 
Université Catholique de Louvain Error Editor (UCLEE; Hutchinson 1996): this program facilitates error tagging thanks to a drop-down menu that makes it possible to select an error tag. It also facilitates the insertion of a corrected form. A new version of the software is currently in preparation. 
Compleat Lexical Tutor (Lextutor; http://www.lextutor.ca) (accessed 22 May 2019): this website, created by Tom Cobb, is mainly aimed at teachers and learners (of English, but also some other languages like French). However, among the many tools it offers, some will be useful to researchers working with learner corpora. VocabProfle, in particular, can analyze (small) learner corpora according to vocabulary frequency bands, making it possible to check whether, say, learners of English tend to rely heavily on the 1000 most frequent words of the English language. 
Further Reading 
Granger, S. 2012. How to use foreign and second language learner corpora. 
In Research methods in second language acquisition: A practical guide, eds. 
Mackey, A., and Gass, S.M., 7–29. Chichester: Blackwell Publishing. 
After briefy introducing learner corpora, this paper clearly presents the different stages that can be involved in a learner corpus study: choice of a methodological approach, selection and/or compilation of a learner corpus, data annotation, data extraction, data analysis, data interpretation and pedagogical implementation. 
Díaz-Negrillo, A., Ballier, N., and Thompson, P., eds. 2013. Automatic treatment and analysis of learner corpus data. Amsterdam: John Benjamins. 
This edited volume covers many important methodological issues related to learner corpora, such as the question of interoperability, multi-layer error annotation, automatic error detection and correction, or the use of statistics in learner corpus research. 
Granger, S, Gilquin, G., and Meunier, F., eds. 2015. The Cambridge handbook of learner corpus research. Cambridge: Cambridge University Press. 
This handbook provides a comprehensive overview of the different facets of learner corpus research, including the design of learner corpora, the methods that can be applied to study them, their use to investigate various aspects of language, and the link between learner corpus research and second language acquisition, language teaching and natural language processing. 
References 
Alexopoulou, T., Geertzen, J., Korhonen, A., & Meurers, D. (2015). Exploring big educational learner corpora for SLA research: Perspectives on relative clauses. International Journal of Learner Corpus Research, 1(1), 96–129. 
Altenberg, B., & Granger, S. (2001). The grammatical and lexical patterning of MAKE in native and non-native student writing. Applied Linguistics, 22(2), 173–194. 
Belz, J., & Vyatkina, N. (2005). Learner corpus analysis and the development of L2 pragmatic competence in networked inter-cultural language study: The case of German modal particles. The Canadian Modern Language Review/La revue canadienne des langues vivantes, 62(1):17– 48. 
Blanchard, D., Tetreault, J., Higgins, D., Cahill, A., & Chodorow, M. (2013). TOEFL11: A corpus of non-native English. Princeton: Educational Testing Service. 
Caines, A., & Buttery, P. (2014). The effect of disfuencies and learner errors on the parsing of spoken learner language. In First joint workshop on statistical parsing of morphologically rich languages and syntactic analysis of non-canonical languages (pp. 74–81). Dublin, Ireland, August 23–29, 2014. 
Callies, M. (2015). Learner corpus methodology. In S. Granger, G. Gilquin, & F. Meunier (Eds.), The Cambridge handbook of learner corpus research (pp. 35–55). Cambridge: Cambridge University Press. 
Castagnoli, S., Ciobanu, D., Kunz, K., Kler, N., & Volanschi, A. (2011). Designing a learner translator corpus for training purposes. In N. Kler (Ed.), Corpora, language, teaching, and resources: From theory to practice (pp. 221–248). Bern: Peter Lang. 
Crossley, S. A., & McNamara, D. S. (2012). Interlanguage talk: A computational analysis of non­native speakers’ lexical production and exposure. In P. M. McCarthy & C. Boonthum-Denecke (Eds.), Applied natural language processing: Identifcation, investigation and resolution (pp. 425–437). Hershey: IGI Global. 
Dagneaux, E., Denness, S., & Granger, S. (1998). Computer-aided error analysis. System, 26(2), 163–174. Dagneaux, E., Denness, S., Granger, S., Meunier, F., Neff, J. A., & Thewissen, J. (2008). Error tagging manual version 1.3. Louvain-la-Neuve: Centre for English Corpus Linguistics. 
de Bot, K., Lowie, W., & Verspoor, M. (2007). A Dynamic Systems Theory approach to second language acquisition. Bilingualism: Language and Cognition, 10(1), 7–21. 
De Felice, R., & Pulman, S. (2009). Automatic detection of preposition errors in learner writing. CALICO Journal, 26(3), 512–528. 
de Haan, P. (1984). Problem-oriented tagging of English corpus data. In J. Aarts & W. Meijs (Eds.), Corpus linguistics: Recent developments in the use of computer corpora (pp. 123–139). Amsterdam: Rodopi. 
de Haan, P. (2000). Tagging non-native English with the TOSCA–ICLE tagger. In C. Mair & M. Hundt (Eds.), Corpus linguistics and linguistic theory (pp. 69–79). Amsterdam: Rodopi. 
De Knop, S., & Meunier, F. (2015). The ‘learner corpus research, cognitive linguistics and second language acquisition’ nexus: A SWOT analysis. Corpus Linguistics and Linguistic Theory, 11(1), 1–18. 
Díaz-Negrillo, A., Meurers, D., Valera, S., & Wunsch, H. (2010). Towards interlanguage POS annotation for effective learner corpora in SLA and FLT. Language Forum, 36(1–2), 139–154. 
Ellis, N. C., Simpson-Vlach, R., Rer, U., O’Donnell, M. B., & Wulff, S. (2015). Learner corpora and formulaic language in second language acquisition research. In S. Granger, G. Gilquin, & F. Meunier (Eds.), The Cambridge handbook of learner corpus research (pp. 357–378). Cambridge: Cambridge University Press. 
Flowerdew, L. (2015). Learner corpora and language for academic and specifc purposes. In S. Granger, G. Gilquin, & F. Meunier (Eds.), The Cambridge handbook of learner corpus research (pp. 465–484). Cambridge: Cambridge University Press. 
Gablasova, D., Brezina, V., McEnery, T., & Boyd, E. (2017). Epistemic stance in spoken L2 English: The effect of task and speaker style. Applied Linguistics, 38(5), 613–637. 
García Lecumberri, M. L., Cooke, M., & Wester, M. (2017). A bi-directional task-based corpus of learners’ conversational speech. International Journal of Learner Corpus Research, 3(2), 175–195. 
Geertzen, J., Alexopoulou, T., & Korhonen, A. (2014). Automatic linguistic annotation of large scale L2 databases: The EF-Cambridge Open Language Database (EFCamDat). In R. T. I. Miller, K. Martin, C. M. Eddington, A. Henery, N. Marcos Miguel, A. M. Tseng, A. Tuninetti, &D.Walter(Eds.), Selected proceedings of the 2012 second language research forum: Building bridges between disciplines (pp. 240–254). Somerville: Cascadilla Proceedings Project. 
Gilquin, G. (2000/2001). The integrated contrastive model: Spicing up your data. Languages in Contrast, 3(1), 95–123. 
Gilquin, G. (2012). Lexical infelicity in English causative constructions. Comparing native and learner collostructions. In J. Leino & R. V. Waldenfels (Eds.), Analytical causatives. From ‘give’ and ‘come’ to ‘let’ and ‘make’ (pp. 41–63). Mchen: Lincom Europa. 
Gilquin, G. (2016). Discourse markers in L2 English: From classroom to naturalistic input. In O. Timofeeva, A.-C. Gardner, A. Honkapohja, & S. Chevalier (Eds.), New approaches to English linguistics: Building bridges (pp. 213–249). Amsterdam: John Benjamins. 
Gilquin, G. (2017). POS tagging a spoken learner corpus: Testing accuracy testing. Paper presented at the 4th Learner Corpus Research Conference, Bolzano/Bozen, Italy, 5–7 October 2017. 
Gilquin, G. (Forthcoming). Hic sunt dracones: Exploring some terra incognita in learner corpus research. In A. .
Cermáková & M. Malá (Eds.), Variation in time and space: Observing the world through corpora. Berlin: De Gruyter. Gilquin, G., De Cock, S., & Granger, S. (2010). Louvain International Database of Spoken English Interlanguage. Louvain-la-Neuve: Presses universitaires de Louvain. Golden, A., Jarvis, S., & Tenfjord, K. (2017). Crosslinguistic infuence and distinctive patterns of language learning: Findings and insights from a learner corpus. Bristol: Multilingual Matters. 
Granger, S. (1996). From CA to CIA and back: An integrated approach to computerized bilingual and learner corpora. In K. Aijmer, B. Altenberg, & M. Johansson (Eds.), Languages in contrast. Text-based cross-linguistic studies (pp. 37–51). Lund: Lund University Press. 
Granger, S. (1998). The computer learner corpus: A testbed for electronic EFL tools. In J. Nerbonne (Ed.), Linguistic databases (pp. 175–188). Stanford: CSLI Publications. 
Granger, S. (2004). Computer learner corpus research: Current status and future prospects. In U. Connor & T. Upton (Eds.), Applied corpus linguistics: A multidimensional perspective (pp. 123–145). Amsterdam: Rodopi. 
Granger, S. (2009). The contribution of learner corpora to second language acquisition and foreign language teaching: A critical evaluation. In K. Aijmer (Ed.), Corpora and language teaching (pp. 13–32). Amsterdam: John Benjamins. 
Granger, S. (2015). Contrastive interlanguage analysis: A reappraisal. International Journal of Learner Corpus Research, 1(1), 7–24. 
Granger, S., Dagneaux, E., Meunier, F., & Paquot, M. (2009). The International Corpus of Learner English (Handbook and CD-ROM. Version 2). Louvain-la-Neuve: Presses universitaires de Louvain. 
Gries, S. T., & Deshors, S. C. (2014). Using regressions to explore deviations between corpus data and a standard/target: Two suggestions. Corpora, 9(1), 109–136. 
Higgins, D., Ramineni, C., & Zechner, K. (2015). Learner corpora and automated scoring. In S. Granger, G. Gilquin, & F. Meunier (Eds.), The Cambridge handbook of learner corpus research (pp. 587–604). Cambridge: Cambridge University Press. 
Hilton, H., Osborne, J., Derive, M. -J., Succo, N., O’Donnell, J., Billard, S., & Rutigliano-Daspet, 
S. (2008). Corpus PAROLE (Parallèle Oral en Langue Étrangère). Architecture du corpus & conventions de transcription. Chambéry: Laboratoire LLS – Équipe Langages, Université de Savoie. http://archive.sf.cnrs.fr/sites/sf/IMG/pdf/PAROLE_manual.pdf. Accessed 22 May 2019. 
Hokamura, M. (2018). The dynamics of complexity, accuracy, and fuency: A longitudinal case study of Japanese learners’ English writing. JALT Journal, 40(1), 23–46. 
Huang, Y., Murakami, A., Alexopoulou, T., & Korhonen, A. (2018). Dependency parsing of learner English. International Journal of Corpus Linguistics, 23(1), 28–54. 
Hutchinson, J. (1996). Université Catholique de Louvain Error Editor. Louvain-la-Neuve: Centre for English Corpus Linguistics, Université catholique de Louvain. 
Izumi, E., Uchimoto, K., & Isahara, H. (2004). The NICT JLE Corpus: Exploiting the language learners’ speech database for research and education. International Journal of the Computer, the Internet and Management, 12(2), 119–125. 
James, C. (1998). Errors in language learning and use: Exploring error analysis. London/New York: Longman. 
Jarvis, S., & Pavlenko, A. (2008). Crosslinguistic infuence in language and cognition.New York/London: Routledge. 
Jucker, A. H., Smith, S. W., & Lge, T. (2003). Interactive aspects of vagueness in conversation. Journal of Pragmatics, 35(12), 1737–1769. 
Liu, E. T. K., & Shaw, P. M. (2001). Investigating learner vocabulary: A possible approach to looking at EFL/ESL learners’ qualitative knowledge of the word. International Review of Applied Linguistics in Language Teaching, 39(3), 171–194. 
Leling, A., Hirschmann, H., & Shadrova, A. (2017). Linguistic models, acquisition theories, and learner corpora: Morphological productivity in SLA research exemplifed by complex verbs in German. Language Learning, 67(S1), 96–129. 
Meunier, F. (1998). Computer tools for interlanguage analysis: A critical approach. In S. Granger (Ed.), Learner English on computer (pp. 19–37). London/New York: Addison Wesley Longman. 
Meunier, F. (2016). Introduction to the LONGDALE Project. In E. Castello, K. Ackerley, & F. Coccetta (Eds.), Studies in learner corpus linguistics. Research and applications for foreign language teaching and assessment (pp. 123–126). Berlin: Peter Lang. 
Meunier, F., & Littré, D. (2013). Tracking learners’ progress: Adopting a dual ‘corpus cum experimental data’ approach. The Modern Language Journal, 97(S1), 61–76. 
Meurers, D. (2015). Learner corpora and natural language processing. In S. Granger, G. Gilquin, & F. Meunier (Eds.), The Cambridge handbook of learner corpus research (pp. 537–566). Cambridge: Cambridge University Press. 
Mler, V. (2017). Language acquisition in CLIL and non-CLIL settings: Learner corpus and experimental evidence on passive constructions. Amsterdam: John Benjamins. 
Myles, F. (2015). Second language acquisition theory and learner corpus research. In S. Granger, 
G. Gilquin, & F. Meunier (Eds.), The Cambridge handbook of learner corpus research (pp. 309–331). Cambridge: Cambridge University Press. Nesselhauf, N. (2004). Learner corpora and their potential in language teaching. In J. Sinclair (Ed.), How to use corpora in language teaching (pp. 125–152). Amsterdam: John Benjamins. 
Osborne, J. (2015). Transfer and learner corpus research. In S. Granger, G. Gilquin, & F. Meunier (Eds.), The Cambridge handbook of learner corpus research (pp. 333–356). Cambridge: Cambridge University Press. 
Paquot, M. (2014). Cross-linguistic infuence and formulaic language: Recurrent word sequences in French learner writing. In L. Roberts, I. Vedder, & J. H. Hulstijn (Eds.), EUROSLA Yearbook 14 (pp. 240–261). Amsterdam: John Benjamins. 
Pendar, N., & Chapelle, C. A. (2008). Investigating the promise of learner corpora: Methodological issues. CALICO Journal, 25(2), 189–206. 
Rayson, P., & Baron, A. (2011). Automatic error tagging of spelling mistakes in learner corpora. In F. Meunier, S. De Cock, G. Gilquin, & M. Paquot (Eds.), A taste for corpora: In honour of Sylviane Granger (pp. 109–126). Amsterdam: John Benjamins. 
Reder, S., Harris, K., & Setzler, K. (2003). The Multimedia Adult ESL Learner Corpus. TESOL Quarterly, 37(3), 546–557. Reznicek, M., Leling, A., & Hirschmann, H. (2013). Competing target hypotheses in the Falko corpus: A fexible multi-layer corpus architecture. In A. Díaz-Negrillo, N. Ballier, & 
P. Thompson (Eds.), Automatic treatment and analysis of learner corpus data (pp. 101–124). Amsterdam: John Benjamins. 
Rer, U. (2004). Comparing real and ideal language learner input: The use of an EFL textbook corpus in corpus linguistics and language teaching. In G. Aston, S. Bernardini, & D. Stewart (Eds.), Corpora and language learners (pp. 152–168). Amsterdam: John Benjamins. 
Rozovskaya, A., & Roth, D. (2010). Training paradigms for correcting errors in grammar and usage. In Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics (pp. 154–162). Los Angeles: Association for Computational Linguistics. 
Seidlhofer, B. (2002). Pedagogy and local learner corpora: Working with learning-driven data. In 
S. Granger, J. Hung, & S. Petch-Tyson (Eds.), Computer learner corpora, second language acquisition and foreign language teaching (pp. 213–234). Amsterdam: John Benjamins. 
Sinclair, J. (1996). Preliminary recommendations on corpus typology (Technical report, EAGLES (Expert Advisory Group on Language Engineering Standards). www.ilc.cnr.it/EAGLES96/ corpustyp/corpustyp.html. Accessed 22 May 2019. 
Spoelman, M. (2013). The (under)use of partitive objects in Estonian, German and Dutch learners of Finnish. In S. Granger, G. Gilquin, & F. Meunier (Eds.), Twenty years of learner corpus research: Looking back, moving ahead (pp. 423–433). Louvain-la-Neuve: Presses universitaires de Louvain. 
Tono, Y. (2012). International Corpus of Crosslinguistic Interlanguage: Project overview and a case study on the acquisition of new verb co-occurrence patterns. In Y. Tono, Y. Kawaguchi, & M. Minegishi (Eds.), Developmental and crosslinguistic perspectives in learner corpus research (pp. 27–46). Amsterdam: John Benjamins. 
Van Rooy, B., & Schäfer, L. (2002). The effect of learner errors on POS tag errors during automatic POS tagging. Southern African Linguistics and Applied Language Studies, 20, 325–335. Zechner, K., Higgins, D., Xi, X., & Williamson, D. M. (2009). Automatic scoring of non-native spontaneous speech in tests of spoken English. Speech Communication, 51(10), 883–895. 
Chapter 14 Child-Language Corpora 
Sabine Stoll and Robert Schikowski 
Abstract Together with experiments, the main method in the study of child lan­guage development is the analysis of behavioral changes of individual children over time. For this purpose recordings of infants’ and children’s naturalistic interactions in a variety of languages spoken in different cultural contexts are key. Language development corpora either are cross-sectional or longitudinal collections of con­versations recorded at predetermined intervals and annotated on various linguistic and multi-modal levels. Here, we discuss the advantages and disadvantages of cross-sectional and longitudinal studies but the focus of this chapter will be on longitudinal studies which are the main tool in corpus analyses of child language. We focus on challenges of corpus design including sampling issues such as the number and choice of participants, amount of recordings, recording situations, transcription, linguistic and multi-modal annotations and ethical considerations. 
14.1 Introduction 
Corpora in language development research are collections of naturalistic interac­tions of children and their surrounding environment. They usually comprise several subcorpora corresponding to different target children. The underlying purpose of developmental corpora is to learn about children’s profciency and to understand how they use language in their natural environment. Thus, corpora allow the researcher to fnd out what children do in natural interaction, in contrast to experiments, which test what children can do. Ideally, a language development 
S. Stoll (•) Department of Comparative Language Science & Center for the Interdisciplinary Study of Language Evolution (ISLE), University of Zurich, Zurich, Switzerland & NCCR Evolving Language, Swiss National Science Foundation Agreement #51NF40_180888 e-mail: sabine.stoll@uzh.ch 
R. Schikowski ETH Zurich, Zurich, Switzerland e-mail: robert.schikowski@sl.ethz.ch 
© Springer Nature Switzerland AG 2020 305 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_14 
S. Stoll and R. Schikowski 
corpus presents an ecologically valid and representative picture of the linguistic development of language learners. To capture how children learn language, the change in their development of vocabulary, grammar (e.g. morphology, syntactic constructions) and pragmatic understanding are monitored over a predefned time window. Thus, developmental corpora are necessarily time series data, whose internal structure is important. To model the child’s profciency at a specifc point in time or over a period of time, a session by session comparison of the child’s and surrounding adults’ constructions is key. 
The raw data in developmental corpora are audio-visual recordings, which are transformed into transcriptions, either phonetic or, more usually, orthographic (see also Chap. 11). In a second step these transcriptions are then enriched with further annotations. 
Two main types of developmental corpora are used in the feld: cross-sectional and longitudinal corpora. In cross-sectional corpora, specifc age points of interest are identifed and a number of children with the respective ages are recorded. Each child is recorded individually in either naturalistic or semi-structured contexts, depending on the purpose of the study. In this type of corpus, development is inferred via between-group comparisons, i.e. by averaging over a large number of participants each of which is recorded only at one point in their development. 
In longitudinal corpora, on the other hand, the development of individual children is estimated based on temporally ordered samples of the same child. Each child is recorded at regular intervals at a number of consecutive time points, usually stretching over several months or even years. Thus, longitudinal corpora portray the individual development of a few children and thereby capture individual differences in developmental curves rather than inferring a general but averaged profle of productivity as it is the case in cross-sectional studies. 
Since recording over several years is time-consuming, a staggered design of several longitudinal studies of children including different age spans is often applied (cf. e.g. the Chintang corpus in Box 1). In this design, several children are observed over a predetermined time span but recordings start and end at different ages for different groups of children. Instead of recording, for instance, two children over two full years, four children of two different ages (e.g. two 2-year-olds and two 3-year-olds) are recorded over one year so that the same age spans are covered. 
Cross-sectional and longitudinal studies both have advantages and disadvantages. Cross-sectional studies require large numbers of children at each interval because the individual variation across children is huge (e.g. Bates et al. 1995). The fewer the children, the more infuence potential outliers or extreme cases with signifcantly different developmental curves can have. Longitudinal data, by contrast, portray real developmental curves of individual children but little information about the effective variation across children at different points of age can be extracted. Thus, longitudinal studies usually remain case studies. Ideally, the two approaches complement each other. 
In the following we focus on longitudinal designs, which are more prevalent in research on language development, but many issues such as annotation layers or metadata are relevant for both corpus types. 
14.2 Fundamentals 
Longitudinal corpora imply regular recordings over at least several months, tran­scriptions of each utterance (ideally not only of the target child, but of all interlocutors), and a multitude of different annotation levels, depending on the questions of the respective project. This makes the development of longitudinal corpora a logistically diffcult, time consuming, and ultimately very expensive endeavor. The ultimate design goal therefore is to create corpora that are sustainable and suitable for quantitative analyses of a wide range of topics rather than focusing on a single research question. In the following we present the main steps and layers in corpus design and their respective challenges. 
14.2.1 Recording and Contextual Setting 
The raw data of longitudinal corpora are audio-visual recordings of natural inter­actions between a target child and her surrounding communicative environment. Video is an important feature not only because the speech of young children is often diffcult to interpret without context but also because language development is multi-modal and involves speech that interacts tightly with gesture and gaze (see Chap. 16). Most corpora nowadays include at least some video recordings. 
A major difference to other spoken corpora is the strict recording scheme. Regular and evenly spaced recordings document how the child’s language develops by adapting more and more to the adult target. 
The naturalness of the recording contexts of existing corpora varies widely. Some researchers prefer to include only the main caretaker and the target child in a play situation. This setting facilitates transcription but sacrifces ecological validity. It is known that the recording context has a strong infuence on the type of constructions used, both in child-directed speech and by the children themselves. Studies that strive for a recording situation that is as natural as possible and thus do not flter for participants or situations are more representative because they allow to better generalize from the recordings to the rest of the child’s linguistic encounters. It is worthwhile to invest into more complex transcriptions that include the typical participants of the natural environment of the child in order to avoid biases in the characterizations of child-surrounding speech. 
To supplement information about speech, researchers have started to use child-mounted cameras to capture the focus of the child’s immediate environment. The main goal of this type of recording is to link the characteristics of the speech of the child to her immediate linguistic and extra-linguistic environment. The visual context is important because the joint attentional frame in conversations as well as the types of responses and world-to word mappings are highly relevant for word learning (e.g. Tomasello 2003; Kuhl 2004). Another approach are recordings with cameras on a tripod, which allows to capture a wider range of the environment 
S. Stoll and R. Schikowski 
including the child herself. This approach has the big advantage that the extralin­guistic context and the actions and reactions of the interactional partners can be included in the analysis. 
14.2.2 Subject Sampling 
The number of children is one of the most important decisions in the design of a study, having the potential both to make data statistically (more) robust and to increase the amount of work put into a corpus ad infnitum. Demuth (1996) recommends three to four children as the minimum and this seems to be the number of children most projects opt for, both small-scale and large-scale. 
While recent corpora tend to include more children, there is (to the best of our knowledge) not a single publication on how to determine the ideal number of children in longitudinal studies. Two children are obviously better than one because one child might happen to be either precocious or a late starter, but two children will also not remedy this problem. However, with more children the question is not if their number improves the reliability of data but rather to what degree it does. This point is crucial considering how additional target children multiply the amount of time and money required to compile a corpus with minimum annotation standards. Numbers of 80 children per community and corpus, as studied in the 0–5 Program (see Representative Corpus 2), are a role model of corpus development but go far beyond the possibilities of most research groups, especially if non-WEIRD (Western, Educated, Industrialized, Rich, Democratic; Henrich et al. 2010) societies are involved. 
The extensive variation in development asks for statistical methods that are suitable for corpora consisting of several case studies. A major factor for the analysis of these corpora is individual variation, i.e. children of the same ages usually vary extremely in their linguistic competence (Bates et al. 1988; Fenson et al. 1994;Lieven 2006). As a consequence, age is a very unreliable predictor for development. Stoll and Gries (2009) propose an incremental method of data analysis that addresses variation up front instead of averaging it out by pooling data of different children. In this approach the individual trajectories of each child are frst compared to their surrounding adults, who are the benchmark for linguistic profciency. Only in a second step are the curves of the children compared and similarities in their development are analyzed. 
Variation is conditioned by a multitude of factors for which the selection of par­ticipants aims to control as far as possible. As a consequence, most developmental studies feature target children of different sexes. However, the differences between sexes reported in the literature are small (Hyde and Linn 1988; Fenson et al. 1994), which means they will frequently be outweighed by individual variation given the small numbers of target children that are common in developmental studies. 
By contrast, a variable that is defnitely worth to control for is socio-economic background since there is large variation in the input to children of different SES groups (e.g. Hart and Risley 1995;Rowe 2008, 2012). Fernald et al. (2013)have shown that already at 18 months there is an enormous gap in processing abilities between infants from high and low-SES families. The conditioning variable for these developmental differences is the amount and quality of input children receive in the frst years of their life (Rowe 2012). 
14.2.3 Size of Corpora and Recording Intervals 
Probably the strongest limiting factor for the results and the conclusions that can be drawn from a corpus is the amount of recordings. Ideally one would record all the speech of the child and her environment. This approach has been taken in the Speechome project (Roy et al. 2006, 2009), in which nearly all linguistic encounters of one child were recorded from birth to age 3. Recordings were made at the home of the child, all rooms of which were wired. Comprehensive as this approach is, it is not feasible for most projects, not only because the research logistics are challenging but also because of the restricted usability of the data due to ethical and privacy issues. In all but the Speechome context we deal with snapshots of the child’s daily encounters and thus with samples of varying sizes, which serve as the basis for our extrapolation to the overall linguistic ecology of a child. 
The type of constructions and vocabulary used by children varies by extra-linguistic contexts and activities. To ensure a faithful assessment of the child’s abilities, recordings ideally include a variety of daily situations. In a recent effort this corpus scheme has been extended to daylong recordings (cf. VanDam et al. 2016, http://www.darcle.org) to obtain deeper insights into daily activities and linguistic interactions (see http://homebank.talkbank.org). 
Daylong recordings of the target child and their surrounding environment are often conducted with LENA devices (LENA Research Foundation, Boulder, Colorado, United States). LENA devices audio-record and simultaneously analyze the utterances produced by the child and her surroundings. They provide automatic estimates about the number of adult words surrounding the child, the number of turns, and the number of child vocalizations. This is very valuable information to estimate the quantity and type of input a child is exposed to. However, the underlying algorithm has only been validated for English, French and Mandarin so far. Further, in audio-only recordings it can be challenging to match the situational context to the respective utterances. If complemented by video, LENA recordings can provide great insights into a large range of situations, provided that all the data is transcribed. This, however, can quickly become a challenge with large numbers of participants and long recording sessions. 
Sample size and sampling regimes have an enormous infuence on the estimation of the child’s development (Tomasello and Stahl’s 2004; Malvern and Richards 1997; Rowland and Fletcher 2006). They are therefore already relevant in the design phase as well as later in the analysis in order to avoid severe biases in the results. Tomasello and Stahl’s (2004:102) estimate that one to two hours of recordings of a 
S. Stoll and R. Schikowski 
single child per month capture no more than 1–2% of her speech. As Tomasello and Stahl point out, the main problem with this is a delay in the detection of rare items. Further, errors, which are a window into developmental strategies, often occur in rare constructions. If infrequent constructions are underrepresented in the sample, errors in frequent constructions become oversampled. Thus, small samples result in both an over-and undersampling of errors (Rowland and Fletcher 2006). In a recent study Bergelson and colleagues (Bergelson et al. 2018) further found that hour-long video recordings may lead to very different estimates about the vocabulary that children hear than daylong audio recordings. They found that the input in the hour-long video recordings was comparatively much greater and much more varied. This is an extremely important result, which confrms conjectures about the relevance of the situational context of recordings with respect to input distributions (Stoll 2016). Bergelson and colleagues show that hour-long video recordings record a special situation, which is presumably not representative for the language a child hears and uses on average over the day. In these hour-long recordings caretakers play with the children and hence provide a much denser input than during other activities typical for the rest of the day. 
This shows that recording intervals and length of recordings can have dramatic consequences for claims about development and productivity if delays are not projected based on underlying frequency distributions. Productivity is one of the most relevant measures in language development research as it captures the acquired competence of the child to use language like a mature native speaker. However, the underlying frequency distributions may likewise not be easily retrievable from corpora that are not dense enough. To avoid this, the following issues are key: 
• 
The recall1 and the hit rate2 for a phenomenon X is a function of the frequency of X and sampling density. 

• 
The same is true for estimations of the frequency and the onset age of X. 

• 
Small samples combined with low-frequency X result in unreliable data. Sooth­ing as this may sound, “small” and “rare” are relative terms. For instance, even a standard sampling density of one hour per week will barely succeed in capturing a not-so-rare X with seven expected instances per week. 

• 
The more frequent X, the steeper the hit rate gain induced by increased sampling density. Note this also entails that the more frequent X, the earlier increases in sampling density do not add signifcantly to hit rates. 


An additional problem stressed by Rowland et al. (2008) is that small samples may not adequately refect the distribution of X. Not only may rare X not be observed at all; short-lived X, such as an error that a child produces only for a short time (e.g. the past tense form goed instead of went), may likewise be missed and distributions over time may appear randomly distorted. This in fact holds more generally for the distribution of lexical items. 
1The recall is the proportion of all instances of a phenomenon that is covered by a sample. 2The hit rate is the probability of fnding at least one instance of a phenomenon in a sample. 
It is worth noting that pooling data in such cases does not necessarily help because it creates new problems. High-frequency items thrown together with low-frequency items will dominate the pool, which may have serious consequences for linguistic interpretation. For instance, English-speaking children produce more errors for rare irregular verbs than for frequent ones, but this fact might be obscured by pooled data because specifc errors by individual children might be misidentifed as rare in the larger sample (Maratsos 2000; Rowland et al. 2008). Pooled data give more weight to verbs with high token frequency with the result of overrepresenting high-frequency tokens (Rowland and Fletcher 2006:9). 
The consequences of these observations for researchers planning to compile or analyze a developmental corpus are serious. Researchers should no longer rely on their intuitions when estimating how robust their data are but prop them up by more reliable quantitative considerations. In the following some more concrete suggestions are given. 
A central notion is sampling coverage, which can be defned as the proportion of the data in question that is captured by the recording scheme. For instance, if we take one week as our reference interval and go with Tomasello and Stahl’s (2004) and Rowland et al. (2008) in assuming that a child is awake for roughly 10 hours per day, this gives us 70 hours of potential interactions per week. If we record 2 hours per week, the sampling coverage will be 2 = 0.03. To calculate the probability 
70 
of capturing linguistic phenomena, we may moreover assume that they follow a Poisson distribution.3 
Below some formulas are listed that make it easy to estimate, for instance, sampling coverage and the probability of capturing a phenomenon. They are derived from the basic terms and calculations presented in Tomasello and Stahl’s (2004). 
• Given a sampling coverage c and an estimated absolute frequency . of X per interval, what will be the average catch . of x per interval? 
. = c ·  
For instance, if we record 2 hours per week and continue to assume that a child talks about 70 hours per week, the sampling coverage will be 2 = 0.03. A
70 
phenomenon that occurs 10 times per week is then expected to be observed 0.03· 10 = 0.3 times per week (in other words, not a single time: on average, a full month will pass until the frst instance is observed). 
3This is in fact a coarse oversimplifcation since the Poisson distribution assumes that events occur at a constant rate and that earlier events cannot infuence the probability of later ones. Both assumptions are obviously very problematic for linguistic interactions so that Tomasello and Stahl’s (2004) approach should be considered as an approximation and a frst step in raising consciousness about sampling issues. These issues need to be tackled by future research on linguistic sampling techniques. 
S. Stoll and R. Schikowski 
• Given an average catch . (calculated from c and . as above), how high is the probability of capturing at least one x (i.e. the hit rate r)? 
.0 · e -. 
r = 1 - P(n = 0) = 1 -= 1 - e -. 
0! 
For instance, let us assume we want to observe an X that we estimate to occur 80 times per day or 560 times per week and we record half an hour per week, so the sampling coverage is 0.5 ˜ 0.01 and the average catch is 0.5 · 560 = 4. Then the 
70 70 
probability of capturing at least one x is 1 - e -4 ˜ 0.98, i.e. very good in spite of the low density. 
• Given an estimated frequency . of X and a number . of X to be captured per interval on average, what should be the sampling coverage c, and what is the number of hours h to be recorded per interval (given the total number of hours spoken by the child per interval, H)? 
. 
c = 
 
. 
h = c · H =· H 
 
For instance, if we estimate that X occurs 500 times per week and would like to capture 10 instances on average, the required sampling coverage is 10 = 0.02.
500 In other words, the number of hours we should record per week is 10 
500 · 70 = 1.4 
(i.e. roughly one and a half hours). 
• Given an estimated frequency . of X and a desired hit rate r, what should be the sampling coverage c and the hours h to be recorded per interval? 
-loge(1 - r) 
c = 
 -loge(1 - r)
h = c · H =· H 
 
For instance, if X is as rare as only occurring 5 times per week and we want to have a 99%4 probability of catching at least one X in an average recording week, the sampling density should be -loge(1-0.99) ˜ 0.92. The number of hours to be 
5 
recorded per week is then 0.92 · 70 ˜ 65. In other words, our goal is unrealistic. 
The last two formulas are especially important because they also allow researchers to derive a recording scheme directly from their research interests. Even if the precise frequency of the phenomenon of interest is not known, estimating 
4While 1.0 may seem to be the most desirable value here, a perfect hit rate requires infnitely dense sampling. In other words, we can never be perfectly sure to capture at least one instance even if we sample everything because the population itself does not guarantee the occurrence of events. 
it and calculating the required sampling density on that base is still much more objective than following a gut feeling or simply choosing the density that is currently most common. 
It is worthwhile mentioning that the concepts of sampling density and coverage used above confate two aspects of sampling which are of great relevance for theory and practice, viz. sampling intervals (e.g. one month between samples) and durations of recordings (e.g. 2 hours per sample). If we were interested in a phenomenon that occurs about 20 times per hour (1400 times per week, 5600 times per month) and wanted to capture 25 instances per week (100 instances per 
25
month), the required sampling densities are 1400 · 70 = 1.25 hours per week or 
100 
5600 · 280 = 5 hours per month. This makes a practical difference: recording 
e.g. a 5-hour sample within a predefned week of the month (the sample can be subdivided in several recordings of different length within this week) will on average be easier to accomplish than recording a 1-hour sample every week, which implies a high demand of discipline both of the recording assistant and the families. In addition, increasing both sampling intervals and durations also has the advantage of increasing the hit rate and improving the sampling density per point in time. In other words, while we lose some granularity in this approach (developments can only be observed in larger steps), we gain confdence in our observations. 
To sum up, this implies that a sampling regime of 4–5 hours within a predefned week per month is preferable to a sampling regime of 1 hour per week even though the same amount of hours is recorded. 
14.2.4 Transcription 
Although transcription may seem to be the precondition for all kinds of data annotation, it can in fact be viewed itself as a kind of annotation or “data reduction” (Brown 1993:154). As has been shown by Ochs (1979, with a focus on language development research), transcriptions have a deep impact on how we interpret data and what kind of analyses are possible based on them. Thus, this task should ideally be treated on a par with other annotation tasks. 
Transcription can be split up into several subtasks, viz. segmentation (detection of time stretches that contain speech), speaker assignment (connecting segments to speaker metadata), and transcription proper (putting text on segments; see also Chap. 11). 
An important linguistic parameter of transcriptions is phonetic granularity. Most developmental studies (apart from studies on phonological development) do not require a high level of phonetic precision. Instead, a simple phonological or orthographic transcription is suffcient. Even when research questions make it necessary to transcribe data phonetically, it is helpful to have an additional tier for coarser transcriptions, which represent less variation and are therefore easier 
S. Stoll and R. Schikowski 
to search. Coarse phonological transcriptions may also serve as a surrogate to full orthographic normalization when the latter is not feasible. 
Transcription is often the bottleneck of corpus development. In the case of under-researched languages, large amounts of data paired with resource pressures often make it seem hard to impossible to transcribe all data at once. Kelly et al. (2015:299) therefore suggest to transcribe only “potentially relevant stretches”, and this is indeed common practice in many corpora. The problem with this practice is that it runs the risk of creating highly biased data where we only fnd what we want to fnd. Calculating frequencies or estimating the age of onset based on such data is impossible. 
Another option to reduce transcription efforts is to transcribe only children’s utterances. While this does not create any statistical problems, it makes it impossible to take child-surrounding speech into consideration. As a consequence there is no way to compare the development of the child to the behavior of mature native speakers and this is all what naturalistic corpora are about. 
Thus, there is no way around transcribing all data in each recording session. In order to be able to spot developments even when the corpus is not (yet) fully transcribed, it is useful to transcribe data according to a systematic “zoom-in” pattern, where one starts with the frst and the last recording of a child, then transcribes the recording in the middle between the two, then the two recordings in the middle of the stretches thus defned, and so on. 
Moving beyond these basic questions, there are a number of transcription problems which are specifc to developmental corpora. An excellent overview of these can be found in MacWhinney (2000), so that below only a short summary and some recommendations are given. 
• The phonology of child language is different from adult language. For researchers interested in phonological development, a simple orthographic transcription therefore will not suffce – they will need an additional tier for phonetic transcriptions. But even researchers interested in other aspects of language face the problem that children frequently produce deviant forms (as 
compared to adult language) such as ['su.] for shoe or ['p.mp.n] for pumpkin. Transcribing only the actual form often makes it diffcult to understand what was said. Transcribing only the target form, on the other hand, obscures the fact that the child produced an alternate form. There are basically two ways to solve this dilemma, depending on research interests and available resources. The maximal solution is to transcribe both levels on independent tiers.5 
The economical alternative is to transcribe only actual forms and to recycle another tier (e.g. morphological analysis) for implicitly specifying the associated target forms. The disadvantage of this latter approach is that it is no longer 
5It is not advisable to transcribe actual and target forms on a single tier as is done in CHAT, since this will often have negative effects on processability and convertability. 
possible to distinguish between children’s mistakes or deviant forms and other cases where the additional tier serves normalization. 
• 
The distinction between actual and target forms is closely connected to error coding, which can, however, also span multiple words and give more precise information regarding the kind of error made. Like all semantic layers that are logically independent of transcription proper, error codes, too, should be specifed as an independent layer (e.g. an additional tier) if a researcher is interested in them. 

• 
It is often desirable to link utterances to stretches on the time line. This makes it easy to locate utterances in a video to review its context and to correct or add annotations. Time links also have theoretical applications in language development, where they can e.g. serve as the base for calculating input frequencies based on real time. Researchers wishing to include time links in their corpus are advised to use a software that creates them as part of the transcription process such as ELAN (https://tla.mpi.nl/tools/tla-tools/elan/)or CHILDES (https://childes.talkbank.org). 

• 
The addressee of utterances is of special interest for language development research because there are important differences between child-directed and adult-directed speech. Thus, it can be useful to code for addressees on a separate tier. 


14.2.5 Metadata 
Any corpus relies on metadata to correlate the speech of the child and her surround­ings with social variables. The most important metadata are the recording date, the identity, birth date, and sex of participants, and the role a participant takes in a given recording (a cover term for kinship relations such as “mother” and functional roles such as “recorder”). Many more felds are provided for in the multitude of XML metadata profles contained in the CMDI6 Component Registry at https://catalog. clarin.eu/ds/ComponentRegistry, which is maintained by the European Union’s CLARIN network (https://www.clarin.eu/). Two CMDI profles that are widely used in documentary linguistics (feld of linguistics providing a comprehensive description of the linguistic practices of individual speech communities) and that are also suitable for developmental research are the IMDI and ELDP profles. 
All widespread metadata standards bundle metadata belonging to different levels (such as sessions or participants) in a single physical fle for each session. While this has the advantage of creating neat fle systems, it also has at least one severe disadvantage. Participant metadata are logically independent of session metadata – for instance, the name and sex of a participant do not change with every recording. Nevertheless, the design of the mentioned standards provides that if a child appears 
6Component Metadata Infrastructure. 
S. Stoll and R. Schikowski 
in 50 sessions, all her metadata are repeated 50 times (in the metadata fle for each session). Our experience shows that this inevitably leads to data corruption in the form of inconsistencies. For instance, one and the same child may appear with slightly altered versions of his name (“Thomas”, “Tommy”), which will make him look like two different participants for automatic processing. 
Participant metadata are commonly linked to the participants’ utterances via short participant IDs that may be numeric or alphabetical. This both makes it easier to type in the speaker of an utterance during transcription and largely anonymizes the data (given that the metadata are stored in a different fle). This is especially relevant if the data is shared more widely. 
14.2.6 Further Annotations 
Most questions require further annotations for making information contained in the data more explicit. Such annotations are the base for automated quantitative analysis. These annotation steps are often referred to as “tagging” or “coding” in the literature. 
One of the most common options for further annotations are utterance-level translations, which are a prerequisite for cross-linguistic research. Without transla­tions, project-external researchers who are not familiar with the language will not be able to use the data. While such researchers might rely on glosses instead (for which see below), glossing is in turn greatly facilitated by utterance-level translations and in many cases only becomes possible through them (e.g. when glosses are done by student assistants, who might not be as familiar with the language as the researchers). This also concerns corpora of languages with strong institutional support: in a globalized world it no longer seems fair to assume that everybody speaks a handful of European languages for which translations would therefore be futile. 
Studies with a focus on the development of grammar rely on grammatical annotations, especially lemmatization, parts of speech, and interlinear glossing (cf. below). 
There are two main glossing standards used in language development corpora. The standard implemented in the CHILDES database (the CHAT tier %mor:) ignores the function of lexical morphemes and the shape of grammatical morphemes and confates what remains on a single tier. Thus, the CHAT version of the Chintang (Sino-Tibetan, Nepal) utterance Pakpak cokkota would look as follows: 
(1) *MOT: Pakpak cokkota. %mor: pakpak ca-3P-IND.NPST-IPFV %eng: ‘He’s munching away at it.’ 
While this saves space, this glossing style also has several disadvantages. First, it does not follow the principle of separating distinct semantic layers (such as 
segments vs. morpheme functions) in the annotation syntax, thus making this format harder to read, process, and convert than others. Second, a researcher who does not speak Chintang will not know what pakpak and ca mean. Even an utterance translation will not help with longer utterances, where it becomes increasingly hard to identify words and morphemes of the object language with the elements of the translation metalanguage. Such types of glosses are thus less reusable and sustainable. 
Further, this format does not allow to search for morphemes in the sense of form-meaning pairs. For instance, a search for the segment ca paired with the gloss ‘eat’ or a search for the form -u paired with the function ‘3P’ will unambiguously select two morphemes with a very low possibility of confusion with other morphemes. However, such searches are only possible in formats with complete form and function tiers. In the CHAT format one might search for ca, but this will include homophones and exclude synonyms. Similarly, a search for the gloss 3P may easily yield other morphemes with the same function even when we are only looking for the one with the underlying shape -u. 
Another common option are true interlinear glosses, which can be seen as a combination of segmentation into morphemes and morpheme-level translations. This type of glosses is also common in general linguistic publications and specifes both the shape and function of all morphemes. Below an example from the Chintang corpus is given (in its native format, Toolbox). The frst tier is an orthographic transcription tier similar to the CHAT example. The second tier segments words into morphemes, given in their underlying form. The third tier assigns a gloss to every morpheme, which is a metalanguage translation in the case of lexical items and a standardized label in the case of grammatical morphemes. The fourth tier contains the English translation of tier 1. 
(2) \tx Pakpak cokkota. \mph pakpak ca-u-kV-ta \mgl without.stopping eat-3[s]P-IND.NPST[3sA]-IPFV \eng ‘He’s munching away at it.’ 
Morpheme-based glosses are a precondition for any searches of basic semantic units without knowing the language. When glosses are used that correspond to a standard such as the Leipzig Glossing Rules (https://www.eva.mpg.de/ lingua/resources/glossing-rules.php) or the Universal Dependencies framework (http://universaldependencies.org/u/feat/index.html), it becomes easy to compare different corpora and languages. The relation between morphemes and glosses is in general much more straightforward than the one between utterances and translations, making it ideal for more objective analyses and for searching for specifc elements (cf. Demuth 1996:21). A corpus with good interlinear glosses may even spare utterance translations. 
Another common semantic layer are parts of speech (POS). POS annotations are crucial for analyses of grammatical development (see Chap. 2). While glosses are highly useful even without POS tags, the reverse is hardly true because POS tags 
S. Stoll and R. Schikowski 
alone do not make it possible to search for specifc functions or morphemes. Thus, POS tagging should be done simultaneously with or after glossing. The combination of the two layers has many applications in developmental research, thus making POS tags another recommendable tier. As always, it is advisable to keep POS and glosses on distinct tiers. 
Beyond the basic layers just discussed, there is a plethora of further possibilities that cannot all be listed here. Some common examples include the annotation of semantic roles or other properties linked to reference, syntactic dependencies, speech acts, errors on various levels, or actions accompanying speech (gaze and pointing). 
(3) 
shows an example for syntactic dependency annotations in a Hebrew corpus created by an automatic parser (Gretz et al. 2015:114; dummy speaker inserted). The relevant tier is %gra: with dependency structures represented as triplets. The frst element in the annotation is an index of the element, the second element is the index of the head of the respective token and the last element indicates the relation of the token. 

(3) 
*CHI: Pan¯i loP roce¯ tipot¯ 


. %mor: pro:person|num:sg neg part|num:sg n|num:pl %gra: 1|3|Aagr 2|3|Mneg 3|0|Root 4|3|Anonagr %eng: ‘I don’t want drops.’ 
Figure 14.1 shows an annotation tier for pointing behavior (index fnger points, hand points etc.) accompanying the speech of the interlocutors taken from the longitudinal corpus of Russian language development (Stoll et al. 2019). The format is ELAN. 
14.2.7 Ethical Considerations 
Language acquisition data are highly sensitive due to several reasons. Children are an especially vulnerable population and are not considered capable of giving informed consent by many legislations. Recordings are typically taken in intimate settings and if more participants than just mother-child dyads are present they will start talking about anything that may be considered taboo by a given society once they have gotten used to the camera. Moreover, longitudinal designs and developmental questions require tracking participants and collecting metadata such as full names, addresses, and birth dates, which makes the raw data quite the opposite of anonymous. 
Ethical considerations should therefore be an integral part of language acquisi­tion research. Besides obtaining ethics clearance from institutional reviewing boards and/or funding agencies, researchers need to have suffcient knowledge of the socio­cultural context and take the time to explain to the participating families in detail what the research implies. In communities speaking underdocumented languages, involving the community itself may be an additional concern and data protection is of special importance since these communities are often tightly knit. 

The described high demands are often directly opposed to long-standing claims for more exchange and interoperability of language acquisition data, which are recently being refueled by the open access and open data movements. Publishing language acquisition data without taking any measures for protecting subjects is ethically highly problematic. Researchers must therefore be aware of data protection techniques such as pseudonymization (also known as “coding”), anonymization, aggregation, and possibly encryption. The organizational design of databases is just as important and should cover aspects such as user roles, access levels, and long­term use. Direct collaboration with data owners helps to build trust and provides the additional advantage of getting access to rich knowledge of the cultural context. 
S. Stoll and R. Schikowski 
Representative Study 1 
Huttenlocher, J., Haight, W., Bryk, A., Seltzer, M., and Lyons, T. 1991. 
Early vocabulary growth: Relation to language input and gender. 
Developmental Psychology, 27(2), 236–248. 
One frequent topic of developmental studies is the development of basic vocabulary. Studies from this area most often rely on transcriptions without considering grammar. In order to understand the enormous variation that children show in this regard, innate predispositions and environmental variables need to be tested in a large number of children. In a large-scale study Huttenlocher et al. (1991) analyzed the vocabulary growth of 11 children from 14 months to 26 months in their natural environment. The corpus consists of two subgroups of children: fve children were recorded from 14 months onwards for 5 hours per month, and six children, from 16 months onward for 3 hours per month. This type of data and sampling allowed Huttenlocher and colleagues to test which variables are mainly responsible for vocabulary growth (including sampling regime) while at the same time controlling for how much the target child and the caregivers talk overall. This is important for calculating the chance of over or underestimating the vocabulary size of a child. 
The authors could show that the relative talkativeness of the mothers is stable over time but an increase in the amount of speech can be explained as an adaption to the age of the child. Between-child models focusing on the relation of group, exposure and gender found that the vocabulary acceleration in Group 1 with a higher recording density is 45% more than in Group 2. This shows impressively how strongly our results depend on our sampling decisions. If group effects are controlled for, the role of exposure is positive and signifcant with girls accelerating faster than boys, i.e. gender is a signifcant variable. 
Representative Study 2 
Abbot-Smith, K. and Behrens, H. 2006. How known constructions 
infuence the acquisition of other constructions: the German passive 
and future constructions. Cognitive Science 30, 995–1026. 
Abbot-Smith and Behrens (2006) illustrate in an impressive high-density study of one German child, age 2;0–5;0 (see Representative Corpus 1) how constructions interact in learning. They show that two German passive con­structions and the German plural, which have similar auxiliary constructions, are learned in a piecemeal fashion at a different pace. A main insight of this study is that construction learning is not an isolated process but previously 
(continued) 
learned constructions have an infuence on later similar constructions, i.e. they either support or impede their development. The authors show that the learning of the German passive construction, which is build with the auxiliary sein, is supported by previously acquired copula constructions which are build with the same auxiliary. The other passive construction built with the auxiliary werden was learned much later and was not supported by the productivity of prior related constructions. In the analysis of the German future tense, which is also build with the auxiliary werden, they found that a semantically similar construction delayed the acquisition of the future tense. Such results can only be obtained by dense corpus studies allowing us to detect relatively rare phenomena such as passive constructions. 
Representative Corpus 1 
The Thomas Corpus (Lieven et al. (2009), https://childes.talkbank.org/ access/Eng-UK/Thomas.html) compiled by the Max Planck Child Study Center at the University of Manchester and the Leo Corpus compiled at the Max Planck Institute for Evolutionary Anthropology, Leipzig (Behrens 2006, http://childes.talkbank.org/access/German/Leo.html) are both dense longitudinal corpora including mainly the interactions of the target child with one caregiver. The corpora contain data of one English-learning child (aged 2;0 to 4;11) and one German-learning child (aged 1;11 to 4;11). The corpora are especially notable for their high sampling density: for the third year of the children’s life, fve 60-minutes recordings were taken every week (four audio, one audio and video) and the later phases consisted of 1 h recording per month. This allows for views into language development with an unprecedented resolution. The corpora are fully lemmatized and contain annotations for morphology and parts of speech. 
Representative Corpus 2 
The Language 0–5 Corpus (https://www.lucid.ac.uk/what-we-do/research/ language-0-5-project/) is currently being built up by Caroline Rowland and colleagues at the University of Liverpool. What makes this corpus remarkable is its broad scope: it follows 80 English-learning children longitudinally from age 0;6 to 5;0, including naturalistic recordings, questionnaires and experiments, thus building the most comprehensive picture of inter-individual variation in one language that language development research has seen so far. This corpus has the potential to change the feld of developmental research radically in allowing sound generalizations from the sample to the population. 
S. Stoll and R. Schikowski 
Representative Corpus 3 
The Chintang Language Corpus (Stoll et al. 2012, 2019, http://www.clrp. uzh.ch) built by Sabine Stoll, Elena Lieven and colleagues is an example for a corpus that strikes a balance between the requirements of sample size and sampling density and that has dealt with some of the typical problems of studies of underresourced languages. It contains data of six children (two each in three rough age ranges: 0;6 to 1;10, 2;0 to 3;5, and 3;0 to 4;3) learning Chintang, a highly polysynthetic, endangered Sino-Tibetan language spoken in a small village in Eastern Nepal. The recordings mainly took place outside on the veranda where the children played and the adults were sitting around. It respects the natural context in which children grow up and includes transcriptions of all the speech of people surrounding the child. For each child, 4 hours were recorded per month within a single week. The corpus is richly annotated, e.g. with translations, Leipzig Glossing Rule-style interlinear glosses and POS tags for all participants. 
14.3 Critical Assessment and Future Directions 
The main goal of language development research is to understand how language can be learned by children. For this we need to know how children and the people surrounding them use language in their natural environment. Observational corpora are the best tool for estimating how distributions in the input relate to learning. However, they come with big costs; the logistics of compiling such corpora are extremely demanding and therefore most corpora include only a small number of children with limited recordings. As a consequence, one of the major impediments to progress in our feld has been the quantity of data we have at our disposal. Quantity here refers both to (i) the size of individual corpora (e.g., number of participants, length and interval of recordings) and (ii) the number (and type) of languages we have data for. 
Why is this relevant? Two issues are key here: 
Corpus size is relevant because we need reliable frequency distributions of a large range of structures and constructions to estimate developmental growth curves. This requires dense corpora such as the Max Planck dense databases. So far, this type of data is only available for a small number of participants in two languages. A main challenge is to extend this approach to a wider number of participants as pioneered in the 0–5 project presented above. Since high density recordings are very demanding both for the people recorded and the researchers, this might not be a valuable avenue at least for a wider range of languages. A potential alternative might be to combine individual high-density studies with large-scale cross-sectional studies. 
The number of languages is relevant because understanding language develop­ment requires understanding how children can learn any language. However, the corpora that are currently available are still heavily biased towards Indo-European languages spoken in Europe. For only about 2% of the languages of the world at least one study of language development is available, and this study often concerns a single feature of the language under investigation (Stoll 2009; Lieven and Stoll 2010). As a result, we only know about the development of a tiny number of features in a tiny number of languages. A complicating fact is that our language sample of “Standard Average European Languages” is in fact rather exotic and unusual from a typological point of view (Dahl 1990). Thus, what language development research so far has been doing is looking at how languages which could be termed as “the odd ball out” are learned rather than making claims about how language is learned independently of individual structural properties. To remedy this problem we need more data from a large variety of typologically diverse languages. This issue has been extensively discussed in typological research, where it is widely acknowledged that statements about language are only possible when a representative sample of languages is considered. 
There is another good reason besides sampling issues why language development research is in dire need of more diverse corpora. As pointed out by Bowerman (1993, 2011), it is frequently assumed that typology and language development are linked in a principled way: the most frequent structures in typology could also be the ones that are easiest to learn, and vice versa (Moran et al. 2017). This, too, can only be proven on the basis of more and more diverse data. Recently a new sampling approach has been proposed focusing on diversity in grammatical structures (Stoll and Bickel 2013) resulting in a database (http://www.acqdiv.uzh. ch/en.html) of longitudinal data of fve groups of grammatically maximally diverse languages (Stoll and Bickel 2013; Moran et al. 2016). This database thus allows us to simulate maximum structural diversity in the languages of the world and therefore is an ideal testing ground for general learning mechanism and input structures. 
To appreciate the role of different structural features in development, we need to be able to compare structures of different languages. Thus, while a large amount of data is already available, it is by no means trivial to explore them in a uniform way. Corpora usually have their own idiosyncratic study designs, including the number and age of children involved, the sampling regime, and the type of contexts in which recordings are taken, let alone the different conventions used for annotations. Comparability is sometimes complicated by rather technical reasons such as different choices of fle formats, corpus formats and syntax, and coding schemes. CHILDES and the CHAT format as well as widespread markup languages such as XML have made a great contribution to unifcation but still leave room for enormous variation, even in areas that are of great interest to many researchers such as word meaning, morphology, or syntax. Thus, more standardization, also linking to standards existing in other corners of corpus linguistics, seems an indispensable step for exploiting the full range of developmental data that we have. A recent effort to unify glossing and initiate collaborative comparative research was undertaken in the above-mentioned cross-linguistic database ACQDIV composed of longitudinal 
S. Stoll and R. Schikowski 
corpora from typologically diverse languages. The database features unifed glosses, POS tags, and other variables that are relevant for comparative studies (for a description of the design see Moran et al. 2016; Jancso et al. 2020). 
To sum up, the two issues of quantity are tightly intertwined. In short, we need more data for more diverse languages. To achieve this goal, we need massive improvement of automatic transcription devices and automatic interlinear glossing. To include a wider set of languages in our samples we will need to focus more on feldwork on less well-known languages. For this, specialists of language development ideally team up with linguists specialized in these languages. For remote and undescribed languages, however, it is usually impossible to conduct high density studies, let alone to record more than a handful of children and subsequently transcribe and annotate the data. In addition, feldwork in culturally diverse settings necessitates a plethora of ethical considerations related to language attitudes, language policies, and privacy issues, which need to be resolved in collaboration between communities and feldworkers (Stoll 2016). 
As of now the feld is also still waiting for innovative ways of reconciling high ethical standards and the demand for open access. One possibility is to implement a sophisticated, multi-layered system of access, to make sensitive data publicly available in an aggregated format that does not allow inferences to associations between individual utterances and speakers, or to fully encrypt the data so that structure is preserved but meaning (including names) is lost. 
Thus, the major challenge for observational language development research is to overcome these impediments and to introduce big data. Only then will we be able to conduct large scale meta-analyses as pioneered by Bergmann et al. (2018). These are developments that hopefully will take place in the not too far future. For now, as we see it, the most pressing step to advance the feld of corpus-based language development research is to strengthen our statistical methods to allow for sound generalizations from the small corpora that are at our disposal. 
14.4 Tools and Resources 

One of the most prominent and important platforms for data sharing and corpus tools in language development research is CHILDES (MacWhinney 2000). CHILDES is part of the TalkBank system, which is a database for studying and sharing conversational data (http://childes.talkbank.org). 
The database currently contains corpora of over 40 languages. The size, depth, design, and format of the corpora vary widely. Data range from frst language devel­opment and bilingual corpora to clinical corpora. CHILDES also provides a number of tools for corpus development including transcription and annotation programs. 
Another recent approach to hosting corpora and initiating collaborative research is the ACQDIV project (http://www.acqdiv.uzh.ch). The project features a database of corpora from maximally diverse and often endangered languages. 
Further Reading 
Behrens, H. (ed.) 2008. Corpora in Language Acquisition Research: Finding Structure in Data (= Trends in Language Acquisition Research 6 (TiLAR)). Amsterdam: Benjamins. 
This collective volume edited by Heike Behrens is a highly valuable collection of various topics on language development corpora, among them also methods for corpus building and the history of this genre. 
Meakins, F., Green, J., Turpin, M. 2018. Understanding linguistic feldwork. Routledge. 
This comprehensive volume on feldwork techniques provides an in-depth practical introduction to feldwork on small and/or endangered languages. It also includes a profound chapter with ample and very useful practical instructions for building up corpora on language development in such environments. 
Acknowledgments The research leading to these results received funding from the European Union’s Seventh Framework Program (FP7/2007–2013), ERC Consolidator Grant: ACQDIV (grant agreement no. 615988, PI Sabine Stoll). 
References 
Abbot-Smith, K., & Behrens, H. (2006). How known constructions infuence the acquisition of other constructions: The German passive and future constructions. Cognitive Science, 30, 995– 1026. 
Bates, E., Bretherton, I., & Snyder, L. S. (1988). From frst words to grammar: Individual differences and dissociable mechanisms. Cambridge: Cambridge University Press. 
Bates, E., Dale, P. S., & Thal D. (1995). Individual differences and their implications for theories of language development. In: Fletcher P. & MacWhinney B. (Eds.), Handbook of child language (pp. 96–151). Oxford: Basil Blackwell. 
Behrens, H. (2006). The input–output relationship in frst language acquisition. Language and Cognitive Processes, 21, 2–24. Bergelson, E., Amatuni, A., Dailey, S., Koorathota, S., & Tor, S. (2018). Day by day, hour by hour: Naturalistic language input to infants. Developmental Science 22(3), 1–10. Bergmann, C., Tsuji, S., Piccinini, P. E., Lewis, M. L., Braginsky, M., Frank, M. C., & Cristia, 
A. (2018). Promoting replicability in developmental research through meta-analyses: Insights from language acquisition research. Child Development, 89(6), 1996–2009. 
Bowerman, M. (1993). Typological perspectives on language acquisition. In E. V. Clark (Ed.), The Proceedings of the Twenty-Fifth Annual Child Language Research Forum, Stanford (pp. 7–15). Center for the Study of Language and Information. 
Bowerman, M. (2011). Linguistic typology and frst language acquisition. In J. J. Song (Ed.), The Oxford handbook of linguistic typology (pp. 591–617). Oxford: Oxford University Press. 
Brown, P. (1993). The role of shape in the acquisition of Tzeltal (Mayan) locatives. In E. Clark (Ed.), The Proceedings of the 25th Annual Child Language Research Forum (pp. 211–220). New York: Cambridge University Press. 
Dahl, Ö. (1990). Standard Average European as an exotic language. In J. Bechert, G. Bernini, 
C. Buridant (Eds.), Towards a typology of European languages. (pp. 3–9). Berlin: Mouton de Gruyter. 
S. Stoll and R. Schikowski 
Demuth, K. (1996). Collecting spontaneous production data. In J. D. Villiers, C. McKee, & H. S. Cairns (Eds.), Methods for assessing children’s syntax (pp. 3–22). Cambridge, MA: MIT Press. 
Fenson, L., Dale, P. S., Reznick, J. S., Bates, E., Thal, D. J., Pethick, S. J., Tomasello, M., Mervis, 
C. B., & Stiles, J. (1994). Variability in Early Communicative Development. Monographs of the Society for Research in Child Development, 59(5), pp. i–185. Wiley: New Jersey. Fernald, A., Marchman, V. A., & Weisleder, A. (2013). SES differences in language processing skill and vocabulary are evident at 18 months. Developmental Science, 16(2), 234–248. Gretz, S., Itai, A., MacWhinney, B., Nir, B., & Wintner, S. (2015). Parsing Hebrew CHILDES transcripts. Language Resources and Evaluation, 49(1), 107–145. Hart, B., & Risley, T. R. (1995). Meaningful differences in the everyday experience of young American children. Baltimore: Paul Brookes Publishing. Henrich, J., Heine, S. J., & Norenzayan, A. (2010). Most people are not WEIRD. Nature, 466(7302), 29. Huttenlocher, J., Haight, W., Bryk, A., Seltzer, M., & Lyons, T. (1991). Early vocabulary growth: Relation to language input and gender. Developmental Psychology, 27(2), 236–248. Hyde, J. S., & Linn, M. C. (1988). Gender differences in verbal ability: A meta-analysis. Psychological Bulletin, 104(1), 53–69. 
Jancso, A., Moran, S., & Stoll, S. (2020). The ACQDIV corpus database and aggregation pipeline. Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), 156–165. 
Kelly, B. F., Forshaw, W., Nordlinger, R., & Wigglesworth, G. (2015). Linguistic diversity in frst language acquisition research: Moving beyond the challenges. First Language, 35(4–5), 286–304. 
Kuhl, P. K. (2004). Early language acquisition: Cracking the speech code. Nature Reviews Neuroscience, 5(11), 831–843. 
Lieven, E. (2006). Variation in frst language acquisition. In K. Brown (Ed.), Encyclopedia of language & linguistics (2. ed., pp. 350–354). Amsterdam: Elsevier. 
Lieven, E., Salomo, D., & Tomasello, M. (2009). Two-year-old children’s production of multiword utterances: A usage-based analysis. Cognitive Linguistics, 20(3), 481–507. 
Lieven, E. V. M., & Stoll, S. (2010). Language. In M. H. Bornstein (Ed.), Handbook of cultural developmental science (pp. 143–160). New York: Psychology Press. 
MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk. Mahwah: Lawrence Erlbaum Associates. 
Malvern, D. D., & Richards, B. J. (1997). A new measure of lexical diversity. In A. Ryan & A. Wray (Eds.), Evolving models of language (pp. 58–71). Clevedon: Multilingual Matters. 
Maratsos, M. P. (2000). More overregularizations after all: New data and discussion on Marcus, Pinker, Ullman, Hollander, Rosen and Xu. Journal of Child Language, 27, 183–212. 
Moran, S., Sauppe, S., Lester, N., & Stoll, S. (2017). Worldwide frequency distribution of phoneme types predicts their order of acquisition. In Paper Presented at the 51st Annual Meeting of the Societas Linguistica Europaea (SLE) 29 Aug–1 Sept 2017. 
Moran, S., Schikowski, R. Pajovic,´ D., Hysi, C. & Stoll, S. (2016). The ACQDIV database: Min(d)ing the ambient language. In Calzolari, N., Choukr, K., Declerck, T., Grobelnik, M., Maegaard, B., Mariani, J., Moreno, A., Odijk, J., Piperidis, S. (Eds.), Proceedings of the Tenth International Conference on Language Resources and Evalutation (LREC 2016) (pp. 23–28). ELRA: Paris. 
Ochs, E. (1979). Transcription as theory. In E. Ochs & B. Schieffelin (Eds.), Developmental pragmatics (pp. 43–71). New York: Academic. Rowe, M. L. (2008). Child-directed speech: Relation to socioeconomic status, knowledge of child development and child vocabulary skill. Journal of Child Language, 35(1), 185–205. Rowe, M. L. (2012). A longitudinal investigation of the role of quantity and quality of child-directed speech in vocabulary development. Child Development, 83(5), 1762–1774. Rowland, C. F., & Fletcher, S. L. (2006). The effect of sampling on estimates of lexical specifcity and error rates. Journal of Child Language, 33(04), 859–877. 
Rowland, C. F., Fletcher, S. L., & Freudenthal, D. (2008). How big is big enough? Assessing the reliability of data from naturalistic samples. In H. Behrens (Ed.), Corpora in language acquisition research: History, methods, perspectives (pp. 1–24). Amsterdam: John Benjamins. 
Roy, B. C., Frank, M.C. , & Roy, D. (2009). Exploring word learning in a high-density longitudinal corpus. In Taatgen, N. & van Rijn H. (Eds.) Proceedings of the Thirty-First Annual Conference of the Cognitive Science Society, July 29-August 1, 2009, Vrije Universiteit, Amsterdam, Netherlands. 
Roy, D., Patel, R., DeCamp, P., Kubat, R., Fleischman, M., Roy, B., Mavridis, N., Tellex, S., Salata, A., Guinness, J., et al. (2006). The human speechome project. Lecture Notes in Computer Science, 4211, 192–196. 
Stoll, S. (2009). Crosslinguistic approaches to language acquisition. In E. Bavin (Ed.), The Cambridge handbook of child language (pp. 89–104). Cambridge: Cambridge University Press. 
Stoll, S. (2016). Studying language acquisition in different linguistic and cultural settings. In 
N. Bonvillain (Ed.), The Routledge handbook of linguistic anthropology (pp. 140–158). New 
York: Routledge. Stoll, S., & Bickel, B. (2013). Capturing diversity in language acquisition research. In B. Bickel, 
L. A. Grenoble, D. A. Peterson, & A. Timberlake (Eds.), Language typology and historical contingency: Studies in honor of Johanna Nichols (pp. 195–260). Amsterdam: Benjamins Publishing Company. 
Stoll, S., & Gries, S. (2009). How to measure development in corpora? An association-strength approach to characterizing development in corpora. Journal of Child Language, 36, 1075– 1090. 
Stoll, S., Bickel, B., Lieven, E., Banjade, G., Bhatta, T. N., Gaenszle, M., Paudyal, N. P., Pettigrew, J., Rai, I. P., Rai, M., & Rai, N. K. (2012). Nouns and verbs in Chintang: Children’s usage and surrounding adult speech. Journal of Child Language, 39, 284–321. 
Stoll, S., Lieven, E., Banjade, G., Bhatta, T. N., Gaenszle, M., Paudyal, N. P., Rai, M., Rai, 
N. K., Rai, I. P., Zakharko, T., Schikowski, R., & Bickel, B. (2019). Audiovisual corpus on the acquisition of Chintang by six children. Electronic resource. Tomasello, M. (2003). Constructing a language: A usage-based theory of language acquisition. Harvard: Harvard University Press. Tomasello, M., & Stahl, D. (2004). Sampling children’s spontaneous speech: How much is enough? Journal of Child Language, 31, 101–121. 
VanDam, M., Warlaumont, A. S., Bergelson, E., Cristia, A., Soderstrom, M., De Palma, P., & MacWhinney, B. (2016). Homebank: An online repository of daylong child-centered audio recordings. Seminars in Speech and Language 37(2), (pp. 128–142). New York: Thieme Medical Publishers. 
Chapter 15 Web Corpora 
Andrew Kehoe 
Abstract This chapter explores the increasingly important role of the web in corpus linguistic research. It describes the two main approaches adopted in the feld, which have been termed ‘web as corpus’ and ‘web for corpus’. The former approach attempts to extract linguistic examples directly from the web using standard search engines like Google or other more specialist tools, while the latter uses the web as a source of texts for the building of off-line corpora. The chapter examines the pitfalls of the entry-level ‘web as corpus’ approach before going on to describe in detail the steps involved in using the ‘web for corpus’ approach to build bespoke corpora by downloading data from the web. Through a series of examples from leading research in the feld, the chapter examines the signifcant new methodological challenges the web presents for linguistic study. The overall aim is to outline ways in which these challenges can be overcome through careful selection of data and use of appropriate software tools. 
15.1 Introduction 
Over the past two decades the internet has become an increasingly pervasive part of our lives, leading to signifcant changes in well-established ways of carrying out everyday tasks, from catching up with the news and keeping in touch with friends to grocery shopping and job hunting. The latest statistics suggest that there are over 4 billion internet users worldwide, with a growth rate of over 1000% since 2000.1 
It should come as no surprise then that, during the same period, the web has had a major impact on well-established ways of doing things in the feld of corpus linguistic research too. Part of this impact has involved the increased availability of 
1http://www.internetworldstats.com/stats.htm. Accessed 22 May 2019. 
A. Kehoe (•) Birmingham City University, Birmingham, UK e-mail: andrew.kehoe@bcu.ac.uk 
© Springer Nature Switzerland AG 2020 329 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_15 
existing resources, with several of the corpora discussed in previous chapters now available online. For example, the British National Corpus (BNC) is searchable on the Brigham Young University website,2 meaning it is no longer essential for researchers to install the corpus and associated software on their own local computers. The BNC is not a ‘web corpus’ as such but it is a corpus that is now available to search via the web. 
What this chapter is more interested in, however, is the growing use of the web itself as a linguistic resource. As the chapter will go on to explain, this research area has diversifed in recent years to include a wide range of different activities but what they have in common is the use of linguistic data from the web, either in place of data from standard corpora like the BNC or to supplement it. The key benefts of the web over such corpora are its size and the fact that it is constantly updated with new texts and, thus, examples of the latest language use. Even a 100 million word corpus like the BNC is too small for some purposes, such as lexicographic and collocational research. Most words in the BNC occur fewer than 50 times, which makes it diffcult to draw frm conclusions about their meaning (Kilgarriff and Grefenstette 2003). Pomikàlek et al. (2009) give specifc examples: the verb hector with only 37 occurrences and the noun heebie-jeebies with none. At the time of writing, a Google search for heebie-jeebies returns over 970,000 hits, which would seem to offer exciting new possibilities for corpus linguistic research. However, as this chapter will go on to illustrate, the web also presents us with a substantial set of new challenges. Some of these are methodological and can be overcome through the use of appropriate techniques and software tools, whereas others require us to rethink the fundamental principles of corpus linguistics as a discipline. 
15.2 Fundamentals 
There are two main approaches to the use of web data in corpus linguistic research, which have been termed ‘web as corpus’ and ‘web for corpus’ (de Schryver 2002). Each has several variants which are described in turn below. It is worth noting here that, although ‘web as corpus’ and ‘web for corpus’ are distinct approaches, the former is sometimes used as an umbrella term for the whole research area, for example in the book title Web As Corpus: Theory and Practice (Gatto 2014)or in the name of the ACL SIGWAC: Special Interest Group on Web As Corpus.3 This refects the fact that the ‘web as corpus’ approach was the frst of the two to emerge in the late 1990s. 
2http://corpus.byu.edu/bnc/. Accessed 22 May 2019. 3http://www.sigwac.org.uk/. Accessed 22 May 2019. 
15.2.1 Web as Corpus 
This ‘entry level’ approach uses commercial search engines such as Google to access the textual content of the web. When considering the term ‘web as corpus’, the frst question we must ask is whether the web can actually be classed as a corpus according to the criteria set out in Chap. 1. On the surface there are similarities between conventional corpora and the web, which have led some researchers to refer to the latter as a ‘cybercorpus’ (Brekke 2000:227) or ‘supercorpus’ (Bergh 2005:26). Like the corpora discussed in previous chapters, the web contains large quantities of textual data that can be explored through dedicated search interfaces. However, if we consider the issue in more depth it becomes clear that the web does not meet several of the key defning criteria of a corpus. Sinclair (2005)offersa succinct summary: 
The World Wide Web is not a corpus, because its dimensions are unknown and constantly changing, and because it has not been designed from a linguistic perspective. At present it is quite mysterious, because the search engines [ ...] are all different, none of them are comprehensive, and it is not at all clear what population is being sampled. 
Sinclair’s frst objection relates to corpus size. From the 1 million word Brown corpus of the 1960s to the 100 million word BNC of the 1990s, the resources used by researchers in the feld have conventionally been of known (usually fnite) size. The web, in contrast, is indeed ‘quite mysterious’ (Sinclair 2005). Over a decade after Sinclair made this statement, although we know that the web has grown in size by many orders of magnitude, we are still no closer to knowing exactly how large it is. Search engine companies such as Google release relevant information publicly from time to time (e.g. Brin and Page 1998) but, for commercial reasons, it is usually in their interests to remain as mysterious as possible. We do know that the web is much larger than any conventional corpus. Early estimates by linguists took the ‘hit’ counts returned by search engines for specifc words then used the frequencies of those words in conventional corpora to extrapolate the size of the web (Bergh et al. 1998). Researchers in other felds tend to measure the size of web collections in terms of number of pages rather than number of words, but their methods are no more advanced. For example, recent research in the feld of ‘Webometrics’ used an almost identical technique to Bergh et al. (1998), estimating the size of the web to be just under 50 billion pages (van den Bosch et al. 2016).4 However, estimates vary wildly, with researchers at Google reporting in March 2013 that their software 
4See the website by the same authors for latest estimates: http://www.worldwidewebsize.com/. Accessed 22 May 2019. 
was aware of 30 trillion pages5; a fgure that had risen to 130 trillion by November 2016.6 
Sinclair’s second objection relates to the composition of the web. When we conduct a search using Google we are given no indication of the status of the matching texts it returns. We do not know whether a text has been through a careful editing process or whether it contains spontaneous thoughts. Often we cannot determine when a text was published and whether it has been edited since (Kehoe 2006). It can be diffcult to discover the intended purpose or audience of a text, and whether it was written by a native-speaker. Sometimes we are given no indication of the author’s identity at all, and it may even be the case that the text was generated or translated automatically by a computer. It is only in the last few years that linguists have begun to answer these questions by developing techniques to analyse web content on a large scale (Biber et al. 2015 – see Representative Study 
1) but there is still work to be done. 
An example will illustrate the limitations we face if we attempt to treat the web as a corpus using conventional search engines. Figure 15.1 shows the results of a Google search for the phrase genius idea, designed to investigate a change in the meaning of genius that has taken place too recently to be found in standard reference corpora like the BNC (most recent text from 1994). According to Rundell (2009), this is a noun ‘hovering on the brink of adjective-hood’, used in contexts were ingenious would previously have been found. The immediate problem we face is that web search engines do not allow grammatical searches so we cannot specify that we want to see only instances of genius used as an adjective. We therefore search for a phrase made up of genius followed by the word idea, which we might expect to collocate with genius in its adjectival sense (we could also try words such as plan, response, touch or move). 
Figure 15.1 was generated in late 2016. If we were to re-run the Google search the results would probably look quite different. A second major problem with accessing the web as a corpus through commercial search engines is that results change frequently as the search engines update their indexes of web content. This has a signifcant effect on the reproducibility of fndings, which is an important consideration in all scientifc research. There is a delicate balance here: as linguists we want to keep pace with the latest developments in language use but we also want to retain control of exactly what appears in our corpus at any given point in time. 
There are other, more specifc problems in Fig. 15.1 too. The frst is that Google ignores case, meaning that some of the ‘matches’ for genius idea are capitalised proper nouns (e.g. ‘Genius Idea Studios, LLC’). Case-sensitivity is vital in linguistic 
5https://search.googleblog.com/2013/03/billions-of-times-day-in-blink-of-eye.html. Accessed 22 May 2019. This refers to the number of pages the Google software was aware of at that time, not the number of pages actually held in its index. 
6https://searchengineland.com/googles-search-indexes-hits-130-trillion-pages-documents-263378. Accessed 22 May 2019. This information has since been removed from the Google website and updated fgures are no longer provided. 

search but there is no way to specify it in Google. Secondly, Google shows only one match from each website within a limited context (the concordance span in corpus linguistic terms). The only way to extract a full set of examples within a usable context is to click on each link in turn, locate each match manually and copy it to a fle. 
Finally, although Google claims to have available ‘About 818,000 results’ in Fig. 15.1, there is no way to view all of these through its web interface. We also need to bear in mind that this ‘hit’ count represents the number of pages in the Google index containing the search term, not the actual frequency of the term in the index. Some researchers (e.g. Keller and Lapata 2003; Schmied 2006) have attempted to use Google hit counts in linguistic research but the results must be treated with caution as we cannot be sure exactly what the fgures reported by Google represent. Indeed, a study by Rayson et al. (2012:29) demonstrated that Google hit counts ‘are often highly misleading, infating results drastically’ and are thus a poor indication of the number of matching pages, let alone of actual search term frequency. 
Since the late 1990s when linguists began to turn to the web as a corpus, search engines have actually become less advanced in terms of the options they offer. Full regular expression search has never been possible but useful features for linguistic study such as wildcards and the ‘NEAR’ operator (for fnding one word in close proximity to another) have gradually been removed over the years. Commercial search engines are geared towards information retrieval rather than the extraction of linguistic data. What they do, they do increasingly well but they are less than ideal for linguistic research. 
This is why several tools have been developed to ‘piggyback’ on commercial search engines and add layers of refnement specifcally for linguistic study, including KWiCFinder (Fletcher 2004), WebCONC (Hing 2001) and the only one still operational at the time of writing WebCorp (Kehoe and Renouf 2002). WebCorp Live, as it is now known, operates by sending the search term to a commercial search engine like Google, extracting the ‘hit’ URLs from the results page, and then accessing each URL directly to gather a full set of matches. These are then presented in the familiar Key Word in Context (KWIC) format, which can be sorted alphabetically or by date. WebCorp also post-processes search engine results to offer case-sensitivity and pattern matching options. An extract of WebCorp Live output for genius idea (case-sensitive and restricted to UK broadsheet newspaper websites) is shown in Fig. 15.2. 
Although WebCorp Live offers advantages over direct use of commercial search engines (and is still widely used for this reason), it does not solve the underlying problems of the web as corpus approach. Quantitative analysis – one of the core activities of corpus linguistic research – is not possible as we do not know the total size of the web ‘corpus’ held on the search engines’ servers. It is also unclear exactly how the search engines decide upon ‘relevant’ matches for a query. Google results are selected and sorted by a proprietary measure of relevance (PageRank: Brin and Page 1998), which is altered regularly in unpredictable and undocumented ways. This problem has become more acute in recent years following the introduction of personalised search results based on factors such as geographic location and previous web activity. 
A fnal, more practical problem is that search engines are becoming increasingly diffcult to use in linguistic study at all. WebCorp Live originally used a process known as ‘web scraping’: the extraction of useful information from the HTML code of a web page, in this case the ‘hit’ URLs from the Google results page and examples of the search term from each of the ‘hit’ pages. Other linguists have used similar techniques, writing scripts in programming languages such as Perl, R, and Python, but this is no longer possible as search engines now block access from software other than recognised web browsers such as Internet Explorer and Chrome. WebCorp Live now uses the Application Programming Interfaces (APIs) provided by search engine companies to give controlled access to their indexes. Unfortunately, in the last few years many popular search engines have either begun to charge a fee for API access 

(e.g. Bing) or have shut down their APIs entirely (Google). This has further reduced the usefulness of search engines for linguistic research. 
Few researchers would now claim that the web is a corpus in any meaningful sense, but the web as corpus approach can still be fruitful for certain kinds of research and it is particularly useful for introducing newcomers to the feld. Perhaps a more suitable term for the activities described in this section is ‘web as corpus surrogate’ (Bernardini et al. 2006). This refects the fact that, although we are aware the web is not a corpus in a conventional sense, it may be the ‘next best thing’ when no other suitable corpora are available and researchers lack the necessary expertise to build them. 
One fnal point to be made in this section is that there has been a growth in recent years in research outside the feld of corpus linguistics which uses web data and standard web search tools to answer what are essentially linguistic questions. So-called ‘culturomics’ (Michel et al. 2011) is the process of mining large digital collections to fnd words and phrases likely to represent important cultural phenomena, for example the fact that the word slavery “peaked during the civil war (early 1860s) and then again during the civil rights movement (1955-1968)” (Michel et al. 2011:177). The corpus surrogate of choice in this feld is often the Google Books archive, which presents a number of problems both in terms of accuracy of digitisation and fexibility of the search interface. A related concept is the rather ill-defned ‘big data’, which has also become a buzz phrase in recent years but which does not appear to offer anything new to corpus linguists. 
15.2.2 Web for Corpus 
The second major strand of web-based linguistic study has seen researchers attempt to overcome the limitations described above. This web for corpus approach has also been referred to as ‘web as corpus shop’ (Bernardini et al. 2006), meaning that the web becomes a store from which particular texts can be downloaded and used to build large off-line corpora. The key advantage over the ‘web as corpus surrogate’ approach is that we have control of what goes into our corpora, making it possible to design a corpus that represents something meaningful. Importantly, we also know exactly how large our corpus is – making quantitative analyses possible – and we can carry out more advanced linguistic processing, such as part-of-speech tagging. Mair (2012) refers to this as a shift from ‘opportunistic’ to ‘systematic’ use of the web as a corpus. 
However, although the web for corpus approach is considerably more systematic than the web as corpus surrogate approach described above, it is not fully com­parable with the conventional corpus compilation process (see Chap. 1). The key issue here is one of representativeness, which ‘means that the study of a corpus (or combination of corpora) can stand proxy for the study of some entire language or variety of a language’ (Leech 2007:135). As Leech points out, this ‘holy grail’ was achievable in conventional corpora because their compilers had a clear idea of the total population, or ‘textual universe’ (Leech 2007:135), from which they were sampling. For instance, the compilers of the Brown corpus (Kucera and Nelson Francis 1967) used library catalogues to sample texts published in the US in 1961. When the textual universe is the web, things are not quite so straightforward. 
In fact, early web navigational aids such as the Yahoo! Directory attempted to impose library-style hierarchical classifcation systems on web texts, with human editors employed to curate content by subject. However, this approach was not scalable to the increasingly vast web and such directories have now largely been replaced by keyword-based search engines like Google. This means we are still largely dependent on such search engines as gatekeepers to the textual content of the web. However, the ‘web for corpus’ approach utilises search engines in a different way, using them at the initial stage of corpus building only. This approach was popularised by Baroni and Bernardini (2004) with their BootCaT tool but it has been used by several other researchers since. 
BootCaT is available as a user-friendly but rather limited front-end or as a series of command line scripts for more advanced users. We will describe the front-end here to introduce the general principles. The frst step in the process is to supply a list of ‘seed’ words from which the corpus will be grown by the software. The type of corpus required will determine what seeds should be chosen. The BootCaT manual gives a simple example for the building of a domain-specifc corpus on dogs, with the seeds dog, Fido, food hygiene, leash, breeds, and pet. The software takes these seeds and combines them into tuples with a length of three, e.g. Fido leash dog. All possible unique tuples are generated from the seeds supplied and each in turn is then sent to Bing using its API (previously Google). The assumption is that it is possible to build a corpus covering a particular domain (in this case dogs) by using a commercial search engine to fnd web pages containing words likely to occur in that domain. As an initial step, BootCaT fetches 10 hits from Bing for each tuple then downloads and processes the corresponding web pages to build a corpus in the form of a text fle. 
Although this example is rather basic, the same underlying principle has been used to build much larger reference corpora, by the BootCaT team and by other researchers. Sharoff (2006a, b) built ‘BNC-like’ 100 million words corpora of English, Chinese, German, Romanian, Ukrainian and Russian from the web. Later, Baroni et al. (2009) built corpora of 1 billion words each for English, German and Italian, while Kilgarriff et al. (2010) built corpora of up to 100 million words each for Dutch, Hindi, Indonesian, Norwegian, Swedish, Telugu, Thai and Vietnamese. More recently, the COW project (Schäfer and Bildhauer 2012) has built ‘gigaword’ corpora using a similar approach. The web has been used to construct corpora of World Englishes too, the best known example being the 1.9 billion word Corpus of Global Web-based English (GloWbE) which includes sub-corpora from 20 countries (Davies and Fuchs 2015; see Representative Corpus 1). 
When building general reference corpora from the web we need to choose seed words that are likely to appear across a wide range of topics, but there has been some debate about exactly what constitute good seeds for a corpus of this type. Sharoff (2006a) used a word frequency list from the BNC, relying on random combinations of four non-grammatical words (e.g. work room hand possible)to ensure that search engine matches were not overly biased towards particular topics. Ciaramita & Baroni (2006:153) experimented with sets of words from the Brown corpus, concluding that the best seeds are ‘words that are neither too frequent nor too rare’. Kilgarriff et al. (2010) are a little more specifc, ignoring the top 1000 high frequency words in the language and using the next 5000 as seeds. 
Whatever seeds are chosen, this is only the frst step in the process of building a corpus from the web. In all but the most basic examples, it is likely that the researcher will want to expand the corpus beyond the initial set of seeds. There are two main ways of achieving this: (i) through the addition of further seeds, or (ii) by using a web crawler. For the frst option, the more advanced BootCaT command line scripts add another stage where further seeds are extracted by comparing the initial corpus with an existing more general corpus, e.g. the basic dogs corpus may include key words such as canine, labrador, and barks which can be combined and sent to the search engine to extract further hits. Hence, the full BootCaT approach is one of ‘bootstrapping’ or iterative refnement. 
This approach is suitable for the building of domain-specifc corpora but the second approach, the use of a web crawler, is more appropriate for building of general reference corpora. In simple terms, crawlers (sometimes known as spiders) start with an initial list of URLs, download the corresponding documents, extract hyperlinks from these documents, then add these to the list of URLs to be crawled. In theory this process could run indefnitely but crawls run for corpus-building purposes tend to be restricted to a fxed time period. The most popular crawler in web for corpus research is the open-source Heritrix system (used by Kehoe and Gee 2007 amongst others). However, there are alternatives available, including the command line tool Wget (used by Kilgarriff et al. 2010) and HTTrack (used by Biber et al. 2015). Another option is SpiderLing (Suchomel and Pomikálek 2012), a crawler designed specifcally for the building of linguistic corpora. 
Whichever tool is chosen, it is important to crawl the web in a responsible manner, observing the robots exclusion standard. This allows website owners to specify (usually in a fle called ‘robots.txt’) which parts of a site should not be crawled. It is also important that crawling takes place as slowly as the name suggests. A crawler should not be confgured to download multiple pages from the same website simultaneously or in quick succession as this may have an impact on access speeds for standard users of the site. 
When the crawl is eventually complete, several other steps are usually carried out to ‘clean up’ the downloaded web documents before they are added to a corpus. The user-friendly BootCaT front-end carries out some of these tasks automatically but most researchers opt to use dedicated tools for these individual tasks to retain more control over the process. There are a number of options available for each task in the corpus building ‘pipeline’, as outlined below: 
Boilerplate Removal The term ‘boilerplate’ is commonly used to refer to features such as navigation menus, copyright notices and advertising banners which do not form part of the main textual content of a web page. As such features are often repeated verbatim across multiple pages on a single website, it is desirable to remove them to prevent the words they contain from becoming unnaturally frequent in the resulting corpus. The boilerplate removal process usually goes hand in hand with the stripping of HTML markup code from web documents as HTML can offer clues about what is boilerplate and what is the main textual content. Several web for corpus projects (e.g. Ciaramita and Baroni 2006; Kehoe and Gee 2007)haveused or adapted the Body Text Extraction (BTE) algorithm for boilerplate removal. An increasingly popular alternative is the linguistically-aware jusText tool, and a further option is texrex, which performs boilerplate removal as well as several other text ‘clean up’ tasks. It is also possible to remove only HTML code without considering boilerplate by using open-source tools such as html2text. Similar tools are available for the extraction of text from PDF and Word documents, formats which are far less likely to contain boilerplate. It is also worth mentioning Apache Tika here: a library designed to extract text from a wide variety of document formats. 
Document Filtering After HTML code and boilerplate have been removed, it is often useful to apply additional flters to the downloaded documents. The exact choice of flters is dependent on the intended nature of the corpus and research aims, but size and language flters are amongst the most common. Size flters are designed to remove very short and very long documents from the corpus. The assumption is that very short documents are unlikely to contain much connected prose, while very long documents are likely to skew the corpus unnaturally. Cavaglia and Kilgarriff (2001) and Ide et al. (2002) specify the minimum acceptable size of a web document as 2000 tokens. This may be overly strict, however, as Kehoe and Gee (2007) found that less than 4% of HTML documents in a large web crawl met the 2000 token threshold. Ciaramita and Baroni (2006) apply an additional flter, which states that for a text to be included at least 25% of its tokens must come from the top 200 most frequent tokens in the Brown corpus. This is intended partly as a spam flter and partly as a way of ensuring that all texts in their corpus are in English. Separate language detection libraries are also available in many programming languages. 
Duplicate Removal The nature of the web means that not all documents are unique. In some cases, multiple URLs will point to the same document, while in others the same document will appear on multiple websites (mirror sites and archives). The latter also happens with content released by news agencies, which is reused on many sites worldwide. As a result, any web-derived corpus is likely to include documents that are either exact duplicates of one another or are very similar. Like boilerplate, such documents can skew word frequency counts so it is desirable to remove them from the corpus. It is possible to do this by writing software that produces a ‘fngerprint’ (or hash) for each document and then compares these to fnd similarities. Software libraries are available in several programming languages to help with this, including the Text::DeDuper module in Perl. An easier alternative is to use an ‘off the shelf’ tool for the automatic detection of duplicate and near-duplicate texts, such as Onion (ONe Instance ONly). 
After these steps have been performed, the corpus can then be tokenised, lemmatised and part-of-speech tagged using the standard approaches described in Chap. 2. One thing to bear in mind when dealing with web data is that it can be rather ‘noisy’. For instance, it may lack punctuation or whitespace and there may be a higher proportion of non-standard spellings than in conventional texts. These factors may have an impact on the accuracy of corpus annotation since the linguistic models underpinning off-the-shelf annotation tools are usually derived from standard written language. For example, most popular part-of-speech taggers are trained on newspaper text, a register which tends to follow fairly strict style guides and contain few errors. Giesbrecht and Evert (2009) offer a useful case study on this topic, examining the effectiveness of fve part-of-speech taggers on a web-derived corpus of German. They fnd that tagging accuracy drops from the advertised 97% to below 93% when applied to web data. The accuracy of such taggers can be improved through additional text ‘clean-up’ or the modifcation of tagging rules, but problems encountered in applying other annotation methods (e.g. dependency parsing) to web data are more diffcult to resolve and few researchers have attempted to do so. 
It is important to stress that the decisions made at each stage of the web corpus 
building process will have a signifcant impact on the resulting corpus, in terms of size but also in terms of composition. Activities such as boilerplate stripping, deduplication and additional fltering can remove a considerable proportion of the documents retrieved through web crawling. In an extreme case, Schäfer and Bild­hauer (2013) found that post-processing eliminated over 94% of the web documents they had downloaded (with over 70% of these being very short documents). This is worth bearing in mind when aiming for a specifc corpus size as it may be necessary to download signifcantly more documents than initially expected. It is also important to monitor all stages of the pipeline carefully to ensure that each annotation and fltering tool is having the intended effect. Relevant questions to consider are likely to include what exactly counts as boilerplate, how similar two documents must be to be considered (near) duplicates, and whether testing for duplicates should occur at sentence, paragraph or whole document level. 
The approach to crawling described so far is one designed to maximise the number of documents downloaded and, thus, maximise the size of the fnal corpus. In recent years most compilers of web corpora have worked on the assumption that maximising corpus size is likely to result in a more representative corpus, even if we have no reliable way of measuring this (see Representative Study 2 for a discussion of the related topic of balance in web corpora). Some researchers have gone further still, dismissing the notion that representativeness is achievable or important in web corpora. Schäfer & Bildhauer (2012:486) adopt ‘a more or less agnostic position towards the question of representativeness’, while Kilgarriff & Grefenstette (2003:11) argue that the web ‘is not representative of anything else. But nor are other corpora, in any well understood sense’. 
An alternative approach to large indiscriminate crawls is to focus on specifc websites, thus limiting the size of the textual universe and increasing the chances of building a representative corpus. For example, some newspapers make archives of articles from previous years available online. These can be downloaded by pointing the crawler to the homepage and instructing it to follow links within the site only, or through the use of an API if the newspaper makes one available. An advantage of this focussed (or ‘scoped’) approach to crawling is that there is unlikely to be much spam or duplication of content and all documents are likely to be in a similar format, making boilerplate removal more straightforward. With news texts it is also much easier to determine publication date: something which can be very diffcult to do on the web in general (see Representative Corpus 1 for information on the NOW corpus). 
Blogs offer similar advantages when it comes to boilerplate removal and text dating (see Representative Corpus 2 for more information on the Birmingham Blog Corpus) and are one variety of so-called ‘Web 2.0’ content, which is being used increasingly in linguistic research. Blogs are similar to conventional web texts in terms of layout and in the fact that they can be located using commercial search engines. This is not true of all Web 2.0 texts though, and corpus linguists have had to devise new methods to access the language of social media sites. There has always been a so-called ‘deep web’ (Bergman 2001) of information beyond the reach of standard search engines in password-protected archives and databases. However, this hidden world below the surface of the web has increased in size substantially in recent years with the growth of social media sites such as Facebook and Twitter which are not fully indexed by Google. In general, it is very diffcult to download textual content from Facebook as it is necessary to be logged in and ‘friends’ with the person who published the content in order to access it. Facebook Pages dedicated to specifc topics and interests are the exception as these can be accessed without logging in (though Facebook’s terms and conditions should always be observed). 
Twitter is easier to access for crawling purposes and it is now being used more widely in linguistic studies as a result. Early studies tended to download tweets manually but there are now a number of automated tools available. Options include the FireAnt package designed specifcally for corpus linguists, as well as general purpose software such as IFTTT, Import.io, and TAGS. The last of these, standing for Twitter Archiving Google Sheet, automatically downloads tweets matching particular search parameters into a spreadsheet which can subsequently be used to build a corpus. Whether existing corpus methods are entirely suitable for the analysis of texts of 280 characters or fewer is a separate question requiring further research, but it is certainly possible to build Twitter corpora and conduct interesting linguistic analyses of them (e.g. Page 2014; Huang et al. 2016). Building a large-scale, publicly-accessible Twitter corpus is a bigger challenge, and previous attempts such as the Edinburgh Twitter Corpus (Petrovi´
cetal. 2010) have met with opposition from Twitter itself. 
As can be seen from the above discussion, the ‘web for corpus’ approach is rather more complex than the previously described ‘web as corpus surrogate’ approach. The building of bespoke corpora from the web usually requires considerable technical skill and, thus, may not be an option for all corpus linguists, especially beginners. Fortunately, there are other options. Several teams of researchers have already built large web-derived corpora and made them available to download or search online (see Sect. 15.4). There is also the commercial Sketch Engine tool, which is now available free of charge to academic users within the EU (2018–2022). Sketch Engine includes a range of pre-loaded corpora, including the TenTen web corpora, and makes these available through a novel search interface. In addition, Sketch Engine provides user-friendly tools that allow linguists to build their own web-derived corpora (based on the BootCaT technology discussed in Sect. 15.2.2). 
Representative Study 1 
Resnik, P., and Smith, N.A. 2003. The Web as a Parallel Corpus. Compu­tational Linguistics 29(3):349–380. 
Until relatively recently, corpus-based studies have tended to focus almost exclusively on the English language, and on British and American English in particular. The web offers new possibilities for the study of World Englishes and other languages for which there are no BNC-style reference corpora 
(continued) 
available. One specifc area where the web has had a transformational impact is in the building of parallel corpora for use in multilingual natural language processing. Work by Resnik & Smith on the STRAND system was pioneering in this feld. 
Parallel corpora are pairs of corpora in two different languages where the texts in one are translations of those in the other (also called bitexts; cf. Chap. 12). STRAND was designed to mine the web to fnd candidate texts automatically, ‘based on the insight that translated Web pages tend quite strongly to exhibit parallel structure, permitting them to be identifed even without looking at content’ (Resnik and Smith 2003:350). The original version of STRAND relied on the AltaVista search engine but, mirroring the wider shift from ‘web as corpus’ to ‘web for corpus’ in the feld, later versions added a bespoke web crawler. One technique used successfully to fnd bitexts was to search for words such as ‘english’, ‘french’, ‘anglais’ and ‘français’ in anchor text: the clickable text within hyperlinks. Another was to compare URLs in the Internet Archive,7 e.g. matching a URL containing ‘/english/’ with an otherwise identical one containing /arabic/. Resnik & Smith used bilingual speakers to assess the quality of the parallel corpora extracted by STRAND, reporting 100% precision and 68.6% recall for English-French web pages. 
More recent research has built on the foundations established by the STRAND project. For example, San Vicente and Manterola (2012)used anchor text, URL matching and HTML structure to build Basque-English, Spanish-English and Portuguese-English parallel corpora. Interestingly, they chose not to remove boilerplate as they found that navigation menus contain useful parallel information. 
Representative Study 2 
Biber, D., Egbert, J., and Davies, M. 2015. Exploring the composition of the searchable web: a corpus-based taxonomy of web registers. Corpora 10(1):11–45. 
The exact composition of the searchable web is something we know surpris­ingly little about as a research community, making it diffcult to assess the representativeness of our web-derived corpora. Related to this is the notion of a balanced corpus: one where ‘the size of its subcorpora (representing 
(continued) 
7http://archive.org/. Accessed 22 May 2019. 
particular genres or registers) is proportional to the relative frequency of occurrence of those genres in the language’s textual universe as a whole’ (Leech 2007:136; cf. also Chap. 1). The major challenge here is that the categorisation of documents by register in a web-scale collection must be done automatically, yet this is not possible unless we frst have a list of the possible categories. 
Biber et al. set out to develop such a list by extracting a random sample of 48,571 documents from GloWbE (see Representative Corpus 1) and using volunteers to classify them. Instead of taking a pre-defned set of registers, the project asked volunteers to identify ‘situational characteristics’, e.g. whether a written text was interactive or non-interactive, and whether non-interactive texts were designed to narrate events, explain information, express opinion, etc. The key fnding was that the majority (31.2%) of web documents belong to the Narrative register, with a further 29% classifed as ‘Hybrid’ (Narration+Information, Narration+Opinion, etc.). Within the Narrative reg­ister, over half the documents were classifed as ‘News report/blog’. 
Some researchers have criticised the approach adopted by Biber et al., pointing out that they remain heavily reliant on Google as gatekeeper to the web because the documents analysed come from a corpus seeded by Google queries (Schäfer 2016). Despite this limitation, however, Biber et al. are still able to offer important new insights on web content, concluding that ‘the most common registers found on the web are not those typically analysed in corpora of published written texts’ and ‘although the most widely analysed registers from published writing can be found on the web, they are typically rare in comparison to other web registers’ (Biber et al. 2015:26). The output from their analysis is available in the form of the CORE corpus (see Representative Corpus 1). 
Representative Corpus 1 
BYU corpora: COCA, GLoWbE, CORE and NOW 
The Corpus of Contemporary American English (COCA), Corpus of Global Web-based English (GloWbE), Corpus of Online Registers of English (CORE), and News On the Web (NOW) corpus are four in a series of corpora released by Mark Davies. 
COCA contains 20 million words of texts each year since 1990, split 
evenly between fve genres: spoken, fction, popular magazines, newspapers, 
and academic journals. It will be noted that these genres are very similar to 
(continued) 
those found in pre-web corpora such as the BNC, and therein lies one of the limitations of COCA. Although all texts were downloaded from the web using the ‘web for corpus’ approach, COCA was designed in such a way that it excludes web-native genres like blogs and social media. COCA is a web corpus in a very loose sense only. It is intended as a monitor corpus yet it does not contain examples of the latest trends in language use, which tend to be found in blogs and other less formal text types. 
This issue is addressed in the 1.9 billion word GloWbE and 50 million word CORE corpora. GloWbE was constructed using ‘web for corpus’ techniques, seeded through search engines queries. As explained in Davies and Fuchs (2015), the most common 3-grams were extracted from COCA (‘is not the’, etc.) and submitted to Google, with 80–100 hits downloaded for each 3-gram. Unlike in COCA, the texts in GloWbE are not restricted to American English and come from 20 different English-speaking countries. Around 60% of them are from blogs which, according to Davies and Fuchs (2015), makes GloWbE comparable with corpora in the International Corpus of English (ICE) family (which have a 60/40% split between speech and writing). Of course, GloWbE is much less carefully designed than the ICE corpora, and it remains to be determined whether blogs are truly comparable with speech, but GloWbE certainly offers a size advantage over the ICE corpora. 
The CORE corpus was derived from GloWbE as part of the project undertaken by Biber et al. (2015) discussed in Representative Study 2. Meanwhile, the NOW corpus has recency as its main focus, containing over 6 billion words of newspaper text from 2010 to present, with around 10,000 new articles added every day. This is thus a good example of the kind of focused crawling discussed in Sect. 15.2.2. 
Representative Corpus 2 
Birmingham Blog Corpus 
The Birmingham Blog Corpus (BBC) is a freely-searchable 630 million word collection downloaded from various blog hosting sites (Kehoe and Gee 2012). Of particular interest is the sub-corpus from WordPress and Blogger, which includes 95 million words of blog posts and a further 86 million words of reader comments on those posts. It was possible to separate posts and comments in this way as, although blogs on WordPress and Blogger are written by a wide range of people and cover many different topics, they make use of a small number of pre-defned templates. It was therefore relatively easy to identify the post and each individual comment during the crawling 
(continued) 
process, without the need for complex boilerplate removal techniques. The crawling for the WordPress and Blogger sub-corpus of the BBC took place without relying on commercial search engines. Instead, lists of popular blogs were taken from the hosting sites themselves and used as the initial set of blogs to crawl. When each post on each blog was processed, links to other WordPress and Blogger blogs were extracted and added to the crawling list, thus widening the scope of the corpus. 
Linguistic research based on the BBC has demonstrated the value of blog data in pragmatic analyses of online interaction (e.g. Lutzky and Kehoe 2017 on apologies). Such research has also identifed a shift in use of the blog format from its original ‘online diary’ focus, meaning that it is now possible to capture a wide variety of topics and communicative intentions in a corpus made up exclusively of blogs. 
15.3 Critical Assessment and Future Directions 
In this chapter we have examined a wide range of approaches which use the web as a linguistic resource, ranging from basic Google searches to large-scale crawling of web content. The web may not be a corpus in a conventional sense but, as we have seen, it can be a valuable corpus surrogate or, increasingly, a source of texts for the building of corpora. However, in their attempts to build large web-derived corpora, researchers continue to be hampered by a reliance on commercial search engines and a lack of detailed knowledge of the web’s structure and content. 
With it becoming increasingly diffcult to use search engines for linguistic research, both directly and as a way of seeding crawls, we must devise new ways of accessing web content. The scoped crawling approach is one solution, allowing us to focus our attention on specifc websites without relying on search engines as gatekeepers to the web. Lists of popular sites such as the Alexa web rankings may be a good starting point. Social media, particularly Twitter, is another increasingly useful source of textual data. 
As we have moved towards the ‘web for corpus’ approach, a new challenge has emerged concerning the legality of distributing corpora crawled from the web. The solutions adopted by corpus compilers vary, from limiting the context shown through the search interface (Mark Davies’ corpora) to releasing corpora for download with the sentences shuffed into a random order (COW corpora). This is still very much a grey area, with different laws applying across the world, and anyone planning to distribute a web-derived corpus should seek local advice. Within the European Union, the General Data Protection Regulation (GDPR)8 introduced in 2018 was designed to standardise laws and give citizens control of their personal data. At the time of writing, the implications of GDPR for web crawling in general and for web corpus building in particular are not completely clear. It remains good practice to confgure web crawlers so that they identify themselves clearly and crawl responsibly, obeying website terms of service. It is also good practice to carry out an assessment of all data being stored – in this case the corpus – to ensure that the chances of an individual person being identifed are minimised. 
In the longer term, the ideal solution for web corpus research would be a search engine designed specifcally for linguistic study: a Google-scale resource providing the search options and post-processing tools we require to extract examples of language in use from the web. There have been attempts in the past but most have proved to be unsustainable as academic funding models do not allow the continuous expansion of disk storage space required for regular web crawling or the bandwidth required to allow multiple users to carry out complex searches on a large corpus simultaneously. With increasing interest in the use of web data in linguistic research and deepening knowledge of web content (e.g. Biber et al. 2015; see also Biber and Egbert 2016, 2018 for multidimensional analyses of web registers), now may be an appropriate time for researchers to pool resources and harness the full potential of the web as a linguistic resource. 
15.4 Tools and Resources 
15.4.1 Web Corpora 
Aranea – searchable and downloadable web-derived corpora in English, French, German, and a range of other languages: http://ucts.uniba.sk/aranea_about/. Accessed 21 May 2019. 
Birmingham Blog Corpus – searchable 630 million word corpus of blog posts and reader comments downloaded from popular hosting sites: http://wse1.webcorp. org.uk/home/blogs.html. Accessed 22 May 2019. 
COCA – over 500 million words of American English texts published since 1990, downloaded from and searchable via the web: http://corpus.byu.edu/coca/. Accessed 21 May 2019. 
CORE – searchable 50 million word sub-set of GloWbE, categorised by register: http://corpus.byu.edu/core/. Accessed 21 May 2019. 
COW – searchable and downloadable web-derived corpora in English, French, German, Spanish, Dutch and Swedish (shuffed sentences only, not full texts): http://corporafromtheweb.org/. Accessed 21 May 2019. 
8https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32016R0679. Accessed 22 May 2019. 
countries: http://corpus.byu.edu/glowbe/. Accessed 21 May 2019. 
Leipzig Collection – searchable web-derived corpora in English, French, German, Arabic and Russian: http://corpora.informatik.uni-leipzig.de/. Accessed 21 May 2019. 
NOW – searchable corpus of news texts newspaper text from 2010 to present, with around 10,000 articles added daily: http://corpus.byu.edu/now/. Accessed 21 May 2019. 
TenTen – a collection of web-derived corpora available through Sketch Engine covering over 30 languages, each containing at least 10 billion words: http:// www.sketchengine.eu/documentation/tenten-corpora/. Accessed 21 May 2019. 
WaCKy – downloadable web-derived corpora in English, French, German and Ital­ian: http://wacky.sslmit.unibo.it/doku.php?id=corpora. Accessed 21 May 2019. 
WebCorp Live – searches the web in real-time for words or phrases, adding refnement options to standard search engines; can also produce a word list and n-grams for any web text: http://www.webcorp.org.uk/. Accessed 21 May 2019. 
15.4.2 Crawling and Text Processing 
Alexa Web Rankings – helpful in locating popular websites as initial seeds for a crawl: http://www.alexa.com/topsites. Accessed 21 May 2019. 
Apache Tika – extracts text and detects metadata from fles: http://tika.apache.org/. Accessed 21 May 2019. 
Body Text Extraction (BTE) – boilerplate removal tool taking into account docu­ment structure: http://www.aidanf.net/posts/bte-body-text-extraction. Accessed 21 May 2019. 
BootCaT – pipeline of tools for seeding and building web corpora: http://bootcat. dipintra.it/. Accessed 21 May 2019. 
FireAnt – freeware designed to build corpora from Twitter, with built-in visualisa­tion tools (time-series, geo-location): http://www.laurenceanthony.net/software/ freant/. Accessed 21 May 2019. 
Heritrix – large-scale web crawler: https://github.com/internetarchive/heritrix3/ wiki. Accessed 21 May 2019. 
HTTrack – downloads a single website to a local computer: https://www.httrack. com/. Accessed 21 May 2019. 
IFTTT – allows use of simple conditional statements to download data from popular websites: https://ifttt.com/. Accessed 21 May 2019. 
Import.io – extracts, processes and visualises data from the web: https://www. import.io/. Accessed 21 May 2019. 
jusText – boilerplate removal tool: http://corpus.tools/wiki/Justext. Accessed 21 May 2019. 
ONe Instance ONly (Onion) – duplicate text remover: http://corpus.tools/wiki/ Onion. Accessed 21 May 2019. 
Sketch Engine – a corpus search infrastructure containing a range of pre­loaded corpora (including TenTen). Includes tools for building web corpora (based on BootCaT). The open-source version NoSketch Engine has a more restricted search interface and no pre-loaded corpora: http://www.sketchengine. eu/nosketch-engine/. Accessed 21 May 2019. 
TAGS – Google Sheets template for automated collection of search results from Twitter: http://tags.hawksey.info/. Accessed 21 May 2019. 
Texrex – full web document cleaning tool; removes duplicates, detects boiler­plate, extracts metadata (part of COW project): http://github.com/rsling/texrex. Accessed 21 May 2019. 
Text::DeDuper – duplicate text remover: https://metacpan.org/pod/Text::DeDuper. Accessed 30 September 2020. Wget – simple HTTP download tool: https://www.gnu.org/software/wget/. Accessed 21 May 2019. 
Further Reading 
Schäfer, R., and Bildhauer, F. 2013 Web Corpus Construction. San Rafael: Morgan & Claypool. 
This book offers a fuller discussion of the technical issues involved in web crawling for linguistic purposes. It includes chapters on web structure, seed word selection and crawling, post-processing, annotation, and corpus evaluation. 
Biber, D., and Egbert, J. 2018. Register Variation Online. Cambridge: CUP. 
Building on the work by Biber et al. (2015) discussed in Representative Study 2, this volume examines the full range of registers found on the searchable web. It explores overall patterns of register variation with a multidimensional analysis and discusses the main lexical, grammatical and situational features of each register, offering important new insights on the language of the web. 
Hundt, M., Nesselhauf, N., and Biewer, C. 2007. Corpus Linguistics and the Web. Amsterdam: Rodopi. 
This was the frst book-length publication to bring together key perspectives in web corpus research. It includes the chapter on representativeness and balance cited in Sect. 15.2.2 as well as chapters on both the new possibilities and new challenges presented by the web as/for corpus approaches. 
References 
Baroni, M., & Bernardini, S. (2004). BootCaT: Bootstrapping corpora and terms from the web. Proceedings of LREC, 2004, 1313–1316. 
Baroni, M., Bernardini, S., Ferraresi, A., & Zanchetta, E. (2009). The WaCky wide web: A collection of very large linguistically processed web-crawled corpora. Language Resources & Evaluation, 43, 209–226. 
Bergh, G. (2005). Min(d)ing English language data on the web: What can Google tell us? ICAME Journal, 29, 25–46. 
Bergh, G., Seppänen, A., & Trotta, J. (1998). Language corpora and the internet: A joint linguistic resource. In A. Renouf (Ed.), Explorations in corpus linguistics (pp. 41–54). Amsterdam: Rodopi. 
Bergman, M. K. (2001). The deep web: Surfacing hidden value. Journal of Electronic Publishing, 7(1). 
Bernardini, S., Baroni, M., & Evert, S. (2006). A WaCky introduction. In M. Baroni & S. Bernardini (Eds.), Wacky! Working papers on the web as Corpus (pp. 9–40). Bologna: GEDIT. http://wackybook.sslmit.unibo.it/pdfs/bernardini.pdf. Accessed 21 May 2019. 
Biber, D., & Egbert, J. (2016). Register variation on the searchable web : A multi-dimensional analysis. Journal of English Linguistics, 44(2), 95–137. 
Biber, D., & Egbert, J. (2018). Register variation online. Cambridge: Cambridge University Press. 
Biber, D., Egbert, J., & Davies, M. (2015). Exploring the composition of the searchable web: A corpus-based taxonomy of web registers. Corpora, 10(1), 11–45. 
Brekke, M. (2000). From BNC to the cybercorpus: A quantum leap into chaos? In J. Kirk (Ed.), Corpora Galore (pp. 227–247). Amsterdam: Rodopi. 
Brin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual web search engine. Computer Networks, 30(1–7), 107–117. 
Cavaglia, G., & Kilgarriff, A. (2001). Corpora from the web (Information Technology Research Institute Technical Report Series (ITRI-01-06)). University of Brighton. https:// www.kilgarriff.co.uk/Publications/2001-CavagliaKilg-CLUK.pdf. Accessed 21 May 2019. 
Ciaramita, M., & Baroni, M. (2006). Measuring web-corpus randomness. In M. Baroni & S. Bernardini (Eds.), Wacky! Working papers on the web as corpus (pp. 127–158). Bologna: GEDIT. http://wackybook.sslmit.unibo.it/pdfs/ciaramita.pdf. Accessed 21 May 2019. 
Davies, M., & Fuchs, R. (2015). Expanding horizons in the study of world Englishes with the 1.9 billion word global web-based English Corpus. English World-Wide, 36(1), 1–28. 
de Schryver, G. (2002). Web for/as corpus: A perspective for the African languages. Nordic Journal of African Studies, 11(2), 266–282. 
Fletcher, W. H. (2004). Making the web more useful as a source for linguistic corpora. In U. Connor & T. Upton (Eds.), Applied Corpus linguistics: A multidimensional perspective (pp. 191–205). Amsterdam: Rodopi. 
Gatto, M. (2014). Web as corpus: Theory and practice. London: Bloomsbury. 
Giesbrecht, E., & Evert, S. (2009). Is part-of-speech tagging a solved task? An evaluation of POS taggers for the German web as corpus. In Proceedings of the 5th Web as Corpus Workshop (WAC5). San Sebastian: Spain. 
Huang, Y., Guo, D., Kasakoff, A., & Grieve, J. (2016). Understanding U.S. regional linguistic variation with Twitter data analysis. Computers Environment and Urban Systems, 59, 244–255. 
Hing, M. (2001). WebCONC. http://www.niederlandistik.fu-berlin.de/cgi-bin/web-conc.cgi (no longer accessible). Accessed 21 May 2019. 
Ide, N., Reppen, R., & Suderman, K. (2002). The American National Corpus: More than the web can provide. In Proceedings of the 3rd language resources and evaluation conference (LREC) (pp. 839–844). Paris: ELRA. 
Kehoe, A. (2006). Diachronic linguistic analysis on the web using WebCorp. In A. Renouf & A. Kehoe (Eds.), The changing face of corpus linguistics (pp. 297–307). Amsterdam: Rodopi. 
Kehoe, A., & Gee, M. (2007). New corpora from the web: Making web text more “text-like”. Towards Multimedia in Corpus Studies. Helsinki: VARIENG. http://www.helsinki.f/varieng/ series/volumes/02/kehoe_gee/. Accessed 21 May 2019. 
Kehoe, A., & Gee, M. (2012). Reader comments as an aboutness indicator in online texts: Intro­ducing the Birmingham blog corpus. Aspects of corpus linguistics: Compilation, annotation, analysis. Helsinki: VARIENG. http://www.helsinki.f/varieng/journal/volumes/12/kehoe_gee/. Accessed 21 May 2019. 
Kehoe, A., & Renouf, A. (2002). WebCorp: Applying the web to linguistics and linguistics to the web. In Proceedings of the 11th international World Wide Web conference. http:// web.archive.org/web/20141206025600/http://www2002.org/CDROM/poster/67/. Accessed 21 May 2019. 
Keller, F., & Lapata, M. (2003). Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3), 459–484. 
Kilgarriff, A., & Grefenstette, G. (2003). Introduction to the special issue on the web as corpus. Computational Linguistics, 29(3), 333–347. 
Kilgarriff, A., Reddy, S., Pomikálek, J., & Avinesh, P. V. S. (2010). A corpus factory for many languages. http://www.sketchengine.co.uk/wp-content/uploads/2015/05/ Corpus_Factory_2010.pdf. Accessed 21 May 2019. 
Kucera, H., & Nelson Francis, W. (1967). Computational analysis of present-day American English. Providence: Brown University Press. 
Leech, G. (2007). New resources, or just better old ones? The holy grail of representativeness. In 
M. Hundt, N. Nesselhauf, & C. Biewer (Eds.), Corpus linguistics and the web (pp. 133–149). Amsterdam: Rodopi. Lutzky, U., & Kehoe, A. (2017). “I apologise for my poor blogging”: Searching for apologies in the Birmingham blog Corpus. Corpus Pragmatics, 1(1), 37–56. 
Mair, C. (2012). From opportunistic to systematic use of the web as corpus: Do-support with got (to) in contemporary American English. In T. Nevalainen & E. C. Traugott (Eds.), The Oxford handbook of the history of English (pp. 245–255). Oxford: Oxford University Press. 
Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., The Google Books Team, et al. (2011). Quantitative analysis of culture using millions of digitized books. Science, 331(6014), 176–182. 
Page, R. (2014). Saying “sorry”: Corporate apologies posted on Twitter. Journal of Pragmatics, 62, 30–45. Petrovi´
c, S., Osborne, M., & Lavrenko, V. (2010). The Edinburgh Twitter corpus. In Proceedings of the NAACL HLT 2010 workshop on computational linguistics in a world of social media (pp. 25–26). 
Pomikàlek, J., Rychly, P., & Kilgarriff, A. (2009). Scaling to billion-plus word corpora. Advances in Computational Linguistics: Special Issue of Research in Computing Science, 41, 3–14. 
Rayson, P., Charles, O., & Auty, I. (2012). Can Google count? Estimating search engine result consistency. In Proceedings of the seventh Web as Corpus workshop (WAC7) (pp. 23–30). http:/ /sigwac.org.uk/raw-attachment/wiki/WAC7/wac7-proc.pdf. Accessed 21 May 2019. 
Resnik, P., & Smith, N. A. (2003). The web as a parallel corpus. Computational Linguistics, 29(3), 349–380. 
Rundell, M. (2009). Genius and rubbish and other noun-like adjectives. MacMillan Dictionary Blog. http://www.macmillandictionaryblog.com/noun-like-adjectives. Accessed 21 May 2019. 
San Vicente, I., & Manterola, I. (2012). PaCo2: A fully automated tool for gathering parallel cor­pora from the web. In Proceedings of the eight international conference on Language Resources and Evaluation (LREC12). http://aclanthology.info/papers/L12-1085/l12-1085. Accessed 21 May 2019. 
Schäfer, R. (2016). On bias-free crawling and representative web corpora. In Proceedings of the 10th Web as Corpus workshop (WAC-X) and the EmpiriST shared task (pp. 99–105). 
Schäfer, R., & Bildhauer, F. (2012). Building large corpora from the web using a new effcient tool chain. In Proceedings of the eighth international conference on Language Resources and Evaluation (LREC) (pp. 486–493). Istanbul: ELRA. 
Schäfer, R., & Bildhauer, F. (2013). Web corpus construction (Vol. 6, pp. 1–145). San Rafael: Morgan & Claypool. 
Schmied, J. (2006). New ways of analysing ESL on the WWW with WebCorp and WebPhraseC­ount. In A. Renouf & A. Kehoe (Eds.), The changing face of corpus linguistics (pp. 309–324). Amsterdam: Rodopi. 
Sharoff, S. (2006a). Creating general-purpose corpora using automated search engine queries. In 
M. Baroni & S. Bernardini (Eds.), Wacky! Working papers on the web as corpus (pp. 63–98). Bologna: GEDIT. http://wackybook.sslmit.unibo.it/pdfs/sharoff.pdf. Accessed 21 May 2019. Sharoff, S. (2006b). Open-source corpora. Using the net to fsh for linguistic data. International Journal of Corpus Linguistics, 11(4), 435–462. 
Sinclair, J. (2005). Corpus and text – Basic principles, and appendix: How to build a corpus. In M. Wynne (Ed.), Developing linguistic corpora: a guide to good practice. Oxford: Oxbow Books. http://ota.ox.ac.uk/documents/creating/dlc/. Accessed 21 May 2019. 
Suchomel, V., & Pomikálek, J. (2012). Effcient web crawling for large text corpora. In Proceedings of the seventh Web as Corpus workshop (WAC7) (pp. 39–43). http://sigwac.org.uk/raw­attachment/wiki/WAC7/wac7-proc.pdf. Accessed 21 May 2019. 
van den Bosch, A., Bogers, T., & de Kunder, M. (2016). Estimating search engine index size variability: A 9-year longitudinal study. Scientometrics, 107, 839–856. 
Chapter 16 Multimodal Corpora 
Dawn Knight and Svenja Adolphs 
Abstract This chapter provides an overview of current advances in multimodal corpus linguistics. It defnes what multimodal corpora are, what they can be used for, how and why they are used, and outlines some of the practical and methodological challenges and concerns faced in the construction and analysis of multimodal corpora. Examples of notable corpora are presented, alongside examples of software tools for multimodal corpus development and enquiry, and the chapter ends with refections on possible future directions for developments within this feld. 
16.1 Introduction 
Developments in technology, particularly the ever-increasing availability of advanced capturing devices for video and audio alongside digital analysis software, have provided linguists with invaluable tools for the construction of multimodal records of human communication. These developments have proven to be particularly benefcial to the emergent feld of multimodal corpus linguistic enquiry. 
A corpus is a principled collection of language data taken from real-life contexts. Modern corpora vary in size and scope and are used by a range of different researchers and professionals: from academics to lexicographers, textbook writers and syllabus designers; and more broadly, they could be used by potentially anyone with an interest in language. As Conrad (2002: 77) argues, “Corpus analysis can be a good tool for flling us in on the bigger picture of language”, providing users both with suffcient data for exploring specifc linguistic patterns, and with the 
D. Knight (•) School of Education, Communication and Language Sciences (ENCAP), Cardiff University, Cardiff, Wales, UK e-mail: KnightD5@cardiff.ac.uk 
S. Adolphs School of English, The University of Nottingham, Nottingham, UK e-mail: svenja.adolphs@nottingham.ac.uk 
© Springer Nature Switzerland AG 2020 353 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_16 
D. Knight and S. Adolphs 
methods of doing so; the corpus linguistic approach (see Stubbs 1996: 41). For more information on corpus design and construction, see Chap. 1. 
While the majority of current corpora are mono-modal in nature, comprising text-based records taken from transcribed spoken language, and written data (e.g. BNC, British National Corpus, see Burnard and Aston 1998 and COCA, Corpus of Contemporary American, see Davies 2008), a surge in the development of multimodal corpora has been witnessed over the past decade or so (including corpora of sign language (e.g. the British Sign Language (BSL) corpus, see Johnston and Schembri 2006) and associated systematic analysis of gesture patterning). Multimodal corpora present a signifcant step-change in the potential for linguistic study, moving away from textual representations of language-in-use, and enabling the analysis of different modes of communication, represented in, for example, video and audio records of communication. Such records help to support the examination of the gestural, prosodic and proxemic features of talk. 
16.2 Fundamentals 
16.2.1 Defning Multimodality and Multimodal Corpora 
Rooted in Halliday’s work on social semiotics (1978), multimodality is often conceptualised as the relationship between different ‘modes’ of communication and how they interact to develop meaning in discourse. A mode, in this context, is defned as “a socially shaped and culturally given semiotic resource for making meaning” (Kress 2010: 79). Examples of semiotic resources include texts, objects, space, colour, image, gesture, prosody, facial expressions, eye gaze and body posture. Multimodal analysis, then, concerns an examination not only of the ‘abstract’ processes utilised in discourse (i.e. bodily movement and speech, see Kress and van Leeuwen 2001), but the ‘media’, the physical mode(s), in which these are conveyed. 
A multimodal corpus is defned as “an annotated collection of coordinated content on communication channels, including speech, gaze, hand gesture and body language, and is generally based on recorded human behaviour” (Foster and Oberlander 2007: 307–308). Multimodal corpora are physical repositories within which records of these behaviours are represented, through the use of multiple forms of media; again, typically the culmination of aligned video, audio and textual representations (i.e. transcriptions) of data within a single digital interface. The catalyst behind the development of multimodal corpora is grounded in the notion that “natural language is an embodied phenomenon and that a deeper understanding of the relationship between talk and bodily actions— in particular gestures— is required if we are to develop a more coherent understanding of the collaborative organization of communication” (Adolphs 2013: 1, also see Saferstein 2004). The construction and analysis of emergent multimodal corpora aim to enable just this, by providing the analyst with the resources which support the investigation of the interplay of different modes in the construction of meaning in discourse. This type of analysis is something that traditional mono-modal corpora, that is, corpora that reduce communication purely to a textual form, are unable to support. 
The majority of current multimodal corpora are forms of ‘specialised’ corpora insofar as they tend to be constructed to help answer a specifc question, examine a particular discursive context and/or to meet the requirements of a particular research area or project. The VACE corpus (Chen et al. 2005), Multimodal Meeting (MM4) Corpus (Mana et al. 2007), Mission Survival Corpus (MMC1, McCowan et al. 2003), NIST Meeting Room Phase II Corpus (Garofolo et al. 2004) and AMI (Ashby et al. 2005) corpora are, for example, taken from meeting room contexts. The CUBE-G (Rehm et al. 2008), Fruit Carts Corpus (Aist et al. 2006), SaGA (Lking et al. 2010) and the UTEP ICT (Herrera et al. 2010) corpora are all (semi)scripted, controlled or task-based multimodal corpora, while the Nottingham Multi-Modal Corpus (NMMC–Knight et al. 2009) is comprised of recordings from an academic context. 
There currently exist no ‘general’ large-scale multimodal corpora, which contain data sampled from a wide range of discursive contexts and/or socio-demographic groups, that are designed in the same vein to existing large-scale monomodal corpora. The specialist nature of multimodal corpora impacts on the potential scalability and reusability of the corpora across different projects. Reusability is an issue that is also often tied to ethical constraints that are commonly attached to multimodal corpora. While text-based corpora can be anonymised so that participants are not easily identifable, this becomes more of an issue with corpora which contain, for example, video and audio data. Audio fles can be muted, and video images can be pixelated, and avatars can be used to conceal the identities of particular individuals. However, such techniques often blur and distort the data, making it diffcult to examine phonetic or prosodic patterns or certain forms of gestures, respectively. Explicit permission needs to be sought from participants not only to video and audio record data, but also to publish and/or distribute it: this is not always possible and is one of the reasons why a large-scale, widely used multimodal corpus is currently not in existence. 
Some recent efforts have been made, however, to attempt to capture more natural­istic records of communication in the construction of a multimodal corpus, moving away from (semi)scripted, controlled or task-based contexts. These are generally smaller scale corpora, particularly in comparison to their contemporary monomodal counterparts. Examples of these include the Multimodal Corpus Analysis in the Nordic Countries project (NOMCO, Paggio et al. 2010 – see Sect. 16.3) and the EVA corpus (Mlakar et al. 2017), both of which include more ‘spontaneous’ interactions, with participants asked to discuss topics of their own choosing (i.e. without specifc prompts or requirements being provided). Similarly, the D64 Corpus (Oertel et al. 2010) includes 4 h of recordings from a two-day period and is a non-scripted, more spontaneous multimodal corpus, with interactions taking place in domestic settings. The developers of D64 aimed to provide “as close to ethnological observation of conversational behaviour as technological demands permit” (Oertel et al. 2010: 
D. Knight and S. Adolphs 
27), although as the corpus was constructed to support the automatic tracking and classifcation of gestures, participants were required to wear refective sticky markers during the recording phase. Again, these markers are somewhat invasive and detrimental to the perceived naturalness of the recorded data, as they are “not only time-consuming and often uncomfortable” to wear but “can also signifcantly change the pattern of motion” enacted by participants (Fanelli et al. 2010: 70; also see Fisher et al. 2003). Thus, the extent to which this corpus can be deemed to be ‘naturalistic’ or truly spontaneous is limited. 
16.2.2 Multimodality Research in Linguistics 
While there is a rich tradition of multimodal research in felds such as psychology, computer science, cultural studies and anthropology, the examination of multimodal communication by linguists is comparatively underdeveloped. Some of the most sig­nifcant developments in research into multimodality and multimodal text analysis in linguistics exist within the discourse analysis tradition and also in sign language research (for examples of relevant works see Tannen 1982; Baldry and Thibault 2001; Kress and van Leeuwen 2001;Lemke 2002; Scollon and Levine 2004; Ventola et al. 2004; Baldry 2004, 2005; van Leeuwen 2005; O’Halloran 2006, 2011; Royce and Bowcher 2006). Within the MDA (Multimodal Discourse Analysis) framework, analysts typically draw on systematic functional theory to examine the relationship between discourse and social practice. Examples of some of the areas where a multimodal approach is used include the examination of text layouts, colour use in texts, identity and voice and embodiment and gesture. Raw data can include, for example, clips of video, still visual images, transcribed text, media cuttings, physical shapes and so on, in small episodes of talk or collections of data, with analysis, again, being qualitatively-led and conducted at the micro level. 
Work in multimodal corpus linguistics is, in contrast, a relatively new feld of research, having only been established in the mid-late 2000s. The key difference between multimodal corpus linguistics and the multimodal analysis of corpora using MDA is that, as with traditional text-based corpus enquiry, multimodal corpus analysis includes not only detailed qualitative analyses, but also quantitative analyses of emerging patterns of language-in-use. 
Gu notes that multimodal corpus linguistic research tends to be rooted either in the social sciences, focusing on providing multimodal and multimedia studies of discourse, or more generally within computational sciences and focusing on speech engineering and corpus annotation (2006: 132). The former of these is concerned with answering specifc questions about language use, while the latter deals with the development of software and tools for the construction of multimodal corpora, and developing approaches to analyse the data systematically. As Knight (2011a: 
55) notes, “few studies concentrate on both of these key concerns in great detail. This is mainly due to the fact that different types of expertise are needed to meet the requirements posed by each of these strands of research”. To provide a holistic overview of the multimodal corpus landscape, the current chapter attempts to draw on, and summarise, key ideas and considerations relevant to each of these strands of research. 
The Representative Study boxes provide examples of three empirical studies that focus on analysing specifc linguistic phenomena using multimodal corpora: discourse markers, turn management in interaction and spoken and non-verbal listenership behaviour. These areas of focus only represent the tip of the iceberg for the potential of multimodal corpus research, as these resources can be used in different areas of research, from examining shoulder shrugging and hesitations (Jokinen and Allwood 2010) to studying emotions in interaction (Cu et al. 2010). As discussed above, the design and specifc contents of multimodal corpora are often tied to the aims and objectives of a given research project, which means that the type of research, and the question that can be addressed by analysing these corpora, are also tiedtothis. 
16.2.3 Issues and Methodological Challenges 
Few open-source and freely available multimodal corpora exist (with the AMI and NOMCO corpora perhaps being exceptions to this – see Sect. 16.3 for further details) and, as already outlined above, existing multimodal corpora are commonly specialist and bespoke. Many of the most pertinent issues and methodological challenges faced in multimodal corpus research are tied to the construction and availability of resources for further research. This dependency is due to the fact that our ability to analyse and examine complex and extensive multimodal corpora in a systematic and accurate way (particularly when integrating traditional corpus-based methods into the approach), is reliant on the availability of appropriate technology. 
The construction of multimodal corpora is a time consuming and technically complex process. Decisions that are being made regarding the design and com­position of these corpora must ensure the aims of the specifc research questions being asked can be examined using the resource that is compiled. Researchers need to consider what forms of data are to be included; what modalities are to be captured and represented; where the data is to be sourced from; what format these are stored in (formats are not always universal and/or transferable) and how they will be synchronised, transcribed and annotated, as well as what kinds of tools/conventions will be used to support these processes. This is because “like transcription, any camera position [or hardware/software used to capture multimodal ‘data’] constitutes a theory about what is relevant within a scene—one that will have enormous consequences for what can be seen in it later— and what forms of subsequent analysis are possible” (Goodwin 1994: 607). 
Achieving balance and representativeness has long been regarded as fundamental to designing reliable and verifable corpora (see Sinclair 2005 – also refer to Chap. 1 for further discussion). A balanced corpus is one that “usually covers a wide range of text categories which are supposed to be representative of the language or language variety under consideration” (McEnery et al. 2006: 16), while 
D. Knight and S. Adolphs 
representativeness in corpus design is best defned as “the extent to which a sample includes the full range of variability in a population”, that is “the range of texts in a language” and the “range of linguistic distributions in a language” (Biber 1993: 243). Balance and representativeness depend on the number of words per sample, number of samples per ‘text’ and number of texts per text type included in a corpus. Again, multimodal corpora are typically specialist insofar as no general, or reference, multimodal corpus exists. While each multimodal corpus may be representative of the specifc context or discourse type that it has been designed to study, it is diffcult to provide a list of current resources that are most representative of the feld. The Representative Corpora below provide some examples of more well known, available, multimodal corpora. 
Once collected, “managing the detail and complexity involved in annotating, analysing, searching and retrieving multimodal semantic patterns within and across complex multimodal phenomena” (O’Halloran 2011: 136) is the next challenge to be faced. As already noted, multimodal corpus analysis is essentially a mixed methods approach, one which seeks to combine quantitative techniques with qualitative textual analyses, as utilised in conventional corpus enquiry. To support detailed quantitative analysis of phenomena in traditional corpus research, data is often marked-up and annotated frst, to make specifc features searchable using concordancing tools. Monomodal text-only corpora in English are sometimes annotated automatically with the use of ‘taggers’ and ‘parsers’. These can tackle morphological, grammatical, lexical, semantic and discourse-level features in talk (see Chap. 2 for further information on corpus annotation). 
These taggers and parsers and their associated annotation systems tend to be unable to support the analysis of language in use beyond spoken discourse. This is because the annotation of non-verbal elements of talk, including gestures, is particularly complex as gestures are not readily made ‘textual units’ (see Gu 2006: 130; for additional works on language and gesture, see Ladewig, 2014a, b; Steen and Turner 2013; Mler et al. 2013 and Rossini 2012). So, while the availability of digital tools and methods supporting the identifcation, representation and analysis of spoken discourse proliferate, there is a lack of these for marking-up non-verbal elements or for integrating this with verbal elements for investigation (particularly in the feld of linguistics). There are also on-going methodological and technical challenges regarding how best to align annotations from the different modalities in a meaningful way to map the temporal and semiotic relationship between these. 
Abuczki and Ghazaleh note that there have been recent moves towards designing international standards “for annotating various features of spoken utterances, gaze movement, facial expressions, gestures, body posture and combinations of any of these features” within multimodal corpora (2013: 94). The most notable work on developing re-usable and international standards for investigating language and gesture-in-use was carried out by researchers involved in the ‘Natural Interaction and Multimodality’ (NIMM) part of the ‘International Standards for Language Engi­neering’ (ISLE) project (Dybkjær and Ole Bernsen 2004, also refer to Wittenburg et al. 2000). The uptake of this has not, as yet, been universal so there remains no formally agreed, conventional prescription for mark-up across the research community. Instead, individual projects often devise their own bespoke schemes, which are designed to meet the specifc requirements of their research. 
A further challenge faced by researchers working on multimodal corpora is the choice of software used to analyse multimodal resources/corpora. Other chapters in this volume have cited a variety of different digital concordancing tools which support the analysis of corpora (see Chap. 8 for more information). These include AntConc (Anthony 2017), #LancBox (Brezina et al. 2015), SketchEngine (Kilgarriff et al. 2014), WMatrix (Rayson 2009) and Wordsmith Tools (Scott 2017). Although these may have some key differences regarding the basic interfaces, query tools and functionalities they offer, they also share many commonalities. We can expect, for example, to be able to search for words, lemmas, clusters/n-grams, POS; generate frequency lists (and, in some cases, word clouds); calculate keywords; calculate and examine patterns of collocation; generate Key Word in Context (KWIC) outputs; chart and visualise results, and/or even plot the incidence of individual items across individual texts and/or sub-corpora using current software. These tools enable quantitative (including statistical) analyses as a potential way-in to the corpus-based examination of text(s) and provide the means for the user to access specifc texts or items of interest for further, more qualitative analyses. However, each of these tools lacks the provisions for querying multimodal corpora, i.e. corpora which do not simply comprise text-based representations of interaction. Note, however, that tools that support the annotation of complex multimodal datasets are widely available. Examples such as ELAN (Wittenberg et al. 2006) and ANVIL (Kipp 2001)are discussed in more detail in Sect. 16.4 below, with a screenshot of ELAN shown in Fig. 16.1. In this screenshot we see ELAN’s capacity to synchronise and organise various forms of aligned time series data for future analysis. This main annotation window includes a video (top left), and a searchable annotation grid and viewer on the top right (here, transcripts can be viewed and searched). In this example, the frst two ‘tiers’ of information (beneath the video) are movement outputs from left and right-hand sensors, followed by an audio output. Beneath this is the transcribed speech from the episode followed by the coded semiotic gestures performed by the right and left hand, then both hands together. Each of these different tiers is synchronised by time and searchable via the facility in the top right corner of the fgure. 
There are calls for the construction of digital tools that enable corpus-based anal­ysis of more complex, multimodal corpora; however, there are various reasons why the development of these digital tools is complicated. The analysis of multimodal corpora presents a whole host of technological challenges especially with regards to the synchronization and representation of multiple streams of information. Early developments that responded to this challenge were made with the construction of the Digital Replay System (DRS, see Brundell et al. 2008). DRS was the frst (and only) software which concentrated on allowing for facilitating corpus-based analyses of synchronised multimodal dataset, using lexical units and/or gestural annotations as the ‘way-in’ to analysis. It enabled users to store, transcribe, code and align different forms of qualitative data and to enable the quantitative and corpus-based analysis of data, including the generation of frequency lists, key-word­
D. Knight and S. Adolphs 

in-context (KWIC) concordance outputs, amongst other utilities. Unfortunately, however, this tool is no longer supported or sustained, so is no longer available to the research community. 
Representative Study 1 
Baiat, G.E., Coler, M., Pullen, M., Tienkouw, S. and Hunyadi, L. 2013. Multimodal analysis of ‘well’ as a discourse marker in conversation: A pilot study. Proceedings of the 4th IEEE International Confer­ence on Cognitive Infocommunications (CogInfoCom 2013), 283–287. Budapest, Hungary. 
This paper adopts a multimodal corpus pragmatic approach to the analysis of the relationship between the spoken discourse marker well and accompanying non-verbal behaviours. It questions whether well is more likely to be used with or without accompanying behaviours, and examines what pragmatic functions can be associated with any patterns of verbal and non-verbal co-occurrence. 
(continued) 
As a ‘pilot’ study, the paper with a corpus only comprising six recordings of circa 7 min each, problematizes the collection, annotation and analysis of multimodal corpora for research. Amongst other key fndings, this initial exploration revealed that “having an averted eye gaze was correlated with the use of this marker regardless of its pragmatic function” (Baiat et al. 2013: 287). 
Representative Study 2 
Navarretta, C. and Paggio, P. 2013. Classifying multimodal turn manage­ment in Danish dyadic frst encounters. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013), 133–146. Oslo, Norway. 
This paper examines multimodal turn management in interactions between Danish speakers who have met for the frst time, so uncovering how individu­als give, take or keep the turn through speech and non-verbal behaviours. The data used in this study is taken from the NOMCO corpus (see Representative Corpus 3), drawing on the Danish unscripted frst encounter dialogues which comprise 12 conversational dyads of circa 5 min, in which participants are instructed to “to talk in order to get acquainted, as if they met at a party” (Navarretta and Paggio 2013: 135). The study revealed that “all kinds of head movement, facial expression and body posture are involved in turn management” and that “the turn management types frequently occurring in this corpus depend on the type of social activity in which the participants are involved” (Navarretta and Paggio 2013: 142). As with Baiat et al. (2013), this work contributes to our understanding of human-human interaction but can also be used for dialogue modelling systems. 
Representative Study 3 
Malisz, Z., Wlodarczak, M., Buschmeier, H., Skubisz, J., Kopp, S., 
Wagner, P. 2016. The ALICO corpus: analyzing the active listener. 
Language Resources and Evaluation 50(2): 411–442. 
This paper contributes to research on spontaneous human-human inter­action, examining the functional relationship between different modalities in interaction, focusing specifcally on the role of the listener and non­
(continued) 
D. Knight and S. Adolphs 
verbal listenership behaviours, including head nods and patterns of gaze. The research presented is carried out using data from the ALICO corpus, which comprises 50 spontaneous, storytelling dialogues carried out by dyads of German speakers, 34 of whom were female and 16 were male. Head movements, gaze and other forms of non-verbal listening behaviours were annotated in conjunction with the spoken discourse to allow for an analysis of the relationship between them. The paper indicates that there is a link “between feedback form and its pragmatic function in both visual and verbal domains” (2016: 413), and the correlation and frequency of specifc listening behaviours underlined the level of attentiveness and involvement of the listener in conversation. For examples of other studies which have examined listenership behaviours, see Dittman and Llewellyn 1968; Duncan 1972; Maynard 1987; Paggio and Navarretta 2010 and Knight 2011a. 
Representative Corpus 1 
The AMI Meeting Corpus is comprised of 100h of recordings taken from meeting room contexts. AMI includes synchronised data from audio recorders (microphones), video cameras, projectors, whiteboards and pens, and associ­ated transcriptions and annotations of this data. The corpus was compiled in 2005 and remains one of the largest freely accessible multimodal corpora in existence. Specifc excerpts from the corpus can be downloaded from http:// groups.inf.ed.ac.uk/ami/download/ and can be searched and analysed using the NITE XML Toolkit, an open-source set of tools which enables the sharing, markup and annotation, representation and analysis of complex multimodal data. The motivation behind the construction of AMI was to help develop the technology that enables group interaction in meeting room contexts, although it is acknowledged that the resource “could be used for many different purposes in linguistics, organizational and social psychology, speech and language engineering, video processing, and multi-modal systems” (Carletta 2006:3). 
Representative Corpus 2 
TalkBank (see: https://talkbank.org/), which includes the Child Language Data Exchange System (CHILDES – MacWhinney 2000; cf. Chap. 14)isan online repository for transcribed audio and video data that “seeks to provide a common framework for data sharing and analysis for each of the many 
(continued) 
disciplines that studies conversational interactions” (MacWhinney 2007:8). This includes data for the examination of, for example, frst and second language acquisition, child phonology, gesture use, bilingualism, aphasia, and classroom interaction. There is a standardised format in which the data is stored and accessed on the site, as well as standardised guidelines for transcription, based on Conversational Analysis (CA) principles (see Sacks et al. 1974), and coding, based on CHAT (Codes for the Human Analysis of Transcripts). The software CLAN (Computerised Language Analysis) was created specifcally to support the creation and analysis of data included in TalkBank. While TalkBank is extensive and is sampled from different contexts and individual projects, the resource is perhaps not a corpus in the traditional sense because it is not a principled collection of data (more a collection of sub-sets of data). The resource is specifcally designed to enable micro-level analyses of data. It is not equipped with corpus query tools which enable a more quantitative analysis of the contents of the data. 
Representative Corpus 3 
The Multimodal Corpus Analysis in the Nordic Countries (NOMCO) project includes conversations in Swedish, Finnish, Danish, Estonian, and more recently, Maltese, which have been video recorded, transcribed and annotated (see Paggio et al. 2010). Each corpus has been constructed in a similar way, with data having been orthographically transcribed using the speech analysis software PRAAT (Boersma and Weenink 2009) and coded for specifc forms of gesture, including head movements, body posture and facial expressions using the video annotation tool ANVIL (Kipp 2001). The corpora were constructed to help support the analysis of a range of phenomena including, for example, “turn management, feedback exchange, information packaging and the expression of emotional attitudes” (Paggio and Navarretta 2017: 463). The corpora are currently available for other researchers to use. More information can be accessed at: http://sskkii.gu.se/nomco/index.php 
16.3 Critical Assessment and Future Directions 
This chapter has provided an overview of the current state-of-the-art in multimodal corpus linguistics. It has highlighted that while multimodal corpus research is gaining some momentum, there are still some areas where further development is required. Perceived shortcomings relating to the size, scope (and representativeness and reusability) and the level of availability of current multimodal corpora have 
D. Knight and S. Adolphs 
been mentioned, along with some of the challenges regarding the representation and analysis of multimodal corpora. A fnal challenge for current multimodal corpus research relates to more recent discussions and developments in this feld. Multi-modal interaction includes a range of different semiotic resources, and multimodal corpora, as already noted, have the potential for enabling the researcher to study the use of language along a continuum of dynamically changing contexts. The focus on dynamic contexts has resulted in a call for the construction of more ‘heterogeneous’ corpora for linguistic research (see Adolphs et al. 2011). Heterogeneous corpora are those which include different forms of media ‘types’ from interaction in virtual environments (instant messaging, entries on personal notice boards etc.), GPS data, to face-to-face situated discourse, phone calls, video calls, sensory data and so on. 
Heterogeneous corpora aim to better capture and represent aspects of the complexity and fuidity of the discursive context for the future analysis of language use (see Tennent et al. 2008; Crabtree and Rodden 2009; Knight et al. 2010; Knight 2011b and Adolphs and Carter 2013). Heterogeneous corpora also aim to provide a better insight into how language is received, as well as produced, by an individual. Accounting for the effect of contextual factors on language use in a systematic way is crucial in this context. As Bazzanella acknowledges, “in real life, context is exploited both in production and in comprehension” (2002: 239; also see Sweetser and Fauconnier 1996). As a vision for the future, Knight et al. (2010: 2) suggest that software that will support the construction and analysis of multimodal and heterogenous corpora will include the following functionalities: 
• 
The ability to search data and metadata in a principled and specifc way (encoded and/or transcribed text-based data). 

• 
Tools that allow for the frequency profling of events/elements within and across domains (providing raw counts, basic statistical analysis tools, and methods of graphing such). 

• 
Variability in the provisions for transcription, and the ability for representing simultaneous speech and speaker overlaps. 

• 
New methods for drilling into the data, through mining specifc rela­tionships within and between domain(s). This may be comparable to current social networking software, mind maps or more topologically based methods. 

• 
Graphing tools for mapping the incidence of words or events, for example, over time and for comparing sub-corpora and domain specifc characteristics. 


As outlined in this chapter, multimodal corpus research is somewhat still in its infancy and as such we can expect a step-change in our description and understand­ing of language based on this research. It will be interesting to see whether some of the major insights generated within monomodal, or text-based, corpus linguistics will be replicated once we are able to truly analyse multimodality at scale. This may include new insights into collocation of multimodal units of meaning across interactions; acquisition of speech-gesture units; and insights into frequencies of specifc multimodal units in different contexts. Advances in this area are likely to be dependent, at least in part, on the development, functionality and availability of technological resources, but also, as Knight notes, on institutional, national and international collaborative interdisciplinary and multidisciplinary research strate­gies and funding (2011b: 409). 
16.4 Tools and Resources 
While there is a dearth in the existence of freely-available and widely used multi-modal corpora, there are, conversely, a wide range of digital tools and resources that exist to support the construction and analysis of bespoke multimodal corpora. The most widely used tools, and the specifc areas of multimodal corpus research that is supported by these tools, focusing on corpus compilation, annotation and analysis, are presented below. All of the tools listed require manual transcription, annotation and analysis of data: these processes are not automated. 
Anvil 
ANVIL is a video analysis software that was built by Michael Kipp in 2000 and recently updated in 2017 (see Kipp 2001 for more details about the key functionalities of this tool). ANVIL provides users with the tools for the man­agement, synchronization, transcription, annotation and analysis of multimodal data (in various different formats, including 3D motion capture videos) across multiple layers (known as tiers), that is, for the construction of synchronization of bespoke multimodal corpora. ANVIL is also equipped with a PRAAT plug-in (for details on PRAAT, see: www.fon.hum.uva.nl/praat (accessed 23 May 2019); cf. also Chap. 11), amongst other tools, supporting phonetic and other forms of linguistic analysis of multimodal corpora. Unfortunately, the current version of ANVIL is not integrated with any specifc corpus enquiry tools such as concordancers or wordlist functionalities, although it does have systematic search capabilities, which operate at a tag/annotation level, as well as at the lexical level. 
ELAN (https://tla.mpi.nl/tools/tla-tools/elan) (accessed 23 May 2019) 
The Max Planck Institute’s ELAN (Wittenberg et al. 2006) is an open-source tool which enables the annotation and analysis of data across multiple ‘tiers’ of information. As with ANVIL, ELAN enables users to upload and synchronise their own multimodal corpora and supports the manual frame-based annotation and analyses of multiple modes of time series data, from audio and video data, to sensor outputs. A particular strength of ELAN is that it is integrated with a particularly user-friendly transcription viewer and some basic corpus analytic tools which allow users to search for specifc lexical items and/or codes within a specifc data ‘project’. As with ANVIL, ELAN is also equipped with a PRAAT plug-in (amongst other plug-ins). 
D. Knight and S. Adolphs 
Transana (www.transana.com) (accessed 23 May 2019) 
Transana supports the qualitative analysis of videos, audio and images. It enables users to integrate, transcribe, categorize and code their own data (i.e. construct their own multimodal corpus) and then search and explore it in more detail (i.e. analyse it). Transana also provides the means for converting fles into a standard format which increases the fexibility of the tool. The transcription tools within Transana are particularly user friendly, and a particular strength of this tool is that it enables real time collaboration via the online version. As Transana is primarily a tool for qualitative analysis, its provisions for, for example, frequency-based or numerical analysis (as utilized by corpus linguists) is somewhat limited. 
Further Reading 
Allwood, J. 2008. Multimodal Corpora. In Corpus Linguistics. An International 
Handbook, eds. Leling, A. and Kyt M., 207–225. Berlin: Mouton de 
Gruyter. 
Allwood’s (2008) chapter includes a particularly detailed focus on the defnition and annotation of gestures and the steps towards developing a standardised approach to such within the feld of multimodal corpus linguistics. Extensive examples of what may be analysed using multimodal corpora are also offered, from the examination of communication and power, emotion, consciousness and awareness to the analysis of multimodality within and across specifc types of media. Allwood also provides some more practical refections on the possible applications of multimodal corpus research, from the development of tools to support human-human and human-machine communication to the construction of multimodal translation and interpretation systems. 
Knight, D. 2011a. Multimodality and Active Listenership: A Corpus Approach. London: Continuum. 
Building on some of the discussions outlined by Allwood (2008), Knight’s mono­graph takes readers through every step of multimodal corpus design and construc­tion and provides a worked example of multimodal corpus analysis. This analysis focuses on the examination of the use of, and the relationship between, spoken and non-verbal forms of backchanneling behaviour, and a particular sub-set of gestural forms: head nods. This investigation is undertaken by means of analysing the patterned use of specifc forms and functions of backchannels within and across sentence boundaries, in 5 h of dyadic (supervision) data taken from the NMMC (Nottingham Multi-Modal Corpus). Knight discusses the validity and applicability of different existing categorisations of backchannels to multi-modal corpus data, and examines the requirements for a redefnition of these considering the fndings resulting from the analyses. 
Gu, Y. 2006. Multimodal text analysis: A corpus linguistic approach to situated discourse. Text and Talk 26(2): 127–167. 
In a similar way to Knight, Gu concentrates on providing some guidelines for an approach to multimodal corpus analysis. This study is situated mainly within the MDA paradigm, so it focuses on small-scale corpora of situated discourse, utilizing discourse analytic methods as the initial point of departure, but discusses how such an approach can be extended with the integration of corpus methods. Gu focuses on different modalities within the analysis, beyond what the majority of multimodal corpus studies typically afford, making this work of particular relevance to the fnal section of this chapter: projections for the future directions of this feld. 
Kipp, M., Martin, J-C., Paggio, P. and Heylen, D. (Eds). 2009. Multimodal Corpora: from models of natural interaction to systems and applications. Lecture Notes in Artifcial Intelligence 5509. Berlin: Springer-Verlag. 
This edited collection provides a state-of-the-art survey of research into multimodal corpus construction and analysis, taken from the ‘Multimodal Corpora: From Models of Natural Interaction to Systems and Applications’ workshop held in con­junction with the 6th LREC (International Conference for Language Resources and Evaluation) conference, held in Marrakech in 2008. The volume presents research from different felds including computational linguistics, human-human interaction, signal processing, artifcial intelligence, psychology and robotics, comprising six contributions from the workshop and additional invited research articles from key multimodal corpora projects, including AMI. The volume discusses a range of topics from corpus construction to analysis, so is invaluable for researchers who are perhaps new to the feld and need some advice on carrying out their own work in this area. 
References 
Abuczki, Á., & Ghazaleh, E. B. (2013). An overview of multimodal corpora, annotation tools and schemes. Argumentum, 9, 86–98. Adolphs, S. (2013). Corpora: Multimodal. In C. A. Chapelle (Ed.), The encyclopaedia of applied linguistics. London: Blackwell. Adolphs, S., & Carter, R. (2013). Spoken corpus linguistics: From monomodal to multimodal. London: Routledge. Adolphs, S., Knight, D., & Carter, R. (2011). Capturing context for heterogeneous corpus analysis: Some frst steps. International Journal of Corpus Linguistics, 16(3), 305–324. 
Aist, G., Allen, J., Campana, E., Galescu, L., Gomez Gallo, C.A., Stoness, S., Swift, M., & Tanenhaus, M. (2006). Software architectures for incremental understanding of human speech, 1922–1925. In: Proceedings of Interspeech 2006, Pittsburgh. 
Allwood, J. (2008). Multimodal Corpora. In A. Leling & M. Kyt(Eds.), Corpus linguistics: An international handbook (pp. 207–225). Berlin: Mouton de Gruyter. Anthony, L. (2017). AntConc (Version 344) [Computer software]. Waseda University, Tokyo. Available from http://www.laurenceanthony.net. Accessed 23 May 2019. 
D. Knight and S. Adolphs 
Ashby, S., Bourban, S., Carlettea, J., Flynn, M., Guillemot, M., Hain, T., Kadlec, J., Karaiskos, V., Kraaij, W., Kronenthal, M., Lathoud, G., Lincoln, M., Lisowska, A., McCowan, I., Post, W., Reidsma, D., and Wellner, P. (2005). The AMI Meeting Corpus, In: Proceedings of measuring behavior conference, Wageningen. 
Baiat, G.E., Coler, M., Pullen, M., Tienkouw, S., & Hunyadi, L. (2013). Multimodal analysis of ‘well’ as a discourse marker in conversation: A pilot study. In: Proceedings of the 4th IEEE International Conference on Cognitive Infocommunications (CogInfoCom 2013), (pp. 283– 287). Budapest, Hungary. 
Baldry, A. (2004). Phase and transition, type and instance: Patterns in media texts as seen through a multimodal concordance. In K. O’Halloran (Ed.), Multimodal discourse analysis: Systemic functional perspectives (pp. 83–108). London/New York: Continuum. 
Baldry, A. (2005). Multimodal corpus linguistics. In G. Thompson & S. Hunston (Eds.), System and Corpus: Exploring connections (pp. 164–183). London/New York: Equinox. 
Baldry, A., & Thibault, P. J. (2001). Towards multimodal corpora. In G. Aston & L. Burnard (Eds.), Corpora in the description and teaching of English, papers from the 5th ESSE (pp. 87–102). Conference Bologna: Cooperativa Libraria Universitaria Editrice Bologna. 
Bazzanella, C. (2002). The signifcance of context in comprehension: The ‘we case’. Foundations of Science, 7, 39–254. 
Biber, D. (1993). Representativeness in corpus design. Literary and Linguistic Computing, 8(4), 243–257. 
Boersma, P., & Weenink, D. (2009). Praat: doing phonetics by computer [Computer Program]. Available from http://www.praat.org. Accessed 23 May 2019. 
Brezina, V., McEnery, T., & Wattam, S. (2015). Collocations in context: A new perspective on collocation networks. International Journal of Corpus Linguistics, 20(2), 139–173. 
Brundell, P., Tennent, P., Greenhalgh, C., Knight, D., Crabtree, A., O’Malley, C., Ainsworth, S., Clarke, D., Carter, R., and Adolphs, S. (2008). Digital Replay system (DRS): A tool for interaction analysis. In: Paper delivered at the International Conference for the Learning Sciences 2008 (ICLS), Utrecht, The Netherlands June–July 2008. 
Burnard, L., & Aston, G. (1998). The BNC handbook: Exploring the British National Corpus. Edinburgh: Edinburgh University Press. 
Carletta, J. (2006). Announcing the AMI meeting corpus. The ELRA Newsletter, 11(1), 3–5. 
Chen, R. L., Rose, T., Qiao, Y., Kimbara, I., Parrill, F., Welji, H., Han, X., Tu, J., Huang, Z., Harper, M., Quek, F., Xiong, Y., McNeill, D., Tuttle, R., & Huang, T. (2005). VACE multimodal meeting corpus. In S. Renals & S. Bengio (Eds.), Proceedings of Machine Learning for Multimodal Interaction (MLMI 2005), Lecture Notes in Computer Science (Vol. 3869, pp. 40– 45). Berlin/Heidelberg: Springer. 
Conrad, S. (2002). Corpus linguistic approaches for discourse analysis. Annual Review of Applied Linguistics, 22, 75–95. 
Crabtree, A., & Rodden, T. (2009). Understanding interaction in hybrid ubiquitous computing environments. Proceedings of the 8th International Conference on Mobile and Ubiquitous Media, Article No1, 1–10. Cambridge: ACM. 
Cu, J., Suarez, M., & Sta, M.A. (2010). Filipino Multimodal Emotion Database. In: Proceedings of 7th Conference on Language Resources and Evaluation (LREC-2010) (pp. 37–42). Malta, May 2010. 
Davies, M. (2008). The Corpus of Contemporary American English (COCA): 560 million words, 1990-present. Available online at https://corpusbyuedu/coca/. Accessed 23 May 2019. 
Dittman, A., & Llewellyn, L. (1968). Relationships between vocalizations and head nods as listener responses. Journal of Personality and Social Psychology, 9, 79–84. 
Duncan, S. (1972). Some signals and rules for taking speaking turns in conversation. Journal of Personality and Social Psychology, 23(2), 283–292. 
Dybkjær, L., & Ole Bernsen, N. (2004). Recommendations for natural interactivity and multimodal annotation schemes. In: Proceedings of the 4th Language Resources and Evaluation Confer­ence (LREC) 2004, Lisbon, Portugal. 
Fanelli, G., Gall, J., Romsdorfer, H., Weise, T., & Van Gool, L. (2010). 3D vision technology for capturing multimodal corpora: chances and challenges. In: Proceedings of the LREC Workshop on Multimodal Corpora, Mediterranean Conference Centre (pp. 70–73). Malta, 18 May 2010. 
Fisher, D., Williams, M., & Andriacchi, T. (2003). The therapeutic potential for changing patterns of locomotion: an application to the acl-defcient knee. In: Proceedings of the ASME Bioengineering Conference (pp. 849–50). Miami, Florida, June 2003. 
Foster, M. E., & Oberlander, J. (2007). Corpus-based generation of head and eyebrow motion for an embodied conversational agent. Language Resources and Evaluation (LREC), 41(3/4), 305–323. 
Garofolo, J.S., Laprun, C.D., Michel, M., Stanford, V.M., & Tabassi, E. (2004). The NIST Meeting Room Pilot Corpus. In: Proceedings of the 4th Language Resources and Evaluation Conference (LREC) 2004, Lisbon, Portugal. 
Goodwin, C. (1994). Professional vision. American Anthropologist, 96(3), 606–633. 
Gu, Y. (2006). Multimodal text analysis: A corpus linguistic approach to situated discourse. Text and Talk, 26(2), 127–167. 
Halliday, M. A. K. (1978). Language as social semiotic: The social interpretation of language and meaning. London: Edward Arnold. 
Herrera, D., Novick, D., Jan, D., & Traum, D. (2010). The UTEP-ICT Cross-Cultural Multiparty Multimodal Dialog Corpus. In: Proceedings of LREC Workshop on Multimodal Corpora.May 2010, Malta. 
Johnston, T., & Schembri, A. (2006). Issues in the creation of a digital archive or a signed language. In L. Barwick & N. Thieberger (Eds.), Sustainable data from digital feldwork (pp. 7–16). Sydney: Sydney University Press. 
Jokinen, K., & Allwood, J. (2010). Hesitation in Intercultural Communication: Some observations on Interpreting Shoulder Shrugging. In: Proceedings of the international workshop on agents in cultural context, (pp. 25–37). The First International Conference on Culture and Computing 2010 Kyoto, Japan. 
Kilgarriff, A., Baisa, V., Bušta, J., Jakubí. r, V., Michelfeit, J., Rychl P., &
cek, M., Kovvá. Suchomel, V. (2014). The sketch engine: Ten years on. Lexicography, 1, 7–36. Kipp, M. (2001). ANVIL – A generic annotation tool for multimodal dialogue, 1367–1370. In: 
Proceedings of 7th European Conference on Speech Communication and Technology 2nd INTERSPEECH Event, Aalborg, Denmark. 
Knight, D. (2011a). Multimodality and active listenership: A corpus approach. London: Blooms­bury. 
Knight, D. (2011b). The future of multimodal corpora. Brazilian Journal of Applied Linguistics (BJAS), 11(2), 391–415. 
Knight, D., Evans, D., Carter, R. A., & Adolphs, S. (2009). Redrafting corpus development methodologies: Blueprints for 3rd generation multimodal, multimedia corpora. Corpora, 4(1), 1–32. 
Knight, D., Tennent, P., Adolphs, S., & Carter, R. (2010). Developing heterogeneous corpora using the Digital Replay System (DRS). In: Proceedings of the LREC 2010 (Language Resources Evaluation Conference) Workshop on Multimodal Corpora: Advances in Capturing, Coding and Analyzing Multimodality. Malta, May 2010. 
Kress, G. (2010). Multimodality–A social semiotic approach to contemporary communication. London: Routledge. 
Kress, G. R., & Van Leeuwen, T. (2001). Multimodal discourse: The modes and Media of Contemporary Communication. London: Arnold. 
Ladewig, S. H. (2014a). The cyclic gesture. In C. Mler, A. Cienki, E. Fricke, S. Ladewig, 
D. McNeil, & J. Bressem (Eds.), Body language communication volume 2: An international 
handbook on multimodality in human interaction (pp. 1605–1618). Berlin: De Gruyter. Ladewig, S. H. (2014b). Recurrent gestures. In C. Mler, A. Cienki, E. Fricke, S. Ladewig, 
D. McNeil, & J. Bressem (Eds.), Body language communication volume 2: An international 
handbook on multimodality in human interaction (pp. 1558–1574). Berlin: De Gruyter. Lemke, J. L. (2002). Travels in hypermodality. Visual Communication, 1(3), 299–325. 
D. Knight and S. Adolphs 
Lking A., Bergman, K., Hahn, F., Kopp, S., & Rieser, H. (2010). The Bielefeld Speech and Gesture Alignment Corpus (SaGA). In: Proceedings of LREC Workshop on Multimodal Corpora, 2010. Malta, May 2010. 
MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk (3rd ed.). Mahwah: Lawrence Erlbaum Associates. 
MacWhinney, B. (2007). The TalkBank project. In J. Beal & K. Moisl (Eds.), Creating and digitizing language corpora: Synchronic databases (Vol. 1). Basingstoke/Hampshire: Palgrave Macmillan. 
Malisz, Z., Wlodarczak, M., Buschmeier, H., Skubisz, J., Kopp, S., & Wagner, P. (2016). The ALICO corpus: Analyzing the active listener. Language Resources and Evaluation, 50(2), 411– 442. 
Mana, N., Lepri, B., Chippendale, P., Cappelletti, A., Pianesi, F., Svaizer, P., & Zancanaro, M. (2007). Multimodal Corpus of Multi-Party Meetings for Automatic Social Behavior Analysis and Personality Traits Detection. In: Proceeding of Workshop on Tagging, Mining and Retrieval of Human-Related Activity Information at ICMI’07 (pp. 9–14). Nagoya, Japan. 
Maynard, S. K. (1987). International functions of a nonverbal sign head movement in Japanese dyadic casual conversation. Journal of Pragmatics, 11, 589–606. 
McCowan, I., Bengio, S., Gatica-Perez, D., Lathoud, G., Monay, F., Moore, D., Wellner, P., & Bourlard, H. (2003). Modelling Human Interaction in Meetings. In: Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 748–751). Hong Kong, April 2003. 
McEnery, T., Xiao, R., & Tono, Y. (2006). Corpus-based language studies: An advanced resource book. London: Routledge. 
Mlakar, I., Ka.ci.c, Z., & Rojc, M. (2017). A Corpus for investigating the multimodal nature of multi-speaker spontaneous conversations – EVA Corpus. WSEAS Transactions on Information Science and Applications, 14, 213–226. 
Mler, C., Bressem, J., & Ladewig, S. H. (2013). Towards a grammar of gestures: A form-based view. In C. Mler, A. Cienki, E. Fricke, S. H. Ladewig, D. McNeill, & S. Teßendorf (Eds.), Body-language-communication: An international handbook on multimodality in human interaction (Vol. 1, pp. 707–733). De Gruyter Mouton: Berlin. 
Navarretta, C., & Paggio, P. (2013). Classifying multimodal turn management in Danish dyadic frst encounters. In: Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013) (pp. 133–146). Oslo, Norway. 
O’Halloran, K. L. (2006). Multimodal discourse analysis – Systemic functional perspectives. London: Continuum. 
O’Halloran, K. L. (2011). Multimodal discourse analysis. In K. Hyland & B. Paltridge (Eds.), Companion to discourse (pp. 120–137). London/New York: Bloomsbury. 
Oertel, C., Cummins, F., Campbell, N., Edlund, J., & Wagner, P. (2010). D64: a corpus of richly recorded conversational interaction. In: Proceedings of the LREC Workshop on Multimodal Corpora, Mediterranean Conference Centre (pp. 27–30). Malta, 18 May 2010. 
Paggio, P., & Navarretta, C. (2010). Feedback in Head Gesture and Speech. In: Proceedings of 7th Conference on Language Resources and Evaluation (LREC-2010) (pp. 1–4). Malta, May. 
Paggio, P., & Navarretta, C. (2017). The Danish NOMCO corpus: Multimodal interaction in frst acquaintance conversations. Language Resources and Evaluation, 51(2), 463–494. 
Paggio, P., Allwood, J., Ahlsén, E., Jokinen, K., & Navarretta, C. (2010). The NOMCO Multimodal Nordic Resource -Goals and Characteristics. In: Calzolari, N., Choukri, K., Maegaard, B., Mariani, J., Odijk, J., Piperidis, S., Rosner, M., & Tapias, D. (Eds.), In: Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10). Valletta, Malta, May 19–21, European Language Resources Association (ELRA). 
Rayson, P. 2009. Wmatrix: A web-based corpus processing environment 7 [computer soft­ware] Computing Department, Lancaster University, Lancaster. Available from: http:// ucrellancsacuk/wmatrix/. Accessed 23 May 2019. 
Rehm, M., André, E., Bee, N., Endrass, B., Wissner, M., Nakano, Y., Lipi, A. A., Nishida, T., & Huang, H.-H. (2008). Creating standardized video recordings of multimodal interactions across cultures. In M. Kipp, J.-C. Martin, P. Paggio, & D. Heylen (Eds.), Multimodal corpora from models of natural interaction to systems and applications, LNAI 5509 (pp. 138–159). New York: Springer. 
Rossini, N. (2012). Reinterpreting Gesture as Language. Amsterdam: IOS Press. 
Royce, T. D., & Bowcher, W. (2006). New directions in the analysis of multimodal discourse. London: Routledge. 
Sacks, H., Schegloff, E. A., & Jefferson, G. (1974). A simplest systematics for the organisation of turn taking for conversation. Language, 50(4–1), 696–735. 
Saferstein, B. (2004). Digital technology and methodological adaptation: Text on video as a resource for analytical refexivity. Journal of Applied Linguistics, 1(2), 197–223. 
Scollon, R., & Levine, P. (2004). Multimodal discourse analysis as the confuence of discourse and technology. Hillsdale: Lawrence Erlbaum Associates. 
Scott, M. 2017. WordSmith Tools version 7 [Computer software]. Lexical Analysis Software, Stroud. Available from: http://www.lexically.net/wordsmith/. Accessed 23 May 2019. 
Sinclair, J. (2005). Corpus and text – Basic principles. In M. Wynne (Ed.), Developing linguistic corpora: A guide to good practice (pp. 1–16). Oxford: Oxbow Books. 
Steen, F., & Turner, M. B. (2013). Multimodal construction grammar. In M. Borknet, B. Dancygier, & J. Hinnell (Eds.), Language and the creative mind. Stanford: CSLI Publications. 
Stubbs, M. (1996). Text and corpus analysis: Computer-assisted studies of language and culture. Oxford: Blackwell. 
Sweetser, E., & Fauconnier, G. (1996). Cognitive links and domains: Basic aspects of mental space theory. In G. Fauconnier & E. Sweetser (Eds.), Spaces, worlds and grammars (pp. 1– 28). Chicago: University of Chicago Press. 
Tannen, D. (1982). Ethnic style in male-female conversation. In J. J. Gumperz (Ed.), Language and social identity (pp. 217–231). Cambridge: Cambridge University Press. 
Tennent, P., Crabtree, A., & Greenhalgh, C. (2008). Ethno-goggles: supporting feld capture of qualitative material. In: Proceedings of the 4th Annual International e-Social Science Conference [online], University of Manchester, Manchester. 
van Leeuwen, T. (2005). Introducing social semiotics. London: Routledge. 
Ventola, E., Cassily, C., & Kaltenbacher, M. (Eds.). (2004). Perspectives on multimodality. John Benjamins: Amsterdam and Philadelphia. 
Wittenburg, P., Broeder, D., & Sloman, B. (2000). Meta-description for language resources EAGLES/ISLE White paper [online report], available at: http://www.mpi.nl/isle/documents/ papers/white_paper_11.pdf. Accessed 23 May 2019. 
Wittenburg, P., Brugman, H., Russel, A., Klassmann, A., & Sloetjes, H. (2006). ELAN: a professional framework for multimodality research. In: Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), 1556–1559. ELRA, Genoa, Italy. 
Part IV Exploring Your Data 
Chapter 17 Descriptive Statistics and Visualization 
with R 
Magali Paquot 
and Tove Larsson 
Abstract This chapter introduces the main functionalities of R, a free software environment for statistical computing and graphics, and its integrated development environment RStudio. It starts with a quick introduction about the basics of R and RStudio (how to install R and RStudio, and get started). Next, it focuses on data handling (how to prepare, load, check, manage and save data with R). The second part of the chapter deals with descriptive statistics (measures of central tendency, measures of dispersion and coeffcients of correlation) and visualization techniques used to explore linguistic data (in the form of bar plots, mosaic plots, histograms, ecdf plots and boxplots). 
17.1 Introduction 
This chapter serves as an introduction to the other chapters in Parts IV and V. Its main objectives are to provide the necessary basic know-how about R (R Core Team 2019) and to introduce descriptive statistics and visualization techniques with R, with a special focus on terminology. 
Section 17.2 provides a short introduction to R and RStudio (RStudio Team 2017) software installationand basic commands. R and RStudio are the two software 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_17) contains supplementary material, which is available to authorized users. 
M. Paquot (•) FNRS – Université catholique de Louvain, Centre for English Corpus Linguistics, Louvain-la-Neuve, Belgium e-mail: magali.paquot@uclouvain.be 
T. Larsson Université catholique de Louvain, Centre for English Corpus Linguistics, Louvain-la-Neuve, Belgium 
Uppsala University, Uppsala, Sweden e-mail: tove.larsson@uclouvain.be 
© Springer Nature Switzerland AG 2020 375 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_17 
M. Paquot and T. Larsson 
tools that will be used in the rest of the handbook to describe a set of statistical techniques available to corpus linguists. Section 17.3 reviews essential steps in data handling with R, from data preparation to data loading, managing and saving. Section 17.4 summarizes the main descriptive statistics and Sect. 17.5 introduces some of the most common data visualization techniques used to explore linguistic data. The main focus is here placed on visualization techniques that may be used to explore a dataset before any statistical test is applied; visualization of statistical results is described in the relevant following chapters. 
The chapter comes with a supplementary R code fle that exemplifes all functions used and provides some more information about additional useful functions. 
17.2 An Introduction to R and RStudio 
17.2.1 Installing R and RStudio 
R is a freely available software environment. While R has a wide variety of applications (see also Chap. 9), we will here focus on its use as a tool for statistical computing and data visualization. In order to install R, follow the steps below: 
1. 
Go to the Comprehensive R Archive Network – CRAN website at https://cran.r­
project.org/. 

2. 
In the “Download and Install R” box, click on the link for your operating system (Windows, Mac OS or Linux) and download the Installer package or follow the instructions for your Linux distribution. 

3. 
Follow the steps to run the installer. 


Although R can be used as standalone software, we will here be using an open-source integrated development environment for R called RStudio, which provides a user-friendly way of accessing the R console and managing R codes, datasets and plots. To download RStudio, follow the steps below: 
1. 
Go to https://www.rstudio.com/. 

2. 
Click on “download” under the RStudio column. 

3. 
Click on the free RStudio Desktop download. 

4. 
Click on the appropriate link for the operating system used on your computer. 

5. 
Follow the steps to run the installer. In the default setting, there are four panels in RStudio (see Fig. 17.1): 


1. 
The top left panel is a code editor (Panel 1) that can be used for writing code (i.e. a sequence of instructions that a computer can process and understand) and for sending it to the console to be executed. Previously saved code fles (or R scripts) can also be opened in this panel. 

2. 
The top right panel (Panel 2) is a workspace that describes the current working environment (i.e. the datasets and variables used) and keeps track of the history; datasets can also be imported from this panel. 

3. 
The bottom left panel is the console, i.e. a text entry and display device where code is run and numerical results are displayed (Panel 3). Code can be run directly in the console or sent to the console from the code editor (see Sect. 17.2.2.1). 

4. 
The bottom right panel (Panel 4) houses the help pages and displays any plots made. This panel also provides an overview of the packages (or libraries) installed (see Sect. 17.2.2.2). 



17.2.2 Getting Started with R 
An elementary understanding of how to write R code is necessary for working in R. In this subsection, an introduction to writing and running code will be presented in Sect. 17.2.2.1, followed by instructions for how to install and load packages (Sect. 17.2.2.2). As will become clear, there is typically more than one way of completing a task in RStudio; for most tasks in this subsection, two different options will therefore be presented. 
17.2.2.1 Writing and Running Code 
To exemplify how to write code and execute commands (i.e. run code), we will here show how R can be used as a calculator. To ask R to display the answer for 31+82, the frst step is to write 31+82 in the code editor (Panel 1) and press [enter]. Comments, for example specifying what the code does for future reference, 
M. Paquot and T. Larsson 


can also be entered here. This is typically done by starting each new line with one or several hash characters #; this way, R will not interpret what follows the hash symbol as code to be run. The next step is to run the code, which can be done in (at least) two different ways. The frst option is to mark the relevant code and press [run] in the upper-right corner of Panel 1. The second option is to either mark the relevant code or just have the cursor somewhere in that line and then press [control] (Windows/Linux/Mac OS) or [command] (Mac OS) + [enter] on the keyboard. The code will now appear along with the answer (i.e. 113) in the console (Panel 3), as shown in Fig. 17.2. 
As mentioned above, the code can also be run directly in the console (the way to do this is to type the code after the prompt (the last angled bracket) and press [enter]); however, it is easier to save and edit code that is written in Panel 1. 
It is often useful or even necessary to create variables or store results using the assignment arrow, <-(i.e. an angle bracket followed by a hyphen). This arrow maps the values on its right to the variable on its left. For example, once a variable y has been assigned the value 31+82 through the following code y<-31+82, typing y into the code editor and then running the code will produce the same result as typing 31+82, as seen in Fig. 17.3. A listing of all assigned variables can be found in the “Environment” tab in Panel 2. 
Tip 1 
If the symbol of the last line of the console (Panel 3) is a plus sign, +, rather than an angle bracket, >, it means that something is missing for the code to run. Very often, what is missing is a closing bracket, ), but this is of course not always the case. To solve the problem, either add what is missing directly into the console or press the escape key on the keyboard to be able to edit the code in the code editor (Panel 1) and then try to run it again. 
To clear the console, put the cursor in this window, press [control] and type the lower-case letter l. 
17.2.2.2 Installing and Loading Packages 
Although R is very useful in its default confguration, additional functionality in the form of packages, i.e. reproducible R code written and made available by independent developers, can be installed to enable more advanced data processing and visualization. Two different ways of installing packages will be described here; the package ggplot2 (Wickham 2009), which can be used for creating more advanced and customizable graphics than will be discussed in this introductory chapter, will be used as an example (see Chap. 23 an example of how to use this package). The frst option is to go to the Tools menu, click on “install packages” and then type ggplot2. In order to install more than one package at the same time, the package names have to be separated by a comma. The second option is to type install.packages(“ggplot2”) in the code editor and run the code.1A list of all installed packages can be found under the “Packages” tab in Panel 4. Once the packages are installed, they have to be loaded for every new session. To load a package, either (i) tick the box next to the name of the package in the “packages” tab in Panel 4 or (ii) type library(“ggplot2”) in the code editor and run the code. 
Tip 2 
To get help with the function install.packages (or any other function), either (i) search for the function using the “help” tab in Panel 4 or (ii) type ?install.packages in the code editor and run the code. 
To know how to cite R, type citation() in the code editor (Panel 1) and run the code. This function can also be used to get information about how to cite packages: citation(“[PACKAGE NAME]”). 
1Note that R is case sensitive. 
M. Paquot and T. Larsson 
17.3 Data Handling in R 
While data can certainly be entered manually, as has been done so far in this chapter, researchers most often import their datasets into R. This section describes how to prepare, load and manage datasets. 
17.3.1 Preparing the Data 
Data need to be stored in a format that makes them easy to manipulate with R. This means that, while it may be practical to enter the data into a spreadsheet software tool (and save the data in the original fle format of that application to preserve colors and other formatting), data should also be saved in a tab-delimited text or csv fle, as such fles are easier to import into R. For most of the statistical techniques illustrated in the next chapters (see Chap. 20, for exceptions), data should also adhere to the case-by-variable format: 
• 
The frst row should contain the names of all variables. 

• 
Each of the other rows should represent a data point, i.e. a single observation of the dependent variable. 

• 
The frst column should be used to uniquely identify each case by numbering all n cases from 1 to n. 

• 
Each of the other columns should represent one variable with respect to which every data point gets annotated. 

• 
Missing data are entered as NA. 


For working with R, Gries (2013:23) also recommends having the variable names in the frst row in all caps, to code the levels of categorical variables as character strings in lower case, and not to use special characters such as spaces, periods, etc., in variable names or levels. 
Tip 3 
To use statistical techniques correctly, it is important to know which type of variables are being analyzed: 
Nominal or Categorical Variables are variables with the least precise level of measurement. Different values (i.e. levels in R) of these variables represent different mutually exclusive categories. For example, the position of adverbs in a sentence can be a nominal variable (e.g. sentence initial, mid-position, sentence fnal). 
(continued) 
Ordinal Variables are ordered or ranked but equal intervals on the scale do not represent equal differences between the levels. As explained by Salkind (2005:276), 
The perfect example is a rank of candidates for a job. If we know that Russ is ranked #1, Sheldon is ranked #2, and Hannah is ranked #3, then this is ordinal arrangement. We have no idea how much higher on this scale Russ is relative to Sheldon than Sheldon is relative to Hannah. We just know that it’s better to be #1 than #2 than #3, but not by how much. 
Another often-cited example is answers in a questionnaire on a fve-point Likert scale (strongly disagree, disagree, no opinion, agree, strongly agree). 
Interval Variables are characterized by the fact that equal intervals on the scale represent equal differences between the values. A common example is temperature on the Celsius or Fahrenheit scale: 
The difference between 20 and 25 degrees is the same as the difference between 25 and 30 degrees. However, it is important that interval variables do not include a zero point, or, if they do, it is arbitrary. That is, the temperature of 0 degrees Celsius does not mean that there is no temperature. This is why it does not make sense to say that twenty degrees Celsius is twice as warm as ten degrees. (Levshina 2015:17). 
Ratio Variables are very similar to interval variables but they include zero on the scale and therefore allow for meaningful comparison of differences and ratios between values. Typical examples include annual salaries, word frequencies or reaction times in milliseconds. 
As will become clear under Sect. 17.3.3, different types of variables are represented and manipulated differently in R. 
Figure 17.4 exemplifes the case-by-variable format. It also provides the frst lines of the dataset that will be used in this chapter to illustrate how data can be described and visualized with the help of R. The dataset comes from Gries (2013) and was compiled to investigate the research question of what independent variables contribute to the ordering of the main clause and the subordinate clause in a given sentence (i.e. the dependent variable).2 Each case represents a sentence where a subordinate clause is introduced by the German or English conjunctions als/when, bevor/before, nachdem/after and weil/because (the nominal/categorical variable CONJ) in parallel corpus data. The variables ORDER, SUBORDTYPE and 
2We are grateful to Stefan Th. Gries for letting us use a dataset from his textbook on ‘Statistics for Linguistics with R’ to exemplify the methods and graphs described in this chapter. 
M. Paquot and T. Larsson 

MORETHAN2CL are three binary variables; i.e. the number of categories (so-called levels) is only two: 
• 
The dependent variable ORDER specifes whether the subordinate clause comes before the main clause (sc-mc) or whether the main clause comes frst (mc-sc). 

• 
The independent variable SUBORDTYPE specifes whether the subordinate clause is causal (caus) or temporal (temp). 

• 
The independent variable MORETHAN2CL specifes whether there are more clauses in the sentence than just the main clause and subordinate clause. 


LEN_MC, LEN_SC and LENGTH_DIFF are three numeric (ratio) variables which represent the number of words in the main clause, the number of words in the subordinate clause and the length difference between the two respectively (see Gries (2013:294) for more information about the dataset). 
17.3.2 Loading and Checking Data 
Once the dataset is prepared, it can be loaded into R. The default function for loading a fle in table format is read.table() but its default settings are not always optimal (e.g. the presence of a header is set to ‘no’ and there is no default separator) and might thus require some adjustments. As a result, other functions may be more practical. The read.csv() and read.delim() functions, for example, are wrapper functions for read.table() (with default argument values set for reading csv and tab-delimited fles respectively) that also include a header. Thus, the following two lines of code can be used interchangeably to load the tab-delimited 17_clauseorders.csv fle and assign its values to the dataframe cl.order, with the file.choose() function opening up a fle selection menu from where the appropriate dataset can be selected: 
> cl.order <-read.table(file.choose(), header = TRUE, sept = “\t”) 
> cl.order <-read.delim(file.choose()) 
More settings can be adjusted, including which character is used in the fle for decimal points (. or ,) and for quotes (see ?read.table() for a complete list of the arguments that the different functions can take). 
Tip 4 
To show R where to look for fles to read in, a working directory can be set. One easy way to do this is to go to the “Files” tab in Panel 4, click on the “ ...” button to the right, choose an appropriate folder, then click on “more” (next to the cogwheel symbol) and then click on “Set as working directory” in the drop-down menu. Another option is to specify the path to the desired folder inside the brackets of the function setwd() and then run the code. To fnd out what the current working directory is, type getwd() in the code editor and run the code. 
A typical way to proceed is to make certain that the fle has been loaded in the correct format using the function head(), which outputs the frst six lines of the fle: head(cl.order). The function str() can be used to see what type of data has been loaded: str(cl.order). A useful way of obtaining an overview of the dataset is to use the summary() function: As can be seen in Fig. 17.5, the type of 

M. Paquot and T. Larsson 
descriptive statistics varies depending on the type of data. For nominal/categorical data, such as ORDER and SUBORDTYPE, only the counts for each level are provided, whereas for numeric variables, such as CASE and LEN_MC, this function provides the values for the minimum, maximum, mean and median, as well as the value for the frst and third quartile (see Sect. 17.4.1 for more information about these measures of central tendency). 
Tip 5 
To access the content of a variable/column or object, R needs to know where that variable/column or object is located. For instance, if one wants to compute the mean of a numeric column in a data frame (e.g. the mean of words in the subordinate clause), R needs to know which data frame contains that column: 
> mean(cl.order$LEN_SC) # the $ separates the data frame and the variable 
One way not to have to type the name of the dataset every time is to attach the data frame to the R search path using the function attach(), in our case: attach(cl.order). When a dataset is attached, you can apply functions on variables (without the name of the data frame they are from) as follows: 
> mean(LEN_SC) 
17.3.3 Managing and Saving Data 
The cl.order data structure is a data frame, i.e. a list of variables represented as vectors and factors of various types. The numeric variables CASE, LEN_MC, LEN_SC and LENGTH_DIFF are numeric vectors (sequences of numbers) and the categorical variables ORDER, SUBORDTYPE, CONJ and MORETHAN2CL are factors. This distinction is important because vectors and factors are not manipulated in the same way in R. Table 17.1 provides a selected list of the most useful functions that take numeric vectors, factors and data frames respectively as arguments. Table 17.2 outlines some of the most useful data manipulation techniques for exploring data frames illustrated with examples from the cl.order dataset. See the accompanying R code for the generated output and Levshina (2015:397–408) for more details. 
Tip 6 
To know what type of object a variable is, the function is() can be used: 
is(LEN_MC) 
[1] "integer""numeric""vector" 
.... 
Numeric vectors 
length()  Check how many elements are included in a vector  
sort()  Sort a vector in increasing order  
table()  Tabulate a vector  

Factors 
as.numeric(as.character())  Turn a factor that contains numbers into a numeric vector  
length()  Check how many elements are included in a factor  
levels()  Return the levels of a factor  
table()  Tabulate a factor  

Data frames 
str()  Return the structure of the data frame  
write.table()  Export a data frame to a fle  
All data structure  

head()  Return the frst lines of a vector, matrix, table, data frame or function  
tail()  Return the last lines of a vector, matrix, table, data frame or function  
rm(a)  Remove the data structure a  
rm(list=ls(all=TRUE))  Clear memory of all data  

Table 17.2 The most useful data manipulation techniques to handle data frames 
cl.order[c(“CONJ”,”ORDER”)]  Select columns CONJ and ORDER  
cl.order[c(7,20,48),]  Select observations or cases (i.e. rows) 7, 20 and 48  
cl.order[SUBORDTYPE == “caus”,]  Select observations which meet specifc criteria, here the subordinate clause is causal.  
cl.order[order(LENGTH_DIFF),]  Sort the data frame by LENGTH_DIFF in ascending order  

17.4 Descriptive Statistics 
The focus of this section will be on the description of numeric variables. See the supplementary R fle for how to explore categorical data with the functions summary(), table() and prop.table(). 
17.4.1 Measures of Central Tendency 
The most straightforward measures of central tendency are the mean and the median. Each provides a different type of information about a distribution of values. The mean is simply the sum of all the values for a given variable, divided by the number of values for that variable. The median is defned as the midpoint in a set of ordered values; it is also known as the 50th percentile (or second quartile) because it is the point below which 50% of the cases in the distribution fall. Other percentiles are useful too, such as the 25% (1/4 of all ranked values are below this value; also 
M. Paquot and T. Larsson 
known as frst quartile) and the 75th percentile (3/4 of all ranked values are below this value; third quartile). A useful R command to get the above-mentioned statistics at once is summary(). It was already used above to summarize the whole data frame but it can also be used to summarize individual variables: 
>  summary(LEN_SC)  
Min. 1st Qu.  Median  Mean  3rd Qu.  Max.  
2.000  5.000  8.000  9.362  12.000  36.000  

One is, however, often not interested in the mean or median for a given independent variable, but instead in the central tendencies at each level of the independent variable. For example, we may want to determine whether the two types of subordinate clause differ with regard to their mean word length. To do so, the function tapply() can be used as follows: 
> tapply(LEN_SC, SUBORDTYPE, mean) caus temp 
10.105528 8.637255 
The function tapply() has three arguments. The frst is a vector or factor (here, LEN_SC) to which we want to apply a function as found in the third argument (here, mean). The second argument is a vector or factor that specifes the grouping of values from the frst vector/factor to which the function is applied. 
Tip 7 
The function table() can also be used to crosstabulate and characterize two or 
more variables and their relation: 
> table(LEN_SC,SUBORDTYPE) 
SUBORDTYPE 
LEN_SC caus temp 
2 26 
3 313 4 924 
(continued) 
5  25  23  
6  18  21  
7  19  19  
8  17  15  

... 
17.4.2 Measures of Dispersion 
Measures of central tendency should never be reported without some corresponding measure of dispersion. Without a measure of dispersion, it is not possible to know how good the measure of central tendency is at summarizing the data. An example from Gries (2013:119–120) will serve to illustrate this. Figure 17.6 shows the monthly temperatures of Town 1 and Town 2 with mean annual temperatures of 
10.25 and 9.83 respectively. A quick glance at Fig. 17.6 shows that the mean of the temperatures for Town 2 (9.83) summarizes the central tendency of Town 2 much better than the mean of the temperatures for Town 1 does for Town 1: the values of Town 1 vary much more widely around the mean with a minimum value of -12 and a maximum value of 23. 

M. Paquot and T. Larsson 
Thus, Gries (2013:119) recommends that the following measures of dispersion always be provided with measures of central tendency: 
• 
the interquartile range or quantiles for the median for interval/ratio-scaled data that (a) do not approximate the normal distribution (see Tip 10), or (b) exhibit outliers, i.e. values that are numerically distant from most of the other data points in a dataset (see Sect. 17.5.5 for a means to visualize outliers); 

• 
the standard deviation or the variance for normally distributed interval/ratio­scaled data. 


The interquartile range is the difference between the third (i.e. 75%) quartile and the frst (i.e. 25%) quartile. The more the frst quartile and the third quartile differ from one another, the more heterogeneous or dispersed the data are. To compute the interquartile range with R, the function IQR() can be used: 
> IQR(LEN_SC) [1] 7 
The standard deviation, sd, of a distribution with n elements is obtained by computing the difference of each data point to the mean, squaring these differences, summing them up, and after dividing the sum by n-1, taking its square root. In R, all that is required is the sd() function: 
> sd(LENGTH_DIFF) [1] 6.851885 
See Gries (2013:120–125) for other measures of dispersion such as the average deviation and the variation coeffcient. 
Measures of central tendency and dispersion can be reported in a table (especially if there are many of them) or in text. Using LENGTH_DIFF as an example, measures of central tendency and dispersion are typically reported as follows: “The mean score was -0.1 with a standard deviation of 6.85”. They can also be summarized as (M =-0.1, SD =6.85), in which M is the mean and SD is the standard deviation. The Publication Manual of the American Psychological Association (American Psychological Association 2010) also illustrates the use of the abbreviated format -0.1(6.85). 
Means (with standard deviations in parentheses) for Trials 1 through 4 were 
2.43 (0.50), 2.59 (1.21), 2.68 (0.39), and 2.86 (0.12), respectively. (p. 117) 
To report the median and interquartile range, the following format can be used: Mdn =0, IQR =-4–3, where IQR is more usefully reported as a range (reporting Q1 and Q3) rather than a value. 
17.4.3 Coeffcients of Correlation 
Coeffcients of correlation are typically used to investigate the relationship between two numeric variables (but there are also coeffcients of correlation used to investigate the relationship between a numeric variable and a categorical variable). A correlation between the numeric variables X and Y is positive if the values of both variables increase and decrease together; the correlation is negative if the values of variable X increase when the values of Y decrease, or if the values of variable X decrease when the values of Y increase. Correlation coeffcients range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with the plus or minus sign indicating the direction of the correlation, and the value refecting the strength of the correlation. To interpret its value, as a very coarse rule of thumb, see which of the following values in Fig. 17.7 the correlation is closest to (e.g. Rumsey 2016). 
Pearson’s product-moment coeffcient r is probably the most frequently used correlation coeffcient but its use is probably best restricted to interval-scaled (or ratio-scaled) variables that are both approximately normally distributed. It requires the relationship between variables to be monotonic (i.e. an increase in the values of variable X is followed by an increase in the values of variable Y; a decrease in the values of variable X is followed by a decrease in the values of variable Y) and linear 
(i.e. the values of variable Y increase or decrease at the same rate as the values of variable X do); it is also very sensitive to the presence of outliers. 
When these conditions are not met, another coeffcient such as Kendall’s t (‘tau’) is preferred. This correlation coeffcient is based only on the ranks of the variable values and is therefore more suited to ordinal variables; it is also less sensitive to outliers. 
To compute a correlation coeffcient with R, the cor() function is used as follows: 
> cor(LEN_MC,LEN_SC, method="kendall") [1] 0.1012148 
Kendall’s t is here selected because the assumptions for using Pearson’s r are not met: the relationship between variables is not monotonic and linear and the two variables are not normally distributed (see Fig. 17.11 for a histogram of LEN_MC). Thus, using the guidelines provided in Fig. 17.7, it can be seen that there is almost no relationship between the length of the main clause and the length of the subordinate clause in the cl.order dataset. 
The relationship between variables could be investigated more closely and we could try to predict values of the dependent variable on the basis of values of one or more independent variables and their interactions. This method is called linear regression and is discussed in Chap. 21. 
M. Paquot and T. Larsson 

perfect strong moderate weak | weak moderate strong perfect 
no linear relationship 
Fig. 17.7 Correlation coeffcients and their interpretation 
17.5 Data Visualization 
Visualization is an essential step in data exploration: it is a helpful way of getting an overview of the data (e.g. the type of distribution represented in the dataset), identifying areas that need attention (e.g. the presence of outliers or many zero values) and checking whether the assumptions of the selected statistical test are met (see more particularly in Chaps. 20–23). As will become clear from the subsequent chapters, data visualization can also provide a means of presenting the results in a reader-friendly manner. 
R offers many different ways of visualizing data and this section will present a selection of commonly used graphs. Section 17.5.1 covers barplots and how these can be customized; Section 17.5.2 introduces a version of barplots for two variables, namely mosaic plots. In these two sections, barplots and mosaic plots will be created using the function plot(). This function is a good function to start with as it chooses what type of graph to use based on the kind(s) of variables to be plotted. The subsequent sections cover graphs that can be used for numeric variables to display dispersion and central tendencies, namely histograms (Sect. 17.5.3), ecdf plots (Sect. 17.5.4), and boxplots (Sect. 17.5.5). The code fle that accompanies this chapter also includes code for making other common graphs, such as scatterplots. 
17.5.1 Barplots 
The plot() function will here be used to create a barplot displaying the fre­quencies of two of the nominal/categorical variables from the cl.order data frame, namely ORDER and CONJ. To graphically present the frequency differences between the two levels of ORDER, namely main clause followed by subordinate clause (mc-sc) and subordinate clause followed by main clause (sc-mc), type plot(ORDER) in the code editor and run the code. The frequencies of ORDER will be plotted as a barplot (Fig. 17.8). The graph will appear in Panel 4, in the “Plot” tab. Another and more customizable alternative would be to use the barplot() function; however, it requires the data to be entered as a vector or matrix, which means that the table() function has to be used for the code to run: barplot(table(ORDER)). 

Tip 8 
In the “plot” tab in Panel 4, users can navigate between recently made graphs utilizing the arrows, magnify the graph by clicking on “Zoom” and save the graph onto the hard drive. To save a graph, click on “Export” and then “Save image”. In the subsequent window, the size of the graph can be modifed, along with the format (JPEG, TIFF, etc.); the directory to which the graph will be saved can also be changed before pressing “Save”. Note that the size of the graph sometimes has to be modifed to ensure that all captions or labels are visible in the saved version of the graph. Figure 17.8 shows this graph saved as a JPG fle (size: 600 by 464 pts). 
While this graph is accurate and can certainly be useful, it is perhaps not suffciently informative for all purposes. Furthermore, the tickmarks on the y-axis do not extend to the size of the bar in this default setting, thereby making the frequency of mc-sc unnecessarily diffcult to read. To remedy some of these problems, the graph can be customized using the following line of code: 
> plot(ORDER, main = "Tokens per clause order", xlab = "Clause order", ylab = "Frequency", ylim=c(0,300), col = c("gray70", "gray40"), col.axis = "gray20") 
The main argument provides a heading for the graph; xlab labels the x-axis; ylim specifes the lower and upper limits of the y axis. The col argument is used to specify the colors of the bars (here, two different shades of gray are used); col.axis changes the color of the x and y axes. The graph can be customized further, for example with the addition of the frequencies on top of the corresponding bar, using the following lines of code: 
M. Paquot and T. Larsson 
Token per clause order 
Frequency 0 50 100 150 200 250 300 

Fig. 17.9 Customized barplot displaying the frequencies across ORDER 
> graph1 <-plot(ORDER, main = "Tokens per clause order", xlab = "Clause order", ylab = "Frequency", ylim=c(0,300), col = c("gray70", "gray40"), col.axis = "gray20"); text(graph1, table(ORDER), table(ORDER), pos=3) 
Here, two lines of code are merged with a semicolon. The frst line of code plots the barplot and assigns the values of the horizontal middles of the bars to the variable “graph1”; the second line of code prints the frequency of each level of ORDER on top of the corresponding bar (i.e. in position 3). This version of the graph can be seen in Fig. 17.9. 
17.5.2 Mosaic Plots 
Graphs can also be used for providing an overview of a data set containing two or more different variables of potential interest. Mosaic graphs are an example of such graphs used for nominal/categorical data. The plot() function will here display the relative frequency of each clause type order (mc-sc or sc-mc) per conjunction (when, before, after or because); that is, this function can be used to plot the variables ORDER and CONJ, using the code below. 
> plot(CONJ, ORDER, col = c("royalblue4","royalblue2"), main= "Clause order per conjunction", xlab = "Conjunction", ylab = “Clause order”) 

In addition to the plot itself, this line of code specifes the colors of the bars, gives the graph a heading and labels the x-axis (Fig. 17.10). Alternatively, the function mosaicplot() can be used, but as for the boxplots, this function requires the table() functiontobeusedaswell: mosaicplot(table(CONJ, ORDER)). 
This plot provides not only the relative frequency of each clause type per conjunction, but also the relative frequencies of the conjunctions (wide bar = more frequent, narrow bar = less frequent). As can be seen, weil/because is the most frequent conjunction, and this conjunction is nearly always found in a sentence where the main clause precedes the subordinate clause (mc-sc; e.g. he left early, because he needed time to think). The least frequent conjunction, bevor/before, also occurs more often in sentences with this clause order, whereas als/when and nachdem/after are more often found in sentences where the subordinate clause precedes the main clause (sc-mc; e.g. when she left, she realized that her laptop was still in her offce). This graph is thus an example of how visualizing data can facilitate interpretation of frequency trends. 
Tip 9 
To learn more about how to customize graphs using the different parameters available, type ?par into the code editor (Panel 1) and run the code. To get a list of the names of all the available colors, type colors() in the code editor and run the code. 
M. Paquot and T. Larsson 
The graphs shown so far have been used to display nominal/categorical variables, but R can of course be used to visualize other kinds of variables too. In the subsequent subsections, the numeric variables LEN_SC and LENGTH_DIFF will be explored. The frst two types of graphs, histograms and ecdf plot, will here be used to display the distribution of the numeric variable LEN_SC (i.e. the length of the subordinate clauses in number of words). 
17.5.3 Histograms 
A histogram is a type of bar graph that groups values into a series of intervals (or bins). While the bins are adjacent, their values are non-overlapping (i.e. each value is included in only one bin). The following customized code can be used to make a histogram in R: 
> hist(LEN_SC, main = "Length distribution of subordinate 
clauses", ylim=c(0, 200), col = "gray", xlab = "Length of 
the subordinate clause (number of words)", labels = TRUE) 
In addition to producing the histogram for LEN_SC, this line of code gives the graph a heading, specifes the limits of the y-axis, specifes the color and adds labels. The graph is shown in Fig. 17.11. 
As can be seen, the bulk of the subordinate clauses are between 5 and 10 words long. We can also note that the vast majority of subordinate clauses contain fewer 
Length distribution of subordinate clauses 

Length of the subordinate clause (number of words) 
Fig. 17.11 Customized histogram showing the distribution of LEN_MC 
than 20 words. Had the data been normally distributed, the histogram would have been bell-shaped and not, as is the case in Fig. 17.11, positively skewed. 
Tip 10 
As mentioned in Sect. 17.4.1 and 17.4.2, it is important to know what the data distribution looks like and whether it approximates a normal distribution or not to choose which measures of central tendency and dispersion to report. A normal distribution is represented by a bell-shaped curve as illustrated by the density plot below. It has the following three characteristics: 
1. 
The mean, median and mode are equal to one another. 

2. 
The bell-shaped (or normal) curve is perfectly symmetrical. 

3. 
The tails of the bell-shaped curve are asymptotic, i.e. they come closer and closer to the horizontal axis but never touch. 



Median Mode 
To check whether data resembles a normal distribution, histograms and Q-Q plots can be used (see functions in accompanying R code). 
As will be seen in Part V, the normal distribution is also often an assumption of statistical tests. When the assumption of normal distribution is not met, it is sometimes recommended to transform data to reduce the impact of outliers and obtain a more representative mean value by normalizing a non-normal distribution. Data transformation can also be employed to equate heterogeneous group variances, another common assumption of the statistical tests described in the next chapters. 
There is a wide range of transformation techniques available (cf. Sheskin 2003, Chap. 11, Sect. VII for a review) but the most commonly used with (often positively skewed) corpus linguistic data is probably the logarithmic transformation (using the function log() in R). As underlined by all authors discussing data transformations, however, there is no one-size-fts-all data 
(continued) 
M. Paquot and T. Larsson 
transformation technique and researchers should compare different transfor­mations and visualize their output. 
17.5.4 Ecdf Plots 
Another kind of graph that provides detailed information about the distribution is ecdf plots (short for empirical cumulative distribution function plots). This kind of graph shows the cumulative frequencies of variable values on the y axis (which ranges from 0 to 1). The following customized lines of code can be used to provide an overview of the distribution of LEN_SC: 
> plot(ecdf(LEN_SC), main="ecdf plot showing the length 
distribution of subordinate clauses", xlab="Length of the 
subordinate clause (number of words)", verticals=TRUE, 
pch="*", col.01line = "blue"); grid() 
The frst line of code specifes the type of graph to plot (here: edcf), the variable (here: LEN_SC), provides a heading, labels the x axis, binds together the points with vertical lines, specifes the plotting character, changes the color of the line that marks the upper and lower limit. The second line of code adds a grid in the background to facilitate reading. The graph is shown in Fig. 17.12. 
This type of graph shows how much of the data are covered for each value on the x axis. For example, as can be seen from the graph, approximately 70% 
Length distribution of subordinate clauses 

Fig. 17.12 Customized ecdf plot showing the distribution of LEN_MC 
of the subordinate clauses in the data are 10 words or shorter; more than 90% of the subordinate clauses in the data contain no more than 20 words. Ecdf plots are particularly useful for displaying the distribution of a dataset that contains many data points that have zero values, as ecdf plots, unlike other graphs such as boxplots, clearly display the proportion of such observations. 
17.5.5 Boxplots 
Boxplots are another type of graph that display a variety of descriptive statistics for the data. In boxplots, the heavy line represents the median, the box displays the interquartile range of the data, meaning that each box in a boxplot represents the 50% of the data closest to the median (see Sect. 17.4.1 and 17.4.2). The vertical lines extending from the boxes mark the minimum and maximum respectively or 
1.5 times the interquartile range; the dots above the boxes represent outliers. 
This kind of graph will now be used to illustrate how the conjunctions (CONJ) differ with regard to the length of the subordinate clauses (LEN_SC). The following code has been used to create the boxplot shown in Fig. 17.13. 
> boxplot(LEN_SC ~ CONJ, main="Length of the subordinate 
clause per conjunction", xlab="Conjunction", ylab="Length", 
col=c("gray40", "gray60", "gray80", "gray95"),ylim=c(0,35)); 
text(1:4, tapply(LEN_SC, CONJ, mean), "x", cex=0.6) 
The frst line of code specifes the graph type (boxplot), the variables to be plotted (LEN_SC as a function of CONJ) and the heading. It also labels the x-and y-axes, 
Length of subordinate clause per conjunction 

als/when bevor/before nachdem/after weil/because Conjunction 
Fig. 17.13 Customized boxplot displaying the lengths of the subordinate clauses per conjunction 
M. Paquot and T. Larsson 
specifes the colors for each of the boxes and the upper and lower limits for the y-axis. The second line of code plots the mean for each box using the character “x”; fnally, the size of this character (here: 60% of its original size) is specifed. 
Tip 11 
The function par() can be used to create paneled graphs (i.e. two or more graphs placed below or next to one another). The frst number specifes the number of rows and the second one the number of columns. The following line of code thus enables placement of two graphs next to one another: par(mfrow = c(1, 2)). This code should be followed by the code creating the graphs themselves. The line par(mfrow = c(1, 1)) is used to restore the default settings once the graphs have been created. 
17.6 Conclusion 
Descriptive statistics and their visualization are an essential part of data exploration but they are not always given the attention they deserve. Yet, many violations of the underlying assumptions of commonly used statistical techniques (see more particularly Chaps. 20–23) could be avoided by applying better data exploration (cf. Zuur et al. 2010). For example and as explained above, a boxplot is a very useful tool to explore the spread of the data and the presence of outliers, and a histogram or a Q-Q plot can be used to check whether the data approximate the normal distribution. 
Descriptive statistics should also be reported with care in research papers, making sure at least the following values are reported: sample sizes, number of cases by main groups (i.e. typically the output of the function table() used for cross-tabulating two variables), measures of central tendency and measures of dispersion (see further discussion in Chap. 26). This is also particularly important if we want our research to contribute to the feld and be subject to meta-analysis (cf. Chap. 27). 
Further Reading 
Chang, W. 2013. R Graphics Cookbook. O’Reilly Media. 
In Chang (2013), step-by-step information about how to generate graphs in R is provided. The book includes 150 “recipes” specifying how to use R (in its default confguration and with packages such as ggplot2) to explore, summarize and interpret data. It also includes information about how to customize graphs. 
Levshina, N. 2015. How to do linguistics with R. Data exploration and statistical analysis. Amsterdam and Philadelphia: John Benjamins. 
This textbook is a very user-friendly introduction to R for linguistics. It comes with two very practical appendixes: (1) Most important R objects and basic operations with them and (2) Main plotting functions and graphical parameters in R. 
Zuur, A. F., Ieno, E. N., and Elphick, C.S. 2010. A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution 1:3–14. 
This article provides a protocol for data exploration, discusses current techniques to detect outliers, heterogeneity of variance, collinearity, etc. and provide advice on how to address these problems when they arise. It also addresses misconceptions about normality, and provides advice on data transformation. 
References 
American Psychological Association. (2010). Publication manual of the American Psychological Association. Washington, DC: American Psychological Association. Gries, S. T. (2013). Statistics for linguistics with R: A practical introduction (2nd ed.). Berlin: De Gruyter Mouton. Levshina, N. (2015). How to do linguistics with R. Data exploration and statistical analysis. Amsterdam/Philadelphia: John Benjamins. 
R Core Team. (2019). R: A language and environment for statistical computing. Vienna: R Foundation for Statistical Computing. https://www.R-project.org/. Accessed 17 June 2019. RStudio Team. (2017). RStudio: Integrated development for R. Boston: RStudio Inc. http:// 
www.rstudio.com/. Accessed 17 June 2019. Rumsey, J. (2016). Statistics for dummies (2nd ed.). Hoboken: Wiley. Salkind, N. (2005). Statistics for people who (think they) hate statistics. London: SAGE. Sheskin, D. (2003). Handbook of parametric and nonparametric statistical procedures.New York: 
Chapman & Hall. Zuur, A. F., Ieno, E. N., & Elphick, C. S. (2010). A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution, 1, 3–14. Wickham, H. (2009). ggplot2: Elegant graphics for data analysis. New York: Springer. 
Chapter 18 Cluster Analysis 
Hermann Moisl 
Abstract Cluster analysis comprises a collection of mathematically-based compu­tational methods for identifying structure in data that is too large or too complex, or both, to be easily interpreted by direct inspection. Its main use in corpus linguistics is hypothesis generation: any structural regularities that cluster analysis fnds in data abstracted from a corpus are used to draw inferences on the basis of which one or more hypotheses about the corpus content can be formulated. After motivating the use of cluster analysis with an example, the fundamentals of cluster analysis are introduced, including the mathematical concepts of vector space and proximity in vector space. Two of the most frequently used methods, k-means and hierarchical analysis, are then described, followed by outlines of two representative case studies. The discussion concludes with a guide to cluster analysis using R, advice on how clustering results should be reported, and some additional readings. 
18.1 Introduction 
The advent of digital technology has generated a rapid increase in the number and size of corpora available to the linguist, and data abstracted from these can be so complex as to be impenetrable to understanding by direct inspection. The aim of this chapter is to outline how cluster analysis can be used for interpretation of such data. Cluster analysis is primarily a tool for hypothesis generation. It identifes structure that is latent in data, awareness of which can be used to draw inferences on the basis of which a hypothesis is formulated. The method has been used in historical, grammatical, geographical, and social variation research 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_18) contains supplementary material, which is available to authorized users. 
H. Moisl (•) Newcastle University, Newcastle upon Tyne, UK e-mail: hermann.moisl49@gmail.com 
© Springer Nature Switzerland AG 2020 401 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_18 
as well as in language processing technologies such as information extraction, question answering, machine translation, and text type identifcation. Examples for research on synchronic grammar are Korhonen (2010), Hauer and Kondrak (2011), Gries (2010), and Gries and Stefanowitsch (2010); for historical linguistics, Kessler (2008), Delmestri and Cristianini (2012); for dialectology Wieling et al. (2011), Meschenmoser and Prl (2012); and for sociolinguistics Ruette et al. (2013), and Grieve et al. (2011). 
The chapter frst presents the fundamentals of cluster analysis and describes two of the most frequently used methods, then briefy outlines two representative clustering applications, and fnally presents a guide to cluster analysis of linguistic data using R together with recommendations for presentation of clustering results and some additional reading. 
18.2 Fundamentals 
18.2.1 Motivation 
Assume a sociolinguist wants to understand the nature of phonetic usage in some speech community of interest, that is, whether there is systematic rather than random variation among speakers. A representative corpus of speech is collected, a set of phonetic variables is defned, and the number of times each speaker uses each of these variables is recorded, thereby building up a body of data. Table 18.1 shows the number of times each of 24 randomly selected speakers, one per row, uses each of the 12 phonetic variables in the columns. What hypothesis would one formulate from visual inspection of this data, taking into account all the variables? And what about, say, 100 speakers and 150 variables? These questions are clearly rhetorical, and there is a moral: human cognition is unsuited to seeing regularities in anything but the smallest collections of numerical data. To see the regularities we need help, and that is what cluster analysis provides: it is a family of computational methods for identifcation and graphical display of structure in data when the data are too large either in terms of the number of variables or of the number of objects described, or both, to be readily interpretable by direct inspection. 
There are numerous clustering methods. A frequently used one is hierarchical analysis, described later in this discussion. Hierarchical analysis of the data in Table 
18.1 yields the results displayed on Figure 18.1. Hierarchical cluster analysis is so called because it arranges data objects into a hierarchy of relative similarities. Each row in Table 18.1 is a profle of a speaker’s phonetic usage as described by the frequency of the various phonetic variables, and visual inspection of the table will show that some profles are more similar to one another than to others. Hierarchical analysis uses these relative degrees of similarity to group the profles into clusters and to display the cluster structure as a tree. How a tree such as the one in Fig. 18.1 is constructed will be described later on. For the moment it suffces to observe that similarity between cluster subtrees is represented by the lengths of the vertical arcs joining them: the shorter the arcs the more similar the clusters. In Fig. 18.1 the arcs labelled A and B are much longer than any others in the tree, which shows that the speaker profles fall into two clusters, where all the profles in cluster A are more similar to one another than to any in cluster B and vice versa. A and B can be further decomposed into subclusters, so that, for example cluster A consists of subclusters C and D, C consists of E and F, and so on. The complete tree gives an exhaustive picture of the similarity structure of the phonetic profles in Table 18.1. Based on Fig. 18.1, a reasonable hypothesis is that there is systematic phonetic variation in the speech community, and more specifcally that the speakers who constitute that community fall into two main groups. 
Speaker  @1  @2  o:  @3  ¨. e ¨. n  a:1  a:2  a ¨. r  w  
g01  3  1  55  101  33  26  193  64  1  8  54  96  
g02  8  0  11  82  31  44  205  54  64  8  83  88  
g03  4  1  52  109  38  25  193  60  15  3  59  101  
n01  100  116  5  17  75  0  179  63  0  19  46  62  
g04  15  0  12  75  21  23  186  57  6  12  32  97  
g05  14  6  45  70  49  0  188  40  0  45  72  79  
g06  5  0  40  70  32  22  183  46  0  2  37  117  
n02  103  93  7  5  87  27  241  52  0  1  19  72  
g07  5  0  11  58  44  31  195  87  12  4  28  93  
g08  3  0  44  63  31  44  140  47  0  5  43  106  
g09  5  0  30  103  68  10  177  35  0  33  52  96  
g10  6  0  89  61  20  33  177  37  0  4  63  97  
n03  142  107  2  15  94  0  234  15  0  25  28  118  
n04  110  120  0  21  100  0  237  4  0  61  21  62  
g11  3  0  61  55  27  19  205  88  0  4  47  94  
g12  2  0  9  42  43  41  213  39  31  5  58  124  
g52  11  1  29  75  34  22  206  46  0  29  34  93  
g53  6  0  49  66  41  32  177  52  9  1  68  74  
n05  145  102  4  6  100  0  208  51  0  22  61  104  
n06  109  107  0  7  111  0  220  38  0  26  19  70  
g54  3  0  8  81  22  27  239  30  32  8  80  116  
g55  7  0  12  57  37  20  187  77  41  4  58  101  
g56  12  0  21  59  31  40  164  52  17  6  45  103  
n07  104  93  0  11  108  0  194  5  0  66  33  69  


18.2.2 Data 
The validity of cluster analysis depends crucially on the characteristics of the data to which it is applied. Data are constructed from observation of things in the world, and the process of construction raises a range of issues that determine the amenability of the data to analysis and the interpretability of the analytical results. On the one hand, nothing can be discovered that is beyond the limits of what the data say about the world. On the other, failure to understand and where necessary to emend relevant characteristics of data can lead to results and interpretations that are distorted or even worthless. It is, therefore, crucial to ensure that, prior to applying any form of cluster analysis, the various data preparation issues have been addressed. These issues cannot be dealt with here, but are covered in detail in Moisl (2015: Chap. 3). 
18.2.3 Clustering 
This section frst discusses the nature of clusters, then presents a geometric concept fundamental to most types of clustering, and fnally describes two of the most frequently used methods, k-means and hierarchical analysis. 
18.2.3.1 Cluster Defnition 
Human perception is optimised to detect patterning in the environment, and clusters are a kind of pattern. Contemplation of a rural scene reveals clusters of trees, of farm buildings, of sheep. Looking up at the night sky reveals clusters of stars, and a casual observer looking at the scatterplots in Fig. 18.2 would say that 18.2a shows a few small concentrations of points but is essentially random, that 18.2b has three identifable clusters of roughly equal size, that 18.2c has two clusters of unequal size, and that 18.2d has two intertwined, semi-circular clusters, all embedded in a random scatter of points. That observer would, moreover, have been able to make these identifcations solely on the basis of innate pattern recognition capability and without recourse to any explicit defnition of the concept ‘cluster’. 
Direct perception of patterns is the intuitive basis for understanding what a cluster is, and is fundamental in identifying the cluster structure of data, but it has two main limitations. One limitation is subjectivity and consequent unreliability. This subjec­tivity stems from the cognitive context in which a data distribution is interpreted: the casual observer brings nothing to the observation but innate capability, whereas the researcher who compiled the data and knows what the distribution represents brings prior knowledge which potentially and perhaps inevitably affects interpretation. In Fig. 18.2c, for example, does the larger cluster on the upper right contain two subclusters? What would the answer be if it were known that the points represent cats in the upper part of the cluster and dogs in the lower? The other limitation is that reliance on innate perceptual capability for cluster identifcation is confned to what can be perceived, and in the case of data this means a maximum dimensionality of 3 or less for graphical representation; there is no way of directly perceiving clusters in data with dimensionality higher than that. 

The obvious way to address these limitations is by unambiguous defnition of what a cluster is, relative to which criteria for cluster membership can be stated and used to test perceptually-based intuition on the one hand and to identify non­visualisable clusters in higher-dimensional data on the other. Textbook discussions of cluster analysis uniformly agree, however, that no one has thus far succeeded in formulating such a defnition. In principle, this lack deprives cluster analysis of a secure theoretical foundation. In practice, the consensus is that there are intuitions which, when implemented in clustering methods, give conceptually useful results, and it is on these intuitions that contemporary cluster analysis is built. 
The fundamental intuition underlying cluster analysis is that data distributions contain clusters when the data objects can be partitioned into groups on the basis of their relative similarity such that the objects in any group are more similar to one another than they are to objects in other groups, given some defnition of similarity. The most commonly used similarity defnition is based on the concept of proximity in vector space. 
18.2.3.2 Proximity in Vector Space 
Once data are represented as a matrix like that in Table 18.1, they have a geometrical interpretation in terms of the concept of vector space. Given a matrix M with m rows and n columns: 
• 
The dimensionality of M, that is, the number n of columns representing the n data variables, defnes an n-dimensional data space. 

• 
Each of the m row vectors is a point in that space. 

• 
The sequence of n numbers comprising each row vector specifes the coordinates of the point in the space. 


This is shown for a two and three dimensional vector spaces in Fig. 18.3, though the idea extends to any dimensionality n.For n = 4 the analogy between mathematical and physical space breaks down in that four and higher dimensional spaces can neither be conceptualised nor represented in terms of physical space. Mathematically and geometrically, however, higher-dimensional spaces are defned and used in the same way as the foregoing lower-dimensional ones, so that a four-dimensional vector is a point in four-dimensional space, a fve-dimensional vector is a point in fve-dimensional space, and so on for any n. The important thing to realise 
n 
n 
3
a 12 b 0.2  0.2  0.2 
0.7  0.4  0.3 
0.8  0.8  0.4 

1 2 1
1 m2 
m2 3 
3 
0.2  0.4  
0.3  0.5  
0.7  0.2  


1 
1 0.8 
0.8 

. [0.8,0.8,0.4]
0.6 
0.6 
0.4
. [0.3,0.5] 0.4 
. [0.2,0.4] 
. [0.7,0.4,0.3] 
0.2 . [0.2,0.2,0.2]
. [0.7,0.2] 
0.2 
0 0 0.5 
0 
0.5
0 0.5 1 
1 
Fig. 18.3 Vector spaces with corresponding matrices. (a) A two-dimensional vector space con­taining three vectors. (b) A three-dimensional vector space containing three vectors 
is that the intuitive concepts of physical space and dimension are useful metaphors for interpreting the corresponding mathematical concepts, but that the metaphors do not constrain what is possible mathematically. 
Given a distribution of vectors in a space V, the geometrical proximity of two vectors v1 and v2 in the space can be measured by determining the angle between them (Moisl 2015:28–47), but is most often done by measuring the distance separating them in the space. There are numerous measures (Deza and Deza 2009), but by far the most often used is Euclidean one, which is the shortest distance between points. It is calculated using the Pythagorean formula for fnding the length of the hypotenuse of a right-angled triangle, as in Fig. 18.4. 
18.2.3.3 Clustering Methods 
Clustering methods are standardly divided into two types: (i) non-hierarchical, which partition data vectors into some number k of disjoint groups such that the members of any given group are closer to one another in the data space than they are to members of any other group, and (ii) hierarchical, which show the distance relations among data vectors as a tree and leave the cluster structure to be 
Fig. 18.4 Finding the 
distance(v1,v2) = 
(7–2)2 + (7–4)2
Euclidean distance between 
two vectors v1 and v2 
10 
v2 = [7,7] 
length = 7 – 4 5 
0 
inferred from the tree by the researcher. Because these types offer complementary information about the structure of data, examples of both are described here, starting with a non-hierarchical one. 
K-means 
K-means clustering is the most frequently used non-hierarchical clustering method. The following account frst describes the standard k-means algorithm and then identifes issues associated with it. 
K-means is based on the idea that, for a given set of vectors V, each cluster is represented by a prototype vector, and a cluster is defned as the subset of vectors in V which are in distance terms closer to the prototype than they are to the prototype of any other cluster. A mathematical function known as an objective function is used to fnd a set of clusters each of which optimally meets this criterion; more is said about the form of such a function below. For V comprising mn-dimensional vectors, V is partitioned into kprototype-centred clusters by the following iterative procedure: 
1. 
Initialise by selecting kn-dimensional prototype locations in the vector space; these can be anywhere in the space. The prototypes are the initial estimates of where the clusters are centred in the space, and their locations are refned in subsequent steps. Placement of initial prototypes and selection of a value for k, that is, of the number of clusters, is non-trivial, and is further discussed below. 

2. 
Assign each of the mdata vectors to whichever of the kprototypes it is closest to in the space using a suitable distance measure such as the Euclidean. This yields kclusters. 

3. 
Calculate the centroid of each of the k clusters resulting from (2), where ‘centroid’ is here understood as the centre of a distribution of vectors in n-dimensional space. The centroid of a set of row vectors in a matrix M is 




Table 18.2 Calculating the centroid of a set of vectors 
Variable 1  Variable 2  Variable 3  
M  V1  0.1  0.2  0.3  
V2  0.4  0.5  0.6  
V3  0.7  0.8  0.9  
V4  0.9  0.8  0.7  
V5  0.6  0.5  0.4  
V6  0.3  0.2  0.1  
Centroid  0.5  0.5  0.5  

calculated by taking the mean of the column vectors of M, as exemplifed in Table 18.2. Each centroid becomes a new cluster prototype. 
4. Repeat (2) and (3) until the objective function is optimised, that is, until the centroids stop changing their locations in the space. 
This procedure is visualised in Fig. 18.5 for 29 data points in a two dimensional space, though it extends to any dimensionality. There are three visually obvious clusters, labelled A – C, and the aim is for the above procedure to fnd them by identifying the cluster centroids in a way that optimises the objective function. 
Figure 18.5a shows a scatter plot of the 29 data points and k = 3 prototypes randomly selected in the space and represented as crosses. The points are assigned to their nearest prototypes, which results in the clusters enclosed by the rectangles in Fig. 18.5b. Cluster centroids are now calculated; their positions, shown in 18.5c, refect the means of the data points in the clusters of 18.5b, and they become the new prototypes. Because the positions of the prototypes have changed, so does the clustering, as shown in 18.5d. The cluster centroids are once again calculated; their new positions are shown in 18.5e and the associated clustering in 18.5f. Further iterations will not change the positions of the centroids, and the procedure terminates. The data points associated with the fnal prototypes correspond to the visually identifable clusters in Fig. 18.5a, and the k-means procedure can therefore be said to have partitioned the data set into three disjoint subsets. The further claim is that, for k = 3, the partition is optimal in the sense that it has minimised an objective function. Where Euclidean distance is used, the objective function is usually the sum of squared errors (SSE): 
SSE = i=1..kx.Ci (x - pi)2 (18.1) 
where x is a data point, Ci is the i’th of k clusters, and pi is the prototype of the i’th cluster. This expression says that the SSE is the sum, for all k clusters, of the Euclidean distances between the cluster prototypes and the data points associated with each prototype. For k-means to have optimised this function, the prototypes have to be placed in the data space so that the Euclidean distances between them and their associated data points is globally minimised across all clusters. It is easy to see that this is what k-means does: the procedure converges on stable cluster centroids, and a centroid is by defnition the minimum distance from all the points on which it is based. 

The k-means algorithm is easy to understand and its results easy to interpret, it is theoretically well founded in linear algebra, and its effectiveness has repeatedly been empirically demonstrated in research applications. It also has well known problems, however; the main ones are outlined below. 
• 
Selection of initial cluster centroids 

K-means requires the user to specify the locations of the initial k prototypes c1 ...ck in the data space, but different sets of initial prototypes can lead to different fnal ones and thereby to different partitions of the data into clusters. In Fig. 18.6 initial prototypes are shown as flled circles and fnal ones as crosses; given the correct k = 3, one initialisation led to a clustering compatible with visual intuition, and the other did not. The standard solution is to conduct a succession of analyses on the same data using a different, usually randomly-generated set of initial prototypes for each analysis, keeping a record of the SSE value in each case. The best analysis, that is, the one with the optimal SSE, is then selected. 

• 
How many clusters? 


K-means requires the user to specify the number of clusters k to identify in the data. If the value chosen for k is incompatible with the number of clusters the data actually contains, however, the result will be misleading because k-means will deliver k clusters whatever the actual number of clusters intrinsic to the data, including none. For example, Fig. 18.7 shows the cluster structure from Fig. 18.6, but with k = 2 and k = 4: in both cases k-means fails to identify the visually-clear cluster structure. 
The obvious solution is to base the selection of k on reliable apriori knowledge of the domain from which the data comes, where available, but this obviates the point of the exercise, which is to identify unknown structure in data. Failing this, some other clustering method such as the one covered next can be used to gain insight into its cluster structure, or one of the range of initialisation heuristics proposed in the literature can be applied (Xu and Wunsch 2009:Chap. 4.3; Mirkin 2011:Chap. 6.2.7). A frequently used heuristic is to conduct multiple analyses on the same data using different values of k and to select the best one, given some defnition of “best” such as SSE: the value of k which optimises SSE is the one selected. Selection of k and selection of prototypes for any given k are obviously 
11 
0.8  0.8  
0.6  0.6  
0.4  0.4  
0.2  0.2  
0  0  
0 0.5  1 0 0.5  1  

(a) (b) 
Fig. 18.6 Effect of initial prototype location on k-means cluster solutions 

k = 2 k = 4 
interrelated and should therefore be done in conjunction (see Sect. 18.3 for how this can be done). 
• 
Cluster shape 

K-means is limited in the shapes of clusters it can identify. This is demonstrated with reference to the clusters in Fig. 18.8.The k-means analysis of the data underlying Fig. 18.8a–d gave the results shown in Fig. 18.9.InFig. 18.9a–d the consensus prototypes arrived at by multiple initialisations of k-means are shown by crosses, and the data points associated with each prototype are enclosed by boxes. The partitions of 18.9a and 18.9b accord with visual intuitions about cluster structure. However, those of 18.9c and 18.9d do not; this failure is symptomatic of a fundamental limitation on the clustering capability of k-means, which has to do with linear separability (Moisl 2015:189–201). 

• 
Outliers 


Because k-means is based on centroids, it is strongly affected by the presence of outliers which distort the location of the centroids in the data space. Outliers in data should therefore be identifed and eliminated prior to analysis. 
Hierarchical Clustering 
Given an m × n matrix M which represents m objects in n-dimensional space, hierarchical cluster analysis constructs a tree which represents the distance relations among the m objects in the space, as shown in Fig. 18.1. 
Construction of a cluster tree is a two-step process: the frst step abstracts a distance table from M, and the second constructs the tree by successive transfor­mations of the table. An intuition for how tree construction proceeds is best gained by working through an example; the example that follows is based on the frst 6 rows of the data matrix in Table 18.1. The Euclidean distances between all possible pairings of these 6 rows were calculated and stored in a 6 × 6matrixD,shown in 

(a) (b) 

(c) 
(d) 

(a) 
(b) 



0 0.25 0 0.25 
0.15 0.15 
+  +  


0  0.25  0  0.25  
(c)  (d)  
Fig. 18.9  K-means solutions for the clusters in Fig. 18.8  

Table 18.3. The Euclidean distance from g01 to itself is 0, g01 to g02 is 116.9, g01 to g03 is 58.9, and so on. To simplify the discussion to follow, it is observed that the table is symmetrical on either side of the zero-values on the main diagonal because the distance between any two row vectors in the data matrix is symmetrical – the distance from g02 to g03 is the same as the distance from g03 to g02. Since the 
Table 18.3 Euclidean distance matrix for the frst six rows of Table 18.1 
g01  g02  g03  g04  g05  g06  
g01  0  116.9  58.9  82.6  103.8  69.7  
g02  116.9  0  113.2  98.8  124.4  116.8  
g03  58.9  113.2  0  79.4  112.9  69.3  
g04  82.6  98.8  79.4  0  108.8  78.9  
g05  103.8  124.4  112.9  108.8  0  112.9  
g06  69.7  116.8  69.3  78.9  112.9  0  

Table 18.4 The Euclidean distance matrix of Table 18.3 with upper triangle values removed 
g01  g02  g03  g04  g05  g06  
g01  0  
g02  116.9  0  
g03  58.9  113.2  0  
g04  82.6  98.8  79.4  0  
g05  103.8  124.4  112.9  108.8  0  
g06  69.7  116.8  69.3  78.9  112.9  0  

upper-right triangle simply duplicates the lower-left one it can be deleted without loss of information. The result is shown in Table 18.4. 
Given m objects to be clustered, construction of a tree begins with m clusters each of which contains a different single object. Thereafter, the tree is constructed in a sequence of steps in which, at each step, two clusters are joined into a superordinate cluster and the distance matrix D is transformed so as to incorporate the newly created cluster into it. The sequence ends when only one cluster, the tree itself, remains and D is empty. The joining of clusters requires some criterion for deciding which of the clusters available at any given step in the tree construction process should be selected for combination. The literature contains a variety of different criteria, and some of these will be presented below; the one chosen for detailed description joins the two clusters with the smallest distance between them in the distance matrix. 
The following sequence of cluster joins and matrix transformations exemplifes this. Initially, each row vector of the data matrix is taken to be a cluster on its own. Fig. 18.10a shows the original distance matrix of Table 18.4. It is searched to fnd the smallest distance between clusters. This is the distance 58.9 between column 1, that is, g01 and row 3, that is, g03, shown bold-face. These are combined into a new composite cluster (1,3). 
Figure 18.10a is now transformed into the one in 18.10b. Rows and columns (1) and (3) are removed from 18.10a and replaced in 18.10b with a single blank row and column to accommodate the new (1,3) cluster; 0 is inserted as the distance from (1,3) to itself. Into the blank cells of the (1,3) row and column of Fig. 18.10b are inserted the minimum distances from (1,3) to the remaining clusters (2), (4), (5), and (6). What does this mean? Referring to Fig. 18.10a, the distance between (1) and (2) is 116.9 and between (3) and (2) it is 113.2; the minimum is 113.2, and that value is inserted into the cell representing the minimum distance between (1,3) and (2) in Fig. 18.10b. The distance between (1) and (4) in 18.10a is 82.6 and between (3) and (4) is 79.4; the latter value is inserted into 10b as the minimum distance between (1,3) and (4). By the same procedure, the minimum distances between (1,3) and 5 and between (1,3) and 6 are inserted. The resulting table is smaller by one row and one column; inserted values are shown in bold-face, and the remaining ones are unchanged. Figure 18.10c is a list showing the sequence of cluster joins together with the distance at which they were combined; this list is the basis for the graphical representation of the cluster tree shown in 18.10d. The scale line below the tree in 18.10d allows the approximate joining distance to be read from the graphical representation. One can, for example, see that (1) and (3) are joined just short of 60. The reduced matrix of Fig. 18.10b is used as the basis for the next step, shown in Fig. 18.11a. 

ThematrixinFig. 18.11a is searched to fnd the smallest distance between clusters. This is 69.3 between (1,3) and (6), and these are combined into a composite cluster ((1,3),6). The matrix is transformed into the one in 18.11b as in the previous step: rows and columns (1,3) and (6) are removed and replaced with a single blank row and column to accommodate the new ((1,3),6) cluster, with 0 inserted as the distance from ((1,3),6) to itself. Into the blank cells of the ((1,3),6) row and column are inserted the minimum distance from ((1,3),6) to the remaining clusters (2), (4), and (5). Referring to Fig. 18.11a, the distance between (1,3) and (2) is 113.2 and between (6) and (2) it is 116.8; the minimum is 113.2, and that value is inserted into the cell representing the minimum distance between ((1,3),6) and (2). The distance between (1,3) and (4) is 79.4 and between (6) and (4) is 78.9; the latter value is inserted into the matrix as the minimum distance between ((1,3),6) and (4). By the same procedure, the minimum distance between ((1,3),6) and 5 is inserted. The resulting table is again smaller by one row and one column; inserted values are highlighted, and the remaining ones are again unchanged. Figure 18.11d shows the tree after the second step together with the distance between (1,3) and (6). The reduced matrix of Fig. 18.11b is used as the basis for the next step. The full derivation is given in Moisl (2015:202–08). 

For a matrix with m rows, there will at any step in the above tree-building sequence be a set of p clusters, for p in the range 2 ...m, available for joining, and two of these must be selected. At the frst step in the clustering sequence, where all the clusters contain a single object, this is unproblematical: simply choose the two clusters with the smallest distance between them. At subsequent steps in the sequence, however, some criterion for judging relative proximity between composite and singleton cluster pairs or between composite pairs is required, and it is not obvious what the criterion should be. The one exemplifed in the foregoing sequence is such a criterion, known as Single Linkage, but there are various others. For simplicity of exposition, it is assumed that a stage in the tree building sequence has been reached where there are p = 3 clusters remaining to be joined. This is shown in Fig. 18.12: 18.12a shows a scatterplot of the data being clustered, and 18.12b the current state of tree construction. 
Which pair of subtrees should be joined next? Based on visual examination of the scatterplot, the intuitively obvious answer is the pair of clusters closest to one another, that is, A and B. Where the data are higher-dimensional and cannot be directly plotted, however, some explicit specifcation of closeness is required. This is what the following cluster-joining criteria provide. 
• The Single Linkage criterion defnes the degree of closeness between any pair of clusters (X,Y) as the smallest distance between any of the data points in X and 

0 10 
(a) (b) 
any of the data points in Y: if there are x vectors in X and y vectors in Y, then, for i = 1 ...x, j = 1 ...y, the Single Linkage distance between X and Y is defned as 
  
Single LinkageDistance(X,Y) = min dist Xi,Yj (18.2) 
where dist(Xi,Yj) is the distance between the i’th vector in X and the j’th vector in Y stated in terms of whatever metric is being used, such as Euclidean distance. The Single Linkage distances between all unique pairs of the p vectors remaining to be clustered are calculated, and the pair with the smallest distance is joined. This is exemplifed for the three clusters of Fig. 18.12 in Fig. 18.13. 
The arrowed lines in Fig. 18.13a represent distances between the points closest to one another in cluster pairs (A,B), (A,C), and (B,C); the one between A and B is shortest, so these two clusters are joined, as in Fig. 18.13b. Single Linkage is also known as Nearest Neighbour clustering. 
• Complete Linkage defnes the degree of closeness between any pair of clusters (X,Y) as the largest distance between any of the data points in X and any of the data points in Y: if there are x vectors in X and y vectors in Y, then, for i = 1 ...x, j = 1 ...y, the Complete Linkage distance between X and Y is defned as 
  
Complete Linkage Distance(X,Y) = max dist Xi,Yj (18.3) 
where dist (Xi,Yj) is the distance between the i’th vector in X and the j’th vector in Y stated in terms of whatever metric is being used. The Complete Linkage distances between all unique pairs of the p vectors remaining to be clustered are calculated, 
C 
B 

A 
A 

C 
B 


(a) (b) 
Fig. 18.13 Single Linkage 
C 
B 

A 
A 
C 

B 


0  16  0  10  
(a)  (b)  
Fig. 18.14  Complete Linkage  

and the pair for which the Complete Linkage distance is smallest is joined. This is exemplifed for the three clusters of Fig. 18.12 in Fig. 18.14. 
The arrowed lines in Fig. 18.14a represent distances between the points furthest from one another in cluster pairs (A,B), (A,C), and (B,C); the one between A and B is shortest, so these two clusters are joined, as in 18.14b. The intuition behind this joining criterion may not be immediately obvious, but is does make sense: fnding and joining the cluster pair with the smallest maximum distance between their members creates a cluster with the smallest diameter at that stage in the clustering procedure, and therefore the most compact cluster. Complete Linkage is also known as Furthest Neighbour clustering. 
• The Average Linkage criterion defnes the degree of closeness between any pair of clusters (X,Y) as the mean of the distances between all ordered pairs of objects in the two different clusters: if X contains x objects and Y contains y objects, this is the mean of the sum of distances (Xi,Yj) where Xi . X,Yj . Y, i = 1 ... x, j = 1 ... y: 
  
Average Linkage Distance (X, Y ) = Si=1...x,j=1...y dist XiYj /xy (18.4) 
where dist is defned as previously; note that distances of objects to themselves are not counted in this calculation, and neither are symmetric ones on the grounds that the distance from, say Xi to Yj is the same as the distance from Yj to Xi. 
There are other linkages, for which see Moisl (2015:209–13). Incidentally, note that dendrograms are more often shown sideways than in the ‘icicle’ or downward-facing format more familiar to linguists. This is a purely practical matter: an icicle format rapidly broadens out as the number of data objects grows, making it impossible to display on a page. 
The main and considerable advantage of hierarchical clustering is that it provides an exhaustive description of the proximity relations among data objects, and thereby more information than a simple partitioning of the data generated by non-hierarchical methods. It has also been extensively and successfully used in numerous applications, and is widely available in software implementations. There are, however, several associated issues. 
• How many clusters? 
Relative to a given tree, how many clusters do the data ‘really’ contain? That is up to the user to decide. Looking at a dendrogram like the one in Fig. 18.15 the answer seems obvious: there are two clusters A and B; each of these itself has some internal cluster structure, but that structure is insignifcant compared to the main A/B partition. This intuition is based on the relative lengths of the lines joining subclusters or, equivalently, on the relative values in the joining table: unusually large intervals between successive merges are taken as an indication that the subclusters in question constitute ‘main’ clusters. What about a structure like the one in Fig. 18.16, however? There are no obvious main clusters, and depending on the joining level one selects, that is, where one ‘cuts’ the tree, two, three, four or more clusters can be identifed. In Fig. 18.16a the cut is placed so that subclusters below a threshold of 100 are not distinguished, yielding two clusters A and B. In 18.16b the cut is at 90, yielding three clusters A-C, in 18.16c there are four clusters A-D for a threshold of 73, and in 18.16d there are fve clusters A-E. Which is the best cut, that is, the one that best captures the cluster structure of the data? There have been attempts to formalise selection of a best cut, but the results have been mixed, and the current position is that the best cut is the one that makes most sense to experts in the subject from which the data comes. 
Fig. 18.15  Hierarchical tree  
showing a clear two-cluster  10  
structure  12  
13  
9  
14  
11  
8  
5  
2  
7  
4  
6  
3  
1  




A 

A
5 

5 
2 
B 
2 
4 
4 
B 
6 
6
C 
3 
3 
1 
1 


(a) (b) 
A 

A
5 
5 
B
2 
2
B 
C
4 
4
C 
D
6 
6 
D 
3 
3
F 

1 
1 

(c) (d) 
Fig. 18.16 Different ‘cuts’ of the same dendrogram (b) Complete linkage 

• Which tree? 
The clustering literature recognises that different cluster joining criteria can and typically do generate different trees for the same data. For example, Fig. 18.17 shows dendrograms for two different analyses of Table 18.1. Both trees show a two-cluster structure consisting of speakers n01 – n07 at the top of each and the remaining ones below, but the structuring of the latter differs greatly. This is hardly surprising. The various joining criteria articulate different views of how data points should be combined into clusters, and these views fnd their expression in different cluster trees relative to the same data. It does, however, raise two questions: (i) given several hierarchical analyses of the same data generated by different joining criteria, which analysis should be preferred, and (ii) why? The traditional answer is that an expert in the domain from which the data was taken should select the analysis which seems most reasonable in terms of what s/he knows about the research area. The obvious objection to this is that it is subjective. It runs the risk of reinforcing preconceptions and discounting the unexpected and potentially productive insights which are the prime motivation for use of cluster analysis in hypothesis generation: given a range of different analyses, one might subconsciously look for what one wants to see. 
The aesthetics of tree structuring might also become a selection factor. Looking at the trees in Fig. 18.17, one might fnd the clear cluster structure of the Complete Linkage tree more appealing than the uninformative chained structure of the Single Linkage one for the g01 – g56 speakers. Ultimately, of course, all human interpretation is subjective, but to be scientifcally convincing, subjectivity needs to be constrained as much as possible. One way of constraining tree selection is to observe that there is a fundamental difference between the Single Linkage criterion and the others referred to above, as explained in Moisl (2015:217–21). The ability of Single Linkage to identify a superset of the cluster structures identifable by the other linkage methods implies that it is more likely than the others correctly to identify the structure latent in any given data, and that it is therefore the most authoritative of the linkage methods: in principle, no matter how aesthetically attractive a non-Single Linkage tree might be, or how much it accords with expert expectation, if it differs substantively from the Single Linkage tree, the suspicion must be that the difference is an artefact of the linkage criterion rather than a refection of intrinsic cluster structure. In practice there is a caveat, however. Single Linkage does not make a distinction between cluster structure and noise in data, on which more below, and this can generate spurious structures. When the given data is known not to contain much or any noise, Single Linkage is authoritative. Where this is not known, selection of the best cluster tree, that is, the one which best captures the intrinsic cluster structure of the data, must be guided by the various cluster validation methods discussed in Moisl (2015:224–49). 
• Outliers and noise 
All the joining criteria are affected by outliers and noise to different degrees and for different reasons, where ‘noise’ designates a variety of factors such as inaccurate measurement or recording, or accidental or deliberate corruption. Outliers are not a problem for Single Linkage because it simply represents them as one-member clusters distant from the other clusters in the tree; this characteristic in fact makes Single Linkage a good way to check for outliers. It is, however, much affected by noise, which results in chaining like that in Fig. 18.17a; if that chaining is indeed a consequence of noise rather than a refection of the similarity structure of the speakers’ pronunciation, a possible cause is inaccurate transcription. For the other linkage criteria noise is less of a problem, but outliers affect them considerably: because Complete Linkage and the various others are all based in one way or another on the notion of a cluster having a centre of gravity or centroid, an outlier pulls that centre away from its natural location among the other data points, thereby affecting calculation of the centre and consequently distorting the structure of the tree. When using hierarchical cluster analysis, therefore, it is important to identify outliers and to eliminate them or at least to be aware of their presence when interpreting results. 
Finally, because of their widespread popularity in data analysis generally, k-means and hierarchical clustering continue to be developed in the hope of at least mitigating their problems. For the interested reader, these have to do with linear separability of clusters and the use of nonlinear distance measurement, for which see Moisl (2015:187–201, 217–23). 
18.2.3.4 Advanced Topics 
Cluster analysis is widely used and highly developed both in terms of the variety of available clustering methods and of theoretical understanding of them (Moisl 2015:Chap. 4). Hierarchical cluster analysis and k-means are a good place to start, but there is extensive potential for application of other clustering techniques in corpus linguistics. These topics, in particular, are worth pursuing: 
• 
As noted, different clustering methods often give substantively different results. The reason might have to do with noise, or lack of intrinsic cluster structure in the data, or some limitation in the clustering method such as those already referred to, or a predisposition of different methods to fnd certain shapes of cluster. In such cases, a principled selection is required; cluster validation is a rapidly developing and necessary aspect of cluster analysis (Moisl 2015:224–49). 

• 
Density-based clustering methods can identify a greater range of cluster shapes than vector space-based ones such as hierarchical and k-means (Moisl 2015:192– 201). 

• 
Nonlinear distance measurement can be more accurate than linear measures such as Euclidean distance, and can be expected by give more reliable results (Moisl 2015:39–45). 


Representative Study 1 
Moisl, H., Maguire, W., and Allen, W. 2006. Phonetic variation in Tyneside: exploratory multivariate analysis of the Newcastle Elect­ronic Corpus of Tyneside English. In Language Variation. European Perspectives , ed. Hinskens, F. Meertens Institute, Amsterdam. 
This paper is a sociolinguistic study of phonetic variation among speakers from Tyneside in North-East England. 
Research question 
Is there systematic phonetic variation in the Tyneside speech community, and, if so, does that variation correlate systematically with social factors? 
(continued) 
Data 
Phonetic data was abstracted from the Diachronic Electronic Corpus of Tyneside English (DECTE), a digital corpus of audio-recorded and transcribed speech from Tyneside in North-East England. DECTE is available online and is fully documented at http://research.ncl.ac.uk/decte/, and is described in Corrigan et al. (2012). DECTE includes phonetic transcriptions of 63 audio speaker interviews using a transcription scheme containing 156 phonetic variables. A phonetic usage profle was created for each speaker by counting and recording the number of times the speaker used each of the 156 phonetic segments, and the 63 profles were assembled into a 63 × 156 data matrix called MDECTE. The MDECTE header contained phonetic segment variable names and corresponding four-digit codes, one per column, and each row has a speaker label. This is exemplifed in Table 18.5. 
MDECTE was transformed in two ways prior to cluster analysis: 
– 
Because there is substantial variation in the lengths of the individual speaker transcriptions, MDECTE was normalised to compensate for this (Moisl 2015:65–71). 

– 
A majority of the phonetic variables are either very infrequently used or show very little variation across speakers. MDECTE was therefore dimensionality reduced to a 63 × 59 matrix (Moisl 2015:71–92). 


Method 
MDECTE was hierarchically cluster analyzed as shown in Fig. 18.18. 
Table 18.5 Schematic of the MDECTE data matrix 
1: d1 0194  2:d2 0198  3: O: 0118  ...  156: E 0164  
g01  31  28  123  ...  0  
g02  22  8  124  ...  0  
...  ...  ...  ...  ...  ...  
n07  19  19  73  ...  0  

(continued) 

Results 
The DECTE speakers fell into two clearly defned main clusters, a larger one G containing speakers g01 – g55 and a smaller one N containing speakers n01 – n07, and G itself has well-defned subclusters. When this cluster structure was correlated with social data about the speakers included in DECTE, all the speakers in cluster G were found to be from Gateshead on the south side of the river Tyne, and all those in N were from Newcastle on the north side. Moreover, G.1 contains speakers with somewhat higher levels of education and employment than those in G.2, all of whom had 
(continued) 
the minimum statutory educational level and were in skilled and unskilled manual employment, and G.2 itself consists of two subclusters for gender, where G.2.2 consists entirely of men and G.2.1 mainly though not exclusively of women. Cluster analysis of MDECTE therefore empirically supports the hypotheses that there is systematic phonetic variation in the Tyneside speech community, and that this variation correlates systematically with social factors. 
Representative Study 2 
Gries, S., and Stefanowitsch, A. 2010. Cluster analysis and the identi­
fcation of collexeme classes. In Empirical and Experimental Methods 
in Cognitive / Functional Research, eds. Rice, S., and Newman, J., 73– 
90. CSLI, Stanford CA. 
A substantial amount of work has been done on inference of grammatical and semantic lexical categories from text (Korhonen 2010). This work is based on the observation that there are restrictions on which words can co-occur within some degree of contiguity in natural language strings, and that the distribution of words in text can therefore be used to infer grammatical and semantic categories and category relatedness for them; as Firth (1957) put it, “a word is characterised by the company it keeps”. This paper addresses the relationship between words and grammatical constructions. 
Research question 
How can the number and nature of semantic classes that are instantiated by a given set of words be determined without recourse to prior assumptions or intuitions? 
Data 
The usual data creation approach of abstracting and counting the collocates of target words from a corpus was modifed by including only the covarying collexemes of target words, that is, words which occur in a defned slot of the same construction as the target word (see Chap. 7). 
Method 
The authors investigated the effectiveness of hierarchical cluster analytic techniques with respect to the research question by determining how well these techniques identify the most prototypical sense(s) of a construction as well as subsenses instantiated by coherent semantic classes of words occurring in it. 
(continued) 
Results 
There was “a relatively coherent classifcation of verbs into semantic groups” superior to a classifcation based solely on collocate-counting. The conclusion was that clustering techniques can be a useful step towards making the semantic analysis of grammatical constructions more objective and precise. 
18.3 Practical Guide with R 
This section contains a guide to k-means and hierarchical cluster analysis using R. The MDECTE data matrix, truncated to 32 rows for convenience of exposition, is used as the basis for exemplifcation. 
18.3.1 K-means 
The k-means analysis described in what follows proceeds in three steps: (i) fnding the appropriate number of clusters k, (ii) carrying out the analysis for the selected k, and (iii) reporting the result. 
(i) As noted earlier, one of the ways of establishing k is to conduct multiple analyses on the same data using different values of k and selecting the one whose SEE is optimal. It was also noted that, because different selections of initial prototypes affect the success with which k-means identifes cluster structure, multiple trials with different random initialisations are conducted for each value of k, and the one with the smallest SSE is selected. Optimisation of prototypes and the value for k selection must, therefore, be integrated: in assessing the optimality of any given k in the succession of candidate k-values, the SSE for that k must itself be optimal. The R code for this follows. 
The frst three lines assign values to variables used subsequently where 
maxk <-15 iterations <-50 lst <-vector() 
• 
maxk is the maximum number of k-values to be assessed. iterations is the number of initial prototypes to be tried for each k. 

• 
lst <-vector() creates an empty vector into which optimal SSE values for the current k will be inserted. 


The list of optimal SSE values is then constructed. 
set.seed(123) for (i in 1:maxk) {mk <-kmeans(m, centers=i, iter.max=10, nstart=iterations) lst[i] <-mk$tot.withinss} 
• 
set.seed(123) is a technicality. It initialises the R random number generator, which is used to generate random prototype vectors in the code that follows. Specifying a seed ensures that the generator outputs the same sequence each time the kmeans function is used, thereby ensuring reproducibility of results for any given data. 

• 
for (i in 1:maxk): for every value in the range 1...maxk. 

• 
mk <-kmeans(m, centers = i, iter.max = 10, nstart = iterations) uses the R function kmeans to calculate mk,the k-means analysis for any given k in the range 1..maxk. 

The meanings of the parameters for kmeans are as follows: 

• 
m is the data matrix. 

• 
centers = i is the number of clusters k for every successive k in the range 1..maxk. 

• 
iter.max = 10 is the number of iterations of the k-means algorithm to apply before giving up on the current k as unviable. 

• 
nstart = iterations is the number of random initial prototypes to try for each k. It is important to note that kmeans automatically selects and outputs the prototype initialisation with the optimal SSE. 

• 
lst[i] <-mk$tot.withinss inserts the optimal SSE for the current k into the SSE list lst, and mk$tot.withinss indexes the SSE which kmeans outputs at each iteration. 


The list of SSE values lst is now plotted, with the result shown in Fig. 18.19: 
plot(lst) 
Figure 18.19 shows SSE decreasing with increasing k. How can this be used to select optimal k? The main thing to realise is that optimal k is not the one with the smallest SSE: if the range had been extended the SSE would have become ever smaller until it reached 0 when k = the number of data objects, at which point every cluster would 
Fig. 18.19 Decrease of SSE for k in the range 1..15 
SSE 

2 4 6 8 101214 K 

Fig. 18.20 Clustering vector for k = 3 
have had 1 member. Optimality is, instead, the k at which the number of clusters is at its most informative, and this is the k for which the rate of decrease of SSE starts to level out. Visual inspection indicates that, in Fig. 18.19 the rate of decrease is largeupto k = 3, and starts to level out thereafter, so the optimal value is taken to be 3. This selection is subjective – it could have been 4 or 5 – but subjectivity is intrinsic to the method. 
(ii) The second step invokes the R function kmeans again, using the selected k via the parameter centers = 3. 
mk <-kmeans(m, centers = 3, iter.max = 10, nstart = 50) 
(iii) Finally, the variable mk contains the result, and includes a range of information whose signifcance is explained in the R help fle for kmeans. Most relevant for present purposes is the clustering vector, which assigns each data object to one of the specifed three clusters, as shown in Fig. 18.20. 
18.3.2 Hierarchical Clustering 
Hierarchical analysis is, again, a three-step process: (i) creation of a distance matrix, 
(ii) clustering, and (iii) presentation of the result. 
(i) Generate the Euclidean distance matrix: 
md <-dist(m) 
• 
md is the distance matrix. 

• 
dist is the relevant R distance calculation function. 


(ii) Cluster the distance matrix: 
mdc = hclust(md, method="average") 
• 
mdc is the list output of the hierarchical cluster analysis. 

• 
hclust is the R clustering function. 

• 
method="average" selects the Average Linkage method; other linkages can be specifed. 


(iii) Draw the dendrogram to display the clustering (Fig. 18.21). There are numer­ous graphical options here; a basic one is: 
plot(mdc, labels=m$V1, hang=-1, cex=0.75) 
• 
plot is self-explanatory. 

• 
labels=m$v1 tells plot that the speaker labels are in column 1. 

• 
hang=-1 shows all the labels in a uniform horizontal row. 

• 
cex=0.75 adjusts the size of the label font; this is useful when there are many labels and too large a font makes them overlap and become unreadable. 


As the number of rows in a data matrix grows, the tree in the ‘icicle’ format shown in Fig. 18.21 soon runs over the edges of the page. It is more convenient to show it 

Average link 
Fig. 18.21 Average linkage hierarchical clustering of MDECTE 
in the arbitrarily-extendible horizontal format as used in Fig. 18.18 for example. To do this and at the same time retain the font-size adjustment facility, it is necessary to extend the native R clustering graphics with a graphics package, one of which is dendextend. 
Convert the tree generated by hclust to an R dendrogram object: 
mdcd <-as.dendrogram(mdc) 
Set the font size: 
mdcd <-set(mdcd, "labels_cex", 0.75) 
Plot the dendrogram (Fig. 18.22): 
plot_horiz.dendrogram(mdcd) 
There are several packages in addition to dendextend which offer an exten­sive range of graphics options, including ggdendro and APE: Analyses of Phylogenetics and Evolution in R language. 
18.3.3 Reporting Results 
K-means and hierarchical clustering are mathematical, not statistical, procedures. As such, there are no statistical measures such as standard deviations or p-value to report when preparing results for publication. This does not, however, mean that it is suffcient to present nothing more than a clustering vector or a dendrogram. It is important to provide the reader with as much information as possible about the framework in which the analysis was conducted. 
• 
Data: How was the data collected? How large is the data set? What does the data matrix look like (including an example)? How, if at all, was the data transformed, for example by removal of outliers, or by variable scaling, or by normalisation? See Chap. 26 for more specifc guidelines on data description. 

• 
Clustering method: As noted, different hierarchical methods can give different results. Simply presenting the one that, for whatever reason, the researcher fnds most appealing is insuffcient. Instead, several methods should be applied to the 


data, agreements and disagreements among them identifed, and reasons for any disparities investigated. Hierarchical results should, moreover, be corroborated via k-means and/or a selection of the many other available clustering methods. 
Finally, validation is increasingly being regarded as essential in reporting of clustering results (Moisl 2015:224–49). 

Fig. 18.22 Fig. 18.18 shown horizontally 
Further Reading 
The literature on clustering is vast and, in general, quite technical, so unless one is mathematically sophisticated it is best to work up the scale of diffculty gradually. 
Divjak, D., and Fieller, N. 2014. Clustering linguistic data. In Corpus Methods for Semantics, eds. Glynn, D., and Robinson, J., 405–441. John Benjamins, Amsterdam. 
Divjak & Fieller’s chapter is the best current follow up to the present chapter. It is aimed at about the same level as the present one, and the two complement one another well by offering independent views and emphases. 
Moisl, H. 2015. Cluster Analysis for Corpus Linguistics. de Gruyter, Berlin. 
This book-length account not only offers coverage of a substantially greater range of methods than that offered by Divjak & Fieller and by the present chapter, but also includes extensive discussion of data issues and cluster validation as well as a detailed overview of cluster analysis applied in various linguistics sub-felds. 
References 
Corrigan, K., Mearns, A., & Moisl, H. (2012). The diachronic electronic corpus of Tyneside English. http://research.ncl.ac.uk/decte/. Accessed 12 June 2019. Delmestri, A., & Cristianini, N. (2012). Linguistic phylogenetic inference by PAM-like matrices. 
Journal of Quantitative Linguistics, 19, 95–120. Deza, M., & Deza, E. (2009). Encyclopedia of distances. Berlin: Springer. Firth, J. (1957). A synopsis of linguistic theory 1930–1955. In J. Firth (Ed.), Studies in linguistic 
analysis. Hoboken: Blackwell. Gries, S. (2010). Corpus linguistics and theoretical linguistics: A love-hate relationship? Not necessarily .... International Journal of Corpus Linguistics, 15, 327–343. 
Gries, S., & Stefanowitsch, A. (2010). Cluster analysis and the identifcation of collexeme classes. In S. Rice & J. Newman (Eds.), Empirical and experimental methods in cognitive/Functional research (pp. 73–90). Stanford: CSLI. 
Grieve, J., Speelman, D., & Geeraerts, D. (2011). A statistical method for the identifcation and aggregation of regional linguistic variation. Language Variation and Change, 23, 193–221. 
Hauer, B., & Kondrak, G. (2011). Clustering semantically equivalent words into cognate sets in multilingual lists. In The 5th international joint conference on Natural Language Processing (IJCNLP 2011), pp. 865–873. 
Kessler, B. (2008). The mathematical assessment of long-range linguistic relationships. Language and Linguistics Compass, 2, 821–839. Korhonen, A. (2010). Automatic lexical classifcation: Bridging research and practice. Philosoph­ical Transactions of the Royal Society A, 368, 3621–3632. 
Meschenmoser, D., & Prl, S. (2012). Fuzzy clustering of dialect maps using the empirical covariance. Grouping maps in dialect corpora based on spatial similarities. International Journal of Corpus Linguistics, 17, 176–197. 
Mirkin, B. (2011). Core concepts in data analysis: Summarization, correlation, and visualization. Berlin: Springer. 
Moisl, H. (2015). Cluster analysis for corpus linguistics. Berlin: de Gruyter. 
Moisl, H., Maguire, W., & Allen, W. (2006). Phonetic variation in Tyneside: Exploratory multivariate analysis of the Newcastle Electronic Corpus of Tyneside English. In F. Hinskens (Ed.), Language variation. European perspectives. Amsterdam: Meertens Institute. 
Ruette, T., Speelman, D., & Geeraerts, D. (2013). Measuring the lexical distance between registers in national varieties of Dutch. In A. Soares da Silva (Ed.), Pluricentric languages: Linguistic variation and sociocognitive dimensions (pp. 541–554). Berlin: Mouton de Gruyter. 
Wieling, M., Nerbonne, J., & Baayen, H. (2011). Quantitative social dialectology: Explaining linguistic variation geographically and socially. PLoS One, 6,9. 
Xu, R., & Wunsch, D. (2009). Clustering. Hoboken: Wiley. 
Chapter 19 Multivariate Exploratory Approaches 
Guillaume Desagulier 
Abstract This chapter provides both a theoretical discussion of what multivariate exploratory approaches entail and step-by-step instructions to implement each of them with R. Four methods are presented: correspondence analysis, multiple correspondence analysis, principal component analysis, and exploratory factor analysis. These methods are designed to explore and summarize large and complex data tables by means of summary statistics. They help generate hypotheses by providing informative clusters using the variable values that characterize each observation. 
19.1 Introduction 
Once corpus linguists have collected sizeable amounts of observations and described each observation with relevant variables, they look for patterns in the data. When the data set is too large, it becomes impossible to summarize the table with the naked eye and summary statistics are needed. This is where exploratory data analysis steps in. 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_19) contains supplementary material, which is available to authorized users. 
G. Desagulier (•) MoDyCo – Université Paris 8, Saint-Denis, France CNRS, Paris, France 
Université Paris Nanterre, Nanterre, France Institut Universitaire de France, Paris, France e-mail: gdesagulier@univ-paris8.fr 
© Springer Nature Switzerland AG 2020 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_19 
Exploring a data set means separating meaningful trends from the noise (i.e. “random” distributions).1 In theory, exploratory data analysis is used to generate hypotheses because the linguist does not yet have any assumption as to what kinds of trends should appear in the data. In practice, however, linguists collect observations in the light of specifc variables precisely because they expect that the latter infuence the distribution of the former. 
I present four multivariate exploratory techniques: correspondence analysis (henceforth CA), multiple correspondence analysis (henceforth MCA), principal component analysis (henceforth PCA), and exploratory factor analysis (henceforth EFA). These techniques rely on dimensionality reduction, i.e. an attempt to simplify complex multivariate datasets to facilitate interpretation. 
19.2 Fundamentals 
CA, MCA, PCA, and EFA are meant for the exploration of phenomena whose real­izations are infuenced by several factors at the same time. Once operationalized by the researcher, these multiple factors are captured by means of several independent variables. When observations of a phenomenon are captured by several variables, the analysis is multivariate. 
19.2.1 Commonalities 
The challenge that underlies the visualizations obtained with dimensionality-reduction methods is the following: we seek to explore a cloud of points from a data set in the form of a rows × columns table with as many dimensions as there are columns. Like a complex object in real life, a data table has to be rotated so as to be observed from an optimal angle. Although the dimensions of a data table are eventually projected in a two-dimensional plane, they are not spatial dimensions. If the table has K columns, the data points are initially positioned in a space R of K dimensions. To allow for easier interpretation, dimensionality-reduction methods decompose the cloud into a smaller number of meaningful planes. 
All the methods covered in this chapter summarize the table by measuring how much variance there is and decomposing the variance into proportions. These proportions are eigenvalues in CA, MCA, and PCA. They are loadings in EFA (and another kind of PCA that is not covered in this chapter).2 
1I am using scare quotes because, as Kilgarriff (2005) puts it, “language is never, ever, ever, random”. 2See Baayen (2008, Sect. 5.1.1). 
All four methods offer graphs that facilitate the interpretation of the results. Although convenient, these graphs do not replace a careful interpretation of the numeric results. 
19.2.2 Differences 
The main difference between these methods pertain mainly to the kind of data that one works with. CA takes as input a contingency table, i.e. a table that cross-classifes observations on a number of categorical variables (see Chap. 20). Entries in each cell are integers, namely the number of times that observations (in the rows) are seen in the context of the variables (in the columns). Table 19.1 is an example of a contingency table. It displays the frequency counts of four types of nouns (rows) across three corpus fles from the BNC-XML (columns). 
MCA takes as input a case-by-variable table such as Table 19.2. The table con­sists of i individuals or observations (rows) and j variables (columns). Historically, MCA was developed to explore the structure of surveys in which informants are asked to select an answer from a list of suggestions. For example, the question “According to you, which of these disciplines best describe the hard sciences: physics, biology, mathematics, computer science, or statistics?” requires informants to select one category. 
PCA takes as input a table of data of i individuals or observations (rows) and j variables (columns). The method handles continuous and nominal data. The continuous data may consist of means, reaction times, formant frequencies, etc. The categorical/nominal data are used to tag the observations. Table 19.3 is a table of six kinds of mean frequency counts further described by three kinds of nominal information. 
Like PCA, EFA takes as input a table of continuous data. However, it does not commonly accommodate nominal data. Typically, Table 19.3 minus the three columns of nominal data can serve as input for EFA. 
Table 19.1 An example of a contingency table (Desagulier 2017, p. 153) 

A1J.xml  A1K.xml  A1L.xml  Row totals  
NN0  136  14  8  158  
NN1  2236  354  263  2853  
NN2  952  87  139  1178  
NP0  723  117  71  911  
Column totals  4047  572  481  5100  

Table 19.2 A sample input table for MCA (Desagulier 2017, p. 36) 
Corpus fle  Mode  Genre  Exact match  Intensifer  Syntax  Adjective  
KBF.xml  Spoken  Conv  A quite ferocious mess  Quite  Preadjectival  Ferocious  
AT1.xml  Written  Biography  Quite a firty person  Quite  Predeterminer  Flirty  
A7F.xml  Written  Misc  A rather anonymous name  Rather  Preadjectival  Anonymous  
ECD.xml  Written  Commerce  A rather precarious foothold  Rather  Preadjectival  Precarious  
B2E.xml  Written  Biography  Quite a restless night  Quite  Predeterminer  Restless  
AM4.xml  Written  Misc  A rather different turn  Rather  Preadjectival  Different  
F85.xml  Spoken  Unclassifed  A rather younger age  Rather  Preadjectival  Younger  
J3X.xml  Spoken  Unclassifed  Quite a long time  Quite  Predeterminer  Long  
KBK.xml  Spoken  Conv  Quite a leading light  Quite  Predeterminer  Leading  

19.2.3 Exploring is not Predicting 
The methods presented in this chapter are exploratory, as opposed to explanatory or predictive. They help fnd structure in multivariate data thanks to observation groupings. The conclusions made with these methods are therefore valid for the corpus only. For example, we shall see that middle-class female speakers aged 25 to 59 display a preference for the use of bloody in the British National Corpus (Sect. 19.3.2). This fnding should not be extended to British English in general. Indeed, we may well observe different tendencies in another corpus of British English. Neither should the conclusions made with exploratory methods be used to make predictions. Of course, exploratory methods serve as the basis for the design of predictive modeling, which uses the values found in a sample to predict values for another sample. Expanding on Gries (2006), Glynn (2014) fnds that usage features and dictionary senses are correlated with dialect and register thanks to two multivariate exploratory techniques (correspondence analysis and multiple correspondence analysis). To confrm these fndings, Glynn (2014) turns to logistic regression. This confrmatory multivariate technique allows to specify which of the 
19 Multivariate Exploratory Approaches 439 Table 19.3 A sample data frame (Lacheret-Dujour et al. 2019) 
Corpus sample  fPauses  fOverlaps  fFiller  fProm  fPI  fPA  Subgenre  Interactivity  Planning type  
D0001  0.26  0.12  0.14  1.79  0.28  1.54  Argumentation  Interactive  Semi-spontaneous  
D0002  0.42  0.11  0.10  1.80  0.33  1.75  Argumentation  Interactive  Semi-spontaneous  
D0003  0.35  0.10  0.03  1.93  0.34  1.76  Description  Semi-interactive  Spontaneous  
D0004  0.28  0.11  0.12  2.29  0.30  1.79  Description  Interactive  Semi-spontaneous  
D0005  0.29  0.07  0.23  1.91  0.22  1.69  Description  Semi-interactive  Spontaneous  
D0006  0.47  0.05  0.26  1.86  0.44  1.94  Argumentation  Interactive  Semi-spontaneous  
...  ...  ...  ...  ...  ...  ...  ...  ...  ...  

usage features and dictionary senses are signifcantly associated with either dialect or register, and determine the importance of the associations. 
Nowadays, many linguists jump to powerful predictive methods (such as logistic regression or discriminant analysis) without going through the trouble of exploring their data sets frst. This is a shame because the point of running a multivariate exploratory analysis is to generate fne research hypotheses, which far more powerful predictive methods can only beneft from. 
19.2.4 Correspondence Analysis 
Correspondence analysis (henceforth CA) is used to summarize a two-dimensional contingency table. The table is a matrix M of counts that consists of i individuals or observations (rows) and j variables (columns). The foundations of CA were laid out by Hirschfeld (1935) and Benzécri (1984). The method gets its name from what it aims to show, namely the correspondence between what the rows and the columns represent. Incidentally, CA also shows the correspondence between the rows and the correspondence between the columns. The basic idea is to group the rows and columns that share identical profles. 
It should be remembered that the linguist makes no assumption as to what kinds of groupings are to be found in the data. In practice, however, a table of data is compiled because meaningful groupings are expected to be found. Therefore, if no meaningful grouping is found, this is because the rows and the columns are independent. In this case, it is advisable to rethink the design of the study, especially the choice of explanatory variables. 
To determine whether rows and columns are independent, CA relies on the .2 test. It tests the signifcance of the overall deviation of the table from the independence model. The test computes the contribution of each cell to .2 and sums up all contributions to obtain the .2 statistic. Because we are interested in determining whether two variables are interdependent, we formulate the hypotheses as follows: 
H0: the distributions of row variables and column variables are independent; H1: the distributions of row variables and column variables are interdependent. 
One calculates the .2 value of a cell in the ith row and the j th column as follows: 
(Oi,j - Ei,j )2 
2
.i,j = (19.1)Ei,j 
where Oi,j is the expected frequency for cell i, j and Ei,j is the expected frequency for cell i, j.The .2 statistic of the whole table is the sum of the .2 values of all cells. 
n 
(O - E)2 
.2 = (19.2)
E 
i=1 
Because the .2 score varies greatly depending on the sample size, it cannot be used to assess the magnitude of the dependence. This is measured with Cramér’s V , which one obtains by taking the square root of the . 2 statistic divided by the product of the sum of all observations and the number of columns minus one: 
.2 Cramér sV = . (19.3)
N(k - 1) 
Central to CA is the concept of profle. To obtain the profle of a row, each cell is divided by its row total. Table 19.4 displays the row profles of Table 19.1.The row profles add up to 1. Likewise, one obtains the profle of a column by dividing each column frequency by the column total (Table 19.5). Again, the column profles add up to 1. 
Table 19.4 The row profles of Table 19.1 

A1J.xml  A1K.xml  A1L.xml  Row total  
NN0  0.8608  0.0886  0.0506  1  
NN1  0.7837  0.1241  0.0922  1  
NN2  0.8081  0.0739  0.1180  1  
NP0  0.7936  0.1284  0.0779  1  
Column average  0.7935  0.1122  0.0943  1  

Table 19.5 The column profles of Table 19.1 
A1J.xml  A1K.xml  A1L.xml  Row average  
NN0  0.0336  0.0245  0.0166  0.0310  
NN1  0.5525  0.6189  0.5468  0.5594  
NN2  0.2352  0.1521  0.2890  0.2310  
NP0  0.1787  0.2045  0.1476  0.1786  
Column total  1  1  1  1  

CA performs an analysis of rows and columns that is both simultaneous and symmetric. A column analysis consists in interpreting the column profles using the rows as reference points on the basis of a table such as Table 19.5. For example, the value in the A1K.xml column for singular common nouns (NN1) is 0.6189. Comparing this value with the average proportion of NN1 in the sample (0.5594), it appears that these noun types are slightly over-represented in A1K.xml by a ratio of 0.6189 ˜ 1.1063. A row analysis consists in interpreting the row profles 
0.5594 
using the columns as reference points on the basis of a table such as Table 19.4, in which the same cell displays a value of 0.1241. In other words, of all the singular common nouns that occur in the corpus fles, 12.41% occur in A1K.xml. On average, A1K.xml contains 11.22% of the nouns found in the sample. The ratio is the same as above, i.e. 0.1241 ˜ 1.1063.
0.1122 
Distances between profles are measured with inertia. It is with the total inertia of the table (f2) that CA measures how much variance there is. f2 is obtained by dividing the .2 statistic by the sample size. CA interprets inertia geometrically to assess how far row/column profles are from their respective average profles. The larger f2, the more the data points are spread out on the map. 
Each column of the table contributes one dimension. The more columns in your table, the larger the number of dimensions. When there are many dimensions, sum­marizing the table becomes very diffcult. To solve this problem, CA decomposes f2 along a few dimensions that concentrate as large a proportion of inertia as possible. These proportions of inertia are known as eigenvalues. 
On top of the coordinates of the data points, two descriptors help interpret the dimensions: contribution and quality of projection (cos 2). If a data point displays a minor contribution to a given dimension, its position with respect to this dimension must not be given too much relevance. The quality of the projection of a data point onto a dimension is measured as the percentage of inertia associated with this dimension. Usually, projection quality is used to select the dimension in which the individual or the variable is the most faithfully represented. 
Individuals and variables can be declared as active or supplementary/illustrative, as is the case with multiple correspondence analysis and principal component analysis (see below). These supplementary rows and/or columns help interpret the active rows and columns. As opposed to active elements, supplementary elements do not contribute to the construction of the dimensions. Supplementary information is generally redundant. Its main function is to help interpret the results by providing relevant groupings. Whether a group of individuals or variables should be declared as active/illustrative depends on what the linguist considers are primary or secondary in the exploration of the phenomenon under study. 
19.2.5 Multiple Correspondence Analysis 
Because MCA is an extension of CA, its inner workings are very similar. For this reason, they are not repeated here. 
As pointed out in Sect. 19.2.2, MCA takes as input a table of nominal data. For this method to yield manageable results, it is best if the table is of reasonable size (not too many columns), and if each variable does not break down into too many categories. Otherwise, the contribution of each dimension to f2 is small, and a large number of dimensions must be inspected. There are no hard and fast rules for knowing when there are too many dimensions to inspect. However, when the eigenvalue that corresponds to a dimension is low, we know that the dimension is of little interest (the chances are that the data points will be close to the intersection of the axes in the summary plot). 
19.2.6 Principal Component Analysis 
As in CA and MCA, the total variance of the table is decomposed into proportions in PCA. There is one minor terminological difference: the dimensions are called principal components. For each component, the proportion of variance is obtained by dividing the squared standard deviation by the sum of the squared standard deviations. 
As exemplifed in this chapter, PCA is based on the inspection of correlations between the variables and the principal components.3 Before one runs a PCA, one should consider standardizing (i.e. centering and scaling) the variables (see Chap. 17). If a table contains measurements in different units, standardizing the variables is compulsory. If a table contains measurements in the same unit, stan­dardizing the variables is optional. However, even in this case, failing to standardize means giving each variable a weight proportional to its variance. Standardizing the variables guarantees that equal weights are attributed to the variables (Husson et al. 2010, p. 45). 
3A second kind of PCA is based on loadings (Baayen 2008, Sect. 5.1.1). Loadings are correlations between the original variables and the unit-scaled principal components. The two kinds of PCA are similar: both are meant to normalize the coordinates of the data points. The variant exemplifed in this chapter is more fexible because it allows for the introduction of supplementary variables. 
19.2.7 Exploratory Factor Analysis 
EFA was made popular in linguistics by Biber’s studies on register variation as part of the multidimensional (MD) approach (Biber 1988, 1995). The goal of the MD approach is to detect register differences across the texts and text varieties of a corpus based on groups of linguistic features that co-occur signifcantly. Technically, this approach starts with a large number of linguistic variables and relies on factor analysis to reduce this number to a few basic functional dimensions that account for differences between texts. MD analysis is featured in a vast number of synchronic and diachronic studies on various discourse domains such as eighteenth century English (Biber 2001), blogs (Grieve et al. 2010), academic English (Biber and Gray 2016), etc. It has been applied to languages other than English such as Somali (Biber and Hared 1992) or Korean (Kim and Biber 1994). For an overview, see Biber and Conrad (2001). 
Although close to PCA, EFA differs with respect to the following. The number of relevant components, which are called factors, is not determined automatically. It must be chosen beforehand. EFA is designed to identify patterns of joint variation in a number of observed variables. It looks for variables that are highly correlated with a group of other variables. These intercorrelated variables are assumed to measure one underlying variable. This variable, which is not directly observed, but inferred, is latent. It is known as a factor. This is an aspect that PCA is not designed to show. One added value of EFA is that “an error term is added to the model in order to do justice to the possibility that there is noise in the data” (Baayen 2008, p. 127).4 
Representative Study 1 
D. Glynn (2014). “The many uses of run.” In: Corpus Methods for Semantics: Quantitative Studies in Polysemy and Synonymy. Ed. by 
D. Glynn and J. A. Robinson. Vol. 43. Human Cognitive Processing. Amsterdam: John Benjamins, pp. 117–144 
Research questions 
Glynn (2014) examines the semasiological variation of run in the light of sociolinguistic variables. The study posits that “even for a lexeme as culturally ‘simple’ and as socially ‘neutral’ as run, one must account for the social dimension of language in semantic analysis” (Glynn 2014, p. 124). 
(continued) 
4Factor analysis of mixed data (FAMD) accommodates data sets containing both continuous and nominal data (Pagès 2014, Chap. 3). In this respect, it should be considered an interesting alternative to standard EFA. For reasons of space, however, this chapter focuses on ‘plain’ EFA. 
Data 
Glynn’s study is based on 500 occurrences of run in British and American English (250 occurrences for each variety). The occurrences break down into conversation and online personal diaries. The diary examples were extracted from the LiveJournal corpus, developed by Dirk Speelman (University of Leuven). The conversation examples were extracted from the British National Corpus and the American National Corpus. 
Method 
Each entry was annotated for dictionary sense, register, and dialect. The data were submitted to correspondence analysis. 
Results 
The frst two dimensions of CA account for 87% of f2, which means that the conclusions based upon their inspection only are reliable. In American conversation, run tends to mean ‘increase’, ‘diffuse’, and ‘motion into diffculty’. In the American diary genre, run is characterized by the following dictionary senses: ‘campaign’, ‘copy’ and, to some extent, ‘metaphoric motion’. Although specifc to American English, ‘meet’ and ‘extend space’ is used in either register. In British English, run is highly and distinctly associated with‘fow’ and ‘extend time’. A relative association with British English is also found with senses such as ‘use up’, ‘cause motion’ and ‘escape’. To further explore the detail of the sociolinguistic variation at work with run, Glynn resorts to multiple correspondence analysis. 
Representative Study 2 
G. Desagulier (2015). “A lesson from associative learning: asymmetry and productivity in multiple-slot constructions.” In: Corpus Lingui­stics and Linguistic Theory. DOI: 10.1515/cllt-2015-0012 
Research questions 
This paper addresses a claim made by Kay (2013) that only fully productive constructions should count as constructions. Desagulier (2015) posits that even patterns that are not fully productive often have subregularities that are. He shows that the AasNP construction (stiff as a board, cool as a cucumber, 
(continued) 
fat as a pancake), which Kay had argued was simply idiomatic, licenses productive coinages when used with particular adjectives or nouns (e.g. black as NP, A as hell). 
Data 
All the occurrences of AasNP were extracted from the BNC-XML, amounting to 1,819 tokens. Only instances of AasNP where the adjective is intensifed were kept. Examples involving a literal comparison and no intensifcation were discarded. Each adjective and noun appearing in AasNP was assigned a range of mean scores based on the following measures: an asymmetric association measure (.P ), a symmetric association measure (collostruction strength indexed on the log-likelihood statistic), type frequency (V ), the frequency of hapax legomena (V 1), potential productivity (P), and global productivity (P *). 
Method 
The individuals consist of all adjective and NP types of AasNP tokens. Each of the 1,278 individuals (402 adjective types and 876 NP types) is examined in the light of four active variables: collostruction strength, the difference .PNP |A - .PA|NP , P, and P * . Three supplementary quantitative variables were also included to verify that no counterintuitive result was obtained with respect to the computation of hapax-based measures: V , V 1, and construction frequency. The data table was submitted to PCA. 
Results 
Three clusters stand out. Globally productive individuals and those that belong to highly associated pairs (i.e. characterized by high collostruction strength and low .P ) cluster along the horizontal axis (frst principal component). The former appear in the upper-right corner of the plot of individuals whereas the latter cluster in the lower-right corner. Individuals that are productive according to P cluster along the vertical axis (second principal component). In other words, individuals that belong to highly associated pairs are among the least potentially productive and the most globally productive. 
Individuals with extreme values for the frst component are mostly nouns (day, night, snow, sheet, etc.). Most nouns with the highest P * values denote paragons whose semantic relation with the adjective can be easily accessed despite its conventional nature, e.g. day is bright in bright as day, sheets 
(continued) 
are white in white as a sheet. Most nouns with the highest collostruction strength denote paragons whose semantic relation with the adjective is less obvious. These lexemes belong to highly conventionalized expressions (bold as brass, safe as houses, etc.). Globally productive individuals are more likely to be used in new AasNP formations than individuals belonging to strongly associated pairs. 
The most productive individuals according to P belong to weakly associ­ated pairs. The most productive subschemas are indexed on adjectives. These adjectives denote basic properties such as colors and shades (black, white, red, clear, bright, pale), texture and constitution (big, sharp, strong, thick, stiff, light), and temperature (cold). There are fewer productive subschemas indexed on nouns. With respect to the most productive subschema, A as hell, the NP has lost its literal meaning to the beneft of an exclusively intensifying function. 
As we move down from the upper-left to the bottom-right part of the plot, productivity declines and conventionalization and autonomy increase. In this study, PCA helps spot distinct loci of constructional productivity at subschematic levels. In other words, productivity is by no means an all or nothing affair. 
Representative Study 3 
D. Biber (1988). Variation across Speech and Writing. Cambridge: Cambridge University Press 
Research questions 
The aim of this work is to spot the patterns of linguistic variation among registers in a corpus of English texts. This landmark study implements an intuition formerly formulated by sociolinguists according to which linguistic features that co-occur signifcantly can discriminate among registers. 
Data 
Biber combines the London-Lund and the Lancaster-Oslo-Bergen corpora to obtain a large and varied corpus that contains a wide variety of spoken and written texts (Biber 1988, Appendix I). 
(continued) 
Method 
Sixty-seven linguistic features are included in the analysis (Biber 1988, pp. 73–75). These are grouped into sixteen classes: (a) tense and aspect mark­ers, (b) place and time adverbials, (c) pronouns and pro-verbs, (d) questions, 
(e) 
nominal forms, (f) passives, (g) stative forms, (h) subordination features, 

(i) 
prepositional phrases, adjectives and adverbs, (j) lexical specifcity, (k) lexical classes, (l) modals, (m) specialized verb classes, (n) reduced forms and dispreferred structures, (o) coordination, and (p) negation. 


First, the corpus is tagged for linguistic features. Next, the frequency counts of all linguistic features are extracted, normalized, and standardized. This guarantees a fair comparison of frequency distributions across texts of unequal lengths. Then, factor analysis is used to identify the dimensions, where each dimension captures a pattern of underlying co-occurrence patterns among linguistic features. A factor loading indicates the extent to which a given feature is representative of the dimension underlying a factor. Dimension scores are calculated for each text sample by adding up standardized frequencies with salient positive loadings and subtracting salient negative loadings on a dimension. Finally, each dimension is interpreted in functional terms. This correspondence between dimensions and functions is facilitated by promax rotation. 
Results 
Because dimensions have a functional basis, each of them is associated with a distinctive pattern of register variation and assigned an interpretive label. In Biber (1988), fve major dimensions are found: 
1. 
involved versus informational production; 

2. 
narrative discourse; 

3. 
situation-dependent versus elaborated reference; 

4. 
overt expression of argumentation; 

5. 
impersonal/asbtract style. 


Each dimension is captured by a distinction between positive and neg­ative features. For example, the positive features of the frst dimension are: verbs, pronouns, adverbs, dependent clauses, and other (contractions, discourse particles, clause coordination, etc.). The negative features of the same dimension are: nouns, long words, prepositional phrases, attributive adjectives, and lexical diversity. Subsequent studies have confrmed that the underlying dimensions of variation and the relations among registers display similar confgurations across languages. 
19.3 Practical Guide with R 
In this section, I show how to run the code to perform CA, MCA, and PCA with FactoMineR. The package should therefore be downloaded and installed beforehand. EFA is run with factanal(), which is part of base R. Therefore, it does not require any extra package. 
19.3.1 Correspondence Analysis 
Leitner (1991) reports a study by Hirschmler (1989) who compares the distribu­tion of complex prepositions in three corpora of English: the Brown Corpus, the LOB Corpus, and the Kolhapur Corpus. The Brown Corpus is a corpus of American English (Francis and Ku.
cera 1964). The LOB Corpus is the British counterpart to the Brown Corpus (Leech et al. 1978, 1986). The Kolhapur Corpus is a corpus of Indian English (Shastri et al. 1986). 
Complex prepositions are multiword expressions (i.e. expressions that consist of several words): ahead of, along with, apart from, such as, thanks to, together with, on account of, on behalf of,or on top of. In Hirschmler’s data, 81 prepositions consist of two words and 154 of three and more, out of a total of 235 complex prepositions. He observes a higher incidence of complex prepositions in the Kolhapur Corpus than in the other two corpora. He also observes that the most complex prepositions (i.e. prepositions that consist of three words and more) are over-represented in the corpus of Indian English. Leitner (1991, p. 224) interprets Hirschmler’s results in the light of the following assumption: 
Their use is often associated with the level of formality (Quirk et al. 1985) or regarded as 
bad style. Since non-native Englishes are often claimed to use a more formal register than 
native Englishes, complex prepositions provide a little studied testing ground. 
Following Leitner (1991), we replicate Hirschmler’s study based on a two-fold assumption: 
• 
complex prepositions are likely to be over-represented in the Kolhapur corpus; 

• 
within the corpus, complex prepositions are likely to be over-represented in the more formal text categories. 


With the code below, we run CA on the preposition data set.5 After clearing R’s memory, we load FactoMineR and import the data fle into R (19_prepositions_brownlobkolh.rds, see companion fles).6 
5On top of FactoMineR, several packages contain a dedicated CA function, e.g. ca (Nenadic 
and Greenacre 2007), and anacor (de Leeuw and Mair 2009). 6Details on how the data were extracted can be found in this blog post: https://corpling.hypotheses. org/284 (accessed 9 June 2019). 
> # clear R's memory > rm(list=ls(all=TRUE)) > # load FactoMineR > library(FactoMineR) > # load the data > dfca <-readRDS(file.choose()) 
The data set has been imported as a data frame. To inspect it, enter str(dfca) and/or head(dfca). It displays the number of times each preposition type is found in a certain context. The table consists of 257 lines (one line per preposition type) and 19 columns (one column per variable). Each column stands for a context where the preposition is found. There are three kinds of columns. The frst three columns correspond to the three corpora. The next ffteen columns correspond to the text categories. The nineteenth column specifes the word length of the prepositions. This last column (prep.length) is loaded as a factor because it contains nominal data (for this reason, it is said to be qualitative). 
The frst three columns are declared as active. Columns 4 to 18 are quantitative and declared as supplementary (col.sup=4:18). These 15 columns correspond to the 15 text categories. Column 19, which corresponds to the complexity of the preposition, is qualitative and therefore supplementary (quali.sup=19). 
> ca.object <-CA(dfca, col.sup=4:18, quali.sup=19, graph=FALSE) 
By default, the CA() function produces a graph based on the frst two dimen­sions. For the time being, these plots are not generated yet (graph=FALSE). Each graph will be plotted individually later, with specifc parameters. 
The output of CA is in ca.object. The frst lines of the ouput give the .2 score and the associated p-value. The . 2 score is very high (10,053.43) and it is associated with the smallest possible p-value (0). The deviation of the table from independence is beyond doubt. Admittedly, the assumptions of the .2 test are not all met. One of them stipulates that 80% of the cells should display expected frequencies that are greater than 5. Our table contains many cells whose expected values are smaller than 
5. Therefore, it does not meet the assumption. While this should be kept in mind, it does not preclude the fact that the choice of a preposition and the variety of English are globally interdependent, given the importance of the score. Furthermore, the .2 test is used in an exploratory context, not a hypothesis-testing context. Just because its conditions are not fully met does not mean it is irrelevant. The intensity of the relationship is defnitely small, but non negligible for this sort of data: Cramér’s V = 
0.111. A score of 1 would be unrealistic as it would attest an exclusive association between the use of prepositions and the dialect of English. 
> ca.object **Results of the Correspondence Analysis (CA)** The row variable has 257 categories; the column variable has 3 categories 
The chi square of independence between the two variables is equal to 10053.43 (p-value = 0 ). 
*The results are available in the following objects: 
name  description  
1  "$eig"  "eigenvalues"  
2  "$col"  "results for the columns"  
3  "$col$coord"  "coord. for the columns"  
4  "$col$cos2"  "cos2 for the columns"  
5  "$col$contrib"  "contributions of the columns"  
6  "$row"  "results for the rows"  
7  "$row$coord"  "coord. for the rows"  
8  "$row$cos2"  "cos2 for the rows"  
9  "$row$contrib"  "contributions of the rows"  
10 "$col.sup$coord"  "coord. for supplementary columns"  
11 "$col.sup$cos2"  "cos2 for supplementary columns"  

12 "$quali.sup$coord" "coord. for supplementary categorical var." 13 "$quali.sup$cos2" "cos2 for supplementary categorical var." 14 "$call" "summary called parameters" 15 "$call$marge.col" "weights of the columns" 16 "$call$marge.row" "weights of the rows" 
The eig object allows to see how many dimensions there are to inspect. Because the input table is simple and because the number of active variables is low, there are only two dimensions to inspect. Indeed, the frst two dimensions represent 100% of the variance of the table. In most other studies, however, we should expect to inspect more than two dimensions. Our decision is based on the cumulative percentage of variance. The inertia (i.e. the sum of eigenvalues) is low (0.0248). This means that there is not much variance in the table and that the tendencies that we are about to observe are subtle. 
> ca.object$eig eigenvalue percentage of variance cumulative percentage of variance 
dim 1 0.020398336 82.34156 82.34156 dim 2 0.004374495 17.65844 100.00000 
In case there are more than two dimensions to inspect, a scree plot is useful. 
> barplot(ca.object$eig[,2], 
+ 
names=paste("dimension", + 1:nrow(ca.object$eig)), 

+ 
xlab="dimensions", 

+ 
ylab="percentage of variance") 


The standard graphic output of CA is a symmetric biplot in which both row variables and column variables are represented in the same space using their coordinates. In this case, only the distance between row points or the distance between column points can be interpreted accurately (Greenacre 2007, p. 72). Only general observations can be made about the distance between row points and column points, when these points appear in the same part of the plot with respect to the center of the cloud of points (Husson, p.c.). Assessing the inter-distance between rows and columns accurately is possible in either an asymmetric plot or a scaled symmetric biplot. In an asymmetric biplot, either the columns are represented in row space or the rows are represented in a column space. In a scaled symmetric biplot, neither the row metrics nor the column metrics are preserved. Rows and columns are scaled to have variances equal to the square roots of eigenvalues, which allows for direct comparison in the same plot.7 
The CA graph is plotted with the plot.CA() function. The rows are made invisible to avoid cluttering the graph with prepositions (invisible="ind"). The prepositions can be plotted together with the column variables by removing invisible="ind". To prevent the labels from being overplotted, autoLab is set to "yes". By setting shadowtext to TRUE, a background shadow facilitates reading. The font size of the labels is adjusted to 80% of their default size (cex=0.8). The active column variables are in magenta (col.col="magenta") whereas the supplementary column variables are in Dodger blue (col.col.sup= "dodgerblue"). Finally, a title is included (title=). Its font size is 80% of the default size (cex.main=.8). 
> plot.CA(ca.object, 
+ 
invisible="row", 

+ 
autoLab="yes", 

+ 
shadow=TRUE, + cex=.8, 

+ 
col.col="magenta", 

+ 
col.col.sup="dodgerblue", 

+
 title="Distribution of prepositions based on lexical complexity 

+ 
in three corpora:\n LOB (British English), Brown (US English), 

+ 
and Kolhapur (Indian English)", + cex.main=.8) 


7This possibility is not offered in FactoMineR. It is offered in the factoextra (Kassambara and Mundt 2017)and ca (Nenadic and Greenacre 2007) packages. 
Distribution of prepositions based on lexical complexity in three corpora: LOB (British English), Brown (US English), and Kolhapur (Indian English) 

-0.2 -0.1 0.0 0.1 0.2 
Dim 1 (82.34%) 
Hirschmler observed the following: (1) complex prepositions cluster in non­fctional texts, a preference that is amplifed in the Kolhapur Corpus; (2) learned and bureaucratic writing shows a more pronounced pattern in the Kolhapur Corpus than in the British and American corpora. The CA plot refects these tendencies (Fig. 19.1). 
The frst dimension (along the horizontal axis) accounts for 82.34% of the variance. It shows a clear divide between Brown and LOB (left) and Kolhapur (right). Large complex prepositions (three words and more: prep.length.3 and prep.length.4) are far more likely to occur in Indian English than in British or US English. No such preference is observed for one-word and two-word prepo­sitions (prep.length.1 and prep.length.2). Very formal text categories cluster to the right, along with the Kolhapur corpus: learned_scientific, press_reviews, and religion, miscellaneous (governmental docu­ments, foundation reports, industry reports, college catalogue, industry in-house publications). The second dimension (along the vertical axis) accounts for 17.66% of the variance. It distinguishes the LOB corpus (upper part of the plot) from the Brown corpus (lower part). All in all, complex prepositions are specifc to the Kolhapur Corpus, especially in formal contexts. 
19.3.2 Multiple Correspondence Analysis 
Schmid (2003) provides an analysis of sex differences in the 10M-word spoken section of the British National Corpus (BNC). Schmid shows that women use certain swear-words more than men, although swear-words which tend to have a perceived ‘strong’ effect are more frequent in male speech. Schmid’s study is based on two subcorpora, which are both sampled from the spoken section of the BNC. The subcorpora amount to 8,173,608 words. The contributions are not equally shared among men and women since for every 100 word spoken by women, 151 are spoken by men. To calculate the distinctive lexical preferences of men and women, while taking the lack of balance in the contributions into account, Schmid’s measures rely on the difference coeffcient, borrowing the formula from Leech and Fallon (1992, 30) and Hofand and Johansson (1982). This formula is based on normalized frequencies per million words. Its score ranges from -1 (if a word occurs more frequently in women’s utterances) to 1 (if a word occurs more frequently in male speech). Absolute frequencies are used to calculate the signifcance level of the differences using the hypergeometric approximation of the binomial distribution. With respect to swear-words, Schmid’s conclusion is that both men and women swear, but men tend to use stronger swear-words than women. 
Schmid’s study is repeated here in order to explore the distribution of swear­words with respect to gender in the BNC-XML. The goal is to see if: 
• 
men swear more than women; 

• 
some swear-words are preferred by men or women; 

• 
the gender-distribution of swear-words is correlated with other variables: age and social class. 


The data fle for this case study is 19_swearwords_bnc.txt (see com­panion fles).8 Unlike Schmid, and following Rayson et al. (1997), the data are extracted from the demographic component of the BNC-XML, which consists of spontaneous interactive discourse. The swear-words are: bloody, damn, fuck, fucked, fucker, fucking, gosh, and shit. Two exploratory variables are included in addition to gender: age and social class.9 
8The code for the extraction was partly contributed by Mathilde Léger, a third-year student at Paris 8 University, as part of her end-of-term project. 9http://www.natcorp.ox.ac.uk/docs/catRef.xml (accessed 9 June 2019). 
> # clear R's memory > rm(list=ls(all=TRUE)) > > #load FactoMineR > library(FactoMineR) > > # load the data > df <-read.table(file=file.choose(), header=TRUE, sep="\t") 
The data set contains 293,289 swear-words. These words are described by three categorical variables (nominal data): 
• 
gender (2 levels: male and female) 

• 
age (6 levels: Ag0, Ag1, Ag2, Ag3, Ag4, Ag5) 

• 
social class (4 levels: AB, C1, C2, DE) Age breaks down into 6 groups: 

• 
Ag0: respondent age between 0 and 14; 

• 
Ag1: respondent age between 15 and 24; 

• 
Ag2: respondent age between 25 and 34; 

• 
Ag3: respondent age between 35 and 44; 

• 
Ag4: respondent age between 45 and 59; 

• 
Ag5: respondent age is 60+. Social classes are divided into 4 groups: 

• 
AB: higher management: administrative or professional. 

• 
C1: lower management: supervisory or clerical; 

• 
C2: skilled manual; 

• 
DE: semi-skilled or unskilled. 


As we inspect the structure of the data frame with str(), it is advisable to keep an eye on the number of levels for each variable and see if any can be kept to a minimum to guarantee that inertia will not drop. 
> str(df) 
'data.frame': 293289 obs. of 4 variables: $ word : Factor w/ 8 levels "bloody","damn",..: 2 2 77727277... $gender :Factorw/2levels"f","m":2222222222... $ age : Factor w/ 6 levels "Ag0","Ag1","Ag2",..: 6 666666666... $ soc_class: Factor w/ 4 levels "AB","C1","C2",..: 1 1 11111111... 
> table(df$word) 
bloody damn fuck fucked fucker fucking gosh shit 
146203 32294 9219 11 467 23487 60678 20930 
The variable word has eight levels. We can group fuck, fucking, fucked, and fucker into a single factor: f -words. With gsub(), we replace each word with the single tag f-words. 
> df$word <-gsub("fuck|fucking|fucker|fucked", "f-words", df$word, ignore.case=TRUE) > table(df$word) 
bloody damn f-words gosh shit 146203 32294 33184 60678 20930 
The number of levels has been reduced to fve. We convert df$word back to a factor. 
> df$word <-as.factor(df$word) 
As in CA, we can declare some variables as active and some other variables as supplementary/illustrative in MCA. We declare the variables corresponding to swear-words and gender as active, and the variables age and social class as supplementary/illustrative. 
We run the MCA with the MCA() function. We declare age and soc_class as supplementary (quali.sup=c(3,4)). We do not plot the graph yet (graph=FALSE). 
> mca.object <-MCA(df, quali.sup=c(3,4), graph=FALSE) 
Again, the eig object allows us to see how many dimensions there are to inspect. 
> round(mca.object$eig, 2) eigenvalue percentage of variance cumulative percentage of variance 
dim 1  0.56  22.47  22.47  
dim 2  0.50  20.00  42.47  
dim 3  0.50  20.00  62.47  
dim 4  0.50  20.00  82.47  
dim 5  0.44  17.53  100.00  

The number of dimensions is rather large and the frst two dimensions account for only 42.47% of f2. To inspect a signifcant share of f2, e.g. 80%, we would have to inspect at least 4 dimensions. This issue is common in MCA. The eigenvalues can be vizualized by means of a scree plot (Fig. 19.2). It is obtained as follows. 
> barplot(mca.object$eig[,1], + names.arg=paste("dim ", 1:nrow(mca.object$eig)), las=2) 
Fig. 19.2  A scree plot  
showing the eigenvalues  20  
associated with each  
dimension  
15  
10  
5  
0  


dim 1
dim 2
dim 3
dim 4
dim 5 
Ideally, we would want to see a sharp decrease after the frst few dimensions, and we would want these frst few dimensions to account for as much share of f2 as possible. Here, no sharp decrease is observed. 
The MCA map is plotted with the plot.MCA() function. Each category is the color of its variable (habillage="quali"). The title is removed (title=""). 
> plot.MCA(mca.object, 
+ 
invisible="ind", 

+ 
autoLab="yes", 

+ 
shadowtext=TRUE, 

+ 
habillage="quali", 

+ 
title="") 


In the MCA biplot (Fig. 19.3), each category is the color of its variable. Let us focus frst on the frst dimension (the horizontal axis) and ignore the second dimension (the vertical axis). Strikingly, the most explicit swear-words (f -words) cluster in the rightmost part of the plot. These are used mostly by men. Female speakers tend to prefer a softer swear word: bloody. Next, we focus on the second dimension and ignore the frst. Words in the upper part (gosh and shit) are used primarily by upper-class speakers. F-words, bloody, and damn are used by lower social categories. Age groups are positioned close to the intersection of the axes. This is a sign that the frst two dimensions bring little or no information about them. 
Combining the two dimensions, the plot is divided into four corners in which we observe three distinct clusters: 
• 
cluster 1 (upper-right corner) gosh and shit, used by male and female upper class speakers; 

• 
cluster 2 (lower-left corner) bloody, used by female middle-class speakers; 


MCA factor map 

Dim 1 (22.47%) 
• cluster 3 (lower-right corner) f -words and damn, used by male lower-class speakers. 
A divide exists between male (m, right) and female (f, left) speakers. However, as the combined eigenvalues indicate, we should be wary of making fnal conclusions based on the sole inspection of the frst two dimensions. The relevance of age groups becomes more relevant if dimensions 3 and 4 are inspected together (Fig. 19.4). To do so, the argument axes=c(3,4) is added in the plot.MCA() call. 
> plot.MCA(mca.object, + axes=c(3,4), 
+ 
invisible="ind", 

+ 
autoLab="yes", 

+ 
shadowtext=TRUE, 

+ 
habillage="quali", 

+ 
title="") 


With respect to dimensions 3 and 4, the male/female distinction disappears (both variables overlap where the two axes intersect). A divide is observed between f ­
MCA factor map 

Dim 3 (20.00%) 
words and bloody (left), used mostly by younger and middle-aged speakers, and gosh and damn (right), used mostly by upper-class speakers from age groups 3 and 5. The most striking feature is the outstanding position of shit in the upper-left corner. Although used preferably by male and female upper class speakers (Fig. 19.3), it is also used, although to a lesser degree, by much younger speakers from lower social classes. 
19.3.3 Principal Component Analysis 
Gréa (2017) compares fve prepositions that denote inclusion in French: parmi ‘among’, au centre de ‘at the center of’, au milieu de ‘in the middle of’, au cœur de ‘at the heart of’, and au sein de ‘within’/‘in’/‘among’. To determine the semantic profle of each preposition, Gréa examines their preferred and dispreferred nominal collocates. He uses an association measure known as calcul des spécifcités (Habert 1985; Labbé and Labbé 1994; Salem 1987), which is based on the hypergeometric distribution. A positive value indicates that the word is over-represented in the construction. The higher the value, the more the word is over-represented. A negative value indicates that the word is under-represented in the construction. The smaller the value, the more the word is under-represented (Gréa 2017, Sect. 2.2). 
To compare the semantic profles of the prepositions, the preferred and dispre­ferred nominal collocates of the prepositions are examined in the FrWaC corpus. The goal is to summarize the table graphically instead of interpreting the data table directly. 
First, we load the data set (19_inclusion_FrWaC.txt). 
> # clear R's memory > rm(list=ls(all=TRUE)) > # load the data (19_inclusion_FrWaC.txt) > data <- read.table(file=file.choose(), header=TRUE, row.names=1, sep="\t") 
As we inspect the data frame with str(), we see that 22,397 NPs were found. 
> # Inspect the structure of the data file > str(data) 'data.frame':22397 obs. of 5 variables: 
$ centre: num  -281 -274 -219 -128 -114 ... 
 $ coeur : num  -651 -913 -933 -368 -330 ... 
 $ milieu: num  -545 -432 -270 -237 -173 ... 
 $ parmi : num  -1685 -1129 -1072 -678 -499 ... 
 $ sein  : num  4226 3830 3490 1913 1522 ...  

The rows contain the nominal collocates and the columns the prepositions. The cells contain the association scores. The assumption is that the semantic profles of the prepositions will emerge from the patterns of attraction/repulsion. 
As in CA and MCA, we can declare some variables as active and some other variables as supplementary/illustrative in PCA. Here, however, we decide to declare all variables as active. We load the FactoMineR package and run the PCA with the PCA() function.10 The table contains measurements in the same unit. Standardizing them avoids giving each variable a weight proportional to its variance. Perhaps some prepositions attract most nouns more than others. The variables are standardized by default. 
> library(FactoMineR) > pca.object <-PCA(data, graph=F) 
We make sure that the frst two components are representative.11 These eigenval­ues are plotted in Fig. 19.5. 
10Several packages and functions implement PCA in R: e.g. princomp() and prcomp() from the stats package, ggbiplot() from the ggbiplot package (which is itself based on ggplot2), dudi.pca() from the ade4 package, and PCA() from the FactoMineR package. Mind you, princomp() and prcomp() perform PCA based on loadings. 
11For this kind of analysis, the frst two components should represent a cumulative percentage of variance that is far above 50%. The more dimensions there are in the input data table, the harder it will be to reach this percentage. 
Fig. 19.5  A scree plot  2.0  
showing the eigenvalues  
associated with each  
component  1.5  

1.0 
0.5 
0.0 

comp 1 
comp 2 
comp 3 
comp 4 
comp 5 
> round(pca.object$eig, 2) eigenvalue percentage of variance cumulative percentage of variance 
comp 1  2.02  40.32  40.32  
comp 2  1.37  27.42  67.74  
comp 3  1.04  20.79  88.52  
comp 4  0.51  10.14  98.67  
comp 5  0.07  1.33  100.00  

In PCA, the variables and the individuals and categories are plotted separately. The graph of variables serves as a guide to interpret the graph of individuals and categories. In the graph of variables, each variable is represented as an arrow. The circle is known as the circle of correlations. The closer the end of an arrow is to the circle (and the farther it is from where the axes intersect at the center of the graph), the better the corresponding variable is captured by the two components, and the more important the components are with respect to this variable. 
> barplot(pca.object$eig[,1], + names.arg=paste("comp ",1:nrow(pca.object$eig)), las=2) 
We plot the graph of variables and the graph of individuals side by side (Fig. 19.6). 
select= "coord 20" select= "contrib 20" 
Dim 2 (27.42%) Dim 2 (27.42%) -500 50 -500 
50 
-100 -50 0 50 100 -100 -50 0 50 100 Dim 1 (40.32%) Dim 1 (40.32%) 
select= "cos2 5" select= "dist 20" 
Dim 2 (27.42%) Dim 2 (27.42%) -500 50 -500 50 
-100 -50 0 50 100 -100 -50 0 50 100 
Dim 1 (40.32%) Dim 1 (40.32%) 
Fig. 19.7 Selecting NPs with select 
Here is what the title of each plot means: 
• 
with select="coord 20", only the labels of the twenty individuals that have the most extreme coordinates on the chosen dimensions are plotted; 

• 
with select="contrib 20", only the labels of the twenty individuals that have the highest contributions on the chosen dimensions are plotted12; 

• 
with select="cos2 5", only the labels of the fve individuals that have the highest squared-cosine scores on the chosen dimensions are plotted13; 

• 
with select="dist 20", only the labels of the twenty individuals that are the farthest from the center of gravity of the cloud of data points are plotted. 


Clear trends emerge: 
12The contribution is a measure of how much an individual contributes to the construction of a component. 13The squared cosine (cos 2) is a measure of how well an individual is projected onto a component. 
•the 
au sein de construction tends to co-occur with collective NPs that denote groups of human beings (entreprise ‘company/business’, équipe ‘team’, étab­lissement ‘institution/institute’, etc.); 

•the 
au centre de and au cœur de constructions tend to co-occur with NPs that denote urban areas (ville ‘city/town’, village ‘village’, quartier ‘district’) and thoughts or ideas (préoccupations ‘concerns/issues’, débat ‘debate/discussion/ issue’); 

•the 
au milieu de and parmi constructions tend to co-occur with plural NPs that denote sets of discrete individuals (hommes ‘men’, personnes ‘persons’, membres ‘members’), among other things. 


The graph displaying the frst two components does a good job at grouping prepositions based on the nominal collocates that they have in common and revealing consistent semantic trends. However, it does not show what distinguishes each preposition. For example, au centre du confit ‘at the center of the confict’ profles a participant that is either the instigator of the confict or what is at stake in the confict. In constrast, au cœur du confit ‘at the heart of the confict’ denotes the peak of the confict, either spatially or temporally. This issue has nothing to do with the PCA. It has to do with the kind of collocational approach exemplifed in the paper, which does not aim to (and is not geared to) reveal fne-grained semantic differences by itself. 
19.3.4 Exploratory Factor Analysis 
The same data set serves as input for EFA, which is performed with factanal(). According to Fig. 19.5, which shows that three principal components are worth investigating, we are tempted to specify 3 factors. Unfortunately, this is not going to work because 3 factors are too many for 5 variables in the kind of EFA that factanal() performs.14 Therefore, we set the number of required factors to 2. A .2 test reports whether the specifed number of factors is suffcient. If the p-value is smaller than 0.05, more factors are needed. If it is greater than 0.05, no more factors are needed. The test reports that the .2 statistic is 12,667.73 on 1 degree of freedom and that the p-value is 0. Although a third factor is required, we have no choice but stick to 2 factors. This means that we should be careful when we interpret the results. 
In base R, we run EFA with factanal().15 The factors argument is set to 
2. By default, the varimax rotation applies. 
14How many factors are considered worth keeping involves a choice based a metric known as SS 
loadings, as explained below. 15The FactoMineR package includes several extensions of factor analysis. Multiple factor analysis (MFA) is used to explore datasets where variables are structured into groups. Like PCA, it can handle continuous and/or categorical variables simultaneously (Pagès 2014). MFA further breaks down into hierarchical multiple factor analysis (Lê and Pagès 2003) and dual multiple factor analysis (Lê and Pagès 2010). Although commonly used in sensorimetrics, these methods are rare in linguistics. 
> fa.object <-factanal(data, factors=2) > fa.object 
Call: factanal(x = data, factors = 2) 
Uniquenesses:  
centre  coeur milieu  parmi  sein  
0.655  0.436  0.849  0.005  0.005  

Loadings: 
Factor1 Factor2 centre 0.587 coeur 0.750 milieu 0.389 parmi -0.147 0.987 sein -0.740 -0.669 
Factor1 Factor2 SS loadings 1.626 1.424 Proportion Var 0.325 0.285 Cumulative Var 0.325 0.610 
Test of the hypothesis that 2 factors are sufficient. The chi square statistic is 12667.73 on 1 degree of freedom. The p-value is 0 
The output displays uniqueness, loadings (the loadings that are too close to zero are not displayed), the proportions of variance explained by the factors, and the .2 test. Factor loadings are the weights and correlations between the variables and the factors. The higher the loading the more relevant the variable is in explaining the dimensionality of the factor. If the value is negative, it is because the variable has an inverse impact on the factor. Au milieu de, au centre de, and au cœur de defne the frst factor. Parmi defnes the second factor. It seems that au sein de defnes both. 
The proportions of variance explained by the factors (i.e. eigenvalues) are listed under the factor loadings. A factor is considered worth keeping if the corresponding SS loading (i.e. the sum of squared loadings) is greater than 1. Two factors are retained because both have eigenvalues over 1. Factor 1 accounts for 32.5% of the variance. Factor 2 account for 28.5% of the variance. Both factors account for 66.9% of the variance. 
In EFA, rotation is a procedure meant to clarify the relationship between variables and factors. As its name indicates, it rotates the factors to align them better with the variables. The two most frequent rotation methods are varimax and promax. With varimax, the factor axes are rotated in such a way that they are still perpendicular to each other. The factors are uncorrelated and the production of 1s 
Fig. 19.8 Loadings with 
parmi 
varimax rotation 
milieucentrecoeur 
sein 
-1.0 -0.5 0.0 0.5 1.0 
Factor1 
Fig. 19.9 Loadings with 
parmi 
promax rotation 
milieu 
centre 
coeur 
sein 
-1.0 -0.5 0.0 0.5 1.0 
Factor1 
and 0s in the factor matrix is maximized. With promax, the factor axes are rotated in an oblique way. The factors are correlated. With promax, the resulting model provides a closer ft to the data than with varimax. In either case, the goal is to arrive at a few common meaningful factors. Rotation is optional as it does not modify the relationship between the factors and the variables. Figure 19.8 is a plot of the loadings of the prepositions on the two factors with varimax rotation. Figure 19.9 is the same plot of the loadings with promax rotation. 
The code below is used to plot the loadings of the prepositions on the two factors with varimax rotation. 
Factor2 Factor2 -0.5 0.0 0.5 1.0 -0.5 0.0 0.5 1.0 
> loadings <-loadings(fa.object) > plot(loadings, type="n", xlim=c(-1,1)) > text(loadings, rownames(loadings)) 
To produce a plot with promax rotation, we run factanal() again but set rotation to promax. 
> fa.object2 <-factanal(data, factors=2, rotation="promax") > loadings2 <-loadings(fa.object2) > plot(loadings2, type="n", xlim=c(-1,1)) > text(loadings2, rownames(loadings2)) 
The distinctive profles we obtain with EFA are similar to those we obtained with PCA. The only major difference is the proximity of au milieu de with au centre de and au cœur de. This may be due to the fact that only two factors are retained in the analysis. As far as this data set is concerned, PCA is clearly a better alternative, all the more so as individuals are not taken into account in the graphic output of this kind of EFA. 
19.3.5 Reporting Results 
When reporting the results of CA, MCA, or PCA, the following elements should be included: 
• 
the cumulative percentage of variance explained by each dimension/component; 

• 
the graph and its interpretation. 


Additionally, numeric descriptors such as contribution and quality of projection can be reported. 
Each methods has its specifcities. In CA, it is customary to report the . 2 test result to see if the table deviates from independence. This result is part of the default output of the CA() function of the FactoMineR package (see Sect. 19.3.1). 
In MCA, the eigenvalues associated with the frst dimensions are often much lower than in CA and PCA. This means that it is often necessary to take more dimensions into account in the analysis. When the dimensionality of a dataset is high, the representation quality of a variable on a given plane is bound to be poor. However, how much a variable contributes to a given dimension is not affected by the high-dimensional nature of the data. Although optional, taking a look at the contribution and reporting the scores might be a good idea. In Sect. 19.3.2,the contribution scores of the variables are accessed by entering the following: 
> mca.object$var$contrib 
Dim1 Dim2 Dim3 Dim4 Dim 5 bloody 1.668529e+01 6.782887e+00 5.964242e+00 4.032829e+00 1.668529e+01 damn 4.740002e+00 3.204420e+01 4.254051e+01 4.924310e+00 4.740002e+00 f-words 2.784165e+01 7.606179e-01 2.789159e+01 4.350051e+00 2.784165e+01 gosh 7.329551e-01 5.436848e+01 1.835285e+01 5.123951e+00 7.329551e-01 shit 1.019655e-04 6.043815e+00 5.250816e+00 8.156886e+01 1.019655e-04 f 2.141147e+01 1.164802e-21 5.381496e-23 1.268778e-21 2.141147e+01 m 2.858853e+01 1.524969e-21 7.838154e-23 1.686021e-21 2.858853e+01 
In PCA, there are two graphs to inspect: the graph of variables and the graph of individuals (see Sect. 19.3.3). The graphs produced with CA, MCA, and PCA should be interpreted by focusing frst on the horizontal axis and second on the vertical axis. 
The output of fa.object in Sect. 19.3.4 is typical of how the results of an EFA should be reported. Therefore, it can conveniently be copied and pasted into the results section of a paper. Biber (1988) offers an excellent example of how the linguist can make sense of the EFA numeric indicators. See Chap. 26 for more general information on how to report the results in a quantitative corpus-based study. 
Further Reading 
R. H. Baayen (2008). Analyzing Linguistic Data: A Practical Introduction to Statistics using R. Cambridge University Press. 
Well-known to quantitative linguists, this textbook explains, among many other methods, how to run CA and PCA with R. It also shows how to run EFA. The data sets are directly relevant to linguistics and provided as part of the languageR package. 
G. Desagulier (2017). Corpus Linguistics and Statistics with R. Introduction to Quantitative Methods in Linguistics. Quantitative Methods in the Humanities and Social Sciences. New York: Springer. 
Chapter 10 of this book presents in greater detail three of the four methods covered in this chapter: CA, MCA, and PCA. Each method is illustrated with a detailed linguistic case study. The corresponding data sets and R scripts are provided in the form of companion fles. 
M. J. Greenacre (2007). Correspondence Analysis in Practice. Vol. 2. Interdisci­plinary statistics series. Boca Raton: Chapman & Hall/CRC. 
This textbook focuses on CA and its variants (joint correspondence analysis, canonical correspondence analysis, co-inertia analysis, co-correspondence analysis) as well as multiple correspondence analysis. Although the book gives priority to practice, the theoretical and mathematical aspects of CA are presented in two appendices (A and B, respectively). The book can be read in combination with the documentation of the ca package (Nenadic and Greenacre 2007). 
F. Husson, S. Lê, and J. Pagès (2010). Exploratory Multivariate Analysis by Example Using R. London: CRC press. 
Like Greenacre (2007), this book’s main thrust is toward practice while making room for the theoretical and mathematical underpinnings of multivariate exploratory methods. It shows how to implement CA, PCA, and MCA with the FactoMineR package featured in the present chapter. 
References 
Baayen, R.H. (2008). Analyzing linguistic data: A practical introduction to statistics using R. Cambridge: Cambridge University Press. 
Benzécri, J.-P. (1984). Analyse des correspondances: Exposé Élémentaire (Vol. 1). Pratique de l’Analyse des Données. Paris: Dunod. 
Biber, D. (1988). Variation across speech and writing. Cambridge: Cambridge University Press. 
Biber, D. (1995). Dimensions of register variation: A cross-linguistic comparison. Cambridge: Cambridge University Press. 
Biber, D. (2001). Dimensions of variation among eighteenth-century registers. In H.-J. Diller & M. Glach (Ed.), Towards a history of english as a history of genres (pp. 89–110). Heidelberg: C. Winter. 
Biber, D., & Conrad, S. (2001). Variation in english: Multi-dimensional studies. London: Longman. 
Biber, D., & Gray, B. (2016). Grammatical complexity in academic english: Linguistic change in writing. Cambridge: Cambridge University Press. 
Biber, D., & Hared, M. (1992). Dimensions of register variation in Somali. Language Variation and Change, 4(1), 41–75. 
de Leeuw, J., & Mair, P. (2009). Simple and canonical correspondence analysis using the R package anacor. Journal of Statistical Software, 31(5), 1–18. 
Desagulier, G. (2015). A lesson from associative learning: Asymmetry and productivity in multiple-slot constructions. In Corpus Linguistics and Linguistic Theory. https://doi.org/10. 1515/cllt-2015-0012. 
Desagulier, G. (2017). Corpus linguistics and statistics with R. introduction to quantitative methods in linguistics. Quantitative methods in the humanities and social sciences. New York: Springer. 
Francis, W. N., & Ku.
cera, H. (1964). A standard corpus of present-day edited American english, for use with digital computers (Brown). Providence: Brown University. 
Glynn, D. (2014). The many uses of run. In D. Glynn & J. A. Robinson (Ed.), Corpus methods for semantics: Quantitative studies in polysemy and synonymy (Vol. 43, pp. 117–144). John Benjamins: Human Cognitive Processing. 
Gréa, P. (2017). Inside in French. Cognitive Linguistics, 28(1), 77–130. 
Greenacre, M. J. (2007). Correspondence analysis in practice (Vol. 2). Interdisciplinary statistics series. Boca Raton: Chapman & Hall/CRC. 
Gries, S. T. (2006). Corpus-based methods and cognitive semantics: The many senses of to run. In S. T. Gries & A. Stefanowitsch (Ed.), Corpora in cognitive linguistics: Corpus-based approaches to syntax and lexis (pp. 57–99). Berlin: Mouton de Gruyter. 
Grieve, J., et al. (2010). Variation among blogs: A multi-dimensional analysis. In A. Mehler, S. Sharoff, & M. Santini (Ed.), Genres on the web. Text, speech and language technology (Vol. 42, pp. 303–322). New York: Springer. 
Habert, B. (1985). L’analyse des formes «spécifques» [bilan critique et propositions d’utilisation]. Mots, 11(1), 127–154. 
Hirschfeld, H. O. (1935). A connection between correlation and contingency. Mathematical Proceedings of the Cambridge Philosophical Society, 31(4), 520–524. Cambridge University Press. 
Hirschmler, H. (1989). The use of complex prepositions in Indian English in comparison with British and American English. In G. Graustein & W. Thiele (Ed.), Englische textlinguistik und varietätenforschung. Linguistische arbeitsberichte (Vol. 69, pp. 52–58). Leipzig: Karl Marx Universität. 
Hofand, K., & Johansson, S. (1982). Word frequencies in British and American English. Norwegian Computing Centre for the Humanities. 
Husson, F., Lê, S., Pagès, J. (2010). Exploratory multivariate analysis by example using R. London: CRC press. 
Kassambara, A., & Mundt, F. (2017). Factoextra: Extract and visualize the results of multivariate data analyses. R package version 1.0.5. 
Kay, P. (2013). The limits of (construction) grammar. In T. Hoffmann & G. Trousdale (Ed.), The Oxford handbook of construction grammar. Oxford: Oxford University Press. 
Kilgarriff, A. (2005). Language is never, ever, ever, random. Corpus linguistics and linguistic theory, 1(2), 263–276. 
Kim, Y.-J., & Biber, D. (1994). A corpus-based analysis of register variation in Korean. In D. Biber & E. Finegan (Ed.), Sociolinguistic perspectives on register (pp. 157–181). New York: Oxford University Press. 
Labbé, C., & Labbé, D. (1994). Que mesure la spécifcité du vocabulaire? Lexicometrica, 3, 2001. 
Lacheret-Dujour, A., et al. (2019). The distribution of prosodic features in the Rhapsodie corpus. In A. Lacheret-Dujour & S. Kahane (Ed.), Rhapsodie: A prosodic and syntactic treebank for spoken French, Chap. 17. Studies in corpus linguistics (Vol. 89, pp. 315–338). Amsterdam: John Benjamins. 
Lê, S., & Pagès, J. (2003). Hierarchical multiple factor analysis: Application to the comparison of sensory profles. Food Quality and Preference, 14(5–6), 397–403. 
Lê, S., & Pagès, J. (2010). DMFA: Dual multiple factor analysis. Communications in Statistics— Theory and Methods, 39(3), 483–492. 
Leech, G., & Fallon, R. (1992). Computer corpora–what do they tell us about culture. ICAME Journal, 16, 29–50. 
Leech, G., Johansson, S., & Hofand, K. (1978). The LOB corpus, original version (1970–1978). Lancaster/Oslo/Bergen. 
Leech, G., et al. (1986). The LOB corpus, POS-tagged version (1981–1986). Lancaster/Oslo/Ber­gen. 
Leitner, G. (1991). The Kolhapur Corpus of Indian English: Intra-varietal description and/or intervarietal comparison. In S. Johansson & A.-B. Stenstr (Ed.), English computer corpora. Topics in english linguistics (pp. 215–232). Berlin: Mouton de Gruyter. 
Nenadic, O., & Greenacre, M. J. (2007). Correspondence analysis in R, with two and three-dimensional graphics: The CA package. Journal of Statistical Software, 20(3), 1–13. 
Pagès, J. (2014). Multiple factor analysis by example using R. Boca Raton: Chapman & Hall/CRC. 
Rayson, P., Leech, G. N., & Hodges M. (1997). Social differentiation in the use of English vocabulary: Some analyses of the conversational component of the British National Corpus. International Journal of Corpus Linguistics, 2(1), 133–152. 
Salem, A. (1987). Pratique des Segments Répétés: Essai de Statistique Textuelle. Paris: Klinck­sieck. 
Schmid, H. J. (2003). Do men and women really live in different cultures? Evidence from the BNC. In A. Wilson, P. Rayson, & T. McEnery (Ed.), Corpus linguistics by the Lune.Ld´z studies in language (pp. 185–221). Frankfurt: Peter Lang. 
Shastri, S. V., Patilkulkarni, C. T., & Shastri, G. S. (1986). The Kolhapur Corpus. India: Kolhapur. 
Part V Hypothesis-Testing 
Chapter 20 Classical Monofactorial (Parametric 
and Non-parametric) Tests 
Vaclav Brezina 
Abstract This chapter focuses on the use of monofactorial statistical tests to compare two or more groups of speakers or two or more corpora. It starts with a discussion of the Null-Hypothesis Signifcance Testing Procedure (NHSTP) and its applications to corpus data. The chapter then offers seven different statistical procedures showing their principles, equations, and underlying assumptions. These procedures include the chi-squared test, the t-test, Mann-Whitney U test, ANOVA, Kruskal-Wallis test, Pearson’s correlation and non-parametric correlations. Effect sizes and confdence intervals are also discussed. A particular attention is paid to the distinction between the parametric and non-parametric tests and their respective assumptions. The ‘Practical Guide with R’ section offers the readers a step-by-step guide on how to run the tests discussed in the chapter in the statistical package R. 
20.1 Introduction 
A statistical test is a mathematical tool which helps us establish whether an observed effect is likely to be due to chance–due to a particular composition of the corpus, i.e. the specifc texts or speakers that have been randomly selected–or whether the effect is likely to exist in the population (language use in general). In the latter case, we say that the effect is statistically signifcant. Imagine that there 
The writing of this chapter has been supported by UK Economic and Social Research Council (grants ES/R008906/1 and EP/P001559/1). 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_20) contains supplementary material, which is available to authorized users. 
V. Brezina (•) Lancaster University, Lancaster, UK e-mail: v.brezina@lancaster.ac.uk 
© Springer Nature Switzerland AG 2020 473 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_20 
are two corpora including the same number of speakers (10), one of male and the other of female speech. We search them for the linguistic variable of our interest, such as swearwords, and we observe that the male corpus includes an average of 16 swearwords and the female corpus 14. The averages (means) are computed from individual relative frequencies1 per speaker sample (text fle). The question is: Do men really use more swearwords than women? On the surface level, 16 is a larger number than 14 so there is an observable difference between the two corpora but at the same time, the difference seems very small and might be due to chance. To answer the question, we have two principled options: First, we can fnd another two samples of the same type of language production and see if we get a similar difference in the use of swearwords between men and women. We would need to repeat this process multiple times until a clear pattern emerged. In practice, this option is often not possible due to limited availability of different data samples (but see Chap. 24 for the description of a resampling method called bootstrapping). Our second option is to use a statistical test. A statistical test will take into consideration 
(i) the size of the effect as well as (ii) the size of the evidence we have (number of observations) and calculate the probability of observing merely due to chance the effect in our corpus (the difference between 16 and 14 in our example) or even a larger effect. We call this probability a p-value and report it in our research reports. A small p-value (usually below the conventional threshold of 0.05) indicates a statistically signifcant result. This chapter focuses on monofactorial statistical tests, statistical procedures that produce p-values, and their interpretation. 
20.2 Fundamentals 
20.2.1 Null-Hypothesis Signifcance Testing (NHST) Paradigm 
Most statistical measures (statistical tests and confdence intervals) discussed in this chapter operate within the Null-hypothesis signifcance testing (NHST) paradigm. This paradigm is based on a simple but rigorous procedure of evaluating empirical evidence (Fisher 1935, see Fig. 20.1). It starts with the formulation of our scientifc (sometimes called alternative) hypothesis (H1), a statement we want to test using empirical evidence. To return to our sociolinguistic example, we may hypothesize that men and women differ in the frequency of use of swearwords in casual speech. The second step of the procedure is to formulate a so-called null-hypothesis (H0). The null hypothesis is the opposite of our scientifc hypothesis. For example, if we hypothesize that there is a difference between the use of swearwords in male and female speech, the null-hypothesis will state that there is no such difference. The reason for formulating H0 is purely technical and is connected with the logic of 
1Relative frequencies are used because speaker samples are of unequal sizes (number of tokens). These are calculated as absolute frequency/number of tokens in the sample x basis for normalisa­tion (e.g. 1000). 

experiments as described by Fisher (1935): we formulate H0 in order to try to reject it with the evidence available. Figure 20.1 shows the process of the NHST at the end of which we receive a p-value, which helps us decide if there is enough evidence to reject the null hypothesis. 
To evaluate the evidence (corpus fndings, e.g. the means of 16 and 14 for men and women, respectively) we employ a particular statistical test. The choice of the statistical test depends on the research design and the shape of the data. All statistical tests produce a p-value. The p-value is the probability of seeing the difference observed in the corpus or even a larger difference if the null-hypothesis were true (Cox and Donnelly 2011:147). We are therefore looking for a small p-value, which would lead us to the rejection of the null hypothesis. As Fisher (1935:16) puts it, “[e]very experiment may be said to exist only to give the facts a chance of disproving the null hypothesis.” It is useful to note that this procedure has certain limitations, which are explained in detail in Sect. 20.2.2.1. 
To demonstrate these principles in practice, the following is an overview of the procedure of statistical testing with our sociolinguistic example: 
Step 1: Formulate H1. 
Men and women differ in the frequency of use of swearwords in casual speech. 
Step 2: Formulate H0. 
There is no difference between how men and women use swearwords in casual speech. 
Step 3: Carry out Research by Analysing corpora. 
The data used consists of two corpora: a male corpus (10 speakers) and a female corpus (10 speakers). We can observe that the mean value of relative frequency of swearwords in the male corpus is 16, while the mean value in the female corpus is 14, see Table 20.1 for details. 
Step 4: Carry out an Appropriate Statistical Test. 
In our particular example, we could use an independent samples t-test or a Mann-Whitney U test (see Sect. 20.2.2) to evaluate the evidence. The results of these tests would be reported as follows: 
T-Test: t (16.77) = 0.67; p = 0.513. Mann-Whitney U Test: U = 70; p = 0.128. 
The reporting of the test might look complicated because different values get reported, such as the test statistics (t = 0.67; U = 70) or the degrees of freedom (16.77). A brief note about the two concepts: The test statistics is “used to assess a particular hypothesis in relation to some population” (Everitt 2006:392). It is the product of an equation of a statistical test. Degrees of freedom is a technical concept introduced by R.A. Fisher, which is used to denote “the number of independent units of information in a sample relevant to the estimation of a parameter or calculation of a statistic” (Everitt 2006:118). For example, if there are ten values that we insert into a calculation of the mean, nine values are free to vary, i.e. can be any values but the last value is determined by the previous nine given a particular mean. For most intents and purposes it is just important to remember that both test statistics and degrees of freedom need to be reported in research reports. For the moment, however, we will focus on the p-values. The tests themselves are described in Sect. 
20.2.2. We can see that the p-value in each case is larger than 0.05, which in linguistic and social science research is a conventional cut-off point. We therefore 
Table 20.1 Frequencies of swearwords in male and female corpora 
Speaker ID  Gender  Relative frequency of swearwordsa  
Sp1  Male  16  
Sp2  Male  16  
Sp3  Male  16  
Sp4  Male  8  
Sp5  Male  8  
Sp6  Male  16  
Sp7  Male  32  
Sp8  Male  16  
Sp9  Male  8  
Sp10  Male  24  
Sp11  Female  7  
Sp12  Female  7  
Sp13  Female  14  
Sp14  Female  14  
Sp15  Female  14  
Sp16  Female  14  
Sp17  Female  14  
Sp18  Female  14  
Sp19  Female  14  
Sp20  Female  28  
MEAN (SD)  Male: 16 (7.5)  
Female: 14 (5.7)  

aRelative frequencies in the table are rounded to the nearest integer. 
conclude that the difference observed in the data is not statistically signifcant. This means that we do not have enough evidence in our sample (corpus) to reject the null hypothesis, which, in our example, states that there is no difference between the male and female use of swearwords. However, this does not mean that there is no difference between the two groups. In the logic of the null hypothesis signifcance testing we can never accept the null hypothesis. What we are always evaluating is the amount of evidence available in the data we have at our disposal. If we, for instance, collect a larger sample (corpus) the same difference (16 and 14) can turn out to be statistically signifcant. To demonstrate this, imagine that instead of a corpus with 10 male and 10 female speakers we have a corpus with 100 male and 100 female speakers, where the difference between the groups is exactly the same (16 and 14). In this case, the results of the statistical tests will be very different: 
T-test: t (184.5) = 2.22; p = 0.028. Mann-Whitney U test: U = 7000; p < 0.001. 
Based on the new set of results above, we can reject the null hypothesis and conclude that there is a statistically signifcant difference between the two groups we have tested–males and females. Another consideration in NHST is whether the hypothesis we formulate is non-directional or directional. So far, an example of a non-directional hypothesis was considered (“Men and women differ in the frequency of use of swearwords in casual speech.”). This hypothesis states that there is a difference between two groups but does not hypothesise in which direction this difference would be. On the other hand, we can restate the hypothesis above as a directional hypothesis. 
H1d: Men use more swearwords than women in casual speech. 
Using a directional hypothesis has implications for the NHST procedure and needs to be theoretically motivated, i.e. there should be evidence in the literature that the difference is in one direction rather than the other. The statistical test that will be used for a directional hypothesis is called a one-tailed test, as opposed to the more usual two-tailed test used with a non-directional hypothesis. In our example, the one-tailed test would test the following null hypothesis: 
H0d: There is no difference between how men and women use swearwords in casual speech or women use more swearwords than men. 
Each statistical test discussed in this chapter has a one-or two-tailed (default) version, which can be selected. In the rest of this chapter, two-tailed versions of statistical tests will be considered. For a discussion of the appropriate use of a one-tailed test see Ruxton and Neuhäuser (2010). 
It is also important to note that NHST, like many other statistical procedures, works with probabilities. As explained above, the key role in NHST is played by the probability value p. When analysing data, we are therefore operating in the realm of likely rather than describing the absolute truth. It is thus possible (but unlikely) to reject a true null hypothesis (known as a type I error or a false positive fnding) or alternatively to fail to reject a false null hypothesis (known as type II error or a false negative fnding). Type I and type II errors are part and parcel of the procedure. They become a problem when their probabilities are infated (due to a violation of a test assumption or multiple testing) above an acceptable level (e.g. 5%). A brief note needs to be made about multiple testing and family-wise errors. With applying multiple tests on the same dataset, each with its own type I error, the probability of rejecting a true null hypothesis (i.e. a false hit) increases dramatically (Kapadia et al. 2005:516ff). For example, with ten statistical tests and the alpha value set to 0.05, the family wise error increases to 0.401, i.e. over 40%. This number is unacceptable in science and different types of corrections for the family wise error (Bonferroni, Sidak etc.) are applied. Bonferroni correction is a fairly conservative procedure, which works best with a smaller number of comparisons but can be too restrictive 
(i.e. infating type II errors) when the number of comparisons becomes very large. It needs to be used thoughtfully (see for guidance see Cabin and Mitchell 2000) with a full understanding of the logic of this procedure. Different options for dealing with family-wise errors are discussed by Shaffer (1995). 
In sum, NHST is a relatively simple procedure, which evaluates the amount of evidence for rejecting the null hypothesis. In addition to looking at p-values, we should also consider the size of the observed effect in the sample and the estimation of the size of the effect in the population, which can be expressed as a confdence interval (see Sect. 20.2.2.1). 
20.2.2 Statistical Tests and their Assumptions 
20.2.2.1 Chi-Squared Test 
The chi-squared test is a powerful tool for exploring categorical data (Balakr­ishnan et al. 2013; Greenwood and Nikulin 1996; Sheskin 2004:493–561; Gries 2013:165ff). Categorical data (see Chap. 17 for different types of data) is a form of data where we divide linguistic features or texts/speakers into different categories. For example, we can search a corpus for active and passive structures and trace their distribution across different genres. Alternatively, we can divide all speakers into those that use a variable of interest (e.g. a swearword) in their linguistic sample and those that do not. When summarizing categorical data, we often use a contingency table such as the one displayed below. The name contingency table refers to the fact that we use this table to show all logical options (or contingencies) of data confguration given the particular variables of interest. 
Table 20.2 shows a contingency table of observed frequencies, frequencies with which the linguistic features (active vs. passive) occur across different genre categories, something we can directly observe in the corpus (hence observed frequencies). 
The chi-squared test compares these observed frequencies with expected frequen­cies, frequencies of occurrence of the individual linguistic features under the null 
Genre Voice  Newspapers  Academic writing  Fiction  Total  
Active  85  75  285  445  
Passive  15  25  15  55  
Total  100  100  300  500  

Table 20.3 Expected frequencies of active vs. passive verb forms 
Genre Voice  Newspapers  Academic writing  Fiction  Total  
Active  100×445 = 89500  100×445 = 89500  300×445 = 267500  445  
Passive  100×55 = 11500  100×55 = 11500  300×55 = 33500  55  
Total  100  100  300  500  

hypothesis, i.e. if there were no effect of the independent variable2 (genre in our example). Expected frequencies are calculated by a simple Eq. (20.1) based on the frequencies in Table 20.2. 
row total × column total 
Expected frequency = (20.1)
grand total 
Table 20.3 shows the values of expected frequencies in our example as well as how these were calculated. 
The equation for a simple form of the chi-squared is as follows. 
(observed frequency - expected frequency)2 Chi-squared = 
across all cells expected frequency (20.2) 
Sometimes, an alternative equation is used, which applies Yates’s (1934) correction for continuity to prevent overestimation of statistical signifcance in small data sets. However, the simple equation above (20.2) is preferable for corpus data, which are usually large. 
Before calculating the chi-squared test we need to review the assumptions and prerequisites of appropriate use: 
1. Independence of Observations: The test assumes that every observation such as the use of the active or passive verb construction in English (see Table 
20.2) is independent of another observation. In corpora, we have to review this 
2An independent variable (also known as an explanatory variable or a predictor variable) is a variable that is used to explain linguistic patterns measured as dependent variables. In corpus research, independent variables are typically related to the context (like the genre in this example) or speaker characteristics (gender, age etc.). 
assumption critically. Corpora are usually sampled at the level of texts, not linguistic features. When categorising linguistic features, which occur (often) multiple times in one text, this assumption will be violated to some extent (Kilgarriff 2005). This violation may lead to an increased number of falsely positive results, i.e. false hits. If this aspect is a major concern in our data, an alternative research design needs to be considered. 
2. Expected Frequencies Greater than 5 (In contingency tables larger than 2 × 2 such as Table 20.3 at least 80% of expected frequencies greater than 5): With corpus data, which is typically large, this assumption is usually not diffcult to meet. However, when this assumption is violated, the Log likelihood test (also known as likelihood ratio test or G test) or the Fisher exact test are more appropriate for the estimation of statistical signifcance (p-values) (cf. Rayson et al. 2004; Sprent 2011; Upton 1992). 
The chi-squared test for the data in Table 20.2 is calculated as follows (using the expected frequencies from Table 20.3): 
(85 - 89)2 (75 - 89)2 (285 - 267)2 (15 - 11)2 
Chi-squared =++ + 
8989 267 11 (25 - 11)2 (15 - 33)2 
++= 32.69 
11 33 
(20.3) 
The test statistic (32.69 in our example) is then compared with the chi-squared distribution with appropriate degrees of freedom (df ) according to the size of the contingency table. The chi-squared test statistic, as every test statistics, has a known distribution under the null hypothesis. Based on a comparison of our value (32.69) with the distribution, we can make a judgement about the statistical signifcance (p­value) of the result. A 2 × 2 table corresponds to df = 1, a 2 × 3 table to df = 2, a3 × 3tableto df = 4 etc. More generally, df = (number of rows – 1) × (number of columns – 1). However, the reader does not need to worry about these technical details because the p-value is automatically provided by R (see below). 
The p-value in this case is very low, p < .001. We can therefore conclude that there is a statistically signifcant difference in the use of the passive and active verb forms across the three genre categories. In the present example, the use of active and passive voice as a function of genre was examined. It is often the case that multiple independent variables or predictors are included in the design. In such cases, logistic regression can be employed. In fact, chi-squared can be seen as a simple (mono­factorial) version of logistic regression (see Chap. 21). 
20.2.2.2 T-test 
The t-test, unlike the chi-squared test, is suitable for quantitative (rather than categorical) data, i.e. data measured at the scale/ratio level. It is a parametric test 
comparing the means of two groups and variation within each group (Sherman 1954; Sheskin 2004:375–423; Gries 2013:215ff). In corpus linguistics, we can use the t-test with relative frequencies of a linguistic variable, e.g. the relative frequencies of occurrence of swearwords in male and female speech as exemplifed in Table 20.1. In simple terms, the t-test compares the means of two groups and takes into account the variance inside each group as well as the number of observations (cases) in each group. The logic behind the test is this: the larger the difference, the smaller the variance and the larger the sample size, the larger the outcome of this test is – i.e. the test statistic. There are different versions of the t-test. The equation below (20.4) shows Welch’s version of t-test, which is preferable for corpus data because this version of the test is robust against the violation of the homoscedasticity assumption (see below) which is often the case in corpus data (Brezina 2018: Chap. 6). 
Mean of group 1 - Mean of group 2 Welch s independent sample t-test = 
Variance of group1 Variance of group2 Number of cases in group 1 + Number of cases in group 2 
(20.4) 
Before calculating the t-test we again need to review the assumptions and the prerequisites of appropriate use: 
1. 
Independence of Observations: Like the chi-squared test (see Sect. 20.2.2.1), the t-test assumes that each observation (text or speech sample) comes from a different (randomly sampled) speaker or writer3 and that the use of language by one speaker/writer in the sample is not affected by (i.e., not correlated with) the use of language by another speaker/writer. We therefore need to make sure that we include each speaker/writer as an observation (case) only once even if multiple texts/transcripts are available. 

2. 
Normal Distribution: The t-test assumes that the linguistic variable of interest is normally distributed in the population; as explained in Chap.17, this symmetrical 


distribution can be visualised as a bell-shaped curve ( 
). However, as shown in the literature (Boneau 1960; Lumley et al. 2002), the t-test is often robust against the violation of this assumption and can therefore be used even with skewed (not normally distributed) samples. An important note needs to be made at this point. The underlying assumptions of a test are connected with the mathematical design of the test. The violation of a particular assumption, as is the case of violation of normality in the t-test, does not automatically imply that the use of the test is invalid in a given situation. On the contrary, as statistical simulation experiments show, the t-test performs well even with skewed distributions. Although traditional textbook wisdom would encourage 
3If we have two or more samples from each speaker and are interested in the difference in their language between sample 1 and sample 2 etc. (e.g. linguistic change/development), we are dealing with a so-called repeated measures design, which requires a different version of the statistic (Crowder 1990). 
the reader to apply a test for normality such as the Kolmogorov-Smirnov goodness of ft test or the Shapiro-Wilk test and then opt for a non-parametric test if non-normal distribution is suspected, this is rarely a good idea. The mathematical reasons for this are complex and the interested reader is referred to the appropriate statistical literature (e.g. Rasch et al. 2011; Schucany and Tony Ng 2006; Zimmerman 1998). The practical reason against this proposal for corpus linguistics lies in the simple fact that a large majority of distributions of linguistic variables are non-normal. This would automatically disqualify the t-test from being used and unnecessarily impoverish the toolbox of the corpus linguist. 
3. Homoscedasticity (or ‘equality of variances’): This assumption applies to the original version of Student t-test. This version of the t-test assumes equality of variances in the two groups that we compare. This means, in simple terms, that this test assumes that there is comparable degree of variation in each of the compared groups. The version of the t-test known as the Welch’s independent sample t-test compensates for unequal variances (different amounts of variation in each of the compared groups); hence the violation of this assumption is not a problem for the version of the t-test introduced here. 
The t-test for the data in Table 20.1 is calculated as follows. Note that variance (SD2) is a squared version of the standard deviation from Table 20.1. 
16 - 14 
Welch s independent sample t-test == 0.67 (20.5) 
56.9 + 32.7 
10 10 
Finally, the test statistic (0.67 in our example) is compared with the t-distribution with appropriate degrees of freedom (df ) to obtain the p-value; Welch’s t-test uses a special formula for adjusting the degrees of freedom to counterbalance the effect of unequal variances (see assumption 3 above). In R, this process happens behind the scenes and a p-value is outputted with the t-test: 
T-test: t (16.77) = 0.67; p = 0.513. 
This p-value, as we have already seen in Sect. 20.2.1, is larger than the conventional 
0.05 and therefore we can conclude that the test did not fnd a statistically signifcant difference between the two groups. 
20.2.2.3 ANOVA 
ANOVA (Analysis of Variance) is a parametric test (sometimes also referred to as F-test) that can be used for comparison of values from multiple groups (Miller 1997; Sheskin 2004:667–757; Gries 2013:280ff). Its simple form, one-way ANOVA, operates with one dependent (linguistic) and one independent variable. Its use is somewhat similar to the use of the t-test (see Sect. 20.2.2.3), but unlike the t-test it can be used for comparison of more than two groups (corpora). 
Table 20.4 Relative frequencies of swearwords in lower, middle and upper-class speakers 
Speaker ID Sp10  Social class Lower  Relative frequency of swearword 24  
Sp20  Lower  28  
Sp7  Lower  32  
MEAN (SD)  28 (4)  
Sp13  Middle  14  
Sp14  Middle  14  
Sp15  Middle  14  
Sp16  Middle  14  
Sp17  Middle  14  
Sp18  Middle  14  
Sp19  Middle  14  
Sp1  Middle  16  
Sp2  Middle  16  
Sp3  Middle  16  
Sp6  Middle  16  
Sp8  Middle  16  
MEAN (SD)  14.8 (1.0)  
Sp11  Upper  7  
Sp12  Upper  7  
Sp4  Upper  8  
Sp5  Upper  8  
Sp9  Upper  8  
MEAN (SD)  7.6 (0.5)  

Table 20.4 shows a recast of the data from Table 20.1; this time, the speakers are divided into three groups according to their social class membership (lower, middle and upper). 
ANOVA is calculated according to the following equation: 
Between group variance 
One-way ANOVA (F) = (20.6)
Within group variance 
Where: cases group 1 × (mean1 - grand mean)2 +cases group2 × (mean2 - grand mean)2 + ... 
Between-group variance = 
number of groups - 1 (20.7) 
and sum of sqared distances for group 1 +sum of sqared distances for group 2 + ... 
Within-group variance = (20.8)
number of cases - number of groups 
Note that grand mean in the equation above is the mean for the whole dataset (all 
groups put together). 
Reviewing the assumptions of ANOVA, (i) independence of observations, (ii) normality and (iii) homoscedasticity, we can see that these are similar to the assumptions of the t-test (see Sect. 20.2.2.2). The ANOVA test is robust against the violation of the normality assumption (Schmider et al. 2010)–see the discussion of the normality assumption in Sect. 20.2.2.2; if the homoscedasticity assumption is violated, Welch’s version of ANOVA, which is robust against the violation of this assumption, can be used. 
ANOVA for the data in Table 20.4 is calculated as follows: 
2
3 × (28 - 15)2 + 12 × (14.8 - 15)2 + 5 × (7.6 - 15)
Between-group variance == 390.6 
3 - 1 
(20.9) 
32 + 11.7 + 1.2 
Within-group variance == 2.6 (20.10) 
20 - 3 
390.6 
One-way ANOVA (F ) == 148 (20.11)
2.6 
As with the previous tests, R also outputs the appropriate p-value for the given F-statistic (with particular degrees of freedom). In our example, the difference is statistically signifcant with p < .001. A note needs to be made here: ANOVA is an omnibus test, which tests the null hypothesis that there is no difference among the groups in question. A signifcant difference means that there is a difference between at least two groups, but ANOVA will not tell us which ones. For this reason, post-hoc tests are carried out to establish where exactly the difference lies. There are different options for post-hoc tests used in different traditions of research (e.g. LSD test, Tukey’s HSD test, Scheffe’s test etc.). The simplest option recommended for corpus data (cf. Shingala and Rajyaguru 2015) is to use a series of t-tests (see Sect. 20.2.2.4) with p-values corrected for multiple testing (e.g. using Bonferroni correction) to avoid the increase in type I error (false positive results; see Sect. 20.2.1). 
20.2.2.4 Mann-Whitney U Test 
Mann-Whitney U test, also known as Wilcoxon rank-sum, is the non-parametric equivalent of the t-test (Corder and Foreman 2009; Mann and Whitney 1947; Sheskin 2004:423–452; Gries 2013:227ff). A non-parametric test is a distribution free method, i.e. a technique “of estimation and inference that [is] based on a function of the sample observations, the probability distribution of which does not depend on a complete specifcation of the probability distribution of the population 
Table 20.5 Rank-ordered speech samples according to the use of swearwords 
Speaker ID Sp1  Gender Male  Relative frequency 8  Rank 17  
Sp2  Male  8  17  
Sp3  Male  8  17  
Sp4  Male  16  6  
Sp5  Male  16  6  
Sp6  Male  16  6  
Sp7  Male  16  6  
Sp8  Male  16  6  
Sp9  Male  24  3  
Sp10  Male  32  1  
Sp11  Female  7  19.5  
Sp12  Female  7  19.5  
Sp13  Female  14  12  
Sp14  Female  14  12  
Sp15  Female  14  12  
Sp16  Female  14  12  
Sp17  Female  14  12  
Sp18  Female  14  12  
Sp19  Female  14  12  
Sp20  Female  28  2  

from which the sample was drawn” (Everitt 2006:129). This means, among other things, that Mann-Whitney does not assume the underlying normal distribution in the population. Instead of working with relative frequencies, Mann-Whitney U tests works with ranks. It is therefore suitable either for ordinal data or quantitative data that can be transformed into ranks. For example, the data in Table 20.1 can be transformed into ranks as follows: 
Note that the ranks are computed for the whole dataset irrespective of the categorisation (Male and Female in our example – see Table 20.5). The largest value 
(32) is ranked as 1. Same values, i.e. ties (e.g. 7) receive a mean rank (19.5 is a mean value of 19 and 20). 
For the Mann-Whitney U test, two U values are calculated, one for each group, from which the smaller value is then taken (Kerby 2014; Mann and Whitney 1947): 
 
cases in group1 × cases in group1 + 1 U1 = sum of ranks for group 1 - 2 
(20.12) 
 
cases in group2 × cases in group2 + 1 U2 = sum of ranks for group 2 - 2 
(20.13) 
We take the smaller of the two values from Eqs. (20.12) and (20.13)asthe U statistic that we report. 
The Mann-Whitney U test has the following assumptions: It assumes inde­pendence of observations. Also, while it does not assume underlying normal distribution, it assumes that “the underlying distributions from which the samples are derived are identical in shape” (Sheskin 2004:423). 
The Mann-Whitney U test for the data in Table 20.5 is calculated as follows: 
10 × (10 + 1)
U1 = 85 -= 30 (20.14)
2 
10 × (10 + 1)
U2 = 125 -= 70 (20.15)
2 
The resulting U, which we report, is 30 because it is the smaller of the two values from the calculations above. The appropriate p-value for this test statistic and sample size is 0.128 (R provides it automatically), hence the result is not statistically signifcant. 
20.2.2.5 Kruskal-Wallis Test 
The Kruskal-Wallis test works on a similar principle as the Mann-Whitney test; it, however, takes into account ranks in multiple (3+) groups (Kruskal and Wallis 1952; Sheskin 2004:757–780). It can be used with straightforward ordinal data or scale/ratio data transformed into ranks. The Kruskal-Wallis test can be considered a non-parametric version of ANOVA because it does not have the assumption about the underlying normal distribution of the dependent variable in the population. However, like the Mann-Whitney U test it assumes that the underlying distributions are of identical shape. 
What follows is an example of how the Kruskal-Wallis test can be used with the data from Table 20.4; this data can be transformed into ranks as shown in Table 20.6: 
The equation for calculating the test statistic is: 
12 
H = 
sample size × (sample size + 1) sum of ranks for group 12 sum of ranks for group 22 
×+ ... 
cases in group1 cases in group2 -3 × (sample size + 1) (20.16) 
The Kruskal-Wallis test for the data in Table 20.6 is calculated as follows: 
Table 20.6 Rank-ordered speech samples according to the use of swearwords in different social-class groups 
Speaker ID Sp10  Social-class group Lower  Relative frequency 24  Rank 3  
Sp20  Lower  28  2  
Sp7  Lower  32  1  
Sp13  Middle  14  12  
Sp14  Middle  14  12  
Sp15  Middle  14  12  
Sp16  Middle  14  12  
Sp17  Middle  14  12  
Sp18  Middle  14  12  
Sp19  Middle  14  12  
Sp1  Middle  16  6  
Sp2  Middle  16  6  
Sp3  Middle  16  6  
Sp6  Middle  16  6  
Sp8  Middle  16  6  
Sp11  Upper  7  19.5  
Sp12  Upper  7  19.5  
Sp4  Upper  8  17  
Sp5  Upper  8  17  
Sp9  Upper  8  17  

12 62 1142 902 
H = ×++ - 3 × (20 + 1) = 14.57 (20.17)
20 × (20 + 1) 312 5 
The p-value for this test statistic and sample size is p < .001 (R provides it automatically). We can therefore conclude that the three social-class groups differ from each other more than by chance alone. 
20.2.2.6 Pearson’s Correlation 
Correlation is a standardised measure of relationship between two variables (She-skin 2004:943–1075; Gries 2013:238ff). For example, we know that the use of nouns and adjectives in text is strongly correlated. This means that the more nouns occur in texts, the more adjectives also appear (and vice versa, because correlations are bidirectional). This fact is not surprising because adjectives typically modify nouns to create complex noun phrases (a beautiful fower). The correlation is, however, not perfect because some adjectives are also used after a copula and without a nominal antecedent (This is beautiful). As introduced in Chap.17, there are different measures of correlation. A very common measure of correlation is Pearson’s correlation (r). Pearson’s correlation is used with quantitative variables (interval/scale) such as the relative frequencies of nouns and adjectives in our example. Pearson’s correlation is a parametric measure which assumes underlying normal distribution of the variables in the population, although the violation of this assumption should not have severe implications (e.g. de Winter et al. 2016; Gayen 1951). However, there is also evidence that under certain conditions non-normally distributed variables can have an effect on the precision of the measure (e.g. Duncan and Layard 1973;Kowalski 1972). If this is a concern, non-parametric Spearman’s correlation, which does not have the normality assumption (see Sect. 20.2.2.7) can be used. 
Pearson’s correlation (r) can be expressed as follows: 
covariance 
r = (20.18)
SD1 × SD2 
This equation shows that we are looking at the amount of covariance (variation that the variables have in common) in the data expressed as the standard deviations (SD1 and SD2) of the two variables that we correlate. As a rough indication, the following interpretation of r has been suggested by Cohen (1988:79–80); however, proper benchmarking specifc to linguistic analyses is desirable (Brezina 2018:275ff): 
0 no effect. ± 0.1 small effect. ± 0.3 medium effect. ± 0.5 large effect. 
Pearson’s and Spearman’s correlation operate on a scale from -1 to 1. A positive value of a correlation indicates a relationship between two variables where if one variable increases the other increases as well and if one variable decreases the other decreases as well. A negative correlation value, on the other hand, indicates an inverse relationship between two variables, where if one variable decreases, the other increases and vice versa. Correlation measures are typically also supplemented with a statistical test which evaluates the null hypothesis that the correlation is 0, that means that there is no relationship between the two variables in question. In addition to the correlation value (r in Pearson’s correlation), a p-value is therefore also reported. A p-value smaller than 0.05 is conventionally considered to indicate statistical signifcance. In such a case, we can conclude that the correlation is likely to be non-zero in the population, which, however, might not be very informative because small (non-zero) correlations have only negligible effect (cf. Schbrodt and Perugini 2013). A better option is to report and use a 95% Confdence interval 
(CI) built around the correlation measure (r), which shows more precisely how large the correlation is likely to be in the population (cf. Chap. 17). For example, with a very small sample of 10, the Pearson’s correlation coeffcient 0.5 would be accompanied by the 95% CI of -0.189, 0.859, opening the possibility of the correlation being 0 in the population (i.e. no correlation at all) because the confdence interval includes 0. With the sample size of 100 the 95% CI is much narrower: 0.337, 0.634 and shows us the interval within which the correlation value is going to be in 95% of the samples drawn from the same population. 
20.2.2.7 Non-parametric Correlation Tests 
Spearman’s correlation (rs), sometimes also denoted by the Greek letter rho (.)is similar to Pearson’s correlation but instead of operating with quantitative variables (interval/scale), it operates with ordinal variables (ranks). It can also be used with interval/scale variables, which get converted into ranks during the process. Spearman’s correlation is non-parametric – it does not assume underlying normal distribution in the population. Spearman’s correlation is therefore calculated as follows: 
6 × sum of squared rank differences 
rs = 1 - (20.19)
number of cases × (number of cases squared - 1) 
Another option for non-parametric correlation is Kendall’s tau (t). However, it needs to be noted that Kendall’s tau generally shows lower values compared to Pearson’s and Spearman’s correlations. For example, “Kendall correlation of 0.8 corresponds to a Pearson correlation of about 0.95” (Bonett and Wright 2000:24). 
20.2.3 Effect Sizes and Confdence Intervals 
Using statistical tests has often been criticized on a number of counts. First, following the binary decision-making procedure of NHST, continuous fndings are routinely interpreted rather crudely as ‘signifcant’ or ‘non-signifcant’. That is, the binary interpretation of p values tells us nothing about the size or magnitude of the effect or relationship of interest. Another weakness of NHST is the arbitrary nature of the cut-off point of 0.05 (Cohen 1995; Plonsky 2015). It is useful, but does not represent the god’s truth (see Sect. 20.2.2). In fact, in some domains of research, particularly under high-stakes conditions (e.g. medical research), a much lower cut-off point of 0.01 or lower4 is applied. A third weakness of the NHST is that statistical signifcance is dependent not only on the size of the effect, but also, importantly, on the sample size (see Sect. 20.2.1) such that any correlation or difference between groups will achieve statistical signifcance given a large enough sample. Furthermore, given the large samples in much of corpus linguistics, this feld in particular must take care not to interpret statistically signifcant results as necessarily large or important. 
4Generally, the cut-off point depends on how much chance we are comfortable with in our discipline. Without testing the whole population (which is usually impracticable), we will always operate in the realm of probability (p-values) and will never have a 100% certainty. Imagine that you have a jar full of sweets of different favours, some of which you like and some dislike. Would you be willing to pick one at random? Would your answer to this question change if you knew that one of these sweets is poisoned? 
The arbitrary nature of considering everything with a p-value above 0.05 (or 0.01) statistically signifcant raises several questions: How should we interpret a p-value of 0.051, which is very close to signifcance? Should we completely disregard this result? Or should we, on the contrary, report it? What about p-values of 0.06, 0.07, 
0.08 etc.? What if two replication studies reach a p-value close to signifcance—does this strengthen or weaken the case? It is important to note that misunderstanding of or overreliance on NHST can be harmful to a discipline because it limits the scope of results reported in research reports, especially those published in books and journals, to statistically signifcant results. Statistically non-signifcant yet important results—although based on rigorous studies asking important research questions— thus become largely underreported; this is a so-called publication bias—see Kepes et al. (2014) for an insightful discussion of this problem. So how do we resolve this dilemma: should statistical tests and NHST be used at all? 
Some extreme solutions recommend not to use statistical signifcance. For instance, the journal Basic and Applied Social Psychology goes as far as putting a ban on all inferential statistics in the submissions to the journal; instead the editors require “strong descriptive statistics” and large sample sizes. The editors of this journal simply claim that the NHST procedure is “invalid” (Trafmow and Marks 2015:1). Instead of abandoning the NHST procedure completely (which, one might argue, is like throwing the baby out with the bathwater), we can search for a useful middle ground. First, we need to realise that statistics offers a wide range of tools which we can use to analyse and interpret our data. In different situations, different tools are appropriate. Statistical tests such as those described in this chapter are useful for effcient checking of our hypotheses. On the other hand, if we want to build on existing results and carry out replication studies and metanalyses (see Chap. 27), we need more sophisticated tools. Such tools are effect sizes combined with confdence intervals. For this reason, it is important to report both p-values, and effect sizes and confdence intervals. 
The idea here is very simple. In addition to any p values, we measure and report the magnitude of the effect we observe in the data or the effect size. We can also report a range of values the effect size is likely to have in the population. We call this range a confdence interval (CI), usually a 95% CI. A 95% confdence interval is an interval that is constructed around a statistical measure (here an effect size) based on the sample in such a way that the true value of this measure lies within this interval for 95% of the samples taken from the same population. We can imagine this as a multiple replication exercise with different samples from the same population. In 95% of the samples, the measure (effect size) will lie within the confdence interval. 
A popular standardised effect size measure is Cohen’s d. It is used to express the difference between two groups. It is very intuitive and simple; it is calculated by taking the difference between the two means and dividing this by the pooled standard deviation (overall SD that combines the individual SDs in the groups we compare) as can be seen from the equation below: 
Mean of group 1 - Mean of group 2 
Cohen’s d = (20.20)
pooled SD 
SD12 × (cases in group1 - 1) + SD22 × (cases in group2 - 1)pooled SD = 
all cases - 2 (20.21) 
In the example of swear-word data from Table 20.1, the calculation of Cohen’s d is as follows: 
16 - 14 16 - 14 
Cohen’s d = == 0.3 (20.22) 
56.89×(10-1)+32.67×(10-1) 6.69 
20-2 
Cohen (1988:40) recommends the following standard interpretation of the d mea­sure: <0.3 small, <0.5 medium, <0.8 large effect. However, the interpretation is largely discipline specifc and requires proper benchmarking for corpus studies. 
When we build the 95% CI around Cohen’s d, we get the following result. 
Cohen’s d: 0.3; 95% CI [-0.65, 1.24]. 
Because the 95% CI is very broad and includes zero, the sample does not provide enough evidence for a non-zero effect, which would show a real difference between the groups. If we, however, take a corpus with 100 male and 100 female speakers, where the difference between the groups is exactly the same (16 and 14), we get more evidence for the effect (the dataset is available from the companion website): d = 0.3 95% CI [0.03, 0.59]. Note that similar change to the CIs but not the effect sizes was discussed in Sect. 20.2.2.6 in relation to the Pearson’s correlation. Unlike p-values, effect sizes and their confdence intervals can be combined in meta-analysis (Brezina 2018:2.3; Cooper et al. 2009; see also Chap. 27). 
Representative Study 1 
Brezina, V., and Meyerhoff, M. 2014. Signifcant or random? A critical review of sociolinguistic generalisations based on large corpora. Inter­national Journal of Corpus Linguistics 19(1):1–28. 
This study reviews the use of statistical techniques in corpus-based soci­olinguistics and demonstrates that the traditional method employing the chi-squared and log likelihood tests for comparison of word distributions in broadly defned subcorpora is inappropriate. 
(continued) 
Research questions 
1. 
What is the performance of the aggregate data method compared to the individual speaker method with real social groups? 

2. 
What is the performance of the aggregate data method compared to the individual speaker method with random speaker groupings? 


Data 
The study used a subset of the British National Corpus (spoken) from which the speech samples of 32 speakers were selected. Sixteen samples came from male speakers and sixteen from female speakers. 
Method 
Brezina and Meyerhoff compared two methods: (1) a traditional method of comparison of broadly defned subcorpora (male vs. female) with the log-likelihood test (similar to chi-squared) and (2) use of the Mann-Whitney U test, which takes into consideration individual variation inside each subcor­pus. The study investigated a range of lexico-grammatical variables including the lemma FUCK, the adverb lovely, the discourse markers you know, sort of, and really, negative concord (e.g. he didn’t say nothing), and the determiners the and some. 
Results 
The results show that the application of the traditional method is largely inaccurate. It overestimates statistical signifcance (type I error) in cases where there is no real difference and the variation is purely due to chance; the type I error rates (false hits) ranged from 48 to 99% depending on the linguistic feature investigated. The Mann-Whitney U test, which takes indi­vidual variation into account performed considerably better with type I error rate (as expected) around 5%. The article demonstrated that inappropriate use of statistical tests can lead to very inaccurate and misleading results. In principle, there is nothing wrong with using the chi-squared test or log likelihood test if the data is suitable for this. We, however, need to critically evaluate the assumptions of the tests before applying them; in the context of corpus linguistic analyses, the assumption of independence of observations is of utmost importance (see Sect. 20.2.2). We have to realise that in corpora we typically sample data at the level of texts/speakers. The data analysis therefore needs to pay attention to this level of individual variation. 
20.3 Practical Guide with R 
This section presents different steps in R from how to analyze data to how to report results for publication. R scripts + data fles are available on the companion website. 
See Chap. 26 for more general info about how to report the results of a quantitative corpus-based study. 
20.3.1 Chi-Squared Test 
Command 
chisq.test(genreData, correct = FALSE) 
Explanation The test assumes cross-tabulated data. This is the type of data shown in Table 20.2. 
R output 
Pearson’s Chi-squared test data: genreData X-squared = 32.686, df = 2, p-value = 7.984e-08 X-squared = 32.686, df = 6, p-value = 1.205e-05 
Reporting For the chi-squared test, three pieces of information need to be reported: 
(i) the test statistic (.2), (ii) degrees of freedom (df ) and the p-value. 
The chi-squared test (.2(6) = 32.686; p < .001) showed a statistically signifcant difference among the three genre groups. 
20.3.2 T-test 
Command 
t.test(RF Gender, paired=FALSE, data = sociolingData) 
Explanation RF refers to relative frequencies of the linguistic variable, while Gender is a grouping variable (male vs. female). This is the type of data shown in Table 20.1. 
R output 
data: RF by Gender t = -0.66832, df = 16.773, p-value = 0.513 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 
-8.320315 4.320315 sample estimates: mean in group Female mean in group Male 
14 16 
Reporting For the t-test, three pieces of information need to be reported: (i) the test statistic (t), (ii) degrees of freedom (df ) and the p-value. In addition, it is also useful to report related descriptive statistics (means, SDs, and the number of observations in each group N1 and N2). 
The Welch’s independent samples t-test (t (16.77) =-0.668; p = .513) did not show a statistically signifcant difference between the two groups. On average, the male group (N = 10) and the female group (N = 10) produced 16 (SD = 7.5) and 14 (SD = 5.7) swearwords per 10,000 words. 
20.3.3 Cohen’s d with 95% Confdence Intervals – To Be Computed with T-test 
Command 
library(compute.es) male<-(subset(sociolingData, Gender==’Male’)$RF); female<-(subset(sociolingData, Gender==’Female’)$RF) mes(mean(male),mean(female),sd(male),sd(female),length(male), length(female), level=95) 
Explanation To compute Cohen’s d with 95% confdence intervals we can use the R package for computing effect sizes compute.es.R. It needs to be downloaded to your computer (just once) from https://cran.r-project.org/web/packages/compute. es/index.html and installed by using the command. 
install.packages(file.choose(), repos=NULL); 
The R function mes takes as input the means for the two groups, their standard deviations as well as the number of cases in each group. 
R output 
Mean Differences ES: d[95%CI] = 0.3 [ -0.65 , 1.24 ] 
Reporting Cohen’s d is reported including the 95% Confdence interval. 
The effect size (Cohen’s d) for the comparison of the two groups showed a small effect: d = .3; 95% CI [-.65; 1.24]. 
20.3.4 ANOVA 
Command 
result<-aov(RF Class, data = sociolingData); summary (result) 
Explanation In the command above, RF is the dependent variable (relative fre­quency of a linguistic feature), while Class is a grouping variable which indicates, under which of the multiple groups the value of the RF variable should be counted. 
R output 
Df Sum Sq Mean Sq F value Pr(>F) 
Group 2 781.1 390.6 148 1.77e-11 *** 
Residuals 17 44.9 2.6 
‘***’‘**’
Signif. codes: 0 0.001 0.01 ‘*’ 0.05 ‘.’ 0.1‘’1 
Reporting For ANOVA, three pieces of information need to be reported: (i) the test statistic (F), (ii) degrees of freedom (df )–note that there are two values of df – and the p-value. In addition, it is also useful to report the omnibus effect size for ANOVA, e.g. eta squared. 
One-way ANOVA (F (2, 17) = 148; p < 0.001) showed a statistically signifcant difference among the three groups; .2 = 0.968. 
Note 1 Reporting p-values should follow the APA guidelines (6th ed.): When reporting p-values, report exact p-values to two or three decimal places. However, report p-values less than 0.001 as p < 0.001. 
Note 2 To calculate eta squared, the overall effect size for ANOVA, use the following command: 
library(lsr); etaSquared(result) 
This expects the library lsr to be installed: 
install.packages("lsr") 
20.3.5 Post-hoc T-Test with Correction for Multiple Testing 
Command 
pairwise.t.test(sociolingData$RF, sociolingData$Class, p.adj = "bonf") 
Explanation The type of data used is shown in Table 20.7. sociolingData$RF is a column with the dependent variable (relative frequencies of a linguistic feature), while sociolingData$Class is a grouping variable. p.adj specifes the type of adjustment of the p-values for multiple signifcance testing. The example shows the strictest one (Bonferroni correction); other options include: holm, hochberg, hommel, BH, BY, fdr, none. 
R output 
Pairwise comparisons using t tests with pooled SD data: sociolingData$RF and sociolingData$Class 
Lower Middle 
Middle 1.5e-09 ­
Upper 1.0e-11 5.9e-07 
P value adjustment method: bonferroni 
Table 20.7 Data format 
ID  Class  RF  
1  Lower  24  
2  Lower  32  
3  Lower  28  
4  Middle  14  
5  Middle  14  
6  Middle  14  
7  Middle  14  
8  Middle  14  
9  Middle  14  
10  Middle  14  
11  Middle  16  
12  Middle  16  
13  Middle  16  
14  Middle  16  
15  Middle  16  
16  Upper  7  
17  Upper  7  
18  Upper  8  
19  Upper  8  
20  Upper  8  

Reporting Post-hoc tests provide additional information to the omnibus test such as ANOVA. They test individual groups pairwise and show where the statistically signifcant differences are. 
The post-hoc tests (t-test with Bonferroni correction) identifed statistically signifcant differences between all tested groups (all p < 0.001). 
20.3.6 Mann-Whitney U Test 
Command 
wilcox.test (RF Gender, paired=FALSE, data=sociolingData) 
Explanation RF refers to relative frequencies of the linguistic variable, while Gender is a grouping variable (male vs. female). This is the type of data shown in Tables 20.1 and 20.5. 
R output 
Wilcoxon rank sum test with continuity correction data: data: RF by Gender 
W = 30, p-value = 0.1282 
alternative hypothesis: true location shift is not equal to 0 
Reporting For the Mann-Whitney U test, two pieces of information need to be reported: (i) the test statistic (U), and the p-value. Note that Wilcoxon rank sum is an alternative name for the same type of test and that the test statistic U = W. 
The Mann-Whitney U test (U = 30; p = 0.128) did not show a statistically signifcant difference between the two groups. 
20.3.7 Kruskal-Wallis Test 
Command 
kruskal.test (RF Class, data = sociolingData) 
Explanation In the command above, RF is the dependent variable (relative fre­quency of a linguistic feature), while Class is a grouping variable which indicates under which of the multiple groups the value of the RF variable should be counted. 
R output 
Kruskal-Wallis rank sum test #data: RF by Class Kruskal-Wallis chi-squared = 15.516, df = 2, p-value = 0.0004272 
Reporting For the Kruskal-Wallis, two pieces of information need to be reported: 
(i) the test statistic (H), and the p-value. 
The Kruskal-Wallis (H = 15.52; p < 0.001) showed a statistically signifcant difference among the three groups. 
20.3.8 Pearson’s and Spearman’s Correlations 
Command 
cor.test(corrData$Nouns, corrData$Adjectives, method="pearson") #Pearson’s correlation 
cor.test(corrData$Nouns, corrData$Adjectives, method="spearman") #Spearman’s correlation 
Explanation In the command above, corrData$Nouns is one linguistic vari­able that is correlated with another linguistic variable corrData$Adjectives. 
R output 
Pearson’s product-moment correlation data: corrData$Nouns and corrData$Adjectives t = 13.682, df = 498, p-value < 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 
(continued) 

0.4558924 0.5836274 
sample estimates: 
cor 
0.5226872 
Spearman’s rank correlation rho data: nouns and adjectives 
S = 9922900, p-value < 2.2e-16 
alternative hypothesis: true rho is not equal to 0 
sample estimates: 
rho 
0.5237004 
Reporting When reporting Pearson’s correlation three pieces of information need to be included: (i) effect size r, (ii) p-value and (iii) 95% confdence interval; for Spearman’s correlation (i) effect size (rs) and (ii) p-value are reported. 
r = 0.523; p < .001; 95% CI [0.456, 0.584]. rs = 0.524; p < .001. 
Further Reading 
Brezina, V. 2018. Statistics in corpus linguistics: A practical guide. Cambridge University Press, Cambridge. 
Brezina (2018: Chaps. 6 and 8) provides more information and real examples (case studies) of the use of the statistical measures discussed in this chapter. The book is intended for beginner and intermediate users of statistical techniques in corpus linguistics and does not presuppose any knowledge of statistics. It is accompanied by Lancaster Stats Tools online, a free and easy to use (‘click and analyse’) statistical tool (http://corpora.lancs.ac.uk/stats) (accessed 14 June 2019). 
Gablasova, D., Brezina, V., and McEnery, T. 2017. Exploring learner language through corpora: Comparing and interpreting corpus frequency informa­tion. Language Learning 67(S1):130–154. doi:10.1111/lang.12226. 
This article offers a critical view on using frequency data in corpus linguistics in the context of language learning; this critique is, however, applicable to other contexts as well. The paper investigates the sources of variation in corpora and shows how these can be dealt with systematically using different statistical and visualisation techniques. 
Gries, S.T. 2013. Statistics for linguistics with R: A practical introduction, 2nd ed. Mouton de Gruyter, Berlin. 
Gries (2013) provides an informative introduction to using R in statistical analysis of language. Chapter 2 will be of interest to anyone new to the statistical package R. The book will appeal to a wide range of users who seek statistical sophistication in their analyses. 
References 
Balakrishnan, N., Voinov, V., & Nikulin, M. S. (2013). Chi-squared goodness of ft tests with applications. Oxford: Academic. Boneau, C. A. (1960). The effects of violations of assumptions underlying the t test. Psychological Bulletin, 57(1), 49–64. https://doi.org/10.1037/h0041412. 
Bonett, D. G., & Wright, T. A. (2000). Sample size requirements for estimating Pearson, Kendall and Spearman correlations. Psychometrika, 65(1), 23–28. https://doi.org/10.1007/ BF02294183. 
Brezina, V. (2018). Statistics in corpus linguistics: A practical guide. Cambridge: Cambridge University Press. 
Brezina, V., & Meyerhoff, M. (2014). Signifcant or random? A critical review of sociolinguistic generalisations based on large corpora. International Journal of Corpus Linguistics, 19(1), 1– 28. https://doi.org/10.1075/ijcl.19.1.01bre. 
Brezina, V., McEnery, T., & Wattam, S. (2015). Collocations in context: A new perspective on collocation networks. International Journal of Corpus Linguistics, 20(2), 139–173. https:// doi.org/10.1075/ijcl.20.2.01bre. 
Cabin, R. J., & Mitchell, R. J. (2000). To Bonferroni or not to Bonferroni: When and how are the questions. Bulletin of the Ecological Society of America, 81(3), 246–248. Cohen, J. (1988). Statistical power analysis for the behavioural sciences (2nd ed.). Hillsdale: Erlbaum. Cohen, J. (1995). The earth is round (p < 0.05): Rejoinder. American Psychologist, 50(12), 1103. https://doi.org/10.1037/0003-066X.50.12.1103. Cooper, H. M., Hedges, L. V., & Valentine, J. C. (Eds.). (2009). The handbook of research synthesis and meta-analysis (pp. 103–126). New York: Russell Sage Foundation. Corder, G. W., & Foreman, D. I. (2009). Nonparametric statistics for non-statisticians: A step-by­step approach.New York:Wiley. Cox, D. R., & Donnelly, C. A. (2011). Principles of applied statistics. Cambridge: Cambridge University Press. Crowder, M. J. (1990). Analysis of repeated measures. New York: Chapman and Hall. 
de Winter, J. C. F., Gosling, S. D., & Potter, J. (2016). Comparing the Pearson and Spearman correlation coeffcients across distributions and sample sizes: A tutorial using simulations and empirical data. Psychological Methods, 21(3), 273–290. https://doi.org/10.1037/met0000079. 
Duncan, G. T., & Layard, M. W. J. (1973). A Monte-Carlo study of asymptotically robust tests for correlation coeffcients. Biometrika, 60(3), 551–558. https://doi.org/10.2307/2335004. 
Everitt, B. (2006). The Cambridge dictionary of statistics (3rd ed.). Cambridge: Cambridge University Press. 
Fisher, R. A. (1935). The design of experiments. London: Oliver and Boyd. 
Gayen, A. K. (1951). The frequency distribution of the product moment correlation in random samples of any size drawn from non-normal universes. Biometrika, 38, 219–247. https:// doi.org/10.2307/2332329. 
Greenwood, P. E., & Nikulin, M. S. (1996). A guide to chi-squared testing (Vol. 280). New York: Wiley. 
Gries, S. T. (2013). Statistics for linguistics with R: A practical introduction (2nd ed.). Berlin: Mouton de Gruyter. 
Kapadia, A. S., Chan, W., & Moyé, L. (2005). Mathematical statistics with applications.Boca Raton: CRC Press. 
Kepes, S., Banks, G. C., & Oh, I. S. (2014). Avoiding bias in publication bias research: The value of “null” fndings. Journal of Business and Psychology, 29(2), 183–203. https://doi.org/10.1007/ s10869-012-9279-0. 
Kerby, D. S. (2014). The simple difference formula: An approach to teaching nonparametric correlation. Innovative Teaching, 3, 1–9. https://doi.org/10.2466/11.IT.3.1. 
Kilgarriff, A. (2005). Language is never, ever, ever, random. Corpus Linguistics and Linguistic Theory, 1(2), 263–275. https://doi.org/10.1515/cllt.2005.1.2.263. 
Kowalski, C. J. (1972). On the effects of non-normality on the distribution of the sample product moment correlation coeffcient. Applied Statistics, 21(1), 1–12. https://doi.org/10.2307/ 2346598. 
Kruskal, W. H., & Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. Journal of the American Statistical Association, 47(260), 583–621. https://doi.org/10.2307/2280779. 
Lumley, T., Diehr, P., Emerson, S., & Chen, L. (2002). The importance of the normality assumption in large public health data sets. Annual Review of Public Health, 23(1), 151–169. https:// doi.org/10.1146/annurev.publhealth.23.100901.140546. 
Mann, H. B., & Whitney, D. R. (1947). On a test of whether one of two random variables is stochastically larger than the other. Annals of Mathematical Statistics, 18(1), 50–60. https:// doi.org/10.1214/aoms/1177730491. 
Miller, R. G. (1997). Beyond ANOVA: Basics of applied statistics. New York: Chapman and Hall/CRC. 
Plonsky, L. (2015). Statistical power, p values, descriptive statistics, and effect sizes: A “back-to­basics” approach to advancing quantitative methods in L2 research. In Advancing quantitative methods in second language research (pp. 43–65). London: Routledge. 
Rasch, D., Kubinger, K. D., & Moder, K. (2011). The two-sample t test: Pre-testing its assumptions does not pay off. Statistical Papers, 52(1), 219–231. https://doi.org/10.1007/s00362-009-0224­x. 
Rayson, P., Berridge, D., & Francis, B. (2004). Extending the Cochran rule for the comparison of word frequencies between corpora. In: Proceedings from 7th international conference on statistical analysis of textual data, JADT 2004, pp. 926–936. 
Ruxton, G. D., & Neuhäuser, M. (2010). When should we use one-tailed hypothesis test­ing? Methods in Ecology and Evolution, 1(2), 114–117. https://doi.org/10.1111/j.2041­210X.2010.00014.x. 
Schmider, E., Ziegler, M., Danay, E., Beyer, L., & Bner, M. (2010). Is it really robust? Rein­vestigating the robustness of ANOVA against violations of the normal distribution assumption. Methodology: European Journal of Research Methods for the Behavioral and Social Sciences, 6(4), 147–151. https://doi.org/10.1027/1614-2241/a000016. 
Schbrodt, F. D., & Perugini, M. (2013). At what sample size do correlations stabilize? Journal of Research in Personality, 47(5), 609–612. https://doi.org/10.1016/j.jrp.2013.05.009. 
Schucany, W. R., & Tony Ng, H. K. (2006). Preliminary goodness-of-ft tests for normality do not validate the one-sample Student t. Communications in Statistics-Theory and Methods, 35(12), 2275–2286. https://doi.org/10.1080/03610920600853308. 
Shaffer, J. P. (1995). Multiple hypothesis testing. Annual Review of Psychology, 46(1), 561–584. 
Sherman, G. R. (1954). The “Student” T test when applied to non-normal populations. Department of Statistics, Stanford University. 
Sheskin, D. (2004). Handbook of parametric and nonparametric statistical procedures (3rd ed.). London: Chapman & Hall/CRC. 
Shingala, M. C., & Rajyaguru, A. (2015). Comparison of post hoc tests for unequal variance. International Journal of New Technologies in Science and Engineering, 2(5), 22–33. 
Sprent, P. (2011). Fisher exact test. In M. Lovric (Ed.), International encyclopedia of statistical science (pp. 524–525). Berlin/Heidelberg: Springer. 
Trafmow, D., & Marks, M. (2015). Editorial. Basic and Applied Social Psychology, 37, 1–2. 
Upton, G. J. G. (1992). Fisher’s exact test. Journal of the Royal Statistical Society. Series A (Statistics in Society), 155(3), 395–402. https://doi.org/10.2307/2982890. 
Yates, F. (1934). Contingency table involving small numbers and the .2 test. Supplement to the Journal of the Royal Statistical Society, 1(2), 217–235. 
Zimmerman, D. W. (1998). Invalidation of parametric and nonparametric statistical tests by concurrent violation of two assumptions. The Journal of Experimental Education, 67(1), 55– 68. https://doi.org/10.1080/00220979809598344. 
Chapter 21 Fixed-Effects Regression Modeling 
Martin Hilpert and Damián E. Blasi 
Abstract This chapter presents fxed-effects regression modeling as a family of methods that describe a dependent variable in terms of one or more independent variables. The chapter focuses on multiple linear regression and on binomial logistic regression, discussing examples of regression analyses on the basis of corpus-linguistic data. The chapter offers descriptions of published studies that have used these methods. Besides explaining the fundamental notions and assumptions of different types of regression, the chapter also illustrates practical aspects of applying regression analyses through the use of artifcially created data sets. In order to give readers a practical introduction to regression modeling, it is shown how manipulations of the underlying data result in different outcomes of the respective analyses. The chapter further discusses how the results of regression analyses can be usefully visualized. A selection of resources for further reading is included to offer readers a starting point for further study of regression modeling. 
21.1 Introduction 
This chapter presents different variants of regression modelling. Regression analysis is a family of methods that aim at describing a dependent variable in terms of other, independent, variables. For example, consider the question of how women and men may differ in the way they use language. Studies that aim to investigate such a 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_21) contains supplementary material, which is available to authorized users. 
M. Hilpert (•) Université de Neuchâtel, Neuchâtel, Switzerland e-mail: martin.hilpert@unine.ch 
D. E. Blasi Universität Zich, Zich, Switzerland e-mail: damian.blasi@uzh.ch 
© Springer Nature Switzerland AG 2020 505 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_21 
M. Hilpert and D. E. Blasi 
relation with regression modeling would start with a linguistic phenomenon, such as perhaps tag questions, and relate observations of variability in speakers’ use of tag questions to another variable, namely the gender of the speaker. These studies would further control for other variables, such as the speaker’s age, socio-economic status, education level, role in their social network, and the formality of the situation in which the recorded conversation takes place. In such a design, the use or non-use of a tag question would be called the dependent variable or response variable, whereas the other variables are the independent variables. The design allows researchers to investigate whether or not the use of tag questions varies with the speaker’s gender. In other words, are women more likely to use tag questions, and if so, how much more likely? Is this generally the case, or only in certain conditions, for example during a certain age bracket? Regression analyses can provide insights into these issues, thereby giving the researcher more than a simple yes-or-no answer. 
By far the most popular use of regression analysis in linguistics is to determine which of the different independent variables have an impact on the dependent vari­able, and how strong that impact is. The uses of regression do however go beyond that. For instance, regression can be used to construct predictive models for values of the independent variables that have not been observed. For practical and pedagogical reasons, this chapter will focus on some of the most basic regression models, which can still yield informative answers to research questions like the ones described above. In doing so, this chapter showcases applications of regression techniques in corpus-linguistic research. In such applications, the dependent variable represents a variable linguistic phenomenon that can be observed in corpus data. For instance, English adjectives such as proud can form the comparative in two ways, namely either as prouder or as more proud. A regression analysis can model that variation by making reference to several characteristics of English adjectives that serve to explain why some are biased towards the morphological comparative, while others show a preference for the periphrastic comparative. Examples of such characteristics include word length, stress pattern, and the syntactic environment (Hilpert 2008). 
This chapter will present three different types of regression modelling that differ with respect to the dependent variable. First, a dependent variable may be represented by measurements on a continuous scale, as for example pronunciation lengths of a given word. In this case, and under a number of assumptions about the data at hand, we can use (multiple) linear regression. In the example of prouder and more proud, the dependent variable is not continuous, but categorical and binary, 
i.e. it has exactly two possible outcomes. This kind of variation can be modelled by means of a binary logistic regression. Finally, the dependent variable may be categorical in such a way that there are more than two possible outcomes. For instance, a speaker may want to express the idea that something is highly positive, and several synonymous lexical items such as great, fantastic,or excellent are available as possible expressions. If the researcher wants to fnd out which factors lead a speaker to choose one particular item out of this set, multinomial logistic regression is one possible analytical tool. Binary logistic regression and multinomial logistic regression draw on the same overall logic, with binary logistic regression being the simplest sub-case of a multinomial logistic regression. 
It is the main aim of this chapter to lay out the fundamental concepts that are at issue and to offer step-by-step instructions for their use. For this purpose, we will use both authentic linguistic datasets and data that is artifcially created. The idea behind using simulated data is that we know exactly what information is contained in a given dataset and what effects should be discovered with a regression analysis. What is more, we can make changes to the underlying data set in order to see how the resulting model changes, for instance when we manipulate aspects of an existing variable, or when we add or leave out another variable. This approach is meant to give readers a feel for the practical use of regression techniques and for the ways in which regression models respond to variation in the data. 
21.2 Fundamentals 
21.2.1 (Multiple) Linear Regression 
Linear regression is used to analyze linguistic phenomena that can be measured on a continuous scale, as for example the pronunciation length of a given word or the time it takes to respond to an experimental stimulus. Below, we will use it to analyze a corpus based measure, namely the type-token ratio (TTR) of different texts that have the same overall length. Texts with high TTR values contain many different word types; texts with low TTR values contain many repetitions of the same word types. A multiple linear regression analysis can determine whether there are factors that allow us to predict whether a given text will have a relatively high TTR value or a relatively low one. The phenomenon that is represented by our measurements, in this case measurements of TTR, represents the dependent variable of the analysis. It is the goal of a linear regression to explain variation in the dependent variable. This is done with the help of at least one independent variable. In many analyses, more than one independent variable is used, since the dependent variable may be infuenced by more than one underlying phenomenon. 
21.2.1.1 An Example of (Multiple) Linear Regression 
Figure 21.1 presents data from Westin (2002:78), who examined the diachronic Corpus of English Newspaper Editorials (CENE). The corpus contains about half a million words from 864 different editorials that appeared in The Times,the Daily Telegraph, and the Guardian between 1900 and 1993 (Westin 2002:12). Westin observed that in 400-word samples of the CENE editorials, texts that were produced later have increasingly higher TTR values. The graph shows average TTR values for editorials from the three different newspapers for each decade. 
The graph visualizes two kinds of variation in TTR values. First, it can be seen that the three newspapers have different average TTR values, with The Times 
M. Hilpert and D. E. Blasi 
showing relatively low values in many decades and the Daily Telegraph showing the highest values. Second, the graph reveals change over time. Later TTR values tend to be higher than earlier values. This means that there are two independent variables that can be used to make predictions about the TTR values of any given text: One independent variable is time, which is a continuous variable expressed in a continuous sequence of numbers that represent successive years. The second independent variable is the type of newspaper, which is a categorical variable, since it falls into three distinct categories, namely The Times,the Daily Telegraph, and the Guardian. Let us focus exclusively on time frst. Linear regression can be used to determine whether there is a relation between the dependent variable TTR and the independent variable time. Linear regression draws on an algorithm that fnds a straight line that captures the relation between the dependent variable and the independent variable with the least amount of error, that is, with the smallest distances between the straight line and the data points. Figure 21.2 shows the data points from Fig. 21.1 together with a regression line, which represents the ftted TTR values that are estimated by a regression model. Also shown are several vertical arrows that illustrate the distances between the respective data points and the regression line. 
As can be seen in Fig. 21.2, many observed TTR values for the Daily Telegraph are higher than the ftted values, while many observed values for The Times are lower than the ftted values. The slope of the regression line is chosen in such a way that these distances are minimized. A common procedure for this purpose is Ordinary Least Squares (OLS), which determines the slope that yields the smallest value for an addition of all squared distance values. In Fig. 21.2, the resulting line has an increasing slope, which means that later years yield higher ftted values for TTR values. The relation between TTR and time is captured by a coeffcient, which 

The Times Daily Telegraph 
Guardian 
1900 1920 1940 1960 1980 time 

1900 1920 1940 1960 1980 
time 
expresses an average estimated increase of TTR per unit of time. In the model that underlies the regression line in Fig. 21.2, TTR increases by 0.44 per decade. 
Some of the fundamental concepts of linear regression can be illustrated on the basis of the R output that is shown below. The supplementary materials to this chapter contain the actual data and the R code that has been used to conduct the analysis, which attempts to predict TTR values exclusively on the basis of time. The summary output of the analysis contains several pieces of information that we will discuss one by one. 
Call: lm(formula = ttr.values ~ time, data = ttr) 
Residuals: 
Min 1Q Median 3Q Max -3.3595 -1.3162 -0.3679 1.2980 3.3214 
Coefficients: Estimate Std. Error t value Pr(>|t|) 
(Intercept) 55.4386  0.6231  88.967  < 2e-16 ***  
time  0.4426  0.1167  3.792 0.000732 ***  
--- 

Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
Residual standard error: 1.836 on 28 degrees of freedom Multiple R-squared: 0.3393, Adjusted R-squared: 0.3157 F-statistic: 14.38 on 1 and 28 DF, p-value: 0.0007321 
M. Hilpert and D. E. Blasi 
The output starts with the call, i.e. a repetition of the formula that was used to specify the regression model. That repetition is followed by an overview of the residuals, that is, the differences between the model’s ftted values and the actual values of the dependent variable. The overview shows residuals for the two worst ftted values (min and max), as well as the median (i.e. the average of residuals 15 and 16 if all 30 residuals are lined up from smallest to largest) and the distribution around that median, as refected in the residuals of the frst and third quartile. The output then shows the coeffcients. There are two lines, one for the intercept, and another one for the effect of time. The intercept shows the estimate for time 0, that is, 1900. At that point in time, the ftted TTR value is 55.44. In the row directly below that number, the estimate of 0.4426 captures the slope, i.e. the ftted TTR increase per decade. This means that for example the estimate for the 1980s can be calculated as 55.44 plus 8 times 0.4426, which yields 58.98. The standard error of 0.1167 is a measure of how confdent the model is of the slope estimate. If we divide the estimate by the standard error, the result is the t-value in the next column, which is used to determine the statistical signifcance of the effect. The p-value in the next column represents that signifcance.1 The last part of the output represents a global assessment of the quality of the regression model. The residual standard error refects how much variability there is in the differences between the model’s predictions and the actual values of the dependent variable. Importantly, the residuals should not be taken as “failures” of the model, as they might represent real variation of the data that is not related to the dependent variables, but to other variables that the researcher did not include in the model. The model is trying to do the best it can, given the available information. The R2-values indicate how much of the variation in the dependent variable has been accounted by the independent variables. The F-statistic assesses globally whether the independent variables can tell us anything about the dependent variable. The corresponding low p-value suggests that this is indeed the case. If the true underlying slope were zero, it would be very unlikely to obtain by chance a slope as large as the one we observe. 
In order to understand this kind of output a little better, it is useful to see how it differs once the second independent variable, the type of newspaper, is entered into a more complex regression model. The output below reports on a model that contains the type of newspaper as a second independent variable, which allows the regression to adjust the estimates for each newspaper type. 
1See Gries (2013:28) for a general discussion of p-values. 
Call: lm(formula = ttr.values ~ time + paper, data = ttr) 
Residuals: 
Min 1Q Median 3Q Max 
-2.2235 -0.9964 -0.0061 0.7278 3.2077 
Coefficients: 
Estimate Std. Error t value Pr(>|t|) 
(Intercept) 56.93827 0.56575 100.642 < 2e-16 *** 
time 0.44261 0.08434 5.248 1.75e-05 *** 
paperGuard -1.38600 0.59336 -2.336 0.0275 * 
paperTimes -3.11300 0.59336 -5.246 1.76e-05 *** 
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
Residual standard error: 1.327 on 26 degrees of freedom Multiple R-squared: 0.6797, Adjusted R-squared: 0.6428 F-statistic: 18.39 on 3 and 26 DF, p-value: 1.309e-06 
In the more complex model, the residuals are smaller and have a median that is closer to zero. Second, the coeffcients indicate that newspaper type is a signifcant independent variable. The estimates for the Guardian and The Times have negative signs, indicating that their TTR values at time 0 are lower than those of the Daily Telegraph, which serves as the reference level of the independent variable newspaper type. The choice of the reference level is by default determined by alphabetical order: The Daily Telegraph is chosen because it precedes the Guardian and The Times in the alphabet. The corresponding t-values show that the differences are signifcant. Lastly, the R2-values show that the more complex model accounts for much more variation in the dependent variable than the simpler model. 
21.2.1.2 Assumptions of Linear Regression 
The interpretation of a (multiple) linear regression depends on a number of assumptions about the data at hand. If these are met, at least two conclusions of practical importance can be made: First, the inferred coeffcients do represent the average change in the expected value of the dependent variable induced by changing one unit of the relevant independent variable, and second, the variation in the data beyond what is explained by our regression model is variation that is not related to our independent variables. The following assumptions have to be met. 
• 
Independence. A frst assumption of the method is that the measurements are independent from one another. This assumption is potentially violated in our case, since we cannot be sure that Westin (2002) only included texts from different authors. In practice, this problem could be addressed by including the authors of the respective texts as a so-called random effect, which is discussed in the following chapter of this book. 

• 
Normality. It is further assumed that the residuals are normally distributed. The distribution of the residuals of a (multiple) linear regression model can be visually inspected by plotting them in a histogram. 

M. Hilpert and D. E. Blasi 

• 
Linearity. Another assumption is that the relation between the dependent variable and a given independent variable is linear. Ideally, a continuous independent variable will relate to the dependent variable in such a way that successive increases in one will correspond to a mirror image of successive changes, i.e. increases or decreases, in the other. 

• 
No multicollinearity. Different independent variables should not stand in linear relationships with each other. For example, if a corpus-based analysis con­tains the length of a phrase as an independent variable, and we introduce the pronominality of that phrase as a second independent variable, these two variables might correlate signifcantly, since full lexical phrases are by necessity longer than single pronouns. As a consequence, the model will reduce the certainty associated with the coeffcients of those variables (as captured by their standard deviations) and potentially misattribute the effect of one variable to 

the other. 

• 
No autocorrelation. For all TTR values in our dataset, the regression model computes predictions that can differ more or less from the actual measurements. These differences are called the residuals. If seeing one residual lets us predict the size of the residual that is next in line, or the one after that, the residuals are autocorrelated. This should not be the case. 

• 
Homoscedasticity. This term signifes that the variation of the residuals along the values of the independent variable needs to be similar. This assumption would be violated, for instance, if the predictions for low TTR values are highly accurate, and the predictions for high TTR values vary greatly in their accuracy. 


21.2.2 Binary Logistic Regression 
Whereas our concern in the previous section was to predict the values of a continuous dependent variable, the goal of a binary logistic regression is to model the logarithm (log) odds ratio that a binary variable takes one value, rather than the other.2 Binary logistic regression is a member of a family of regression techniques that is known as generalized linear modeling, or GLM for short. In a linguistic case study, this allows us to predict how a speaker will behave when faced with the choice between two linguistic resources. In most cases, the choice will not be determined by a single underlying factor, but rather by a complex ecology of interacting factors. Binary logistic regression serves to identify the factors that 
2This is just but one possible regression model for binary outcomes (Agresti and Kateri 2011). Furthermore, it might appear as a bit arbitrary that the response variable is the log odds-ratio rather than a more “intuitive” measure (like the arithmetic difference of probabilities or so.) The reason is that this particular model can be analyzed (and computationally treated) within the framework of generalized linear models (Gelman and Hill 2006). 
matter to a given linguistic choice. As in (multiple) linear regression, the goal is to explain a dependent variable in terms of several independent variables. The most important difference is the nature of the dependent variable, which is continuous in a (multiple) linear regression, whereas it is categorical and binary in a binary logistic regression. 
21.2.2.1 An Example of Binary Logistic Regression 
An example of a binary choice in English is the variation between will and be going to as expressions of the future. A binary logistic regression analysis requires a dataset that contains tokens of the dependent variable in their two different realizations. Our discussion here draws on a dataset of will and be going to from the Diachronic Corpus of Present-Day Spoken English (Wallis et al. 2006) that is discussed in Hilpert (2013:45). The dataset is included in the supplementary materials for this chapter. Each token is annotated in terms of the dependent variable (will vs. be going to) and the independent variables that inform the analysis, which in this case are time period (early, late), gender (female, male), text type (informal, mid-level, formal), and verb semantics (agentive, non-agentive). All four independent variables will be treated here as categorical. The example below shows an example sentence from the DCPSE together with its annotation: 
(1)  a.  so yes I’m sure it will be quite different  
b.  Dependent variable:  WILL  
Period:  late  
Speaker gender:  female  
Text formality:  informal  
Verb semantics:  non-agentive  

With a dataset that is annotated for dependent and independent variables and their values, a binary logistic regression can determine whether the independent variables have a measurable effect on the choice in the dependent variable, and how the independent variables differ in relative impact. For each token, the analysis will produce a predicted value of the dependent variable. Given that we have an example from a young female speaker who speaks in an informal setting and uses a non-agentive verb, what are the log odds of that speaker producing will rather than be going to? The coeffcients of the independent variables will yield a numerical value that expresses the log odds. Whereas the ftted values in a (multiple) linear regression can theoretically range from negative to positive infnity, this is not the case with binary logistic regression. Since it is the purpose of the method to model a categorical choice between two outcomes, the ftted values of a binary logistic regression only range between 0 (be going to is completely unlikely) and 1 (it is completely certain that the speaker will use be going to). This is accomplished through the inverse logit function, which transforms log odds values into probabilities (Gries 2013:294). The inverse logit function exemplifes the notion 
M. Hilpert and D. E. Blasi 
of a link function. Different types of GLM use different link functions to map values of the independent variables onto predicted values of the dependent variable. 
Below, we discuss the R output of a binary logistic regression model that has been produced on the basis of the DCPSE dataset of will and be going to. The model attempts to predict whether a speaker will use will or be going to on the basis of four independent variables: age, gender, verb semantics, and text formality. The supple­mentary materials contain a dataset called ‘21_FixedEffectsRegression_future.csv’ as a csv-fle and R code for the analysis. To read the data into R by using the code below, users should put the fle in their current working directory. 
future<-read.delim("21_FixedEffectsRegression_future.csv", sep=";") 
Call: glm(formula = future ~ period + gender + semantics + formality, 
family = binomial, data = future) 
Deviance Residuals: 
Min 1Q Median 3Q Max -2.0497 -0.9519 -0.7700 1.0976 1.6496 
Coefficients: Estimate Std. Error z value Pr(>|z|) 
(Intercept)  0.04637  0.12511  0.371  0.711  
periodlate  -0.19773  0.08760  -2.257  0.024 *  
gendermal  0.14426  0.09711  1.485  0.137  
semanticsnonag 0.36329  0.08715  4.168 3.07e-05 ***  
formalityfor  1.41616  0.16108  8.791  < 2e-16 ***  
formalityinf  -0.91277  0.09573  -9.535  < 2e-16 ***  
--- 

Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
(Dispersion parameter for binomial family taken to be 1) 
Null deviance: 3398.0 on 2452 degrees of freedom Residual deviance: 3036.6 on 2447 degrees of freedom AIC: 3048.6 
Number of Fisher Scoring iterations: 4 
Similar to the outputs that were discussed above, the formula that was used to specify the regression model is followed by an overview of the residuals. The table of coeffcients starts with a row for the intercept and then lists the coeffcients for the independent variables. The independent variable of period is signifcant and has a negative sign, which means that usage in the later period of the DCPSE gravitates towards be going to. Since the label “early” precedes the label “late” in the alphabet, the former is chosen as the reference level and the table of coeffcients only shows a value for the later period. Moving on in the table, the coeffcient for the independent variable of gender is not signifcant, which indicates that in the dataset under investigation, men and women are indistinguishable in their use of future marking. The coeffcient for verb semantics shows that non-agentive verbs signifcantly bias speakers towards will. The independent variable of formality is represented by two rows. Formal texts show a bias towards will, informal texts show a bias towards be going to. 
In more complex analyses, analysts might employ a process called model selec­tion, which aims to determine which independent variables and which interactions between them should be included in the regression model. Different strategies can be employed in model selection. A general discussion that also includes criticisms of model selection can be found in Gries (2013:253). The strategy of forward selection starts with a minimal model, to which independent variables are added and retained if the resultant models are superior to the simpler ones. Backwards selection starts with a maximal model in which all potentially signifcant independent variables are included at frst. Simpler models are adopted if they are not signifcantly worse than their more complex competitors. 
How can the coeffcients of a logistic regression be interpreted? For categorical independent variables, the coeffcient represents the change in the log odds of the outcome as compared to the reference level. For example, in contexts with non-agentive verbs the log odds of will increase by about 0.36, as compared to contexts with agentive verbs. The exponential function turns these values into odds ratios, which can be interpreted more easily. In the case of non-agentive verbs, the log odds value of 0.36, transformed by the exponential function, yields an odds ratio of 
1.433. What this means is that the odds for non-agentive verbs to be used with will are 43.3% higher than the odds for agentive verbs. 
21.2.2.2 Assumptions of Binary Logistic Regression 
A basic requirement of binary logistic regression is a dependent variable that has exactly two possible outcomes. Apart from this requirement, the assumptions that underlie a binary logistic regression analysis are a subset of the ones that were discussed for multiple regression analysis above. 
• 
Independence. Each observation should be independent, that is, it should not be infuenced by other observations that are contained in the same dataset. In our data, it could be argued that this assumption is problematic for at least two reasons. First, we implicitly allowed for several observations from the same speaker. Second, it is well-known that speakers’ choices between two variants are infuenced by earlier uses of those variants in the preceding discourse, a phenomenon that is known as structural priming (Szmrecsanyi 2005). The infuence of structural priming can be assessed through a separate predictor variable; the idiosyncrasies of individual speakers can be addressed in a mixed-effects regression model (cf. Chap. 22). 

• 
Linearity. To the extent that a binary logistic regression model involves contin­uous independent variables, those variables should stand in a linear relationship with the log-transformed predictions of the regression model. 

• 
No multicollinearity. The co-presence of highly similar independent variables can have problematic results. A model selection process that drops different confgurations of variables from the datasets under investigation can be used to keep this problem in check. 


M. Hilpert and D. E. Blasi 
21.2.3 An Extension of Binary Logistic Regression: Multinomial Logistic Regression 
Multinomial logistic regression is an extension of binary logistic regression that shares its general characteristics and assumptions, but that can handle cases of variation with more than two variants. The introduction mentioned the example of adjectives that convey the idea of a highly positive assessment. Here, a speaker of English has the choice between adjectives such as great, excellent, and perfect, amongst others. Multinomial logistic regression can be used to investigate the variables that infuence speakers’ subconscious choices between such a group of adjectives. The dependent variable is thus a categorical variable with three or more levels.3 The general logic of the analysis is directly analogous to binary logistic regression. Based on a dataset that includes the dependent variable and several predictor variables, the analysis tries to predict the outcome of the dependent variable based on the values of the predictor variables. As with logistic regression, we model the response as a log odds ratio, with the difference that this time one of the levels of the dependent variable (for example great) is chosen as the reference level, and the log odds ratios of the remaining levels (excellent, perfect) are computed in relation to that level. 
Representative Study 1 
Tily, H., Gahl, S., Arnon, I., Kothari, A., Snider, N., and Bresnan, J. 2009. Syntactic probabilities affect pronunciation variation in spontaneous speech. Language and Cognition 1/1, 47–165. 
Research questions 
Is the preposition to pronounced more quickly or more slowly depending on its context? What are the factors that infuence pronunciation length? Tily et al. hypothesize that to is pronounced more quickly in cases where it has a high probability of occurrence. For example, when a sentence starts with the words John sent a letter, a following to is very likely. By comparison, if the sentence starts with John gave the cat, a following to is less likely, even though an utterance such as John gave the cat to his daughter is possible. These relative likelihoods are a function of several underlying factors, such as the referents’ animacy, their length, and their discourse-givenness, amongst others. Bresnan et al. (2007) offer a detailed study of the relative importance of these factors. 
(continued) 
3If there is a natural ordering of the response variable (e.g. we assume that excellent > per­fect > great in terms of their valency) then ordinal regression might be a more informative tool (Harrell 2015). 
Data 
Tily et al. (2009) extracted all examples of the prepositional dative con­struction (John gave the book to Mary, Bob sent an invoice to the client) from the Switchboard corpus and determined the pronunciation length of the preposition to in each example. A set of measurements from 446 sentences serves as the dependent variable of the analysis. 
Method 
Linear regression was used to determine whether the syntactic probability of to has an infuence on pronunciation length. Syntactic probability was operationalized as the relative likelihood of the prepositional dative construc­tion vis-à-vis its syntactic alternative, the ditransitive construction (John gave Mary the book, Bob sent the client an invoice). For each example from the Switchboard corpus, a relative likelihood was calculated on the basis of the results from Bresnan et al. (2007). In order to assess the relative infuence of syntactic probability as an independent variable, Tily et al. controlled for a range of other variables, including speech rate, the phonological segment to the left and right of to, the probability of to given the verb in the sentence, and the probability of to given the word to its immediate left and right. 
Results 
Tily et al.’s results (2009:156) are summarized in the following table of coeffcients. Syntactic probability has a signifcant effect on pronunciation length in the expected direction even when other contextual variables, such as the likelihood of to given the following word or a previous vowel, are controlled for. Speech rate, forward bigram probability, and verb bias did not reach signifcance and are hence not listed in the table, which represents the minimal adequate model. 
Coefficients: 
Estimate Std. Error t value Pr(>|t|) Intercept 0.17557 0.01220 14.397 0.000000 *** Outcome probability 0.34147 0.13782 2.478 0.013603 * Backward bigram 6.92303 2.12904 3.252 0.001235 ** Previous vowel 0.02486 0.01038 2.396 0.017001 * 
M. Hilpert and D. E. Blasi 
Representative Study 2 
Diessel, H. 2009. Iconicity of sequence. A corpus-based analysis of 
the positioning of temporal adverbial clauses in English. Cognitive 
Linguistics 19:457–482. 
Research questions 
In English, the syntactic order of adverbial clauses and their main clauses is variable. The sentence Before you leave, please check that the lights are out is just as grammatical as its alternative Please check that the lights are out before you leave. How do speakers decide where to place the adverbial clause? Diessel (2009) hypothesized that speakers design their complex sentences so that they iconically mirror the temporal sequence of events and states of affairs in the real world. In the example above, the event of checking the lights happens before the event of leaving, which yields the prediction that speakers have a preference for the second variant. 
Data 
In order to investigate whether iconicity of sequence infuences the position­ing of English adverbial clauses, Diessel retrieved 600 complex sentences with the conjunctions when, after, before, once, and until from the ICE-GB corpus. 
Method 
The sentences were coded for the dependent variable and four predictor variables. The dependent variable is the relative position of the adverbial clause, which can either be anterior or posterior to the main clause. The frst independent variable is the conceptual order of the two clauses, which was coded as either iconic or non-iconic. The second independent variable is syntactic complexity, which captures whether or not an adverbial clause is internally complex. The third independent variable takes meaning into account, distinguishing temporal meaning from conditional meaning and the meaning of causation or purpose. The fourth independent variable is the length of the adverbial clause, measured as a percentage of the length of the entire complex sentence. A binary logistic regression was conducted to assess the effects of these independent variables. 
Results 
The logistic regression fnds signifcant effects for all variables except syntac­tic complexity. The coeffcient estimates in the table below (Diessel 2009:19) are positive for variables that lead speakers to place the adverbial clause after the main clause; a negative sign indicates that speakers are biased towards an anterior positioning. Crucially, the effect of conceptual order is in line with Diessel’s hypothesis. If an adverbial clause encodes an anterior event, speakers show a preference for anterior verbalization. The variable of meaning 
(continued) 
shows that its three levels (temporal, causal/purpose, conditional) behave in signifcantly different ways. Causal adverbial clauses have a tendency to follow their main clauses, while conditional clauses tend to precede them. Finally, longer adverbial clauses have a greater likelihood to follow their main clauses, which is consistent with the well-known principle of end weight. 
Coefficients:  
Conceptual order Meaning a. causal/purpose b. conditional  Estimate 1.902 -2.775 1.364  Wald X2 73.69 41.07 7.27 31.20  df 1 2 1 1  p0.001 0.001 0.007 0.001  odds ratio 6.70 0.06 3.91  
Length  -1.343  7.39  1  0.001  0.19  

21.3 Practical Guide with R 
21.3.1 Multiple Linear Regression 
21.3.1.1 Creating an Artifcial Dataset for a Multiple Linear Regression 
Let us imagine that we just completed an investigation on the basis of a corpus of spoken English that extracted all tokens of don’t from a set of spontaneous conversations. Our research question, similar to the one asked by Tily et al. (2009), is whether there is variation in pronunciation length of don’t that can be explained in terms of several independent variables. The basic data for our fctitious investigation consists of 1000 tokens of don’t for which we have measurements of pronunciation length. Earlier actual research (Bybee and Scheibman 1999) has identifed several factors that we want to include as independent variables. A frst independent variable concerns the element that precedes don’t. In many instances, that element is a pronoun (e.g. I don’t), whereas in others, it is a lexical element (e.g. Republicans don’t). We model this variation as a binary categorical variable, thus distinguishing pronominal and lexical preceding elements. A second independent variable concerns the right context of don’t. Unless don’t is used as a pro-verb, the right context will contain a verb in the infnitive, as in I don’t know. In our fctitious dataset, we capture a difference between examples with high-frequency verbs in the right context (i.e. verbs such as know, want, think,or have) and examples that do not have such an element in the right context (i.e. examples with lower-frequency verbsornoverbatall). 
The R code below creates a data frame that forms the basis for our analysis. We frst create two columns for the independent variables. We fx the values so that both independent variables are evenly distributed with regard to their respective levels, 
i.e. with 500 examples for each level. We then set the pronunciation times in such a 
M. Hilpert and D. E. Blasi 

lex.other pron.other lex.hifreqv pron.hifreqv 
way that we manufacture the effects of the two independent variables into the data. We generate numbers around the mean of 300 ms, minus 80 ms for tokens with a preceding pronoun, and minus 50 ms for tokens with a following high frequency verb. We include a certain amount of noise by generating values with a standard deviation of 30 ms. We further make sure that R is treating the two independent variables as factors, which is crucial when factor levels are coded with numbers such as 0 and 1. R should not interpret the difference between the two as a numerical distance. The fnal line of code visualizes the artifcially created data, yielding the boxplot that is shown in Fig. 21.3. 
# Clean the workspace and set the random number generator.rm(list = ls())set.seed(1234) 
# Create a data frame with the two independent variables. 
linreg.data <-data.frame(pronoun = rep(c(0, 1), 500), hi.freq.v = c(rep(0, 500), rep(1, 500)) 
) 
# Add the fictional pronunciation length measurements.linreg.data$pron.length = rnorm(1000, 300-80*linreg.data$pronoun­50*linreg.data$hi.freq.v, sd = 30) 
# Turn the two independent variables into factors. linreg.data[,1:2] <-lapply(linreg.data[,1:2], as.factor) 
# A visualization of the effects that we have created 
boxplot(pron.length ~ pronoun + hi.freq.v, data = linreg.data, ylab="pronunciation time", names=c("lex.other", "pron.other", "lex.hifreqv", "pron.hifreqv")) 
The boxplot shows that examples with a preceding element that is lexical (lex) and a following element that is not a high frequency verbs (other) are distributed around the value of 300 ms. As we specifed in the code above, examples with a preceding pronoun are, on average, 80 ms faster. Examples with a following high frequency verb are roughly 50 ms faster. These two effects are additive, so that examples with a pronoun and a following high frequency verbs are about 130 ms faster. We can now run a multiple linear regression analysis in order to see how the manufactured effects are picked up by a statistical model. 
21.3.1.2 Running a Multiple Linear Regression 
The code below executes a multiple linear regression model that attempts to predict the value of each measurement in our dependent variable on the basis of the information that is given by the independent variables. 
# A first regression model of the artificially created data 
linreg.mod1 <-lm(pron.length ~ pronoun + hi.freq.v, data = linreg.data) summary(linreg.mod1) 
Call: lm(formula = pron.length ~ pronoun + hi.freq.v, data = linreg.data) 
Residuals: 
Min 1Q Median 3Q Max 
-102.106 -19.695 -0.313 19.582 95.653 
Coefficients: 
Estimate Std. Error t value Pr(>|t|) 
(Intercept) 299.886 1.640 182.89 <2e-16 *** 
pronoun1 -79.662 1.893 -42.07 <2e-16 *** 
hi.freq.v1 -51.706 1.893 -27.31 <2e-16 *** 
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
Residual standard error: 29.94 on 997 degrees of freedom Multiple R-squared: 0.7162, Adjusted R-squared: 0.7156 F-statistic: 1258 on 2 and 997 DF, p-value: < 2.2e-16 
The table of coeffcients reproduces the effects that we manufactured into the data. The intercept is very close to the 300 ms that we specifed; examples with pronouns are about 80 ms faster; examples with high frequency verbs are about 52 ms faster. The residual standard error corresponds to the standard deviation of 30 ms that we selected. 
21.3.1.3 What Happens If We Make the Effect Sizes Smaller? 
How do the results change when we re-engineer the effects of the two independent variables? The code below reduces the strength of both effects by 30 ms. We then re-run the analysis and obtain a new output. 
M. Hilpert and D. E. Blasi 
# Clean the workspace and set the random number generator. rm(list = ls())set.seed(1234) 
# Create a new data frame with the two independent variables. 
linreg.data2 <-data.frame( pronoun = rep(c(0, 1), 500), hi.freq.v = c(rep(0, 500), rep(1, 500)) 
) 
# Add the fictitious pronunciation length measurements, with reduced effect sizes. linreg.data2$pron.length = rnorm(1000, 300-50*linreg.data2$pronoun­20*linreg.data2$hi.freq.v,    sd = 30) 
# Turn the two independent variables into factors.linreg.data2[,1:2] <-lapply(linreg.data2[,1:2], as.factor) 
# Run a second regression model, with reduced effect sizes. Output the 
results. linreg.mod2 <-lm(pron.length ~ pronoun + hi.freq.v, data = linreg.data2) summary(linreg.mod2) 
Call: lm(formula = pron.length ~ pronoun + hi.freq.v, data = linreg.data2) 
Residuals: Min     1Q  Median 3Q Max -102.106 -19.695 -0.313 19.582   95.653 
Coefficients: 
Estimate Std. Error t value Pr(>|t|) (Intercept)  299.886    1.640 182.89 <2e-16 *** pronoun1 -49.662 1.893 -26.23 <2e-16 *** hi.freq.v1   -21.706      1.893 -11.46 <2e-16 *** 
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
Residual standard error: 29.94 on 997 degrees of freedom Multiple R-squared:  0.4511, Adjusted R-squared:  0.45 F-statistic: 409.7 on 2 and 997 DF,  p-value: < 2.2e-16 
Expectably, the new coeffcient estimates correspond to the adjustments that we made to the underlying data. Since we did not change the inherent variability of the adjustments, the standard errors remain virtually identical to the ones that were obtained above. The t-values drop according to the diminished sizes of the estimates, but the p-values remain signifcant. With regard to the global indicators of model quality, the residual standard error stays the same, but the R2-values drop considerably. 
21.3.1.4 What Happens If We Make the Effects “Noisier”? 
The way in which we set the values of the dependent variable above included a certain amount of statistical noise, which we specifed by setting the standard deviation to 30 ms. This statistical noise explains why the models’ coeffcients do not correspond exactly to the number of milliseconds that we specifed for each type of example. What happens when we increase the amount of noise, i.e. the amount of unsystematic, unpredictable variation that goes into an analysis? The code below increases the inherent variability of our two independent variables by setting the standard deviation to 50 ms instead of 30 ms. 
# Clean the workspace and set the random number generator.rm(list = ls())set.seed(1234) 
# Create a new data frame with the two independent variables. 
linreg.data3 <-data.frame( pronoun = rep(c(0, 1), 500), hi.freq.v = c(rep(0, 500), rep(1, 500)) 
) 
# Add the fictitious pronunciation length measurements, with an increased amount of noise. linreg.data3$pron.length = rnorm(1000, 300-80*linreg.data3$pronoun­50*linreg.data3$hi.freq.v, sd = 50) 
# Turn the two independent variables into factors. linreg.data3[,1:2] <-lapply(linreg.data3[,1:2], as.factor) 
# Run a third regression model, with noisier measurements. linreg.mod3 <-lm(pron.length ~ pronoun + hi.freq.v, data = linreg.data3) summary(linreg.mod3) 
Call: lm(formula = pron.length ~ pronoun + hi.freq.v, data = linreg.data3) 
Residuals: Min 1Q Median 3Q Max -170.177 -32.826 -0.522 32.637 159.421 
Coefficients: 
Estimate Std. Error t value Pr(>|t|) (Intercept) 299.810 2.733 109.70 <2e-16 *** pronoun1 -79.436 3.156 -25.17 <2e-16 *** hi.freq.v1 -52.844 3.156 -16.75 <2e-16 *** 
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
Residual standard error: 49.9 on 997 degrees of freedom Multiple R-squared: 0.4783, Adjusted R-squared: 0.4773 F-statistic: 457 on 2 and 997 DF, p-value: < 2.2e-16 
The results show a few differences compared to those of the frst model that we constructed. First of all, the residuals are larger. While the coeffcients as such have not changed much, the standard errors of the coeffcients are larger and the t-values correspondingly lower. The global assessments show a higher residual standard error and lower R2-values. This shows that high R2-values depend not only on the average size of an effect, but also on the clarity of that effect. 
21.3.1.5 Manufacturing an Interaction Effect 
A useful feature of regression modelling is that interdependencies between inde­pendent variables can be taken into account. These interdependencies are called interactions, and in this section, we will modify the dataset in such a way as to create an interaction effect. For this purpose, let us consider a third independent variable for our fctitious data, namely the presence of an adverb to the immediate right of don’t. Like the other two independent variables, it is a categorical variable that can take on two values. Either an adverb is present or not. Let us assume that the presence of an adverb modulates the effect of high frequency verbs in the right 
M. Hilpert and D. E. Blasi 
context, such that intervening adverbs between don’t and the high frequency verb, as in I don’t even know him, attenuate the faster pronunciation times of don’t. Let us further assume that an intervening adverb does not change the pronunciation times in any way when no high frequency verb is present in the right context. This setup yields an interaction effect: adverbs slow down pronunciation times, but only if a high frequency verb is present. 
The R code below frst creates a data frame with our three independent variables. We add another column to the data frame in which we identify all examples with a high frequency verb and an intervening adverb, essentially dummy-coding the interaction. We fx the pronunciation length measurements in such a way that the effects for pronouns and high frequency verbs remain the same as above, but we selectively slow down examples that have both a high frequency verb and an intervening adverb. 
# Clean the workspace and set the random number generator.rm(list = ls())set.seed(1234) 
# Create a new data frame with three independent variables. 
linreg.data4 <-data.frame( 
pronoun = rep(c(0, 1), 500), 
hi.freq.v = c(rep(0, 500), rep(1, 500)), 
adverb = rep(c(0, 1, 1, 0), 250) ) 
# Identify examples with high frequency verbs and adverbs 
linreg.data4$interaction = ifelse(linreg.data4$hi.freq.v==1 & 
linreg.data4$adverb==1, 1, 0) 
# Create the fictitious pronunciation length measurements.
linreg.data4$pron.length = rnorm(1000, 300-80*linreg.data4$pronoun­
50*linreg.data4$hi.freq.v+50*linreg.data4$interaction, sd = 30) 
# Turn the three independent variables into factors. linreg.data4[,1:3] <-lapply(linreg.data4[,1:3], as.factor) 
Let us take a frst pass at analyzing the data in a way that is, strictly speaking, inappropriate. The R code below specifes a regression model that includes the new independent variable, alongside the two other ones, as a main effect. This means that the model tests for an effect of adverbs across all other independent variables. As we will discuss below, main effects need to be distinguished from interaction effects. 
# A fourth regression model, including the new variable linreg.mod4 <-lm(pron.length ~ pronoun + hi.freq.v + adverb, data = linreg.data4)summary(linreg.mod4) 
Call: lm(formula = pron.length ~ pronoun + hi.freq.v + adverb, data =linreg.data4) 
Residuals: Min 1Q Median 3Q     Max -97.377 -20.992 0.442  21.123  94.652 
Coefficients: 
Estimate Std. Error t value Pr(>|t|) (Intercept)  286.545 2.011  142.50 <2e-16 *** pronoun1 -79.662 2.011 -39.62 <2e-16 *** hi.freq.v1   -26.706 2.011 -13.28 <2e-16 *** adverb1       26.682 2.011   13.27  <2e-16 *** 
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
Residual standard error: 31.79 on 996 degrees of freedom Multiple R-squared:  0.6586, Adjusted R-squared:  0.6576 F-statistic: 640.6 on 3 and 996 DF,  p-value: < 2.2e-16 
The table of coeffcients indicates that there is a signifcant effect of adverbs, which are estimated to slow down pronunciation times by about 27 ms. While this is true as an average, it does not capture the fact that adverbs selectively slow down examples that have a high-frequency verb in the right context. In order to bring this to light, we need to specify what is known as an interaction term in the regression model. This is done in the R code below. In the formula that specifes the regression model, the asterisk between the variable of high frequency verb and the variable of adverb indicates that we are testing for the main effects of these variables as well as for a possible interaction between them. The last line in the table of coeffcients, which links both variables with a colon, represents that interaction. 
# A fifth regression model, including an interaction term linreg.mod5 <-lm(pron.length ~ pronoun + hi.freq.v * adverb, data = linreg.data4)summary(linreg.mod5) 
Call: lm(formula = pron.length ~ pronoun + hi.freq.v * adverb, data =linreg.data4) 
Residuals: Min 1Q Median 3Q     Max -99.575 -19.756  -0.589 18.909 93.122 
Coefficients: 
Estimate Std. Error t value Pr(>|t|)   (Intercept)    297.355      2.115 140.607   <2e-16 *** pronoun1 -79.662   1.892 -42.115  <2e-16 *** hi.freq.v1         -48.327  2.675 -18.066  <2e-16 *** adverb1 5.062   2.675   1.892 0.0588 . hi.freq.v1:adverb1 43.241    3.783  11.430  <2e-16 *** 
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
Residual standard error: 29.91 on 995 degrees of freedom Multiple R-squared:  0.6983, Adjusted R-squared:  0.6971 F-statistic: 575.7 on 4 and 995 DF,  p-value: < 2.2e-16 
M. Hilpert and D. E. Blasi 
What can be seen from the output is that the adverb variable by itself is no longer a signifcant predictor of the pronunciation times. The variable does however participate in a signifcant interaction with the variable of high frequency verbs: The interaction term in the last line of the coeffcient states that when both an adverb and a high frequency verb are present, pronunciation times are slowed down by about 43 ms, which represents the effect that we engineered into the data. 
21.3.2 Binary Logistic Regression 
21.3.2.1 Creating an Artifcial Dataset for a Binary Logistic Regression 
Let us assume that we conduct a corpus-based analysis of how speakers address each other in conversation. Languages such as French or German make a grammatical distinction between second person pronouns that are intimate and second person pronouns that are more distant and polite. By way of abbreviation, these pronouns are often referred to as T-forms (French tu) and V-forms (French vous). Our research question is whether and how variation between these forms is governed by conditioning factors. Below, we create a fctitious dataset of 1000 examples. The dataset features four independent variables. The frst independent variable concerns the type of conversation from which the examples are drawn. We classify these into formal and casual. The second independent variable is speaker gender. The third one is the relation between speaker and hearer. Here, we distinguish conversations between friends and conversations between strangers. The last independent variable is speaker age. 
The R code below specifes different biases for these variables. We set the log odds of examples from formal conversations to 0.6, which means that the odds of the V-variant in formal conversations are about 82% higher than the odds of the V-form in casual conversations (exp(0.6) ˜ 1.82). The log odds for examples produced by male speakers are adjusted by -0.2, so that the odds of the V-form in male-produced speech become about 82% of the odds of the V-form in female-produced speech (exp(-0.2) ˜ 0.82). For the independent variable of relation, we adjust the log odds of speech towards friends to -2, so that the odds are only 13% of the odds of a V-form in conversations with strangers. For the continuous independent variable age, we tweak the log odds by adding 0.01 for each additional year of age. This means that a one-year difference in age will increase the odds of a speaker choosing the V-form by about 1% (exp(0.01) ˜ 1.01). On the basis of the log odds, we calculate for each example the probability that the outcome of the dependent variable is a V-form. We then create a column for the dependent variable, i.e. the pronouns that our fctitious speakers use. Based on the probabilities of a V-form, we take random samples from a binomial distribution to assign to each example a value of either a V-form or a T-form. 
# Clean the workspace and set the random number generator.rm(list = ls())set.seed(1234) 
# Create a data frame with two independent variables. 
logreg.data = data.frame( formality = rep(c("formal", "casual", "formal", "casual"), each=250), gender = rep(c("female", "female", "male", "male"), each=250) , relation = sample(c("friends", "strangers"), 500, replace=T), age = sample(c(20:50), 500, replace=T)) 
# Create a column with log odds for each example. logreg.data$logodds = 
0.6 * ifelse(logreg.data$formality=="formal", 1, 0) + -0.2 * ifelse(logreg.data$gender == "male", 1, 0) + -2 * ifelse(logreg.data$relation == "friends", 1, 0) + 
0.01 * logreg.data$age 
# Create a column with the probability of a V-form for each example. logreg.data$prob.of.V = plogis(logreg.data$logodds) 
# Create a column for outcomes of the dependent variable. r.binom = function(x) rbinom(1, 1, x) successes = sapply(logreg.data$prob.of.V, r.binom)logreg.data$pronouns = factor(ifelse(successes==1, "V", "T")) 
21.3.2.2 Running a Binary Logistic Regression 
The R code below executes a binary logistic regression that tries to predict whether a speaker will use a V-form or a T-form based on the information that is contained in the independent variables. 
# A binary logistic regression model logreg.model = glm(pronouns ~ formality + gender + relation + age, data=logreg.data, family="binomial")summary(logreg.model) 
Call: glm(formula = pronouns ~ formality + gender + relation + age,family = "binomial", data = logreg.data) 
Deviance Residuals: Min 1Q Median 3Q Max -1.6385 -0.6998 -0.5276 0.9706 2.2054 
Coefficients: 
Estimate Std. Error z value Pr(>|z|) (Intercept) -2.099777 0.331502 -6.334 2.39e-10 *** formalityformal 0.502578 0.149865 3.354 0.000798 *** gendermale -0.438238 0.148967 -2.942 0.003263 ** relationstrangers 2.195431 0.158461 13.855 < 2e-16 *** age 0.009005 0.008072 1.116 0.264613 
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
(Dispersion parameter for binomial family taken to be 1) 
Null deviance: 1333.9 on 999 degrees of freedom Residual deviance: 1086.3 on 995 degrees of freedom AIC: 1096.3 
Number of Fisher Scoring iterations: 4 
M. Hilpert and D. E. Blasi 
The output shares several components with the outputs that were discussed in the previous section. It shows the formula of the regression equation, an overview of the distribution of the residuals, and fnally a table with the coeffcients. The coeffcients indicate that three of the four effects that we manufactured into the data were picked up by the logistic regression model. There are signifcant effects of formality, gender, and relation, such that formal texts show a bias towards V-forms, male speakers have a bias against V-forms, and a conversation between strangers comes with a strong bias towards V-forms. Age does make speakers gravitate towards the V-form, but not signifcantly so. The coeffcients of the model differ somewhat from the values that we specifed above, which is due to our sampling from the binomial distribution.4 
21.3.2.3 Visualizing the Effects of a Binomial Logistic Regression 
In order to understand the regression model in more detail, it is useful to visualize the effects that are found. The R code below installs and loads a package that allows the creation of effect plots for binary logistic regression models. Figure 21.4 visualizes the effects of formality, gender, relation, and age. 
# Install the "effects" package install.packages("effects") library(effects) 
# Create effect plots plot(allEffects(logreg.model)) 
The panels of Fig. 21.4 show the predicted probabilities of the V-form on the y-axis. The values of the independent variables are mapped onto the x-axis. For the categorical independent variables formality, gender, and relation, the plots juxtapose the different levels and show predicted probabilities with a 95% confdence interval around those probabilities. A comparison between the panels for formality, gender, and relation reveals that the latter variable has a much stronger effect than the other two. Since age is a continuous independent variable, the plot shows a scale on the x-axis. While the plot indicates an increase of the probability of the V-form with age, the 95% confdence interval is very broad and would accommodate a horizontal line, which aligns with the non-signifcant result for age that is listed in the table of coeffcients. 
4We acknowledge that the artifcially created data set suffers from overdispersion, i.e. the variation is higher than statistically expected. We do not discuss this problem or its consequences here, as that would go beyond the intended scope of the chapter. 

21.3.2.4 Manufacturing and Visualizing an Interaction Effect 
Like (multiple) linear regression, binary logistic regression allows the researcher to test for interactions between the independent variables. A possible effect of this kind with regard to the choice between T-forms and V-forms could be that the independent variables of formality and age interact, so that younger speakers are not very sensitive to the difference between formal and informal situations, whereas older speakers are more tradition-minded and hence more sensitive. The R code below creates a dataset that emulates this phenomenon, increasing the effect of formality by 5% for each additional year of age. The code also runs a regression model that includes the relevant interaction term, and visualizes the interaction effect. 
M. Hilpert and D. E. Blasi 
# Clean the workspace and set the random number generator. rm(list = ls())set.seed(1234) 
# Create a data frame with four independent variables. 
logreg.data2 = data.frame( formality = rep(c("formal", "casual", "formal", "casual"), each=250), gender = rep(c("female", "female", "male", "male"), each=250), relation = sample(c("friends", "stranger"), 500, replace=T), age = sample(c(20:50), 500, replace=T)) 
# Create a column with log odds for each example. 
logreg.data2$logodds = (0.6 * ifelse(logreg.data2$formality=="formal", 1, 0)) *logreg.data2$age/min(logreg.data2$age) + -0.2 * ifelse(logreg.data2$gender == "male", 1, 0) + -2 * ifelse(logreg.data2$relation == "friends", 1, 0) +0.01 * logreg.data2$age 
# Create a column with the probability of a V-form for each example. logreg.data2$prob.of.V = plogis(logreg.data2$logodds) 
# Create a column for outcomes of the dependent variable. r.binom = function(x) rbinom(1, 1, x) successes = sapply(logreg.data2$prob.of.V, r.binom) logreg.data2$pronouns = factor(ifelse(successes==1, "V", "T")) 
# A binary logistic regression modelwith the interaction term logreg.model2 = glm(pronouns ~ formality * age + gender + relation + age, data = logreg.data2, family="binomial")summary(logreg.model2) 
Call: glm(formula = pronouns ~ formality * age + gender + relation + age, family = "binomial", data = logreg.data2) 
Deviance Residuals: Min 1Q  Median 3Q Max -2.1352 -0.8137  -0.4438 0.9700 2.2062 
Coefficients: 
Estimate Std. Error z value Pr(>|z|)(Intercept)         -1.984308 0.441634 -4.493 7.02e-06 *** formalityformal -0.557678  0.594677 -0.938  0.34836 age 0.004913   0.011676   0.421  0.67392 gendermale          -0.465573 0.149965 -3.105 0.00191 ** relationstranger     2.253275   0.161140  13.983  < 2e-16 *** formalityformal:age 0.045296   0.016380   2.765  0.00569 ** 
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
(Dispersion parameter for binomial family taken to be 1) 
Null deviance: 1369.9  on 999  degrees of freedom Residual deviance: 1075.1  on 994  degrees of freedom AIC: 1087.1 
Number of Fisher Scoring iterations: 4 
# Visualize the interaction between formality and age.plot(allEffects(logreg.model2)[3]) 

The effect plot in Fig. 21.5 shows different trend lines for casual and formal speech, which refects the interaction effect between formality and age. The left panel shows that age makes no difference in casual speech: Young speakers and old speakers do not differ in their probabilities of selecting the V-form. In formal speech, however, there is a clear trend: The older a speaker is, the greater the probability of a V-form. The table of coeffcients for the logistic regression model indicates that the interaction between formality and age is signifcant, while neither formality nor age are judged to have signifcant main effects. 
21.3.3 Reporting the Results of Regression Analyses 
When writing up the results of regression analyses, the central element is the table of coeffcients, which should include the coeffcient estimates, standard errors, and signifcance values for each coeffcient as well as the intercept. The table should be accompanied by further information about the regression model. For the results of a linear regression, the write-up needs to include the R2-value, the F-statistic, the degrees of freedom, and a global p-value of the regression model. Confdence intervals for the coeffcients are a useful addition. For the results of a binary logistic regression, the write-up should provide goodness-of-ft statistics such as the concordance index C or Nagelkerke’s R2 (cf. Gries 2013:304; Levshina 2013:259) in addition to the degrees of freedom and a global p-value. The classifcation accuracy of the binary logistic regression model is another useful piece of information. As for visualization, we recommend the use of effects plots for means and slopes as in Figs. 21.4 and 21.5. 
M. Hilpert and D. E. Blasi 
Further Reading 
Baayen, R.H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics. Cambridge University Press, Cambridge. 
Baayen’s introduction to statistics for linguistics is a cornerstone reference that not only covers the regression techniques that were discussed in this chapter, but also offers a more thorough discussion of nonlinear predictors, model criticism and validation, as well as regression with breakpoints. 
Gries, S.T. 2013. Statistics for Linguistics with R: A Practical Introduction. Second edition. De Gruyter, Berlin. 
Gries’s book includes several chapters on regression techniques that are illustrated with concrete data sets. The chapters gradually build up in complexity, presenting analyses with binary, categorical, and numerical predictors. The chapters further offer insights into ordinal, multinomial, and Poisson regression. 
Levshina, N. 2015. How to do Linguistics with R: Data exploration and statistical analysis. John Benjamins, Amsterdam. 
Levshina’s book offers a particularly learner-friendly discussion of regression techniques. The book presents these methods as ways of approaching linguistic questions, and it places emphasis on data visualization. The examples that are covered include an application of logistic regression to the choice between the Dutch causative constructions with doen and laten. 
References 
Agresti, A., & Kateri, M. (2011). Categorical data analysis. Berlin: Springer. Bresnan, J., Cueni, A., Nikitina, T., & Baayen, R. H. (2007). Predicting the dative alternation. In 
G. Boume, I. Kraemer, & J. Zwarts (Eds.), Cognitive foundations of interpretation (pp. 69–94). Amsterdam: Royal Netherlands Academy of Science. Bybee, J., & Scheibman, J. (1999). The effect of usage on degrees of constituency: The reduction of don’t in English. Linguistics, 37(4), 575–596. Diessel, H. (2009). Iconicity of sequence. A corpus-based analysis of the positioning of temporal adverbial clauses in English. Cognitive Linguistics, 19, 457–482. Gelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge: Cambridge University Press. Gries, S. T. (2013). Statistics for linguistics with R: A practical introduction (2nd ed.). Berlin: De Gruyter. Harrell, F. (2015). Regression modeling strategies: With applications to linear models, logistic and ordinal regression, and survival analysis. Berlin: Springer. 
Hilpert, M. (2008). The English comparative – Language structure and language use. English Language and Linguistics, 12(3), 395–417. 
Hilpert, M. (2013). Constructional change in English: Developments in allomorphy, word forma­tion, and syntax. Cambridge: Cambridge University Press. 
Szmrecsanyi, B. (2005). Language users as creatures of habit: A corpus-linguistic analysis of persistence in spoken English. Corpus Linguistics and Linguistic Theory, 1(1), 113–150. 
Tily, H., Gahl, S., Arnon, I., Kothari, A., Snider, N., & Bresnan, J. (2009). Syntactic probabilities affect pronunciation variation in spontaneous speech. Language and Cognition, 1(1), 47–165. 
Wallis, S. A., Aarts, B., Ozon, G., & Kavalova, Y. (2006). The Diachronic Corpus of Present-Day Spoken English (DCPSE). London: Survey of English Usage. 
Westin, I. (2002). Language change in English newspaper editorials. Amsterdam: Rodopi. 
Chapter 22 Mixed-Effects Regression Modeling 
Roland Schäfer 
Abstract In this chapter, mixed-effects regression modeling is introduced, mostly using alternation modeling as an example. It is one option to deal with cases where observations vary by groups (such as speakers, registers, lemmas) by introducing so-called random effects into the model specifcation. It is stressed that using a categorical variable as a random effect is just an alternative to using it as a normal fxed effect in a Generalised Linear Model (GLM) as introduced in Chap. 21,but that the two options have different mathematical advantages and disadvantages. Simple random intercepts are introduced, which capture per-group tendencies. However, random slopes (for situations where fxed effects vary per group) and multilevel models (for situations where group-wise tendencies can be predicted from other variables, for example when lemma frequency is useful to predict lemma-specifc tendencies) are also introduced. Criteria for including random effects in models and for evaluating the model ft (for example through pseudo-coeffcients of determination) are discussed. The demonstration in R uses the popular lme4 package. 
22.1 Introduction 
Mixed effects modeling is an extension of (generalised) linear modeling, of which logistic regression (see Chap. 21) is an instance. A common characterisation of mixed-effects modeling is that it accounts for situations where observations are “clustered” or “come in groups”. In corpus linguistics, there could be clusters of observations defned by individual speakers, registers, genres, modes, lemmas, etc. 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_22) contains supplementary material, which is available to authorized users. 
R. Schäfer (•) Department of German Studies and Linguistics, Humboldt-Universität zu Berlin, Berlin, Germany e-mail: mail@rolandschaefer.net 
© Springer Nature Switzerland AG 2020 535 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_22 
Instead of estimating coeffcients for each level of such a grouping factor (so-called 
“fxed effects”), in a mixed model it can alternatively be modeled as a normally distributed random variable (a so-called “random effect”) with predictions of group-wise tendencies being made for each group. This chapter introduces readers to the situations where mixed-effects modeling is useful or necessary. The proper specifcation of models is discussed, as well as some model diagnostics and ways of interpreting the output. Readers are assumed to be familiar with the concepts covered in Chap. 21. 
22.2 Fundamentals 
22.2.1 When Are Random Effects Useful? 
In (Generalised) Linear Mixed Models (GLMMs) – or, more generally speaking, “multilevel models” or “hierarchical models” (see Gelman and Hill 2007 and Sect. 22.2.1.2 below) – the purpose of including random effects is usually said to be the modeling of variance between groups of observations. A single observation (alternatively “data point”, “measurement”, or “unit”) is one atomic exemplar enter­ing into the statistical analysis of a study. In corpus linguistics, single observations can be understood as single lines in a concordance. These concordance lines could contain, for example, clauses or sentences in which one of the alternants of a morpho-syntactic alternation occurs, the goal being to model the infuence of diverse properties of the clauses or sentences on the choice of the alternants. Along similar lines, they could contain occurrences of a contracted or a non-contracted form of words (like am and ’m in English). As another example, the concordance lines could contain NPs where two pre-nominal adjectives are used, the goal being to determine the factors infuencing their relative ordering. When such observations are grouped, it is often plausible to assume that there is some variance in the choice of the alternating forms or constructions between groups (or “at the group level”), such as the level of the individual speakers, registers, lemmas, etc. 
Groups can be defned by any linguistically relevant grouping factor, such as the individual speakers (or authors, writers, etc.), the regions where they were born or live, social groups with which they identify, but also time periods, genres, styles, etc.1 Specifc lexemes often have idiosyncratic affnities towards alternants in alternating constructions. Therefore, exemplars containing specifc lexemes also constitute groups. In cases like the dative alternation in English, individual verbs co-occur with the alternants to different degrees. 
1Trivially, grouping factors should never be ordinal variables. They are always categorical. Termi­nologically, the “groups” are the collections of observations, and each such group corresponds to the “levels” of the categorical grouping factor. 
If this is the case and the grouping factor is not included in the model, the error terms within the groups will be correlated. Put simply, the error terms would have a certain tendency per group. We would fnd group-wise tendencies, for example, if sentence length were modeled (for example using the types of constructions or lexemes occurring in it) and individual speakers represented in the data had tendencies to form longer or shorter sentences. If the speaker variable were not included as a predictor in the model under such conditions, the model would make predictions averaged over all speakers, and for short-sentence speakers, these predictions would tend to be too high, whereas for long-sentence speakers, they would be too low. However, the algorithms used for estimating the parameters of GLMs (so-called estimators) work under the assumption of non-correlated errors, an assumption often called “independence (of errors)”. Thus, the prediction errors should not have group-wise tendencies as described above. 
If the assumption of independence does not hold, standard errors for model coeffcients will typically be estimated as smaller than they nominally are, leading to increased Type I error rates in inferences about the coeffcients. This gets even worse when there are within-group tendencies regarding the direction and strength of the infuence of the other regressors, i.e., when there is an interaction between them and the grouping factor (e.g., Schielzeth and Forstmeier 2009). This is why known variation by group should be accounted for in the model. Random effects are one convenient way to do so with some advantages and disadvantages compared to fxed effects. 
The crucial question in specifying models is not whether to include these grouping factors at all, but rather whether to include them as fxed effects or as random effects. Random effect structures are very suitable for accounting for group-level variation in regression, but formulaic recommendations such as “Always include random effects for speaker and genre!” are inappropriate. The choice between fxed and random effects should be made based on an analysis and understanding of the data set at hand and the differences and similarities in the resulting models. The remainder of Sect. 22.2.1 introduces three important points to consider about the structure of data sets in this context. Then, Sect. 22.2.2 provides a moderately technical introduction to the actual modeling. 
22.2.1.1 Crossed and Nested Effects 
This section discusses a distinction that arises when there is more than one grouping factor. When this is the case, each pair of grouping factors can be “nested” or “crossed”. By way of example, we can group exemplars (such as sentences) by the individual speakers who wrote or uttered them, and we can group speakers by their region of birth. Such a data set would intrinsically be “nested”, as Table 22.1 illustrates. Since speakers have a unique region of birth, Tyneside is the unique region value for the speakers Daryl and Riley, and Greater London is the unique region value for Dale and Reed. In this example, the region factor nests the speaker factor. This example was chosen because the nesting is conceptually necessary. 
Table 22.1 Illustration of nested factors 
Exemplar  Speaker  Region  
1  Daryl  Tyneside  
2  Daryl  Tyneside  
3  Riley  Tyneside  
4  Riley  Tyneside  
5  Dale  Greater London  
6  Dale  Greater London  
7  Reed  Greater London  
8  Reed  Greater London  

Table 22.2 Illustration of crossed factors 
Exemplar 1  Speaker Daryl  Mode Spoken  
2  Daryl  Written  
3  Riley  Spoken  
4  Riley  Spoken  
5  Dale  Written  
6  Dale  Written  
7  Reed  Spoken  
8  Reed  Written  

However, even when a data set has a nested structure by accident, standard packages in R will also treat them as nested (see Sect. 22.3.1). 
When grouped entities do not uniquely belong to groups of the other grouping factor, the factors are “crossed”. Continuing the example, crossed factors for speaker and mode are illustrated in Table 22.2. While there are only spoken sentences by Riley and only written sentences by Dale in the sample, there is one spoken and one written sentence each by Daryl and Reed. There is a many-to-many relation between speakers and modes, which is characteristic of crossed factors. In Table 22.1,the relation between speakers and regions is many-to-one, which is typical of nested factors. 
With more than two grouping factors, there can be nesting within nesting, etc. Mode could nest genre if genres are defned such that each genre is either exclusively spoken or written. Similarly, in a study on adjectives we might want to describe adjectives as being either intersective or non-intersective. Within the two groups, a fner-grained semantic classifcation which itself nests single adjective lexemes might be nested. However, not all of these structures should be modeled as nested random effects. In the latter case, for example, the low number of levels of one factor (intersective vs. non-intersective) predestines it as a so-called “second-level predictor” rather than a nesting random effect (see Sect. 22.2.2). 
22.2.1.2 Hierarchical/Multilevel Modeling 
This section describes the types of data structures which require the use of multilevel models, which represent a generalisation of the simple mixed effects models discussed so far. Such data structures are similar or in some special cases identical to nested data structures. Let us assume that we wanted to account for lexeme-specifc variation in a study on an alternation phenomenon such as the dative alternation in English by specifying the lexeme as a random effect in the model. Additionally, we entertain the hypothesis that a lexeme’s overall frequency infuences its preferences for occurring in the construction alternants. A similar situation would arise in a study of learner corpus data (even of the same alternation phenomenon) with a learner grouping factor if we also knew that the number of years learners have learned a language infuences their performance with regard to a specifc phenomenon. In such cases, variables like the frequency and the number of learning years are constant for each level of the grouping factor (lexeme and learner, respectively). In other words, each lexeme has exactly one overall frequency, and each learner has had a fxed number of years of learning the language. 
Such variables are thus interpretable only at the group level. Table 22.3 illustrates such a data set (fctional in this case). It might be a small fraction of the data used to predict whether a ditransitive verb is used in the dative shift construction or not. The NP length is measured in words, the lemma frequencies are actual logarithm-transformed frequencies per one million tokens taken from ENCOW14A (Schäfer and Bildhauer 2012). The outcome column encodes whether alternant 1 or 2 was chosen. As the table shows, the givenness status and the NP length vary at the level of observations. To capture verb-lemma-specifc tendencies, a verb lemma grouping factor is added. The verb lemma frequency necessarily varies at the group level because each lemma has a unique frequency. In such cases, an adequately specifed multilevel model uses these so-called “second-level fxed effects” to partially predict the tendency of the grouping factor, which is why they are also 
Table 22.3 Illustration of a fctional data set which requires multilevel modeling 
Level of observations  Group level  Outcome  
Exemplar  Givenness  NP length  Verb  Verb freq.  Alternant  
1  New  8  give  6.99  Shift  
2  Old  7  give  6.99  Shift  
3  Old  5  give  6.99  No-shift  
4  Old  5  grant  5.97  No-shift  
5  New  9  grant  5.97  Shift  
6  Old  6  grant  5.97  No-shift  
7  New  11  promise  5.86  No-shift  
8  New  10  promise  5.86  Shift  
9  Old  9  promise  5.86  No-shift  

called “second-level predictors”.2 Put differently, the idiosyncratic effect associated with a lexeme, speaker, genre, etc. is split up into a truly idiosyncratic preference and a preference predictable from group-level variables (such as the frequency of the lemma or the number of learning years of the speaker). In Sect. 22.2.2.4,it will be explained briefy that in such cases, a second model has to be specifed which predicts the group-level tendency from the second-level effects. The data look similar to multilevel nesting, but (i) second-level models can account for continuous numerical predictors at the group level, which nesting cannot. Also, (ii) there might be situations where specifying even categorical second-level grouping factors as fxed effects in a second-level model is more appropriate than adding nested random effects (see Sect. 22.2.2). 
22.2.1.3 Random Slopes as Interactions 
This section introduces the data patterns that give rise to “varying intercepts” and “varying slopes”. Varying intercepts are an adequate modeling tool when the overall tendency in the outcome variable changes with the levels of the grouping factor. 
We assume that we are looking at an alternation phenomenon like the dative alter­nation, wherein we are interested in the probability that, under given circumstances, the dative shift construction is chosen. In the examination of the data, it turns out that the probability of the dative shift changes for old and new dative NPs. The verb lemma also infuences the probability of either variant being used. The situation can now be as in the left or the right panel of Fig. 22.1. In the situation depicted in the left panel, the overall level in probability changes with the verb lemma, but for each verb lemma, the values change roughly by the same amount in exemplars with old and new dative NPs. Note that the lines are not perfectly parallel because the fgure is supposed to be an illustration of a data set rather than a ftted model, and we always expect some chance variation in data sets. In the situation depicted in the right panel, however, the overall tendency also varies between lemmas, but additionally the lemma-specifc tendencies also vary between exemplars with old and new NPs. This is in fact nothing but an interaction between two factors (verb lemma and givenness), and we could use a fxed-effect interaction to take it into account. However, if the verb lemma factor is used as a random effect, the interaction is modeled as a so-called “random slope”. In Sect. 22.2.2, it is shown how all the different types of data sets discussed so far can be modeled using fxed effects models or, alternatively, using mixed effects models. Which one is more appropriate will be argued to be better understood as a technical rather than a conceptual question. 
2The fxed effects discussed so far, which are interpreted at the level of observations, are consequently called “frst-level fxed effects” or “frst-level predictors”. 

22.2.2 Model Specifcation and Modeling Assumptions 
In this section, it is discussed how the specifcation of mixed models differs from that of fxed effects models, and that for each model with random effects there is an alternative model with only fxed effects. A major focus is on the question of when to use fxed and random effects. The amount of technicality and notation is kept at the absolute minimum. Particularly, the specifcation of models in mathematical notation is not always shown, and models are introduced in R notation. For an appropriate understanding of model specifcation, readers should consult a more in-depth text book, for example Part 2A of Gelman and Hill (2007) (pp. 235–342). Without any knowledge of the mathematical notation conventions, it is impossible to understand many advanced text books and much of the valuable advice available online. 
22.2.2.1 Simple Random Intercepts 
Readers with experience in fxed effects modeling should be able to see that a grouping factor encoding the verb lemma and all the other potential grouping factors discussed in the previous sections could be specifed as normal fxed effects in a GLM. This section introduces the main difference between the fxed-effect approach and the random-effect approach. Logistic regression examples are used throughout this section, and we begin with the fctional corpus study of the dative alternation introduced in Sects. 22.2.1.2 and 22.2.1.3. 
First, we specify a minimal model with only the LEMMA grouping factor and one other predictor, namely GIVENNESS, both as fxed effects. 
glm(Construction~1+Lemma+Givenness, 
data=alternation.data, 
family=binomial(link=logit)) 
In the case of logistic regression in alternation modeling, CONSTRUCTION is binary (levels 0 or 1, corresponding to the two alternants). Furthermore, LEMMA has m levels (one for each lemma), and GIVENNESS is also binary (levels 0 and 1, corresponding to “not given” and “given”). A model specifcation like this encodes a theoretical commitment to what the researcher thinks is the mechanism that determines which alternant is chosen. Concretely, it encodes the assumption that the probability of the outcome which is labeled as 1 can be predicted from the additive linear term specifed as 1+LEMMA+GIVENNESS. Because the infuence of the regressors on the outcome is not linear in many cases, the additive linear term is transformed through the link function, here given as the inverse logit function in the FAMILY parameter of the GLM function together with the specifcation of the distribution of the residuals (assumed to be binomial). For logistic regression models, we assume that the distribution of the prediction errors follows the binomial distribution. If another distribution (such as the Poisson distribution) and another link function (such as the logarithm, which is the default for Poisson models) is chosen, the specifcation of the model formula remains the same in R. 
In any type of GL(M)M, the additive linear term consists of a number of sub-terms which are simply added up. Each of these sub-terms (except for the intercepts) consists of the multiplication of the (estimated) coeffcient with an observed value of one of the variables. However, R notation for model for­mulæ simplifes the specifcation of the actual linear term. First of all, the 1 in Construction 1+Lemma+Givenness is R’s way of encoding the fact that an intercept is part of the model. An intercept is a constant sub-term to which all other sub-terms are added, and it can be seen as the reference value when all other sub-terms (corresponding to categorical or numeric regressors) assume 0 as their value. 
For binary regressors like GIVENNESS, the only coeffcient that is estimated directly encodes the value added to (in case of a positive coeffcient) or subtracted from (in case of a negative coeffcient) the linear term when the value of the regressor is 1 (in the example, when the referent is given). When the value of the regressor is 0 (for example, when the referent is not given), 0 is added to the intercept. The intercept thus encodes (among other things) something like a default for a binary regressor. If the default corresponds to, as in the example, non-givenness, phrases like “non-givenness is on the intercept” or “‘givenness equals zero’ is on the intercept” are often used. 
However, a grouping factor such as LEMMA is usually a categorical variable with more than two levels. In such a case, the m levels of the grouping factor are “dummy-coded”, and for all but one of these binary dummy variables, a coeffcient is estimated. Dummy coding is a way of encoding a categorical variable as a 
Table 22.4 Dummy coding 
Value of... 
of a categorical variable LEMMA with four levels, resulting in the binary dummy variables l1,l2,l3 
Lemma  l1  l2  l3  
1  0  0  0  
2  1  0  0  
3  0  1  0  
4  0  0  1  

number of binary variables (see Table 22.4), and R takes care of dummy-coding automatically. Because the frst of the m levels of the grouping factor is encoded by all dummy variables assuming the value 0, only m - 1 sub-terms are added to the model, which means that only m - 1 coeffcients have to be estimated. The frst level of the grouping factor is thus “on the intercept” and becomes the reference to which all other levels are compared.3 
To sum up and clarify, if in a given study LEMMA has four levels dummy-coded as l1, l2, l3, and GIVENNESS is binary and coded as g with g = 1ifthe referent is given, the formula corresponding to the R code above looks like (22.1) in mathematical notation, where the linear additive term is enclosed in [], each sub-term appears in (), and c encodes the choice of the two alternants. 
 
Pr(c = 1) = logit-1  a0 + (ßl1 · l1) + (ßl2 · l2) + (ßl3 · l3) + (ßg · g) (22.1) 
In plain English: the probability Pr that the alternant of the construction coded as 1 is chosen Pr(c = 1) is calculated as the inverse logit of the linear term. The linear term is just the sum of the intercept a0 and the measured values, each multiplied by its corresponding coeffcient labelled ß. 
In such a model, the effect of each verb lemma is treated as a fxed population parameter, just like the effect of givenness. In other words, the algorithm which estimates the coeffcients for the m - 1 dummy variables tries to fnd a fxed value for each of them without taking the variation between them into account. With many levels, this requires a lot of data, and levels for which only a few observations are available in the data set have very imprecise coeffcient estimates with large confdence intervals. 
This is where random effects come into play as an alternative. If we treat the same grouping factor as a random intercept, we let the intercept vary by group, i.e., each group is allowed to have its own intercept. Furthermore, we give the varying intercepts a (normal) distribution instead of estimating m - 1 fxed population parameters. This means that the group-wise intercepts are assumed to be normally 
3Choosing one dummy as a reference level is necessary because otherwise, infnitely many equivalent estimates of the model coeffcients exist, as one could simply add any arbitrary constant to the intercept and shift the other coeffcients accordingly. However, the estimator works under the assumption that there is a unique maximum likelihood estimate. This extends to any other appropriate coding for categorical variables. 
distributed around 0. This and nothing else is the conceptual difference between a fxed effect and a random effect.4 
In R, the model specifcation then is as follows, where 1| can be read as “an intercept varying by”. 
glmer(Construction~1+Givenness+(1|Lemma), 
data=alternation.data, 
family=binomial(link=logit)) 
The sub-term GIVENNESS remains the same as in the GLM specifcation above, and it is still treated as a fxed effect. The sub-term (1|LEMMA) encodes that an intercept will be added to the linear term depending on which lemma is observed. This is obvious in mathematical notation corresponding to the above R code as shown in (22.2). In addition to the overall intercept a0, there is another constant term a[Lemma], which is chosen appropriately for each level of LEMMA. 

-1  
Pr(c = 1) = logit a0 + a[Lemma]+ (ßg · g) (22.2) 
Crucially, instead of estimating a batch of m - 1 coeffcients for the levels of the grouping variable, a varying intercept (assumed to come from a normal distribution) is predicted for each of its m levels. Technically, the varying intercepts are predicted from their own linear model. All more complex model structures to be discussed below are extensions of this approach. 
22.2.2.2 Choosing Between Random and Fixed Effects 
One commonly given reason to use a random effect instead of a fxed effect is that “the researcher is not interested in the individual levels of the random effect” (or variations thereof). Such recommendations should be taken with a grain of salt. Gelman and Hill (2007, 245–247) summarise this as well as other diverging and par­
tially contradictory recommendations for what should be a random effect as found in the literature. They conclude that there is essentially no universally accepted and tenable conceptual criterion for deciding what should be a random effect and what a fxed effect. The author of this chapter agrees with their conclusion that random effects should be preferred whenever it is technically feasible. Understanding when it is technically feasible requires at least some understanding of two major points. First, the variance in the intercepts needs to be estimated if a random effect is used. Second, the random intercepts can be understood as a compromise between ftting separate models for each level of the grouping factor (“no pooling”) and ftting 
4There is one other practical difference. If models are used to make actual predictions (which is rarely the case in linguistics), a random effect allows one to make predictions for unseen groups. See Gelman and Hill (2007, 272–275). 
a model while ignoring the grouping factor altogether (“complete pooling”), see Gelman and Hill (2007, Ch. 12). 
As was stated above, the random intercepts are assumed to come from a normal distribution, and therefore the variance between them has to be estimated with suffcient precision. From the estimated variance and the data for a specifc group, the estimator predicts the “conditional mode” in a GLMM or the “conditional mean” in a LMM for that group (see Bates 2010, Ch. 1). The conditional mode/mean for a group is the value of the varying intercept for this group. It is the numerical value shown by R packages like lme4 for each level of a random intercept variable. This procedure, however, requires that the number of groups not be too low. As a rule of thumb, if there are fewer than fve levels, a grouping factor should be included as a fxed effect, regardless of its conceptual interpretation. Although one often fnds default recommendations telling practitioners to use a grouping variable for speakers as a random effect, it would be ill-advised to do so if there are exemplars from less than fve speakers in the sample. Along the same lines, the distinction between intersective and non-intersective adjectives (Sect. 22.2.1.1) is not a suitable grouping factor for use as a random effect because it has too few levels. Hence, if such a grouping factor is used in a model where it nests fner-grained groups of another factor (such as adjective lemmas), the nested factor with the low number of levels should be used as a second-level fxed effect rather than a nesting random effect (see Sect. 22.2.2.4). 
If the number of levels is reasonably large, the next thing to consider is the number of observations per level. Alternatives to using a random effect would be to estimate a separate model for each level of the grouping factor, or to include it as a fxed effect. In both cases the effects are not treated as random variables, and fxed coeffcients per group are estimated without taking the between-group variance into account. With a random effect, however, the conditional modes/ means are pulled (“shrunken”) towards the overall intercept (“shrinkage”). When the number of observations in a group is low, the conditional mode/mean is simply shrunken more strongly towards 0, predicting only a small deviation from the overall tendency.5 On the other hand, fxed effect estimates would become inexact and would probably be dismissed because of growing uncertainty in the estimate (large confdence intervals, high p-values) when the number of observations in a group is low. Thus, low numbers of observations in all or some groups are often detrimental for using fxed effects grouping factors. Random effects are much more robust in such situations because of shrinkage. On the downside, a conditional mode that was strongly shrunken (due to a low number of observations) cannot be distinguished straightforwardly from a conditional mode of a group which simply does not deviate a lot from the average tendency. For fxed effects, we have both a parameter estimate and a possible signifcance test, but for random effects, we only have the prediction 
5Shrinkage is thus stronger (and the conditional mode/mean is closer to 0) if there is less evidence that a group deviates from the overall tendency. The lower the number of observations per group, the less evidence there is. 
of the conditional mode/mean. However, so-called “prediction intervals” can be 
calculated for individual per-group intercepts, and we return to them in the following section. 
22.2.2.3 Model Quality 
Signifcance 
It is not adequate to do any kind of signifcance testing on the individual levels of the random effect because they are not estimates in the conceptual and technical sense.6 There are ways of calculating “prediction intervals” (which are not the same as confdence intervals) for conditional modes in order to specify the quality of the ft (see Sect. 22.3), but they should not be misused for talking about signifcance. The fact that we do not do signifcance tests for single levels of the grouping factor does, however, not mean that we are not interested in the individual conditional modes, which is proven by the fact that they are often reproduced in research papers, for example in the form of a plot. Also, we can still get a good idea of the per-group tendencies by looking at the conditional modes/means. Additionally, a random effect allows the researcher to quantify the between-group variance, which is not possible for fxed effects. 
Relevance of the Random Effects 
A related question is whether the inclusion of the random effect improves the model quality. It is recommended here to include all random effects as required by the design of the study (after having decided based on the criteria given in Sect. 22.2.2.2 whether they should be random rather than fxed). Only if they clearly (and beyond doubt) have no effect, practitioners should consider removing them. To check whether they have an effect, the estimated between-group variance is the frst thing to look at. If it is close to 0, there is most likely not much going on between groups, or there simply was not enough data to estimate the variance. In LMMs, it is possible to compare the residual (item-level) variance with the between-group variance to see which one is larger, and to which degree. If, for example, the residual variance is 0.2 and the between-group variance is 0.8, then we can say that the between-group variance is four times larger than the residual variance, which would indicate that the random effect has a considerable impact on the response. This comparison is impossible in GLMMs because their (several types of) residuals do not have the same straightforward interpretation as those of LMMs. 
6Again, we do not assume them to be fxed population parameters, which would be the case for true estimates such as fxed effects coeffcients. 
Furthermore, models can be compared using likelihood ratio (LR) tests. In such tests, a model including the random effect and a model not including it are compared, similar to LR tests for the comparison of fxed effects. Such pairs of models, where one is strictly a simplifcation of the other, are called “nested models” (not to be confused with “nested effects” discussed in Sect. 22.2.1.1). A sometimes more robust alternative to the ordinary LR test are parametric bootstrap tests. 
With all this, it should be kept in mind that it is never appropriate to make formal comparisons between a GLMM with a random effect and a GLM with the same factor as a fxed effect using any test or metric (including so-called information criteria such as Akaike’s or Bayes’). Informally comparing coeffcients of determination (R2) between such pairs of models is somewhat useful, as will be shown below. 
Quality of the Fit 
To measure how well a GLMM fts the data, any metric that is based on prediction accuracy can be used in the same way as with GLMs. For example, the rate of correct predictions on the data used for model estimation or cross-validation methods are appropriate. 
Coeffcients of determination (pseudo-R2) can be used to give some idea of the overall model ft. For GLMMs, Nakagawa and Schielzeth (2013) have proposed a method that distinguishes between “marginal” R2 (only fxed effects) and “condi­tional” R2 (fxed and random effects). This has become a de facto standard. In cases where an effect works well as a fxed or a random effect (for example, if it has between fve and ten levels with enough data points for each level), the marginal and conditional R2 measures for the GLMM converge in an expected way with Nagelkerke’s R2 for corresponding GLMs. The marginal R2 for a GLMM estimate is roughly the same as Nagelkerke’s R2 for a GLM estimate where the grouping factor is ignored. Also, the conditional R2 for a GLMM estimate is roughly the same as Nagelkerke’s R2 for a GLM estimate which includes the grouping factor as a fxed effect. 
22.2.2.4 More Complex Models 
Varying Intercepts and Slopes 
In Sect. 22.2.1.3, it was shown under which conditions a varying-intercept and varying-slope (VIVS) model might be useful. While it is possible to have just a varying slope, this is rarely useful, and we discuss only VIVS models. 
A random slope is a good choice when the strength or direction of some fxed effect varies by group. We extend the simple model from Sect. 22.2.2.1 to include random slopes for GIVENNESS varying by LEMMA using R notation. Each variable from the fxed effects part of the formula which we expect to vary by LEMMA is simply repeated before the | symbol. 
glmer(Construction~1+Givenness+(1+Givenness|Lemma), 
data=alternation.data, 
family=binomial(link=logit)) 
With this model specifcation, a fxed coeffcient for GIVENNESS will still be estimated. However, an additional value will be predicted for each lemma, and this value has to be added to the fxed coeffcient. In mathematical notation, this is very transparent, as shown in (22.3). The varying slope for GIVENNESS to be chosen appropriately for each LEMMA is specifed as ßg[Lemma]. 

-1  
Pr(c = 1) = logit a0 + a[Lemma]+ ((ßg + ßg[Lemma]) · g) (22.3) 
A source of problems in VIVS models is the fact that, in addition to the variance in the intercepts and slopes, the covariance between them has to be estimated. If in groups with a higher-than-average intercept, the slope is also higher than average, they are positively correlated, and vice versa. These relations are captured in the covariance. Technically speaking, the joint distribution of the random intercepts and the random slopes is assumed to follow a multivariate normal distribution with means, variances, and covariances to be estimated. The number of variance parameters to be estimated thus obviously increases with more complex model specifcations, and the estimation of the parameters in the presence of complex variance-covariance matrices requires considerably more data than estimating a single variance parameter. The estimator algorithm might terminate, but typi­cally covariance estimates of -1 or 1 indicate that the data was too sparse for a successful estimation of the parameter. In this case, the model is “over­parametrised” and needs to be simplifed (see Bates et al. 2015a; Matuschek et al. 2017). 
Nested and Crossed Random Effects 
The difference between nested and crossed random effects is only defned when there are two or more random effects. As it was explained in Sect. 22.2.1.1, nested random effects are appropriate tools when the levels of a grouping fac­tor are nested within the levels of another grouping factor. Technically, while varying slopes can be understood as interactions between a fxed and a random effect, nested random intercepts can be understood as interactions between two or more random effects. Crossed random effects are just several unrelated random effects.
In the model specifcation, there is no difference between a crossed and a nested random effect. Both are specifed like independent random effects. The following code provides an example in R notation. 
glmer(Construction~Givenness+(1|Lemma)+(1|Semantics), data=alternation.data, family=binomial(link=logit)) 
SEMANTICS could be a factor encoding the semantic classes which nest individual lemmas. It could also be a (crossed) grouping factor completely unrelated to the lemmas, for example encoding some semantic property of the whole sentence containing the construction. As was mentioned in Sect. 22.2.1.1, the question on the practitioner’s side is rather how the data are organised. If the data have a nested structure, the estimator will treat them as nested, otherwise as crossed. Data have a nested structure whenever each level of a (nesting) random effect can always be determined entirely from the levels of another (nested) factor. 
Second-Level Predictors 
In Sect. 22.2.1.2, situations were introduced where the random effects themselves can be partially predicted from second-level fxed-effects. In R notation, the true model structure is entirely blurred, and practitioners even run the risk of working with second-level predictors without realising it. 
We add a numeric second-level fxed effect which specifes the token frequency for each level of LEMMA in the following R code. 
glmer(Construction~1+Givenness+Lemmafrequency+(1|Lemma), 
data=alternation.data, 
family=binomial(link=logit)) 
This is the only way to specify second-level predictors in standard R notation. The data set has to be organised as shown in Table 22.3, where for each data point a level of LEMMA is specifed and the appropriate frequency value for this level of LEMMA is given in a separate column.7 R treats LEMMAFREQUENCY as a second-level predictor automatically under such conditions. Simply speaking, this means that the random intercept for LEMMA will now be predicted from its own linear model. In Sect. 22.3, a model with second-level effects will be used for illustration purposes. 
7There are, of course, elegant ways of pulling the frequency values from another data frame on the fy in R. 
Representative Study 1 
Wolk, C., Bresnan, J., Rosenbach, A. and Szmrecsanyi, B. 2013. Dative and genitive variability in Late Modern English: Exploring cross-constructional variation and change. Diachronica 30(3): 382–419. https://doi.org/10.1075/dia.30.3.04wol. 
Research questions 
The authors aim to achieve two things. First, they want to compare changes in two word order-related alternations in the history of English between 1650 and 1999: the dative alternation and the genitive alternation. They look for infuencing features shared in both cases as well as construction-specifc features. Second, they aim to show that historical data fts well into a probabilistic, cognitively oriented view of language. 
Data 
The authors use the ARCHER corpus (Biber et al. 1994), which contains texts from various registers from 1650 to 1999. For both constructions, carefully designed sampling protocols were used. For the annotation of the data, both available corpus meta data were used (text ID, register, time in fractions of centuries, centered at 1800) as well as a large number of manually coded variables (constituent length, animacy, defniteness, etc.). Furthermore, the possessor head lemma (genitive alternation) and the verb lemma (dative alternation) were coded. 
Method 
Two mixed effects logistic regression models are estimated. For the genitive alternation, the text ID and the possessor head lemma are used as crossed random effects. The authors state on p. 399 that they collapsed all head noun lemmas with less than four occurrences into one category because otherwise “diffculties” would arise. However, it is the advantage of random effects modeling that it can deal with a situation where categories have low numbers of observations (see shrinkage, Sect. 22.2.2.2). For the dative alternation, the model includes the text ID, the register (which nests the text ID) as well as the lemma of the theme argument and the verb. 
Results 
It is found that many factors have a shared importance in both alternations, e.g., defniteness, animacy, construction length. It is also argued that the observed tendencies – such as short-before-long and animate referents frst – are in line with synchronic corpus-based and experimental fndings about gen­eral cognitive principles underlying the framework of probabilistic grammar. These principles remain in effect, but the strength of their infuence changes over time. 
Representative Study 2 
Gries, S. 2015. The most underused statistical method in corpus linguis­tics: Multi-level (and mixed-effects) models. Corpora 10(1): 95–126. https://doi.org/10.3366/cor.2015.0068. 
Research questions 
The paper is programmatic in nature. The author re-analyses data from a previously published study on verb particle placement in English. He uses a GLMM instead of a fxed-effects logistic regression to show that including random effects in order to account for variation related to mode, register, and subregister increases the quality and predictive power of the model. He also argues that by not doing so, corpus linguists risk violating fundamental assumptions about the independence of the error terms in models. 
Data 
The data are 2,321 instances of particle verbs showing either verb–direct object–particle or verb–particle–direct object order, taken from the ICE-GB. The grouping factors derived from the structure of the corpus are mode (only two levels), register (fve levels), and subregister (13 levels). They are nested: mode nests register, which nests subregister. Additionally, verb and particle lemma grouping factors are annotated. Finally, two fxed effects candidates are annotated (the type of the head of the direct object and the logarithmised length of the direct object in words). 
Method 
The author uses the model selection protocol described in Zuur et al. (2009) to frst fnd the optimal random effects structure using ANOVAs and AIC comparisons as well as analyses of the estimated variance for single random effects. He then goes on to fnd the optimal fxed effects structure. Additionally, he compares the pseudo-R2 measure of the resulting mixed models. 
Results 
Gries fnds that the verb and particle lemma as well as the subregister play signifcant roles. The variance estimate for mode is close to 0 from the beginning of the model selection procedure. This is not surprising, as two levels are not nearly enough in order for the variance to be reliably estimated, and it should be used as a second-level predictor instead. The R2 values of the fnal model are high, with a large difference between marginal R2 = 0.57 and conditional R2 = 0.748, which indicates that the random effects improve the 
(continued) 
model ft. It is also shown that the classifcation accuracy is improved over that of a GLM without random effects, but differently for different lexical groups and subregisters. The paper thus shows that it is not appropriate to ignore lexical grouping factors and grouping factors derived from the corpus structure, especially as both are easy to annotate automatically. 
22.3 Practical Guide with R 
22.3.1 Specifying Models Using lme4 in R 
This section focuses on lme4, an often used package to do multilevel modeling in R with maximum likelihood methods (Bates et al. 2015b). 
22.3.1.1 Overview of the Data Set 
The data used here for illustration purposes comes from Schäfer (2018), where a binary case alternation in German measure phrases is modeled using data from the DECOW corpus (Schäfer and Bildhauer 2012). In the frst alternant, the kind-denoting noun (here Wein ‘wine’) is in the genitive as in (4a). In the second alternant, the kind-denoting noun is assigned the same case as the head measure noun as 
in (4b).  
(4)  a.  Wir trinken [[ein Glas]Acc [guten  Weins]Gen]Acc.  
we  drink  a  glass  good  wine  
We drink a glass of good wine.  
b.  Wir trinken [[ein Glas]Acc [guten Wein]Acc]Acc.  

The infuencing frst-level factors derived from theory-driven analysis and previous accounts comprise the numeric stylistic indicator variables Badness (a measure of document quality available for all DECOW documents; see Schäfer et al. 2013) and Genitives (a measure of the frequency of genitives), a binary variable Cardinal encoding whether the NP is modifed by a cardinal or not, and the three-level variable Measurecase encoding the case of the head noun. Furthermore, there are two crossed random intercepts for the kind noun (Kindlemma) and the measure noun (Measurelemma). These random intercepts come with second-level models including a number of fxed second-level effects. For Kindlemma, there are: Kindfreq (numeric, z-transformed), which encodes the lemma frequency; Kindgender (binary), which encodes the grammatical gender of the kind noun; Kindattraction (numeric, z-transformed), which encodes the infuence of neighbouring constructions. For Measurelemma, there are: Measurefreq and Measureattraction, which correspond to the simi­larly named variables for Kindlemma; Measureclass (fve-level categorical), which encodes the broad semantic class of the measure noun. Finally, the dependent variable Construction is coded as 1 if the genitive is used and as 0 if there is case identity. 
22.3.1.2 A Simple Varying Intercept Instead of a Fixed Effect 
Fitting and Evaluating the Model 
First, it is shown how a grouping factor can be specifed as a fxed or a random effect. The following is the standard glm() call to estimate a model with the measure lemma (150 levels) as a fxed effect. For illustration purposes, not all available regressors are used here. 
glm.01 <-glm(Construction~1 
+Measurelemma 
+Badness 
+Cardinal 
+Genitives 
+Measurecase, 
data=measure, 
family=binomial(link=logit)) 
The output of the summary(glm.01) command (not shown here) shows that the estimates for the 149 fxed effects corresponding to Measurelemma have extremely high standard errors and are virtually unusable. The Nagelkerke coeff­cient of determination for this model can be calculated using the NagelkerkeR2 (glm.01) function from the fmsb package, and it is 0.397. 
As the above example shows, grouping factors with many levels like Measurelemma are usually not suitable fxed effects. Hence, the following specifcation re-estimates the model as a GLMM using the glmer function with Measurelemma as a varying intercept. The glmer function is the standard function from the lme4 package for estimating mixed models. 
glmm.01 <-glmer(Construction~1 
+(1|Measurelemma) 
+Badness 
+Cardinal 
+Genitives 
+Measurecase, 
data=measure, 
family=binomial(link=logit)) 
The output of the summary(glmm.01) command is as follows (abbreviated). 
Random effects: Groups Name Variance Std.Dev. Measurelemma (Intercept) 1.252 1.119 
Number of obs: 5063, groups: Measurelemma, 150 
Fixed effects: Estimate Std. Error z value Pr(>|z|) 
(Intercept)  -2.32135  0.17867 -12.992  < 2e-16 ***  
Badness  -0.14065  0.04474  -3.144  0.00167 **  
CardinalNo  1.35673  0.13947  9.727  < 2e-16 ***  
Genitives  -0.73886  0.04239 -17.429  < 2e-16 ***  
MeasurecaseAcc -0.01923  0.08821  -0.218  0.82740  
MeasurecaseDat  0.25047  0.12045  2.079  0.03758 *  

R outputs the standard coeffcient table for the fxed effects including the overall intercept. Above this coeffcient table, there is a summary of the random effects. The number of groups for Measurelemma is correctly given as 150, and the variance in the random intercepts is 1.252.8 As a rule of thumb, the larger the variance between the intercepts, the larger are the differences between the groups. For the variance estimate, confdence intervals can be obtained with either one of the following commands, where the frst one uses the profle method (based on likelihood ratio tests) and the second one uses the parametric bootstrap, which is sometimes considered more robust.9 
confint(glmm.01, parm="theta_", method="profile") 
confint(glmm.01, parm="theta_", method="boot", nsim = 250) 
For the frst command, the output (95% confdence interval) is 0.887 and 1.414. Without applying formal signifcance testing, this is a reasonably narrow interval, and it does not extend to 0. Thus, the effect should remain part of the model specifcation. Beyond this, practitioners should not do model selection for random effects. 
8The variance-covariance matrix of glmm.01 can also be extracted directly using the 
VarCorr(glmm.01) command. 9Since the bootstrap (especially with smaller original sample sizes) tends to run into replications where the estimation of the variance fails and is thus returned as 0, the bootstrap interval is sometimes skewed towards 0 when the profle confdence interval frames the true value symmetrically. The bootstrap is thus not always more robust or intrinsically better. Comparing both methods is recommended. 
Single conditional modes (see Sect. 22.2.2.2) for the levels of the grouping factor can be extracted using the ranef command. The following command stores a list of conditional modes for Measurelemma in glmm.01.ranef. 
glmm.01.ranef <-ranef(glmm.01, condVar = TRUE, drop = TRUE)$Measurelemma 
If the options condVar = TRUE and drop = TRUE are passed as above, then conditional variance-covariance estimates are returned as attributes of the result. They have to be accessed using the attributes function as shown below. 
attributes(glmm.01.ranef)$postVar 
These can be used to construct prediction intervals around the predicted conditional modes in order to display them in tabular form or plot them. While some ready-made functions exist to plot them, it is good to have a custom plotting function. If the random effect has many levels, it might only be possible to plot a selection (random or informed) of the conditional modes, and there is no ready-made function which supports this. The R script accompanying this chapter contains a maximally simple example using only standard plotting functions which creates a plot with dots representing the estimate (measured on the y axis) and the prediction intervals as horizontal bars around the plot for a random subset of the conditional modes. An example is given in Fig. 22.2, where the width of the prediction intervals corresponds negatively to the number of exemplars observed in the different groups. Turning to the quality of the overall model ft, Nakagawa & Schielzeth’s coeff­cients of determination can be calculated with the r.squaredGLMM(glmm.01) command (from the MuMIn package). The output is as follows. 
R2m R2c 0.2004865 0.4209018 
This informs the user that the fxed effects cumulatively account for a proportion of 
0.200 of the total variance in the data. Taking also the random effect into account, the model explains a proportion of 0.421 of the total variance. The random effect thus appears to be relevant. Comparing the conditional R2 to the Nagelkerke R2 of the GLM with Measurelemma as a fxed effect (which was 0.397), we see that the difference is not substantial, although the individual coeffcient estimates in the GLM were unreliable. 
Measure lemma random effects with 95% prediction intervals 

-2 -1 0 1 2 3 
Prediction of conditional mode 
Furthermore, readers are encouraged to compare the estimates of the fxed effects for glm.01 (except Measurelemma) and for glmm.01.10 The fxed-effects coeffcient estimates (except for the intercept, which is heavily offset in glm.01)do not differ much between the GLM and the GLMM. However, the standard deviations (and consequently the confdence intervals as well as the p-values) change. 
Reporting the Results 
Journals and conferences in corpus linguistics do not enforce strict guidelines when it comes to reporting the results of GLMM fts. While a coeffcient table for the fxed effects is a de-facto standard for GLMMs just as much as for GLMs, there is 
10Again, the accompanying script contains all necessary code. 
no such de-facto standard as to how random effects should be reported. Everything that should be reported for a GLM should also be reported for a GLMM, such as the coeffcient table for the fxed effects (which should at least contain the coeffcient estimate, the standard error, and possibly bootstrapped confdence intervals) and variance infation factors (Fox and Monette 1992; Zuur et al. 2010). In addition, the present author recommends to report (either in the running text, in tabular form, or in the caption of the coeffcient table): 
1. 
the estimate of the random effect variance (and covariance) parameters 

2. 
(bootstrapped) confdence intervals for the above 

3. 
Nakagawa & Schielzeth’s R2 coeffcients of determination 

4. 
optionally all or some conditional modes with prediction intervals in tabular form or as a plot (see Fig. 22.2) 

5. 
(bootstrapped) p-values for random effects if absolutely necessary, and only if the model comparison is possible between nested GLMMs and does not involve the direct comparison of a GLM and a GLMM (see for example the PBmodcomp function from the pbkrtest package; Halekoh and Hsgaard 2014)11 


22.3.1.3 More Complex Models 
The glmer call as used in Schäfer (2018) is as follows. 
glmm.03 <-glmer(Construction~1 +(1|Measurelemma) # Random. +(1|Kindlemma) +Badness # Item-level. +Cardinal +Genitives +Measurecase +Kindattraction # Kind lemma level. +Kindfreq +Kindgender +Measureattraction # Measure lemma level. +Measureclass +Measurefreq, data=measure, family=binomial(link=logit), na.action = na.fail, control=glmerControl("bobyqa")) 
11This entails that GLMMs with only one simple random effect cannot be compared with a model without it, as such a model would be a GLM and not a nested GLMM. 
Since the model has a relatively high degree of complexity, the option control= glmerControl("bobyqa") is required. It selects a different optimiser (an algorithm used by the estimator). In general, BOBYQA optimisers are highly robust, and using a BOBYQA is the frst step to try when the estimator does not converge and convergence errors are reported at the console. 
In Schäfer (2018), the results are reported as follows (given here in adapted and abridged form)12: 
A multilevel logistic regression model was ft which models the infuence of the regressors on the probability that the genitive is chosen over case identity. The measure lemma and the kind noun lemma were specifed as varying-intercept random effects. The sample size was n=5,063 with 1,134 cases with the genitive and 3,929 cases with case identity. The intercept estimated at -3.548 comprises CARDINAL=YES,MEASURECASE=NOM, KINDGENDER=MASC,MEASURECLASS=PHYSICAL, and 0 for all numeric z-transformed regressors. The coeffcient table is shown in Table 22.5. Given the coding of the response variable, coeffcients leaning to the positive side can be interpreted as characterising a con­fguration typical of the genitive. I ran a parametric bootstrap (using the CONFINT.MERMOD function from LME4) with 1,000 replications and using the percentile method. The resulting 95% bootstrap confdence intervals are reported in Table 22.5 in columns “CI low” and “CI high”. For each regressor, a p-value pPB was obtained by dropping the regressor from the full model, re-estimating the nested model, and comparing it to the full model. A drop-in bootstrap replacement for the Likelihood Ratio Test in the form of the function PBMODCOMP from the PBKRTEST package (Halekoh and Hsgaard 2014) was used. Nakagawa & Schielzeth’s pseudo-coeffcients of determination are R2 = 0.409 (marginal) and R2 = 0.495 (conditional). The lemma intercepts have standard deviations of sMeasurelemma = 0.448 and sKindlemma = 0.604. Of all predictors, only KINDFREQ (pPB = 0.095) could be seen as slightly too high to be convincing, failing at sig=0.05. 
Finally, in order to illustrate the interpretation of the conditional modes and the fxed effects coeffcients in such a model, there is code in the accompanying script which extracts all relevant values and calculates a predicted value for item 99 (arbitrarily chosen for illustration purposes) from the measure data set. For example, the overall intercept can be extracted via the following command. 
coef(summary(glmm.03))['(Intercept)','Estimate'] 
To this intercept, the sub-terms for frst-level fxed effects are added. They can be calculated as follows, using Badness as an example. The result is 0.0183. 
coef(summary(glmm.03))['Badness', 'Estimate'] * measure[99, 'Badness'] 
12Notice that the results reported in the paper differ slightly from the sample script included with this chapter because the random number generator was in a different state. 
22 Mixed-Effects Regression Modeling 559 Table 22.5 Coeffcient table from Schäfer (2018) 
Model level  Regressor  pPB  Factor level  Coeffcient  CI low  CI high  
Observations  Badness  0.002  -0.152  -0.247  -0.061  
Cardinal  0.001  No  1.189  0.862  1.466  
Genitives  0.001  -0.693  -0.768  -0.592  
Measurecase  0.001  Acc  0.030  -0.150  0.212  
Dat  0.705  0.455  0.944  
Kindlemma  Kindattraction  0.020  0.225  0.049  0.393  
Kindfreq  0.095  0.146  -0.023  0.301  
Kindgender  0.001  Neut  0.021  -0.367  0.392  
Fem  1.269  0.800  1.709  
Measurelemma  Measureattraction  0.001  0.282  0.106  0.447  
Measureclass  0.001  Container  0.252  -0.265  0.788  
Rest  0.421  -0.209  1.063  
Amount  0.831  0.215  1.432  
Portion  1.217  0.675  1.684  
Measurefreq  0.005  -0.231  -0.363  -0.079  

In other words, we extract the fxed-effect coeffcient estimate for Badness and multiply it with the Badness value observed for item 99. 
In order to calculate the contribution of the second-level effects, which will be added to the overall intercept and the frst-level fxed-effects sub-terms, we frst need to extract the appropriate group-level intercept. The following code extracts the Kindlemma random intercept for item 99, which is -0.159 for the lemma Wasser ‘water’. 
ranef(glmm.03)$Kindlemma[ as.character(measure[99, 'Kindlemma']), '(Intercept)'] 
To these group-level intercepts, the second-level fxed-effects sub-terms are added, and they can be calculated very much like their frst-level equivalents. For example, the following code calculates the sub-term for Kindfreq, which is -0.044 (the z-transformed logarithmised frequency per one million tokens of Wasser)inthis case. 
coef(summary(glmm.03))['Kindfreq', 'Estimate'] * measure[99, 'Kindfreq'] 
All in all, the prediction for the Measurelemma second-level model is -0.027. For the Kindlemma second-level model, it is 0.125, and for the frst-level fxed-effects part of the model (including the overall intercept), it is -3.382. Added up, the linear term is predicted to be -3.284. This result needs to go through the inverse logit link function (implemented as invlogit in the car package, for example), which results in 0.036. Given the coding of the response variable, this means that the model predicts a probability of 0.036 that the genitive construction is chosen in the given example. Readers are advised to go through the full calculations in order to understand what the different numbers in their reported GLMMs represent. 
Further Reading 
Gelman, A. and Hill, J. 2007. Data analysis using regression and multilevel/hier­archical models. Cambridge: Cambridge University Press. 
Chapters 1–15 and Chaps. 20–24 of this book are a highly recommended read, especially for R and lme4 users. 
Zuur, A. F., Ieno, E. N., Walker, N., Saveliev, A. A. and Smith, G. M. 2009. Mixed effects models and extensions in ecology with R. Berlin etc.: Springer. 
This practical guide has a reputation among R users of mixed effects models in many felds. 
Bates, D. M. 2010. Lme4: mixed-effects modeling with R. http://lme4.r-forge.r­
project.org/lMMwR/lrgprt.pdf. 
Bates, D., Mächler, M., Bolker, B. and Walker, S. 2015. Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67(1): 1–48. https:// doi.org/10.18637/jss.v067.i01. 
These two are obligatory reads for users of lme4, (co-)authored by Douglas Bates, the author of lme4. 
References 
Bates, D. M. (2010). Lme4: Mixed-effects modeling with R. http://lme4.r-forge.r-project.org/ lMMwR/lrgprt.pdf. Bates, D., Kliegl, R., Vasishth, S., & Baayen, R. (2015a). Parsimonious mixed models. https://arxiv. org/abs/1506.04967. Bates, D., Mächler, M., Bolker, B., & Walker, S. (2015b). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01. 
Biber, D., Finegan, E., & Atkinson, D. (1994). Archer and its challenges: Compiling and exploring a representative corpus of historical english registers. In U. Fries, P. Schneider, & G. Tottie (Eds.), Creating and using english language corpora (pp. 1–13). Amsterdam: Rodopi. 
Fox, J., & Monette, G. (1992). Generalized collinearity diagnostics. Journal of the American Statistical Association, 87, 178–183. https://doi.org/10.2307/2290467. 
Gelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge: Cambridge University Press. 
Halekoh, U., & Hsgaard, S. (2014). A Kenward-Roger approximation and parametric bootstrap methods for tests in linear mixed models – the R package pbkrtest. Journal of Statistical Software, 59(9), 1–30. https://doi.org/10.18637/jss.v059.i09. 
Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing type I error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/ 10.1016/j.jml.2017.01.001. 
Nakagawa, S., & Schielzeth, H. (2013). A general and simple method for obtaining R2 from generalized linear mixed-effects models. Methods in Ecology and Evolution, 4(2), 133–142. https://doi.org/10.1111/j.2041-210x.2012.00261.x. 
Schäfer, R. (2018). Abstractions and exemplars: The measure noun phrase alternation in German. Cognitive Linguistics, 29(4), 729–771. https://doi.org/10.1515/cog-2017-0050. 
Schäfer, R., Barbaresi, A., & Bildhauer, F. (2013). The good, the bad, and the hazy: Design decisions in web corpus construction. In S. Evert, E. Stemle, & P. Rayson (Eds.), Proceedings of the 8th Web as Corpus Workshop (WAC-8) (pp. 7–15). Lancaster: SIGWAC. 
Schäfer, R., & Bildhauer, F. (2012). Building large corpora from the web using a new effcient tool chain. In N. C. (Chair), K. Choukri, T. Declerck, M. U. Doan, B. Maegaard, J. Mariani, A. Moreno, J. Odijk, & S. Piperidis (Eds.), Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12) (pp. 486–493). Istanbul: European Language Resources Association (ELRA). 
Schielzeth, H., & Forstmeier, W. (2009). Conclusions beyond support: Overconfdent estimates in mixed models. Behavioral Ecology, 20(2), 416–420. https://doi.org/10.1093/beheco/arn145. 
Zuur, A. F., Ieno, E. N., & Elphick, C. S. (2010). A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution, 1(1), 3–14. https://doi.org/10.1111/j. 2041-210x.2009.00001.x. 
Zuur, A. F., Ieno, E. N., Walker, N., Saveliev, A. A., & Smith, G. M. (2009). Mixed effects models and extensions in ecology with R. Berlin: Springer. 
Chapter 23 Generalized Additive Mixed Models 
R. Harald Baayen and Maja Linke 
Abstract In this chapter we introduce the Generalized Additive Model (GAM). GAMs enable the analyst to investigate non-linear functional relations between a response variable and one or more predictors. Furthermore, GAMs provide a principled framework for studying interactions involving two or more numeric predictors. GAMs are an extension of the generalized linear model, and can therefore be used not only for Gaussian response variables, but also for binomial and Poisson response variables (and many others). Corpus linguists will fnd GAMs useful for coming to a detailed understanding of nonlinear patterns in their data, which can range from historical change (Ellegård 1953) to the effects of corpus-based measures on acceptability ratings (Baayen and Divjak 2017). 
23.1 Introduction 
The generalized additive model (GAM) offers the analyst an outstanding regression tool for understanding the quantitative structure of language data. Generalized additive models were frst introduced in a monograph by Hastie and Tibsharini (1990). The book by Wood (2006), a revised and expanded version of which appeared in 2017, provides the reader with both a thorough mathematical treatment, and a large number of detailed examples. Many of these come from biology, where the analyst faces challenges very similar to those faced by the corpus linguist. If one is interested in the density of mackerel eggs in the Atlantic east of France and the British Isles, one is faced with data that are unevenly spread out over a large area, where the ocean varies in depth, the gulf stream is of variable strength, and average temperature changes as one moves from Brittany to Scotland. A linguist interested in 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_23) contains supplementary material, which is available to authorized users. 
R. H. Baayen (•) · M. Linke Department of Linguistics, University of Tingen, Tingen, Germany e-mail: harald.baayen@uni-tuebingen.de;maja.linke@uni-tuebingen.de 
© Springer Nature Switzerland AG 2020 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_23 
R. H. Baayen and M. Linke 
language use as it evolved in North America, as attested in the Corpus of Historical American English Davies (2010), similarly faces naturalistic data with a bewildering variety of properties. How the language changes over time varies with register, the education level of the writer, with the gender of the writer, with time, with social changes that come with immigration, and with technological developments. Crucially, one can hardly expect that effects of numerical predictors (henceforth covariates) will be strictly linear. Furthermore, covariates may interact nonlinearly with factorial predictors and with other covariates in ways that are diffcult or even impossible to predict before initiation of data analysis. 
Whereas a decade ago, bringing random effect factors into generalized additive models was not straightforward, recent versions of the mgcv package for R offer the analyst an excellent toolkit for dealing with multiple sources of noise relating to speakers and linguistic units (Wood 2017). Working with the mgcv package is also substantially facilitated thanks to the itsadug package (van Rij et al. 2017). 
Within linguistics, GAMs have been found useful in dialectometry and sociolin­guistics (Wieling et al. 2011, 2014), phonetics (Wieling et al. 2016; Tomaschek et al. 2018), psycholinguistics (Linke et al. 2017; Milin et al. 2017), cognitive linguistics (Divjak et al. 2017, in press; Baayen and Divjak 2017, in press) and historical linguistics (Baayen et al. 2017, in press). The goal of this chapter is to provide the reader with suffcient background to be able to understand the GAMs presented in these studies, and to start working with GAMs oneself. To this end, this chapter has three main parts, frst a general introduction into common use cases that beneft from the application of generalized additive models, followed by a practical introduction to working with GAMs and a non-technical introduction to how GAMs work. 
23.2 Fundamentals 
In an ordinary least squares regression model, a response yi is modeled as a weighted sum of p predictors and an error term that follows a normal distribution with zero mean. 
yi = ß0 + ß1xi1 + ß2xi2 + ...+ ßpxip + i,i ~ N(0,s). (23.1) 
Although the linear predictor .i, 
.i = ß0 + ß1xi1 + ß2xi2 + ...+ ßpxip, (23.2) 
may provide an adequate model for the functional relation between a response and its predictors, there are many cases in which the assumption of linearity is inadequate. Reaction times in lexical decision task, for instance, tend to decrease in a non-linear way as a function of words’ frequency of occurrence in corpora. Modeling a non-linear response function as if it were linear not only results in inaccurate predictions, but also in structured errors that depart from the modeling assumptions about the relation between the variance and the mean. For Gaussian models, for instance, the errors may show heteroskedasticity, and when this happens, the validity of signifcances reported by the linear model is no longer assured and p-values listed in model summaries will be unreliable. 
Consider, by way of example, Fig. 23.1, which graphs acceptability ratings on a 5-point Likert scale for Polish sentences against three predictors: the frequency of the verb, construction-verb reliance (the frequency of a verb × construction combination given the frequency of the verb), and rater generosity, which gauges the extent to which participants tend to prefer the higher end of the rating scale. The frst two predictors were transformed in order to avoid adverse effects of outliers. Figure 23.1 was obtained with ggplot, using its default method for visualizing nonlinear trends (geom_smooth). 
ggplot(polish, aes(LogFrequencyVerb, AcceptabilityRating))+ geom_smooth() # left panel of Fig. 1 
For each of the three panels, we observe departures from linearity. The left and center panels shows quite wiggly curves, and although the right panel reveals a nearly linear pattern, there is some leveling off for the highest values of the predictor. For two out of three predictors, a linear model appears to be inappropriate. 
Figure 23.1 illustrates a property of GAMs which requires special attention: For the diagnostic plots shown, we used the ggplot2 library default smoother geom_smooth, which defaulted to a smoothing method gam. The left and center panels of Fig. 23.1 are overly wiggly, suggesting that ggplot2’s default settings for smoothing are overftting and might actually not be appropriate for the Polish dataset. Although geom_smooth does provide a set of parameters to address this problem, adequate modifcation of the parameters is only feasible to an analyst equipped with a high level of understanding of the model and the data. 
Consequently, the goal of this chapter is to provide the reader with suffcient background to be able to understand the GAMs presented in these studies, to start exploring working with GAMs oneself, and to evaluate whether GAMs have 
4.0 
4 
3.5 3.0 
3.5



AcceptabilityRating
3 
2 
2.5 
3.0 
2.5 
1.5 
1.0 
2.0 
1 
Fig. 23.1 Smooths for acceptability ratings as a function of frequency (left), construction verb reliance (center), and rater generosity (right) using the default smoother of ggplot2, geom_smooth 
R. H. Baayen and M. Linke 
been used appropriately. What this chapter does not aim to do is provide analysis guidelines for reporting research results. Interpretation of models presented in this chapter requires a detailed understanding of the model, its implementation and a careful assessment of how both interact with the data set at hand. In what follows, we begin with recapitulating the basic concepts of the generalized linear model. Next, we introduce key concepts underlying the generalized additive model. We then present a worked example of how GAMs can be used to obtain a thorough understanding of the quantitative structure of linguistic data. 
23.2.1 The Generalized Linear Model 
Central to the generalized linear model is the idea that a response variable Yi for a datapoint i that is described by p predictors x1,x2,...,xp is a random variable. For real-valued response variables, we assume that the probability Pr(Yi = yi|xi1,xi2,...,xip)follows a normal distribution with variance s2 and mean .i: 
2
Pr(Yi = yi|xi1,xi2,...,xip) ~ N(.i,s ), (23.3) 
where the linear predictor .i is given by an intercept ß0 and a weighted sum of the p predictor values: 
.i = ß0 + ß1xi1 + ß2xi2 + ...+ ßkxip. (23.4) 
The means µi = .i are linear functions of x (see the left panel of Fig. 23.2). For each value of x, 20 randomly drawn values are shown. Note that the Gaussian model provides, for each value of x, the probability of the response. The most probable value is the mean. The scatter of the observed values around the mean is constant across the full range of the predictor. 
For count data, a Poisson model is often used, with the same linear predictor .i: 
Pr(Yi = m|xi1,xi2,...,xip) ~ Poisson(e.i). (23.5) 
Thus, the logarithm of the observed count is linear in the predictors. In this way, we ensure that predicted counts can never be negative. As can be seen in the center panel of Fig. 23.2, the expected counts themselves are a nonlinear function of x.The variance of the counts, which for Poisson random variables is equal to the mean, increases with x.
When the response variable is binary (as for successes versus failures, or correct versus incorrect responses), we are interested in the probability of a success, which we model as a binomial random variable with a single trial and a probability of success e.i/(1 + e.i), i.e., 
0.0 0.2 0.4 0.6 0.8 1.0 
p = exp(.)/(1+exp(.)) 
0 20406080 100 0 20406080 100 -40-20 0 20 40 
xxx 
Fig. 23.2 Gaussian (left), Poisson (center), and binomial (right) models. Across models, 20 random values were generated for each value of the predictor x. For the Gaussian model, the linear predictor is . = 300 +5x.For x = 60, mean (µi = .i) and 95% confdence interval are highlighted. For the Poisson model, the linear predictor (in red) is . = 3 +0.05x.The linear predictor specifes the logarithm of the Poisson parameter ..For x = 96, the mean of the predicted count (exp(.i)) is shown together with its 95% confdence interval. For the binomial model, the linear predictor (in red) is . =0.1+0.3x. The linear predictor specifes the log odds, the corresponding probabilities are shown in gray. The blue dots represent failures (0) and successes (1); jitter was added to bring out individual outcomes. The vertical gray line is at x =-0.33, here . =0and p =0.5. The horizontal gray line is at . =0.1, for which p =0.52 
 
.i
e 
Pr(Yi =1|xi1,xi2,...,xip) ~binom ,1 , (23.6)
.i
1 +e 
where the linear predictor .i again is defned exactly as above. In this case, the log odds (i.e., the logarithm of the ratio of successes to failures) is linear in the predictors. As can be seen in the right panel of Fig. 23.2, for binomial random variables, the variance is greatest for p = 0.5, which in this example is the case when x =-0.1/0.3 =-0.33. Here, we observe the greatest overlap (with respect to x) for (jittered) failures and (jittered) successes. 
The linear predictor is not restricted to expressing a “linear” functional relation between . and the predictors. For instance, the linear predictor 
.i =ß0 +ß1xi1 +ß2x 2 (23.7)
i1 
specifes a parabola rather than a straight line. In fact, very wiggly curves can be obtained by adding multiple powers of xas predictors. This is illustrated in Fig. 23.3. Instead of writing 
12 s
.i =ß0x 0 +ß1x +ß2x +...+ßsxi, (23.8)
ii 
we can state the model more succintly as 
s 
	 
.i = ßjxj =f(xi). (23.9)
i j=1 
R. H. Baayen and M. Linke 
y = 1 + 2x y = 1 + 2x - 3x2 y = 1 + 2x - 3x2 + 3x3 - 4x4 + 2x7 

xxx 
Unfortunately, when f(x) is set up as a polynomial of x, as we have done here, with the aim of using this polynomial as a predictor of the response, it turns out that doing so comes with substantial disadvantages. Polynomial functions require far too many parameters (the s weights ß), and they can perform very poorly when predicting the response for unseen data. In other words, polynomial functions overft one’s data, and cannot be used for prediction and generalization. This is where the generalized additive model comes in. 
23.2.2 The Generalized Additive Model 
The generalized additive model takes the linear predictor .i of the generalized linear model and enriches it with functions of one or more predictors, as, for instance: 
.i = ß0 + ß1xi1 + f1(xi2) + f2(xi3,xi4). (23.10) 
   
parametricpart non - parametricpart 
The parametric part is familiar from the generalized linear model. The non­parametric part specifes two functions, one of which takes x2 as argument, and one of which takes two predictors, x3 and x4, as arguments. Instead of using polynomial functions, GAMs use smoothing splines for functions such as f1 and f2. A smoothing spline with one predictor as argument is used for ftting wiggly curves. A smoothing spline with two predictors can be used for ftting a wiggly surface. Splines can take more than two arguments, in which case wiggly hypersurfaces are modeled. Given a linear predictor with appropriate smooths, this linear predictor can be used to model Gaussian response variables, or Poisson or binomial responses. GAMs can also accommodate ordinal responses as well as multinomial responses. 
In order to use GAMs appropriately, it is useful to have a general understanding of how smoothing splines work. Here, we illustrate one particular spline that is the default of the mgcv package (Wood 2017), the so-called thin plate regression spline. 
The lower right panel of Fig. 23.4 presents a thin plate regression spline smooth f(x)estimated for the effect of a predictor x on the response. This effect is known as the partial effect of x, as in models such as (23.10) there typically are many other predictors that also contribute to the predicted value of the response. The partial effect shown in the lower right panel is a weighted sum of the curves in the other 9 panels of Fig. 23.4. Such elementary curves are known as basis functions. 
The frst two basis functions (in the upper left) are straight lines that are completely smooth. For predictors that have a strictly linear effect, these two basis functions suffce. Each basis function is associated with a weight (shown in square brackets on the vertical axes). For straight lines, the weight for the frst basis function is the intercept, and the weight for the second basis function is the slope. 
In this example, the predictor has a nonlinear effect, so we need more basis functions than just the frst two. Figure 23.4 illustrates 7 additional basis functions, which become increasingly wiggly as we proceed from panel 3 to panel 9. The curve in panel 10 is the sum of all nine (weighted) basis functions. Thus, the vertical position of the black dot on the red curve in panel 10 is the sum of the vertical positions of the other black dots in panels 1–9 (the scaling on the vertical axes already incorporates the weighting). 
The exact mathematical form of the wiggly basis functions is determined by the total number of basis functions requested by the user. Below, we return to the practical question of how to choose the number of basis functions. Here, we proceed on the assumption that a suffciently large number of basis functions has been requested by the user. 
The question that arises at this point is how to avoid the situation in which we have so many basis functions that we edge too close to the datapoints and start ftting the model to noise rather than signal. In other words, we need to fnd a good balance between undersmoothing (resulting in too much wiggliness) and oversmoothing 
12345 
basis function [0] basis function [0.65]
-0.1 0.0 0.1 0.2 -0.022 -0.018 -0.014 -0.010 

basis function [0.14] basis function [-0.34]
-0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 
basis function [-0.02] basis function [0.17]
-0.004 -0.002 0.000 0.001 0.002 0.4 0.6 0.8 1.0 
basis function [0.41] basis function [-0.13]
-0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 -0.6 -0.4 -0.2 0.0 0.2 
partial effect basis function [-0.18]
0.0 0.5 1.0 1.5 0.00 0.05 0.10 0.15 0.20 0.25 0.30 
xxxxx 6 7 8 9 10 





xxxxx 
Fig. 23.4 A smooth using thin plate regression spline basis functions for the partial effect of a predictor x on the response. The frst 9 panels represent the basis functions, multiplied by their weights (given in square brackets on the vertical axes). The sum of these weighted basis functions results in the spline smooth in the lower right panel 
R. H. Baayen and M. Linke 
(resulting in missing out on signifcant wiggliness). The solution offered by GAMs is to balance two constraints, one constraint demanding that we want to stay faithful to the data (by minimising the summed of squared errors), and one constraint that demands we keep our model as simple as possible. This second constraint can be rephrased as a prior belief that the truth is likely to be less wiggly rather than very wiggly. This belief is a restatement of Occam’s razor, in that we don’t want to make our theory more complex (by adding basis functions and associated weights) than is absolutely necessary. The two opposing constraints lead to a model cost Cf for a smooth f , 
n  Cf = (yi - f(xi))2 + . f (x)2dx, (23.11) i=1 
   
faithfulnesstothedata Occam srazor 
that we want to keep as small as possible. This cost consists of two parts. To the left of the + we have the sum of squared deviations between the observed values yi and the values f(xi) that are predicted by the smooth. This is what an ordinary least squares regression model ftted with lm minimizes. To the right of the +, we have the integral of the squared second derivative of the smooth, weighted by a smoothing parameter .. The integral over the squared second derivative is a measure of the wiggliness of the smooth which we also want to keep as small as possible. The parameter . regulates the balance between the desire to remain faithful to the data and the desire to keep wiggliness down. 
The introduction of the smoothing parameter . raises a new question, namely, how to estimate .. A frst step towards a solution is to assume that the weights of the basis functions follow a normal distribution with mean zero and some unknown standard deviation ss . It turns out that the choice of . co-determines ss . This leads to the second step, namely, to choose some ., sample from the (normal) distribution of weights for the smooth implied by ., and keep tuning . until an optimal ft is obtained. This typically results in weights for a smooth that are smaller than if . were zero, i.e., when there is no penalization for wiggliness and all that counts is faithfulness to the data. This method has been shown to also yield good estimates for confdence intervals (Nychka 1988; Marra and Wood 2012). 
Importantly, penalization for wiggliness can reduce the weights of basis functions to zero, in which case the pertinent basis functions are apparently unnecessary. For instance, when a smooth is ftted to data for which the functional relation between the response and a predictor is truly linear, all the weights for the wiggly basis functions in Fig. 23.4, i.e., the basis functions in panels 3–9, are driven to zero, leaving untouched only the completely smooth (i.e., completely non-wiggly) frst two basis functions. These two basis functions jointly determine a straight line. The horizontal basis function is merged into the intercept specifed in the parametric part of the model, to ensure that the model remains identifable.1 Thus, the only weight of the smooth that remains is that for the slanted line. This weight is simply the slope of the regression line. 
When the functional relation between response and a predictor is in fact non­linear, penalization will retain at least some wiggly basis functions, but with weights that are reduced compared to an unpenalized smooth with . = 0. The proportion of the original weight of a basis function that is retained after penalization is referred to as the effective degree of freedom (edf) of that basis function. The sum of the effective degrees of freedom of all basis functions used to construct a smooth constitutes the effective degrees of freedom of that smooth. Summary tables for the smooth terms in a GAM list these effective degrees of freedom, which enter into a special F -test that is used to evaluate the signifcance of a smooth. 
The edf of a smooth cannot be larger than k, the number of basis functions that is set by the user. If the edf for a smooth is close to k, hardly any penalization has occurred, and it is likely that a larger value of k should be chosen (see the documentation of choose.k of mgcv for detailed discussion). When penalization leaves a predictor with 1 edf, its effect is likely to be linear: All wiggly basis functions will have been taken out of commission by setting their weights to zero, and only the weight for the second basis function is retained (i.e., the slope of the regression line). 
GAMs also accommodate random effect factors as predictors. The way in which this is done is different from the method used by the linear mixed model as implemented in the lme4 package. The GAM implementation that we discuss here makes use of the mechanism of penalization to estimate the parameters of random-effect factors, using a so-called ridge penalty. This ridge penalty takes the sum of the absolute values of the random-effect coeffcients, and seeks to keep this sum as small as possible. In this way, parameters are shrunk towards zero, just as in the linear mixed effect model. However, computation times for mixed GAMs (henceforth GAMMs) are typically longer than for the corresponding models ftted with lme4, which is due to GAMMs not making any simplifying assumptions about the structure of the random effects. 
Proper inclusion of random effects in the model specifcation protects against overly wiggly curves. Recall that Fig. 23.1, obtained with ggplot2’s default smoother, produced highly wiggly and undulating curves for two out of three predictors. However, once Verb is included as random-effect factor, the partial effects of the predictors become much less wiggly (compare the left and right panels of Fig. 23.5). A highly wiggly curve with narrow confdence bands is replaced by a shallow curve with wide confdence bands. The fact that the horizontal axis is included in the 95% confdence band for nearly all values of verb frequency indicates that a main effect of verb frequency is highly unlikely to be signifcant. 
1If there are two specifcations for the intercept in the model, an arbitrary amount d can be taken from the one and added to the other without changing model predictions; in this case there are infnitely many models too choose from, with no principled reason for selection. 
R. H. Baayen and M. Linke 

6 8 10121416 6 8 10121416 
LogFrequencyVerb LogFrequencyVerb 
Fig. 23.5 Thin plate regression spline smooths for log verb frequency as predictor of acceptability ratings. Left panel: a model without by-verb random intercepts, right panel: a model that does include by-verb random intercepts 
Indeed, the p-value for this smooth provided by the model summary (not shown) is 0.22. 
The reason that the smooth in the model without a random effect for Verb produces such a wiggly curve is that for each verb frequency we have as many repetitions as there are subjects. As a consequence, each distinct frequency value comes with substantial evidence that stands in the way of proper penalization. By including random intercepts for Verb, the idiosyncracies of individual verbs can be taken into account, and for the present example, the evidence for an undulating effect due to frequency evaporates. 
Representative Study 
Baayen, R. H. and Divjak, D. (2017). Ordinal GAMMs: a new window on human ratings. In Thoughts on Language: Studies in Cognitive Linguistics in Honor of Laura A. Janda., Makarova, A., Dickey, S. M., and Divjak, D. S., 39–56. Bloomington: Slavica Publishers. 
Research questions 
Divjak (2016) investigated the extent to which speaker’s experience con­tributes to the acceptability ratings for infnitival and fnite that-complements in Polish. Baayen and Divjak (2017, in press) reanalyzed these data using ordinal GAMs. Key questions are whether the frequency of occurrence of the verb (in the Polish National Corpus), and the conditional probability of the construction given the verb (reliance) are predictive for acceptability ratings. 
(continued) 
Rater generosity (i.e., the extent to which a rater is prone to give high ratings) was included as a control variable. 
Data 
The data set contains off-line acceptability ratings for verbs that occur with low frequency in that-constructions. A total of 95 verbs in that-constructions was presented to 285 undergraduate students of English/German in Poland. Participants were asked to rate “how Polish a sentence sounds” on a 5-point Likert scale. Each verb was responded to by 15 participants. 
Method 
Ratings elicited on a Likert scale yield ordinal data. Baayen and Divjak (2017, in press) therefore used an ordinal GAM, which models the probability that a rating is r (r = 1,2,...,5) through the probability that a latent variable u falls into the r-th interval defned on the real axis. Three models were compared: a model with (nonlinear) main effects only, a model with all pairwise interactions, and a model with a three-way interaction. 
polish.gam = gam(AcceptabilityRating ~ te(RankConstructionVerbReliance, LogFrequencyVerb, 
RaterGenerosity) + s(Verb, bs = "re") , data = polish, family = ocat(R = 5)) 
Results 
Of the three models, the model with the three-way interaction, ftted with a tensor product smooth, provided the best ft to the ratings. Subjects with lower rater generosity showed stronger effects of frequency and reliance. Furthermore, ratings decreased for increasing frequency when reliance was low, and ratings increased only with reliance for high-frequency verbs (see the inset contour plot below). The GAM analysis succeeded in bringing together within one model a series of fndings that Divjak (2016) could account for only in part with an ordinal linear model. 
R. H. Baayen and M. Linke 
23.3 Practical Guide with R 
The dataset that we use to illustrate how to work with GAMs is taken from the Buckeye corpus of conversational American English as spoken in Columbus, Ohio (Pitt et al. 2005). In what follows, we restrict ourselves to the data of one speaker (S40). For this speaker, we compared the words as realized by this speaker with the corresponding dictionary pronunciations. For the frst word uttered, allright,the dictionary form (in ARPAbet notation) is aa l ray t. The speaker actually pronounced ao r eh t. For each word, we extracted its successive diphones for the dictionary form as well as those of the form actually produced (ObsDiphone), and checked for each dictionary diphone (DictDiphone) whether it was absent in the actual realization, irrespective of its position in the word. For a particular utterance of allright, results look as follows. 
DictDiphone DictDiphonePosition ObsDiphone DictDiphoneAbsent 
1  #aa  1  #ao  TRUE  
2  aal  2  aor  TRUE  
3  lr  3  reh  TRUE  
4  ray  4  eht  TRUE  
5  ayt  5  t#  TRUE  
6  t#  6  - FALSE  

This information was collected for all words, in the order in which they appear in the corpus, resulting in a table with 27062 observations, one for each diphone. For each of these 27062 diphones, we considered the following variables: DictDiphoneAbsent, with values TRUE or FALSE, depending on whether the dictionary diphone was realized by the speaker; this is the response variable for our analyses; DictDiphonePosition, an integer indicating the position of the dictionary diphone in the word; DictDiphoneCount, an integer with the number of dictionary diphones in the word; PhraseInitial, with values TRUE or FALSE, indicating whether the word carrying the diphone is phrase-initial; PhraseFinal, with values TRUE or FALSE, indicating whether the word carrying the diphone is phrase-fnal; PhraseLength, an integer indicating the length in words of the phrase; PhraseRate, the speech rate (number of syllables per second); LogDuration, the logarithm of the duration of the word (in seconds); DictDiphoneActDiversity, a measure, based on discriminative learning, gauging the lexical uncertainty caused by the diphone; WordActDiversity, a measure gauging the lexical uncertainty of the carrier word in a semantic vector space model derived with discriminative learning; SemanticTypicality, the extent to which the semantic vector of the carrier word is similar to the average semantic vector; and CorpusTime, the position of the diphone in the corpus (ranging from 1 to 27062) but scaled and centered to make this variable commensurable with the other numeric predictors. A detailed description of these predictors is available in Tucker et al. (2018). 
load("23_GAMM.RData") head(buckeye) # variables for ‘allright’ 
PhraseInitial PhraseFinal PhraseLength WordActDiversity 1 TRUE TRUE 1 0.05973968 2 TRUE TRUE 1 0.05973968 3 TRUE TRUE 1 0.05973968 4 TRUE TRUE 1 0.05973968 5 TRUE TRUE 1 0.05973968 6 TRUE TRUE 1 0.05973968 
SemanticTypicality Count PhraseRate LogDuration DiphonePosition 
1  0.0401964  6  5.12885  -0.9417342  1  
2  0.0401964  6  5.12885  -0.9417342  2  
3  0.0401964  6  5.12885  -0.9417342  3  
4  0.0401964  6  5.12885  -0.9417342  4  
5  0.0401964  6  5.12885  -0.9417342  5  
6  0.0401964  6  5.12885  -0.9417342  6  

DictDiphoneActDiversity CorpusTime DictDiphoneAbsent 1 2.5053406 -1.540405 TRUE 2 2.4392060 -1.540301 TRUE 3 0.3853351 -1.540198 TRUE 4 1.9609812 -1.540095 TRUE 5 2.1347266 -1.539991 TRUE 6 2.1825928 -1.539888 FALSE 
Note that many variables are ‘piece-wise’ constant by word. For allright, the only variables (out of 12) that are not repeated 6 times (once for each diphone in the dictionary pronunciation) are DictDiphonePosition, DictDiphoneActDiversity, CorpusTime, and the response variable, DictPhoneAbsent. 
23.3.1 A Main-Effects Model 
We begin with ftting a standard logistic model in which the log odds is assumed to vary linearly with the numeric predictor variables. We use the bam function from the mgcv package (Wood 2017) (version 1.8.26), but exactly the same results are obtained with the glm function of base R. 
R. H. Baayen and M. Linke 
m1 = bam(DictDiphoneAbsent ~ PhraseInitial + PhraseFinal + PhraseLength + PhraseRate + LogDuration + DictDiphoneCount + DictDiphonePosition + WordActDiversity + SemanticTypicality + DictDiphoneActDiversity + CorpusTime, data = buckeye, family = "binomial") 
With the exception of PhraseRate (p = 0.0704), all predictors receive good support (the model summary is available in the supplementary materials). The AIC for this baseline model is: 
AIC(m1) [1] 29939.64 
The assumption that a covariate has a strictly linear effect may be true, but it may also be unjustifed. Often, exploratory data analysis will be required to establish whether, and for which variables, the linearity assumption is inappropriate. The following model relaxes the linearity assumption for all covariates using the s smoothing function from mgcv. The amount of wiggliness that a smooth allows for is controlled by the number of basis functions k, which has 10 as default value. This default is not motivated theoretically, and hence is a heuristic starting point. What is important is that k has to be set to an integer value (the ‘dimension’ of the smooth) that is large enough. How large an initial value of k should be depends on the number of different values of the predictor for which a spline is required. If there is only a handful of different values, one may want to set k to 3 or 4. If there are thousands of different values, a possible value would be 200. 
For the numeric predictors in the present data, we proceed as follows. We have two counts with a limited range, DictDiphoneCount (7 distinct values) and DictDiphonePosition (8 distinct values). The dimension of a smooth should be lower than the number of distinct values, so we choose k = 5. For PhraseRate (1171 distinct values), LogDuration (5842 distinct values), PhraseLength (29 distinct values, we take the logarithm of this variable to reduce its rightward skew), DictDiphoneActDiversity (604 distinct values), WordActDiversity (825 distinct values), and SemanticTypicality (825 distinct values) we go with the default. CorpusTime, however, has no less than 27062 distinct values, and the default value of k therefore comes with the risk of oversmoothing. We therefore set k to 100. 
buckeye$LogPhraseLength = log(buckeye$PhraseLength) 
m2 = bam(DictDiphoneAbsent ~ PhraseInitial + PhraseFinal + s(DictDiphoneCount, k = 5) + s(DictDiphonePosition, k = 5) + s(LogPhraseLength) + s(PhraseRate) + s(LogDuration) + s(DictDiphoneActDiversity) + s(WordActDiversity) + s(SemanticTypicality) + s(CorpusTime, k = 100), 
data = buckeye, family = "binomial") 
It is noteworthy that by allowing predictors to have nonlinear effects, we have obtained a substantially improved ft: 
AIC(m2) [1] 27722.93 
with a decrease in AIC of no less than 2216.7. 
Figure 23.6 presents the partial effects of the covariates. These partial effects are centered around zero, and represent deviations from the group means defned by the factorial predictors (here PhraseInitial and PhraseFinal) when other covariates are held constant at their most typical value. In other words, the partial effect of a term in the model specifcation is the contribution of that term to the model predictions. The command 
plot(m2, pages=1) 
presents all smooths in the model in a one-page multipanel fgure. Figure 23.6 also presents the partial effects, but uses customized code (available in the supplementary materials) that adds histograms or density plots and that also highlights, by means of vertical red lines, the 5, 25, 75, and 95 percentiles of the predictors. 
Panels 1 and 2 of Fig. 23.6 indicate that there is no effect of PhraseLength and PhraseRate: the X-axis (the horizontal red line) is within the 95% confdence interval for the full range of predictor values. The effects of all other predictors are non-linear. 
123 
partial effect partial effect partial effect 
-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 

partial effect partial effect partial effect 
-2 -1 0 1 2 3 -2 -1 0 1 2 3 -1.5-1.0-0.50.0 0.5 1.0 1.5 

partial effect partial effect partial effect 
-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 

0.00.51.01.52.02.53.03.5 0 2 4 6 8 10 12 14 2 3 4 5 6 7 8 
LogPhraseLength PhraseRate DictDiphoneCount 456 



1 2 3 4 5 6 7 8 -4-3-2-1 0 1 2 3 0.00.51.01.52.02.53.0 
DictDiphonePosition LogDuration DictDiphoneActDiversity 789 


0 1 2 3 -1.5 -1.0 -0.5 0.0 0.5 1.0 
WordActDiversity SemanticTypicality CorpusTime 
-0.1 0.0 0.1 0.2 0.3 0.4 
Fig. 23.6 Partial effects of predictors in a GAM (m2) for the log odds of diphone deviation for speaker 40 in the Buckeye corpus 
The log odds of deviation from the dictionary norm increases with the number of diphones, but levels off for words with more than 5 diphones (panel 3). Panel 4 clarifes that the later the position of the diphone in the word is, the greater the log odds of deviation. Apparently this effect reverses for positions 7 and 8 (upper center panel). Here, however, data are sparse. 
Unsurprisingly, the log odds of diphone deviation decreases with word duration (panel 5). As documented in detail by Johnson (2004), segment and even syllable deletion is common in this corpus, and as words that do not have deletions typically will be longer, a negative trend for the bulk of the data is expected. However, the distribution of durations has a few large-valued outliers, which give rise to the upward swing in the right-hand side of the plot. Due to the sparsity of data, the confdence intervals are wide. The reason that these outliers nevertheless are taken seriously by the GAM is that the same (log) duration is repeated as many times for the six words with log duration exceeding zero as these words have diphones. As a consequence, this handful of words has stronger support than just the small number of words would suggest. 
We see here an important advantage of GAMs over models that force the effect of duration to be linear. In such models, outliers may exert high leverage on the regression, and typically have to be removed from the data set. By contrast, the GAM clarifes that outliers behave differently, highlights the associated uncertainty with wide confdence intervals, and at the same time does not let the outliers infuence conclusions about the effect of a predictor for the bulk of the data. In other words, GAMs provide the full picture, and protects the analyst against models based on fattened and simplifed data. 
The distribution of DictDiphoneActDiversity (panel 6) has a long tails. Here, we fnd an S-shaped curve. For the interquartile range (the center 50% of the data highlighted by the center vertical red lines), we observe an increase in the log odds with increasing activation diversity. The effect goes back to zero, however, for the frst and third quartiles. Strongly undulating patterns are likewise visible for WordActDiversity (panel 7) and SemanticTypicality (panel 8). 
The wiggliness of these curves is diffcult to interpret theoretically. For cases such as these, the analyst has two options. The frst option is to accept that these undulations are real, and that our theoretical understanding is too limited, or that our predictor is theoretically fawed. The second option is to reduce the dimension of the smooth. For the present data, such a reduction has some justifcation because of the abovementioned problem that lexical and phrasal variables are constant within words, which is a problem that often arises when working with observational data from corpora. Since data points are not independent in the way one would like them to be, some conservatism with respect to nonlinearity is justifed. When the model is reft with k set to 5, the functional form of these effects becomes much simpler and easier to understand, as we shall see below (Fig. 23.7). Simpler curves come at the cost of a reduction in the quality of the ft (difference in AIC: 454), but the model remains far superior to the model imposing linearity on the relation between the response and the predictors (difference in AIC: 1762.73). 
The effect of Corpus Time in the lower right panel is quite wiggly, but as we are dealing with a predictor with 27062 distinct values, and as we have no a-priori hypothesis about how deviation probabilities might change in the course of the interview, we accept the smooth as providing a description of real changes over time in diphone deviation behavior. 
23.3.2 A Model with Interactions 
Thus far, we have considered models with main effects only. In this section, we consider interactions involving numerical covariates. There are two basic types of interaction: an interaction of a covariate with a factor, and an interaction of two covariates. First consider the interaction of a numerical predictor with a factorial predictor such as PhraseFinal. PhraseFinal has two levels (TRUE/FALSE), and an interaction of PhraseFinal with a covariate, say SemanticTypicality, requests two smooths for this covariate, one for phrase-fnal words and one for words that are not phrase-fnal. We request the two curves from the bam function with the by directive in the call to s: 
1234 
partial effect -2-10 1 2

partial effect -2-10 1 2

partial effect -2-10 1 2

partial effect -2-10 1 2


Word Act Diversity Word Act Diversity Word Act Diversity Word Act Diversity 
Fig. 23.7 Partial effects for interactions in a GAM for the log odds of diphone deviation for speaker S40 in the Buckeye corpus. In the contour plots, dotted green lines indicate 1 SE up, and dashed red lines 1 SE down from a contour line (panels 5 and 7); in panels 6 and 8, darker shades of blue indicate lower values, and darker shades of yellow higher values 
s(SemanticTypicality, k = 5, by = PhraseFinal) 
An update of model m2 that includes several interactions, 
m4= bam(DictDiphoneAbsent ~ PhraseInitial+ PhraseFinal+ s(DictDiphoneCount, k = 5) + s(DictDiphonePosition, k = 5) + s(LogPhraseLength) + s(LogDuration) + te(WordActDiversity, PhraseRate, 
k = 5, by = PhraseFinal) + s(SemanticTypicality, k = 5, by = PhraseFinal) + s(DictDiphoneActDiversity, k = 5, by = PhraseFinal) + s(CorpusTime, k = 100), data = buckeye, family = "binomial") 
AIC(m4) 
[1] 28103.35 
and which offers an improved ft (AIC: 28103.35) compared to model m3 (AIC: 28176.91). The partial effects of m4 are presented in Fig. 23.7. Panels 1 and 2 indicate that the effect of SemanticTypicality is stronger in phrase-fnal position. The effect of this variable appears to be present primarily across its fourth quartile. Panels 3 and 4 reveal an effect of DictDiphoneActDiversity that is U-shaped for the bulk of the data points. The largest effect is again present for diphones in words that are in phrase-fnal position. The downward swing for low activation diversity in the left of panel 4 appears due to a small number of outliers. 
The bottom panels of Fig. 23.7 illustrate the three-way interaction of this model, which involves two covariates, WordActDiversity and PhraseRate, and one factor, PhraseFinal. This interaction was specifed with the model term 
te(WordActDiversity, PhraseRate, by = PhraseFinal, k = 5) 
Here, te requests a tensor product smooth, which estimates a wiggly surface that we visualize with contour plots. In these contour plots, just as in geographical maps indicating terrain height, contour lines connect points with the same partial effect. There are two ways in which the contour map can be shown: one in which 1 SE confdence regions are added (panels 5 and 7), and one in which color coding is used to represent the magnitude of the partial effect (panels 6 and 8). In panels 5 and 7, dotted green lines are 1 SE up from their contour lines, and dashed red lines are 1 SE down. In panels 6 and 8, darker shades of blue indicate lower values, and darker shades of yellow, higher values. With the directive by=PhraseFinal,we requested two wiggly surfaces, one for diphones in words that are not phrase-fnal (panels 5 and 6), and one for diphones in phrase-fnal words (panels 7 and 8). In panels 6 and 8, contour lines are 0.2 units apart. Comparing the color shadings, it is clear that effects are much stronger in phrase-fnal position. Comparing panels 5 and 7, it is also clear that 1 SE confdence regions are considerably tighter in phrase-fnal position. Unlike panels 6 and 8, panels 5 and 7 are informative about where there is a signifcant gradient. In panel 5, for instance, confdence regions of adjacent contour lines begin to overlap for high phrase rates, indicating the absence of a signifcant effect. 
For understanding contour plots, it can be useful to trace changes in the value of the response with imaginary lines that are parallel to the axes. For instance, for PhraseRate to have an effect, contour lines should be crossed when moving in parallel to the y-axis. For words that are not phrase fnal, this does not happen for low values of WordActDiversity. It is only for higher values of this activation measure that an effect becomes visible, with larger phrase rates indexing reduced log odds of diphone deviation. When we consider imaginary horizontal lines, we cross more contour lines for low phrase rates than for high phrase rates, indicating that there is a stronger gradient up for WordActDiversity when PhraseRate is relatively low. It is noteworthy that for phrase-fnal words, the 
R. H. Baayen and M. Linke 
Table 23.1 Summary table of model m4 ftted to the log odds of diphone deviation for speaker 40 in the Buckeye corpus of American English as spoken in Columbus, Ohio 
A. parametric coeffcients  Estimate  Std. Error  t-value  p-value  
(Intercept)  -0.8919  0.0184  -48.4523  <0.0001  
PhraseInitialTRUE  -0.2342  0.0767  -3.0548  0.0023  
PhraseFinalTRUE  0.5695  0.0795  7.1608  <0.0001  
B. smooth terms  edf  Ref.df  F-value  p-value  
s(DictDiphoneCount)  3.9681  3.9992  807.0225  <0.0001  
s(DictDiphonePosition)  3.9465  3.9980  767.0938  <0.0001  
s(LogPhraseLength)  1.0001  1.0002  0.0456  0.8310  
s(LogDuration)  7.3465  8.3297  1491.4192  <0.0001  
te(WordActDiversity,PhraseRate):PhraseFinalFALSE te(WordActDiversity,PhraseRate):PhraseFinalTRUE s(SemanticTypicality):PhraseFinalFALSE  3.4963 5.1997 3.6784  3.8133 6.0181 3.9381  75.7274 101.2842 485.0057  <0.0001 <0.0001 <0.0001  
s(SemanticTypicality):PhraseFinalTRUE  3.3121  3.7477  160.0053  <0.0001  
s(DictDiphoneActDiversity):PhraseFinalFALSE s(DictDiphoneActDiversity):PhraseFinalTRUE  3.9554 3.8722  3.9985 3.9880  297.9520 56.4501  <0.0001 <0.0001  
s(CorpusTime)  34.2773  42.5874  714.6877  <0.0001  

effect of PhraseRate reverses, such that higher phrase rates predict increasing instead of decreasing log odds of diphone deviation. 
Table 23.1 provides a summary of model m4, obtained by applying R’s general summary function to the model object (summary(m4)). The upper part of the table provides the statistics familiar from the generalized linear model for the parametric part of the model. The lower part of the table provides an evaluation of the signifcance of the smooth terms, using tests that are described in Wood (2013a,b). In Table 23.1, LogPhraseLength is associated with 1 effective degree of freedom (edf), suggesting a linear effect of this predictor. However, from the high p-value (0.79) it is clear that the slope of this regression line is effectively zero. Signifcant linear effects will show up with 1 edf and a low p-value. To obtain an estimate of the actual slope of a regression line, the model can be reftted without the smooth, in which case the slope will be listed in the parametric part of the model. An important property of GAMs is that if a predictor has a truly linear effect, the algorithm will discover this, and remove all wiggliness, leaving a straight line. GAMs only admit wiggliness where wiggliness is truly justifed. 
Table 23.1 lists summary statistics for six smooths, two smooths for each of the three interactions with PhraseFinal. The p-values for these smooths inform us about whether these individual smooths are likely to be just a fat horizontal line, or a fat surface. Importantly, the table does not inform us about whether the interaction itself is signifcant. In other words, the situation is the exact parallel of a linear model with a two-level factor and a covariate that is specifed in R as 
Fig. 23.8 The difference Difference TRUE - FALSE curve, given model m4,for SemanticTypicality 
for words that are and that are not in phrase-fnal position 

formula(Response ~ Factor+ Factor:Covariate) 
The summary of this model informs us about whether the two regression lines for the two levels of the factor have slopes that differ signifcantly from zero, but it does not tell us whether there are signifcant differences between the slopes. To assess whether there truly is an interaction in a GAM, a possible frst step is to plot the difference curve with the plot_diff function from the itsadug package (van Rijetal. 2017), as shown in Fig. 23.8. 
library(itsadug)} 
plot_diff(m4, 
view = "SemanticTypicality", 
comp = list(PhraseFinal = c("TRUE", "FALSE"))) 
When this difference curve is added to the effect of SemanticTypicality for diphones in words that are not phrase-fnal (the reference level), the curve is obtained for its effect in phrase-fnal position. Consistent with the signifcant main effect for SemanticTypicality in Table 23.1, the 95% confdence interval of the difference curve does not include the horizontal axis: the log odds of diphone deviation is consistently higher for phrase-fnal words. Given the large effect for greater values of SemanticTypicality and the relatively 
R. H. Baayen and M. Linke 
constrained confdence interval, it is clear that the interaction is unlikely to be reducable to just a main effect. We check this by taking model m4, replacing the term s(SemanticTypicality, k=5, by=PhraseFinal) by the term s(SemanticTypicality, k=5), and comparing the goodness of ft of this new, simpler model (m5, model not shown) with that of m4, the more complex model, using the compareML function from the itsadug package. 

compareML(m4, m5) 
Model Score Edf Difference Df p.value 1 m5 38537.64 29 2 m4 38507.85 31 29.795 2.000 1.148e-13 
Although m4 requires 2 more effective degrees of freedom, these additional degrees of freedom enable it to bring the fREML score down by 29.8. The small p-value indicates that the increase in goodness of ft outweights the increased complexity of the model. 
23.3.3 Random Effects in GAMs 
It is straightforward to include random effects in generalized additive models ftted with mgcv. By-subject random intercepts are requested with s(subject, bs="re") (notation in lme4: (1|subject)). By-subject random slopes for a covariate are specifed as s(covariate, subject, bs="re") (lme4: (0+covariate|subject)). For factors, s(factor, subject, bs="re") directs the model to estimate, for each subject, random sum contrasts (lme4: (1|factor:subject)). Hence, no separate term for by-subject random intercepts should be requested. The variance components of a GAMM and associated confdence intervals are obtained with gam.vcomp. Unlike lme4, mgcv does not offer tools for modeling correlation parameters for random effects. 
For corpus data, a random effect factor such as Word can cause serious problems for the analyst. Recall that in the present dataset predictors at the word level are repeated in the dataset for each of a word’s diphones. One might think that adding by-word random intercepts would alleviate this problem. Technically, we can add the model term s(Word, bs="re") to m4, resulting in a new model, m6 (not shown) that appears to provide an improved ft (for instance, AIC is down by 3470.3). However, of the 829 word types, 383 occur once only (46.2%). As a consequence, nearly half of the words have only one occurrence but are predicted by no less than three factorial variables: PhraseInitial, PhraseFinal, and a random intercept. In addition, there are several covariates that will further be specifc to a given word, such as LogDuration and WordActDiversity. Thus, we have far too many predictors to one observation. As a consequence, model m6 is severely overspecifed. 
The adverse effects of this overspecifcation become apparent when we consider the concurvity of the model. Concurvity is a generalization of co-linearity, and causes similar problems of interpretation, in the sense that when concurvity is high, it is diffcult to say which variables are driving the model’s predictions. As when co-linearity is present, concurvity can also make estimates somewhat unstable. The concurvity function of mgcv provides several measures of concurvity, each of which is bounded between zero and one. Values close to or equal to 1 indicate there is a total lack of identifability. The index we consider here, which Wood describes as in some cases potentially too optimistic, is based on the idea that a smooth can be decomposed into a part g shared with other predictors, and a part f that is entirely its own unique contribution. The greater part g is compared to part f, the greater the concurvity. The observed index of concurvity is based on the square of the ratio of the Euclidian lengths of vectors g and f evaluated at the observed values of the predictors. 
When we extract this measure from the output of concurvity(m6),we obtain the concurvity values shown in Fig. 23.9 in blue. The same fgure shows, in red, the concurvity values of the corresponding terms of model m4, the model that is otherwise identical, except that m4 lacks by-word random intercepts. Concurvity values for model m6 are higher across the board, and are extremely and unacceptably high for DictDiphoneCount and for the interactions of WordActDiversity and SemanticTypicality by PhraseFinal. 
It is clear that m6 is an overspecifed model that must be simplifed. We there­fore completely remove PhraseFinal and PhraseInitial from the model specifcation, as this will attenuate the adverse consequences of hapax legomena occurring with only one value for these predictors. After further simplifcation, model m7, with good support for all predictors, is obtained, the concurvity values of which are presented in Fig. 23.9 in green. Concurvity is now much reduced. 
m7 = bam(DictDiphoneAbsent ~ WordActDiversity + s(DictDiphonePosition, k = 5) + s(LogDuration) + s(DictDiphoneActDiversity, k = 5) + s(CorpusTime, k = 100) + s(Word, bs = "re"), data = buckeye, family = "binomial", discrete=T) 
Given that linguistic covariates tend to be tightly correlated, and especially so for observational data from corpora that have not been hand-curated to minimize variation in specifc dimensions, model m7 appears to keep concurvity within bounds that are perhaps reasonable. Unfortunately, m7 confronts us with another problem: the random intercepts it has estimated for Word should follow a Gaussian 
R. H. Baayen and M. Linke 
m6 m4 m7 
observed concurvity 0.0 0.2 0.4 0.6 0.8 1.0 

s(DictDiphoneCount)s(DictDiphonePosition) s(LogPhraseLength) s(LogDuration) te(WordActDiversity,PhraseRate):PhraseFinalFALSE te(WordActDiversity,PhraseRate):PhraseFinalTRUE s(SemanticTypicality):PhraseFinalFALSE s(SemanticTypicality):PhraseFinalTRUE s(DictDiphoneActDiversity):PhraseFinalFALSE s(DictDiphoneActDiversity):PhraseFinalTRUE s(CorpusTime) 


s(Word) 
Fig. 23.9 Observed concurvity for models m6 (blue), m4 (red), and m7 (green). Model m7 does not have interactions with PhraseFinal, its concurvity value for DictDiphoneActDiversity is shown for the PhraseFinal=FALSE interaction term in the other two models 
distribution, but as shown by Fig. 23.10, they fail to do so. The tails of the observed distribution are too short for it to be Gaussian. That a Gaussian random effect is not really appropriate for the present data is also apparent from the fact that the by-word random intercepts can be predicted from SemanticTypicality, DictDiphoneCount, and WordActDiversity (all coeffcients positive, all p 0.0001, adjusted R-squared 0.263). In other words, what should be random noise is in fact structured variation. Part of the problem is the Zipfan distribution of words’ frequencies: Random-effect factors with a Zipfan frequency distribution are not well suited for modeling with Gaussian random effects (Douglas Bates, p.c.). 
As George Box famously said, “all models are wrong, but some are useful” (Box 1976). Model m7, although far from perfect, is perhaps useful in two ways. First, it clarifes that there is substantial variation tied to individual words. Second, it is useful as a strong adversarial attach on the predictors of model m4 that are 
Fig. 23.10 A quantile-quantile plot of the by-word random intercepts of model m7 shows marked departure from normality 
effects -3 -10123 
-3-2-10 1 2 
Gaussian quantiles 
tied to the word. The survival of WordActDiversity in m7, albeit only as a linear effect, is, from this perspective, an index of its robustness. At the same time, model m4, although also not perfect, provides much more insight into how words’ properties (rather than sublexical properties) may co-determine the log odds of diphone deviation. Because m4 is a logistic model, there is no assumption that the errors of this model should be Gaussian and that they should be independently and identically distributed. Nevertheless, model m4 is overly optimistic because the observations from which it is constructed are not independent but, as shown by the analysis of concurvity, cluster by word in linguistically meaningful ways. 
One important new kind of random effect that mgcv makes available is the factor smooth. In a model with covariates with linear effects, it is possible that regression lines for individual subjects differ both with respect to their slopes and with respect to their intercepts. The nonlinear counterpart of this situation is that subjects have their own wiggly curves. Factor smooths implement such wiggly random effects. For example, 
s(CorpusTime, Speaker, bs = "fs", m = 1) 
requests wiggly curves for log odds as a function of CorpusTime for all speakers in the corpus. Figure 23.11 illustrates by-speaker factor smooths in the Buckeye corpus. Each line represents a speaker. Some speakers show considerable wiggliness, whereas other speakers show only small local ups and downs. As the curves have their own intercepts, in a model with factor smooths, no separate term for by-speaker random intercepts should be included. 
As is the case for random effects in the linear mixed model, the factor smooths are subject to shrinkage. Importantly, factor smooths are set up in such a way that if there is no wiggliness, horizontal straight lines are returned. In this case, the model has become a model with just straightforward random intercepts. 
R. H. Baayen and M. Linke 
In the linear mixed model, a model with by-subject random intercepts as well as by-subject random slopes will provide population estimates for intercept and slope. In the case of factor smooths, it is possible to request both a general, speaker-independent smooth, together with by-speaker factor smooths. 
s(CorpusTime) + s(CorpusTime, Speaker, bs = "fs", m = 1) 
In this case, mgcv issues a warning, as in general multiple smooths for the same covariate should be avoided. For this special case, however, this warning can be ignored (Simon Wood, p.c.). For large datasets, it should be kept in mind that estimating factor smooths for large numbers of speakers or words can be computationally very expensive. 
It is both an empirical and a theoretical issue whether a separate smooth that is supposed to be common to all speakers, such as s(CorpusTime) in the above specifcation, is really necessary and makes sense. Is it theoretically justifed to expect that when speakers go through a one-hour interview, there truly should be a common way in which the diphones they realize in their speech deviate from the standard language? If not, perhaps the main effect for CorpusTime should be removed. 
It is noteworthy that the interpretation of the individual curves estimated by a factor smooth is different from that of random intercepts and slopes in the linear mixed model. The individual curves provide an estimate of how a given speaker went through her/his interview, but how the same speaker would behave in a replication interview is unlikely to be a variation of the same curve with greater or smaller amplitude. Instead, the expectation is that the speaker will show a similar amount of wiggliness, but with ups and downs at different moments in time. 
Thus, GAMs not only offer the analyst new possibilities for understanding complex relations in large volumes of data, they also confront us with new 
S22 S30 S40 

CorpusTime CorpusTime CorpusTime 
challenges. For instance, in Fig. 23.11, speaker S40 shows substantial fuctuations in the log odds of diphone deviation. Such large deviations are unlikely to be due to just chance, and require further explanation and further refection on the temporal dynamics of language use. 
23.3.4 Extensions of GAMs 
The toolkit of smoothing splines (see the documentation for smooth.terms for an overview of the many different splines that mgcv implements) is available for Gaussian models, as well as for Poisson and Binomial responses, using the family directive familiar from the generalized linear model (glm). GAMs allow for a more complex linear predictor ., but otherwise the linear predictor is used exactly as in the generalized linear model, as summarized in Sect. 23.2.1. When the residuals of a Gaussian model follow a t-distribution rather than a normal distribution, the family directive can be set to scat, which requests a scaled t model for the residuals. For multinomial logit models, the family directive is set to multinom, and for the modeling of ordinal response variables, family is set to ocat.The documentation for scat, multinom, and ocat provides further detail on these extensions of the generalized additive model and their use. 
Further Reading 
As model interpretation and model criticism with GAMMs require a high level of understanding of both the method and the theoretical concepts it builds on, it is advisable to engage in a deeper exploration of the issues at hand prior to applying the models in a productive research environment. Here, we provide an overview of key readings, that might come in handy to the analyst exploring various types of data and uncovering and addressing effects commonly found in human response data. 
Wood, S. N. (2017). Generalized Additive Models. Chapman & Hall/CRC, New York. 
This book is a standard reference on GAMs, provides a necessary background in linear models linear mixed models and generalized linear models and introduction to the theory and applications of GAMs, complemented by a wide range of exercises. 
Baayen, R. H., van Rij, J., de Cat, C., and Wood, S. N. (2018). Autocorrelated errors in experimental data in the language sciences: Some solutions offered by generalized additive mixed models. In Speelman, D., Heylen, K., and Geeraerts, D., editors, Mixed Effects Regression Models in Linguistics, 49–69. Springer, Berlin. 
R. H. Baayen and M. Linke 
The article illustrates on three data sets how human factors like within-experiment learning or fatigue may interact with predictors of interest, both factorial and metric, and demonstrate why ftting maximally complex models is not an advisable strategy, especially within the framework of the generalized additive mixed effects model. 
Wieling, M. (2018). Analyzing dynamic phonetic data using generalized addi­tive mixed modeling: A tutorial focusing on articulatory differences between L1 and L2 speakers of English. Journal of Phonetics, 70:86–116. 
This paper offers a hands-on tutorial, including the original data and all R commands, for analysing dynamic time series data on the example of articulator trajectories observed using electromagnetic articulography. The paper leads the reader through the steps of data exploration, visualization, modeling of complex interactions and model criticism, introducing a wide variety of techniques and strategies with a detailed and comprehensive rationale for the modeling decisions, offering the reader an opportunity to replicate the analyses and gain more under­standing about the material. 
References 
Baayen, R. H., & Divjak, D. (2017, in press). Ordinal GAMMs: A new window on human ratings. In Makarova, A., Dickey, S. M., & Divjak, D. S. (Eds.), Thoughts on language: Studies in cognitive linguistics in honor of Laura A. Janda. Bloomington: Slavica Publishers. 
Baayen, R. H., Tomaschek, F., Gahl, S., & Ramscar, M. (2017, in press). The Ecclesiastes principle in language change. In Hundt, M., Mollin, S., & Pfenninger, S. (Eds.), The changing English language: Psycholinguistic perspectives. Cambridge: Cambridge University Press. 
Box, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71, 791–799. Davies, M. (2010). The Corpus of Historical American English (COHA): 400+ million words, 1810–2009. Divjak, D. (2016). The role of lexical frequency in the acceptability of syntactic variants: Evidence from that-clauses in Polish. Cognitive Science. https://doi.org/10.1111/cogs.12335:1--29. 
Divjak, D., Milin, P., & Baayen, R. H. (2017, in press). A learning perspective on individual dif­ferences in skilled reading: Exploring and exploiting orthographic and semantic discrimination cues. Journal of Experimental Psychology: Learning, Memory and Cognition. 
Ellegård, A. (1953). The auxiliary do: The establishment and regulation of its use in English. 
Stockholm: Almquist & Wiksell. Hastie, T., & Tibsharini, R. (1990). Generalized additive models. London: Chapman & Hall. Johnson, K. (2004). Massive reduction in conversational American English. In Spontaneous 
Speech: Data and Analysis. Proceedings of the 1st Session of the 10th International Symposium, Tokyo (pp. 29–54). The National International Institute for Japanese Language. Linke, M., Broeker, F., Ramscar, M., & Baayen, R. H. (2017). Are baboons learning “orthographic” representations? probably not. PLOS-ONE, 12(8), e0183876. Marra, G., & Wood, S. N. (2012). Coverage properties of confdence intervals for generalized additive model components. Scandinavian Journal of Statistics, 39, 53–74. Milin, P., Feldman, L. B., Ramscar, M., Hendrix, P., & Baayen, R. H. (2017). Discrimination in lexical decision. PLOS-One, 12(2), e0171935. 
Nychka, D. (1988). Bayesian confdence intervals for smoothing splines. Journal of the American Statistical Association, 83, 1134–1143. 
Pitt, M., Johnson, K., Hume, E., Kiesling, S., & Raymond, W. (2005). The Buckeye corpus of conversational speech: Labeling conventions and a test of transcriber reliability. Speech Communication, 45(1), 89–95. 
Tomaschek, F., Tucker, B., & Baayen, R. H. (2018). Practice makes perfect: The consequences of lexical profciency for articulation. Linguistic Vanguard, 4, 1–13. 
Tucker, B. V., Sims, M., & Baayen, R. H. (2018). Opposing forces on acoustic duration. Manuscript, University of Alberta and University of Tingen. 
van Rij, J., Wieling, M., Baayen, R. H., & van Rijn, H. (2017). itsadug: Interpreting time series and autocorrelated data using GAMMs. R package version 2.3. 
Wieling, M., Montemagni, S., Nerbonne, J., & Baayen, R. H. (2014). Lexical differences between Tuscan dialects and standard Italian: Accounting for geographic and socio-demographic variation using generalized additive mixed modeling. Language, 90(3), 669–692. 
Wieling, M., Nerbonne, J., & Baayen, R. H. (2011). Quantitative social dialectology: Explaining linguistic variation geographically and socially. PLoS ONE, 6(9), e23613. 
Wieling, M., Tomaschek, F., Arnold, D., Tiede, M., Brer, F., Thiele, S., Wood, S. N., & Baayen, 
R. H. (2016). Investigating dialectal differences using articulography. Journal of Phonetics, 
59, 122–143. 
Wood, S. N. (2006). Generalized additive models. New York: Chapman & Hall/CRC. 
Wood, S. N. (2013a). On p-values for smooth components of an extended generalized additive 
model. Biometrika, 100, 221–228. Wood, S. N. (2013b). A simple test for random effects in regression models. Biometrika, 100, 1005–1010. Wood, S. N. (2017). Generalized additive models. New York: Chapman & Hall/CRC. 
Chapter 24 Bootstrapping Techniques 
Jesse Egbert and Luke Plonsky 
Abstract Bootstrapping is a statistical technique that relies on randomly sampling with replacement from a set of observed values. Bootstrapping makes it possible to measure the accuracy and reliability of sample estimates and is often recommended for small samples and samples with unknown or non-normal distributions. In corpus linguistics, bootstrapping has also been proposed as a method for quantifying the degree of homogeneity in a corpus sample, for validation of statistical results, and as a methodological step in random decision forests, an advanced classifcation method. However, to date bootstrapping techniques have seldom been used with corpus data. We argue in this chapter that bootstrapping is underused in corpus linguistics, and that quantitative corpus linguists would do well to add this tool to their repertoire. This chapter includes an introduction to the fundamentals—both conceptual and practical—of bootstrapping methods. We address several applica­tions of bootstrapping, including the measurement of sample estimate accuracy, the validation of statistical models, the estimation of corpus homogeneity, and random forests. We include an overview of two representative studies that have successfully used bootstrapping techniques with corpus data. Finally, we demonstrate how to perform bootstrapping on corpus data using R, and how to visualize and interpret the results. 
24.1 Introduction 
Bootstrapping is a method of simulating a sampling distribution for a parameter of interest and estimating its accuracy. This is done through resampling with replace­ment from an observed data set (see Sect. 24.2.1). Bootstrapping can improveon 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_24) contains supplementary material, which is available to authorized users. 
J. Egbert (•) · L. Plonsky Northern Arizona University, Flagstaff, AZ, USA e-mail: Jesse.Egbert@nau.edu; Luke.Plonsky@nau.edu © Springer Nature Switzerland AG 2020 593 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_24 
J. Egbert and L. Plonsky 
our ability to measure the accuracy and reliability of sample estimates (e.g. Gries 2006) and is often recommended for small samples and samples with unknown or non-normal distributions (see Gries 2013; Plonsky et al. 2015; Egbert and LaFlair 2018). In corpus linguistics, bootstrapping has also been proposed as a method for quantifying the degree of homogeneity in a corpus sample (e.g. Gries 2006), for validation of statistical results (e.g. Baayen 2008), and as a methodological step in advanced classifcation methods, such as random decision forests (e.g. Szmrecsanyi et al. 2016b). However, to date bootstrapping has seldom been used with corpus data. 
We argue in this chapter that bootstrapping is underused relative to its potential in corpus linguistics, and that quantitative corpus linguists would do well to add this tool to their repertoire. We begin this chapter by introducing the fundamentals— both conceptual and practical—of bootstrapping methods. We then present an overview of several applications of bootstrapping that have been used in prior corpus linguistics research. For each of these methods, we describe the underlying assumptions that must be met, the types of data used, and a general overview of the methodological steps. We then include two study boxes that provide an overview of two representative studies that have successfully used bootstrapping techniques with corpus data. Next, we demonstrate how to perform bootstrapping on corpus data using R, and how to visualize and write about the results for publication. We conclude with a summary of the chapter and some comments on potential applications of bootstrapping in corpus linguistics that have yet to be attempted. 
24.2 Fundamentals of Bootstrapping 
24.2.1 Objectives and Methods 
The overarching goal of statistical analysis is to estimate parameters (e.g. mean, standard deviation) of a population by measuring those parameters in a sample. In most cases, population parameters are unknowable. However, they can be estimated with a certain degree of confdence from an observed sample drawn from the population. In most cases, the observed data in a sample provides the best possible insight into the population parameters. If we can assume that a sample is representative of a given population, it is safe to assume that (a) values observed in the sample are possible in the population, and (b) the distribution of those values in the sample can be used to estimate their distribution in the population. Although we can get a parameter estimate from this sample, we have no way of measuring the accuracy of our estimate without knowing the sampling distribution. A sampling distribution is the theoretical probability distribution for a parameter of interest that accounts for the full range of samples of the same size that we could have drawn from the population. For example, we can calculate the mean rate of occurrence for nouns in a single corpus sample of size n. However, if this sample is biased in some way, as most samples are, then our estimated rate of occurrence (i.e. mean) for nouns will be different from the actual rate of nouns in the population. Assuming that the entire population of texts is inaccessible, our next best option for measuring the degree of bias in our current sample is to collect additional samples of size n from the population and measure the mean rate of nouns in those corpora. The mean rates for nouns drawn from every possible sample of size n that can be drawn from the population constitute the sampling distribution for this parameter. Since the actual value for our parameter depends on the population, which is unknown, we are unable to estimate the accuracy of our parameter estimate. Thus, sampling distributions can offer crucial insights (at least theoretically) into the nature of the population. 
Moreover, we are often constrained in what we can learn about a population from a sample because of several limitations: small sample size, unknown or non-normal distribution, the presence of statistical outliers, and the effects of potential model overftting (see Sect. 24.2.2). While this seems to paint a dismal picture regarding the use of samples in inferential statistics, developments in statistical theory and computation have provided workable solutions for these challenges. One of these solutions is bootstrapping. Bootstrapping can be used for a variety of purposes, such as quantifying uncertainty, estimating statistical distributions, measuring homogeneity/heterogeneity across samples, validating statistical models, and protecting against statistical threats (e.g. model overftting, sensitivity to extreme values). Explanations and examples of these applications of boostrapping in corpus linguistics are included in Sect. 24.2.2. 
Bootstrapping is a statistical technique that relies on randomly resampling with replacement from a set of observed values in order to estimate the accuracy of statistical parameters. Bootstrapping relies on principles of frequentist statistical theory and computational power to simulate the sampling distribution of a statistical parameter (Efron and Tibshirani 1993: 395). The logic behind bootstrapping is as follows. The population distribution (F) for a parameter of interest (T) may be unknown, but we do have the sample distribution (Fˆ ) we observed in our data. One way of estimating the accuracy of parameter estimate for T is to simulate a sampling distribution (F*)for T by drawing many samples (with size n) through repeatedly re-sampling with replacement from Fˆ . This process results in a large number of samples that are variants of Fˆ and which are assumed to be drawn from F.We can then measure T in each of these samples and generate a simulated sampling distribution for T. Decades of extensive research has shown that this distribution will approximate how T would vary across many independent samples drawn from F (e.g., Chernick 1999;Efron 1992; Efron and Gong 1983; Efron and Tibshirani 1986, 1993). Thus, we can use F* to measure the accuracy of our parameter estimate for T, based on a sample size of n, in the form of a standard error or confdence interval (cf. Chap. 20 for an introduction to confdence intervals). 
Both authors of this chapter have taught and presented on bootstrapping methods in numerous classrooms and conferences. In our experience, it seems to be a natural reaction for people to be skeptical when frst introduced to bootstrapping. However, this skepticism is typically allayed once it is made clear what bootstrapping actually 
J. Egbert and L. Plonsky 
can and cannot do. To that end, we now turn to a discussion of what bootstrapping does not do, as well as what it does do. 
1. 
Bootstrapping does not fx sampling problems. Observations about an unrepre­sentative sample are not generalizable to the target population, and bootstrapping cannot change that. 

2. 
Bootstrapping does not increase the n size of a sample. Each bootstrapped sample has the same number of observations as the original sample. Thus, it cannot artifcially increase statistical power (i.e. the probability of correctly rejecting a null hypothesis). 

3. 
Bootstrapping does not automatically shrink the confdence interval for a sample. Bootstrapped confdence intervals are more accurate, but not necessarily nar­rower. The width of a confdence interval is based on a sample’s n size and the standard deviation for the parameter of interest, neither of which are changed during the process of bootstrapping. 


To summarize these three points: bootstrapping cannot provide better data or more data from the population than what is contained in the sample. No researcher who understands the theory and methods of bootstrapping would claim that it does these things. Unfortunately, some of these misconceptions seem to persist. Despite its limitations, bootstrapping can provide a wealth of information about the distribution of a sample. 
1. 
Bootstrapping does estimate the sampling distribution for statistical parameters by simulating the data sets that might be drawn from the population over many repeated samples (see, e.g., Chernick 1999,7). 

2. 
Bootstrapping does provide a reliable measure of accuracy for those parameters 


(i.e. more accurate confdence intervals) (Efron 1987). 
3. Bootstrapping does provide an accurate estimate of the validity of a statistical model (Baayen 2008;Efron 1979; Efron and Tibshirani 1997). 
To summarize, bootstrapping augments the amount and quality of information we can extract from the observed data in a sample. 
24.2.2 Applications of Bootstrapping in Corpus Linguistics 
Bootstrapping has been used for a wide range of applications in many scientifc disciplines, and this list is growing all the time (see Chernick 1999). However, the use of bootstrapping in corpus linguistics has been much more limited. In this section, we introduce four major applications for which bootstrapping has been used in corpus linguistics to date: the estimation of sampling distribution, the measurement of corpus homogeneity, the validation of statistical models, and random forest classifcation models. At the end of this section, we revisit this topic and propose potential applications of bootstrapping in future corpus linguistics research. 
24.2.2.1 Estimating Sampling Distributions 
The most basic application of bootstrapping in corpus linguistics is that of simu­lating sampling distributions to measure the accuracy of parameter estimates (see, 
e.g. Gries 2006). In this application, a computer program is used to re-sample with replacement many times (e.g. 10,000) from an observed data set. For each of these bootstrapped samples, a parameter of interest (e.g. mean, frequency count, effect size) is measured and recorded. This results in a large vector of parameter values that is used to estimate a sampling distribution. Using this distribution, the researcher can calculate measures of accuracy (e.g. standard error, confdence interval) for the statistical parameter. 
The use of bootstrapping for the purpose of estimating sampling distributions has been proposed by several scholars in corpus linguistics, especially in cases where large samples are diffcult or impossible to obtain (see Säily 2014; Hinneburg et al. 2007; Mannila et al. 2013). To our knowledge, however, this application of bootstrapping has only been used in a small number of studies. In Wagner et al. (2015) coders manually identifed the function of general extenders (e.g. or whatever, and things like that) extracted from corpora of English, and the researchers used bootstrapping to measure the accuracy of inter-rater reliability estimates. Another line of research has used bootstrapped word frequencies to measure keyness (cf. Chap. 6), or the signifcance of word frequencies in one corpus when compared with another (see Lijffjt et al. 2011, 2012, 2016; Säily 2014). These studies conclude that their bootstrapping approach generates better keyword lists than traditional keyness measures. 
24.2.2.2 Measuring Corpus Homogeneity 
Gries (2006) proposed a novel application of bootstrapping methods to explore corpus data which not only estimates the sampling distribution for a particular linguistic feature, but also uses bootstrapped samples to identify underlying dimen­sions of linguistic variation and quantify homogeneity across corpus parts (e.g. registers, sub-registers, texts). Gries began his study by showing that exhaustive permutation (i.e. exploring all possible combinations of sub-registers) is unwieldy and computationally ineffcient. He then demonstrated how bootstrapping can be used as the basis for an objective, bottom-up method for clustering corpus data (e.g. texts) at any level of granularity. This approach maximizes homogeneity within groups and heterogeneity between groups (see Representative Study 1 for more info). 
24.2.2.3 Validating Statistical Models 
The bootstrap can also be used to validate statistical models. This application has been more widely used by corpus linguistics researchers than the previous two 
J. Egbert and L. Plonsky 
applications. Statistical models are prone to overftting to the sample they are based on (i.e. the training data set) (Baayen 2008: 146, 283). Overftting occurs when the statistical model attempts to account for extreme values (i.e. noise) as well as the more ‘normal’ data points (i.e. signal) (Baayen 2008: 205). This can result in an invalid model. Two common approaches for evaluating the validity of a statistical model are cross-validation and bootstrap validation. 
Cross-validation relies on a test data set that is separate from the training data set. Once a statistical model has been trained using the training data set, it is applied to the test data set to assess how well it fts to data it was not trained on. There are a variety of methods for selecting the test and training data sets, but most of them require the researcher to divide the data set into two parts, usually a larger training set and a smaller test set. One advantage of this approach is that the researcher can test the statistical model on a new data set. However, this results in reduced sizes for the training and test data sets (for an in-depth discussion see, e.g., Baayen 2008). 
In bootstrap validation individual observations are bootstrapped from the original data set to assess the possibility of overftting. The bootstrapped model, which is based on a large number of data sets that have been re-sampled with replacement from the original data set, is then compared with the original model to evaluate whether the effects are retained. This is often done by measuring optimism, or the magnitude of the differences between the parameters in the original model and the bootstrap model. Bootstrap validation has the advantage of allowing the researcher to use the entire data set for training. Additionally, extensive research has shown that bootstrap validation actually outperforms cross-validation (see, e.g., Efron 1979; Efron and Tibshirani 1997). This method works because effects that do not survive bootstrap validation are typically those that were based on extreme values/noise in the data set that were not replicated in the bootstrap samples. Of course, as with all applications of bootstrapping, the usual disclaimers apply. Bootstrap validation based on a poorly designed sample will have the same problems and limitations as the original data set. 
Bootstrap validation for corpus linguistic analysis is introduced and explained in Baayen (2008). A number of published corpus linguistic studies have used this method. A few examples will suffce to illustrate how bootstrap validation is used to validate different statistical models. Berez and Gries (2009) and Gries (2010) use bootstrapping to validate hierarchical cluster analyses based on senses of polysemous words. Wolk et al. (2013) perform bootstrap validation of mixed-effects logistic regression models to account for variability and diachronic change in the use of genitives and datives. A similar method was used in Heller et al. (2017) to account for variation in the use of the genitive across world English varieties, and in Szmrecsanyi et al. (2016a) to account for variation and change in the use of the ’s-genitive, of-genitive, and pre-modifying nouns. 
24.2.2.4 Random Forest Analysis 
The fnal application of bootstrapping we discuss in this chapter is its use in random forest analysis, a machine learning method (see Chap. 25). There are three major steps in random forests: decision trees, tree bagging, and random forests. Decision trees are commonly used in machine learning as a method for modeling many different types of data. Decision trees are powerful due to their ability to model patterns that are irregular and nuanced. However, this characteristic makes them prone to overftting their training sets (Hastie et al. 2008). One method that has been proposed for addressing this problem is tree bagging, in which many decision trees are constructed based on bootstrapped samples (i.e. samples based on different parts of the data set). 
As with other applications of bootstrapping, random forest analysis makes it possible to distinguish between signal and noise in the data because extreme values are not likely to occur in as many of the bootstrap samples as more typical values. In addition to bootstrapping the decision trees, random forests sample a random subset of the original variables (e.g. linguistic features) at each stage in the learning process. This helps to mitigate the effect of bootstrapped decision trees that are strongly correlated simple because they are based on features that are extremely strong predictors of variability in the data set (Breiman 2001;Ho 2002). 
Random forest analysis was not used on corpus data until relatively recently. While it is not found in a large number of studies, its use seems to be on the rise to answer questions in corpus-based variationist linguistics. We briefy mention three of these studies here. Bernaisch et al. (2014) use random forests to model dative alternation with the verb give across different world English varieties. Deshors and Gries (2016) extend the recently proposed MuPDAR (Multifactorial Prediction and Deviation Analysis with Regression) method to random forests to model patterns of verb complementation across different varieties of world English. Finally, Szmrecsanyi et al. (2016b) also apply random forests to explore variation in three syntactic alternations (genitives, datives, and particle placement) across four world English varieties (see Chap. 25, Representative Study 2). 
24.2.2.5 Additional Applications of Bootstrapping 
We believe that the applications of bootstrapping in corpus linguistics we have discussed in the previous four sections are just a beginning for corpus linguistics. There are many applications of the principles and methods of bootstrapping that need to be further explored by corpus researchers. Here we discuss two such applications. 
Bootstrapping could be used as a method for evaluating the linguistic repre­sentativeness of a corpus (cf. Chap. 1). The method proposed by Gries (2006) was focused primarily on analyzing language use across the parts of a corpus. A natural extension of this method could be used for the purpose of estimating the linguistic representativeness of a corpus (see, e.g., Biber 1993). In most cases, 
J. Egbert and L. Plonsky 
the linguistic populations from which language corpora are sampled are poorly defned and unknowable. This does not mean that corpus creators and users should ignore the importance of representativeness, especially when their goal is to make generalizations from the corpus to a larger population of interest (see Egbert et al. forthcoming). Resampling with replacement from a corpus sample and exploring the distribution of linguistic features within and between these resamples can provide valuable insights into the internal consistency or stability of the sample. Based on these fndings, inferences could be made about whether the corpus sample represents a single, homogeneous population and the degree to which it represents the target population(s). This could be done with many different linguistic features to determine whether the sampling design and size of the corpus is adequate for studies of particular linguistic features. Biber (1993) demonstrated that the degree to which a corpus is linguistically representative depends, to a large extent, on the linguistic variable(s) that a researcher is interested in. Additionally, as demonstrated by Gries (2006), this could also be applied at many levels of granularity to evaluate the representativeness of particular strata and/or texts. 
Bootstrapping could also be used in (applied) corpus linguistics for the creation of vocabulary lists. The creation and use of vocabulary lists seems to be increasing at an accelerated pace (see, e.g. Gardner and Davies 2013; Brezina and Gablasova 2013; Nation 2016). Existing vocabulary lists include words that have high fre­quency and dispersion in a corpus. However, a major weakness of most lists is that they prioritize frequency over dispersion to the point that dispersion is typically accounted for using a dispersion index and a simple minimum threshold (cf. Chap. 4). Most lists rely on Juilland’s D, a particularly problematic dispersion measure that has been criticized in recent research (Biber et al. 2016; Burch et al. 2017;cf. also Chap. 5). An alternative approach would be for researchers to use bootstrapping to collect many resamples of texts from the corpus, storing a word list each time. The simplest way of creating a word list using these resampled word lists would be to include any word that occurred in at least N% of the resampled lists. This approach combines the measurement of frequency and dispersion into a single step. It would be a much more robust method that would be less dependent on the design and contents of the original corpus sample. This approach is similar to the word frequency studies discussed in Sect. 24.2.1 (see, e.g., Lijffjt et al. 2016), with the exception that this application would focus on extracting word lists from a single corpus instead of comparing word frequencies between corpora. 
As we mention above, the applications we propose here represent just two of the many potential applications of bootstrapping in future corpus-based research. Although the use of bootstrapping methods in corpus linguistics is still in its infancy, writing this book chapter has given us reason to be optimistic about the future of bootstrapping in corpus linguistics. What the feld lacks in quantity of bootstrapping studies, it makes up for in quality. During the process of reviewing the corpus-based research that uses bootstrapping, we have seen no evidence of bootstrapping results being misinterpreted or misrepresented. So while these authors have ample anecdotal evidence to suggest that bootstrapping methods need to be better understood, generally, it seems that their application in corpus linguistics research has gotten off to a great start. The high quality of bootstrapping methods in corpus linguistic research may actually be due in part to the slow and cautious pace at which they have been adopted. It is far more important that corpus researchers apply and interpret bootstrapping appropriately, than that they use it a lot. Having said that, we hope to see both of these things happening in corpus linguistic research. Our hope is that this chapter helps to move the feld in that direction. 
Representative Study 1 
Gries, S.T. 2006. Exploring variability within and between corpora: Some methodological considerations. Corpora 1(2):109–151. 
Research questions 
If one performs a corpus-linguistic quantitative analysis of phenomenon X in a corpus using the statistical parameter P, 
1. 
From a descriptive perspective, which degree of variability of P was observed in one’s data set and how do we quantify it? And, how do the present results concerning P compare to those of other studies? These questions inevitably lead to the next: 

2. 
How homogeneous is the corpus that was used for the study of phe­nomenon X? 

3. 
From an exploratory, bottom-up perspective, how can one identify (some of) the (most) relevant sources of the observed variability of P?Ifwe rephrase that from a hypothesis-testing, top-down perspective: are the vari­ables A, B, C, responsible for a signifcant proportion of P’s variability? (Gries 2006: 113) 


Data 
In order to address these questions, Gries used the International Corpus of English-Great Britain (ICE-GB), sub-divided into 13 sub-registers, within 5 registers, within 2 modes (see Gries 2006, Table 2). 
Methods 
From this corpus, he calculated the present perfect verbs (as a percentage of all verb forms) for each corpus fle. From the list of percentages (one per fle), Gries drew 50,000 random samples with replacement for each sub-register, matching the sample size (number of fles) in the original sub-register. He then stored a vector (i.e. list) of 50,000 mean percentages of present perfect verbs for each sub-register, and performed a hierarchical cluster analysis (see Chap.18), where the 13 sub-registers are clustered based on similarities in their use of present perfect verbs across 50,000 bootstrapped samples. 
(continued) 
J. Egbert and L. Plonsky 
Results 
Gries’ results revealed one cluster that contained written, informational sub-registers (fewer present perfects), and a second cluster that contained interactive, less formal sub-registers (more present perfects). He determined that persuasive writing tied into Cluster 2, showing that it approximated conversational discourse, and that broadcast speech was an outlier. These results reveal that this method can be used to cluster data based on a single variable (e.g. present perfect verbs) at any level of granularity. The use of the bootstrap in this study provided a sampling distribution on which to base the cluster analysis and related interpretations. He showed that the single variable of present perfect verbs was able to identify register patterns that are strikingly similar to those found along two of Biber’s (1988) dimensions. This use of bootstrapping is a creative extension of the methods for estimating sampling distributions described in Sect. 24.2.2.1. 
Representative Study 2 
Szmrecsanyi, B., Grafmiller, J., Heller, B., & Rhlisberger, M. 2016. 
Around the world in three alternations. English World-Wide 37(2), 
109–137. 
Research questions 
1. 
Do the varieties of English we study here share a core probabilistic grammar? 

2. 
Can ecology account for probabilistic similarity between varieties of English—for example, 

3. 
Do we fnd a split between native and non-native varieties of English? 


Do the alternations under study differ in terms of their probabilistic sensitivity to variety effects? (Szmrecsanyi et al. 2016b: 110) 
Data 
To answer these questions, they drew on data from the International Corpus of English (ICE), including the following four varieties: British English (ICE-GB), Canadian English (ICE-CAN), Indian English (ICE-IND), and Singapore English (ICE-SIN). 
Methods 
From each of these four sub-corpora, they extracted tokens of three alternat­ing linguistic features, coded for particle placement (5414 interchangeable 
(continued) 
tokens), genitive (4701 interchangeable tokens), and dative (3958 inter­changeable tokens). For each of the three features, the researchers modeled the data using conditional inference trees by recursively partitioning the data into smaller and smaller subsets according to those predictors that co-vary most strongly with the outcome. Informally, binary splits in the data are made by trying to maximize the homogeneity or “purity” of the data partitions with respect to the values of the outcome (e.g. all s-genitives vs. all of-genitives)” (Szmrecsanyi et al. 2016b: 113–4). 
They then applied random forest analysis, based on bootstrap-sampled trees (tree bagging) and random subsets of predictors (random forests) to further analyze the variation in the data set. 
Results 
The fndings from this study revealed internal homogeneity within the four varieties and external heterogeneity between them. The directions of the effects are stable across English varieties, but the effect sizes are variable. While the researchers did fnd some evidence of a native/non-native divide in the use of the three features, they call for future research into this question. The results suggest that variety of English is one of the strongest predictors of variation in the use of all three features. However, this effect varies across feature, with variety having the strongest effect on particle placement and the weakest effect on genitive variation. The use of bootstrapping methods in this study made it possible for the researchers to identify the independent variables that had the strongest effect on the choice of particle placement, dative, and genitive forms. 
24.3 Practical Guide with R 
In this section we demonstrate how to bootstrap corpus-based data using R. We also provide a case study to illustrate how to interpret and report on key visuals and quantitative results. For case study, we use the corpus of Academic Written English (AWE), which contains balanced samples from three publication types (journal articles, university textbooks, popular science books) in two disciplines (biology, history) (see Egbert 2015). Table 24.1 contains text and word counts for the strata in the corpus. The AWE corpus is relatively small, both in terms of texts (N = 146) and words per text (M = 566). The reason for this is that the original purpose for this corpus was to explore the relationship between the linguistic characteristics of published academic texts and readers’ perceptions of those texts. Due to practical constraints, it was determined that study participants could only be asked to read c. 500 word excerpts of 146 texts (see Egbert 2015). 
J. Egbert and L. Plonsky 
Each text in the corpus was read by 25 independent participants, each of whom reported their perceptions of the texts using an instrument called the Stylistic Perception Survey, composed of 38 semantic differential items on 6-point scales 
(e.g. readable—unreadable; technical—not technical). Each of the texts was also analyzed using a comprehensive set of linguistic features (tagged using the Biber Tagger). 
One of the purposes of Egbert’s (2015) study was to explore correlations between the perceptions of readers towards the texts and the linguistic features used by the authors of those texts. For example, the study revealed that the mean scores for reader perceptions of readability is negatively correlated with the normed rate of pre-modifying nouns (r =-0.43) and positively correlated with the percentage of high frequency words (500 most frequent in the Corpus of Contemporary American English) (r =0.62). In other words, texts perceived as more readable tend to have fewer pre-modifying nouns and larger proportions of high frequency vocabulary. These results are interesting, but they provide no information about the accuracy of these correlations, based on the sample used. 
In this case study, we re-analyze this data set using bootstrapping to estimate the sampling distribution of these two correlation coeffcients (readability x pre­modifying nouns and readability x high frequency vocabulary). Using a large number of bootstrap samples we can calculate mean correlations, as well as confdence intervals, allowing us to estimate the amount of variance we expect to see if we collected many samples of the same size from the population. 
1. 
Install the boot package in R. 

2. 
Load the boot package. 


install.packages("boot") library(boot) 
3. Read in the data set (Note: use setwd() function to set the correct working directory; cf. Chap. 17). 
Table 24.1 Text and Word Counts in the Academic Written English (AWE) Corpus 
Sub-register  Biology  History  Total  
Popular science Textbooks  25 (14,306) 25 (13,875)  25 (14,349) 24 (13,480)  50 (28,655) 50 (27,917)  
Journal articles  25 (14,067)  23 (12,557)  50 (28,336)  
Total  75 (42,248)  71 (40,386)  1146 (82,634)  

AWE <-read.csv("24_bootstrapping_AWE.csv", header = TRUE) 
4. 
Write a function (named CORstat). Functions are not used where they are placed in the code, rather they are placed above the code that calls on them, and called on later in the code. The ‘data’ in the code will be replaced by the data under study. 

5. 
Create temporary vectors of bootstrap samples from the two variables. 

6. 
Calculate the correlation between the two variables, and return the correlation coeffcient.1 


CORstat_NN <-function(data, i) { 
temp <-data[i,] 
return(cor(temp$readable, temp$NN, method = "pearson")) } 
7. 
Use the set.seed function to establish the seed in R’s random number generator. This ensures that the random simulation is reproducible. 

8. 
Use the boot function with the following parameters: (1) the name of the data frame containing the data, (2) the function we created to perform the correlation, and (3) the number of bootstrap samples (in this case, 10,000). 

9. 
Print the results. 


set.seed(10000) CORboot_NN <-boot(AWE, CORstat_NN, 10000) print(CORboot_NN) 
This produces the following output2: 
1It should be noted that there are different ways of writing code in R to perform bootstrapping. Another good approach would be to create a loop that stores results from bootstrapped samples, rather than use a function. 
2Because bootstrapping is based on random re-samples, the output will be slightly different each time we repeat the procedure. 
J. Egbert and L. Plonsky 
ORDINARY NONPARAMETRIC BOOTSTRAP Call: 
boot(data = AWE, statistic = CORstat, R = 10000) Bootstrap Statistics : 
original bias std. error 
t1* -0.4310868 0.002769969 0.07145907 
The ‘original’ column contains the correlation coeffcient for the original data set. The ‘bias’ column reveals the difference between the mean correlation coeffcient of the 10,000 bootstrap samples and the original coeffcient. The ‘std. error’ column contains the standard error for the 10,000 bootstrap samples. 
10. 
Next, we use the boot.ci function to compute a 95% confdence interval around the mean of the bootstrap samples, using the ‘bias-corrected and accelerated’ bootstrap method.3 

11. 
And print the results. 


CORboot.ci_NN <-boot.ci(CORboot_NN, conf = 0.95, type = "bca") print(CORboot.ci_NN) 
This produces the following output. 
BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS 
Based on 10000 bootstrap replicates CALL : 
boot.ci(boot.out = CORboot, conf = 0.95, type = "bca") 
Intervals : 
Level BCa 
95% (-0.5629, -0.2877 ) 
Calculations and Intervals on Original Scale 
3The bias-corrected and accelerated (BCa) method is preferred because it corrects for bias and skewness in the bootstrap distribution (see Davison and Hinkley 1997). 
Histogram of t 

0.3 0.4 0.5 0.60.7 0.8 –4 –2 0 2 t* Quantiles of Standard Normal 
Fig. 24.1 Histogram and Q-Q plot for bootstrap sampling distribution 
This reveals that the 95% confdence interval ranges from r =-0.565 to r =-0.283.4 
12. Finally, we can plot the bootstrap samples in the form of a histogram and Q-Q plot. The interpretability of the mean, standard error, and confdence intervals for our bootstrap samples depend on their normality. These plots will help us to evaluate the shape and attributes of the bootstrap sampling distribution (cf. Chap. 17). 
plot(CORboot_NN) 
Now that we have explained the steps for bootstrapping in R using the variable of pre-modifying nouns, we will demonstrate how to report bootstrapping results, based on the results for a different relationship: readability x high frequency vocabulary. 
The variables of perceived readability and the amount of high frequency vocabulary are correlated at r = 0.62. In order to estimate the accuracy of that statistic, we carried out a bootstrap estimation approach with 10,000 samples, using 95% BCa confdence intervals. Based on a review of the histogram and Q-Q plot for the bootstrap samples, we determined that the bootstrap sampling distribution approximates a normal distribution (see Fig. 24.1). 
The results revealed only a small amount of bias (-0.004) and confrmed a moderate to strong correlation between perceived readability and percent of high frequency vocabulary, M =0.6196, SE =0.0614, 95% CI =[0.4802, 0.7215].5 
4This can be compared with the original 95% CI of [-0.555, -0.289]. 5Compare with the original 95% CI of [0.508, 0.711]. 
J. Egbert and L. Plonsky 
This section has demonstrated how to create a bootstrapped sampling distri­
bution for a correlation coeffcient and present the results. We should emphasize here that bootstrapping can be applied not only to correlations, but to nearly any statistical parameter, including parameters related to descriptive statistics (mean, median, standard deviation, kurtosis, etc.), group comparisons (t-statistic, F-statistic, etc.), regression parameters (e.g. slope, intercept, coeffcients), effect size indices (Cohen’s d, R2 , etc.), just to name a few. For a tutorial on how to carry out bootstrapping for a variety of statistical parameters (in R and SPSS), see LaFlair et al. (2015). See also Chap. 26 for more general info about how to report the results of a quantitative corpus-based study. 
Further Reading 
LaFlair, G.T., Egbert, J., and Plonsky, L. 2015. A practical guide to bootstrap­ping descriptive statistics, correlations, t tests, and ANOVAs. Advancing Quantitative Methods in Second Language Research, 46, New York: Rout-ledge. 
This chapter motivates the use of bootstrapping for analyzing data in second language research. It covers practical considerations to account for when using bootstrapping and provides step-by-step demonstrations of how to analyze data to calculate bootstrap means, standard deviations, correlation coeffcients, t-statistics, and F-statistics. 
Egbert, J., and LaFlair, G.T. 2018. Statistics for categorical and distribution free data. In Handbook of Applied Linguistics Research Methodology, eds. De Costa, P., Phakiti, A., Starfeld, S., and Plonsky, L. London: Palgrave Macmillan. 
This chapter focuses generally on statistical techniques for analyzing non-traditional and distribution-free data, including a section on the use of the non-parametric bootstrap to analyze such data and the advantages of using bootstrapping over alternatives such as permutation tests. 
References 
Baayen, R. H. (2008). Analyzing linguistic data: A practical introduction to statistics using R. Cambridge: Cambridge University Press. 
Berez, A. L., & Gries, S. T. (2009). In defense of corpus-based methods: A behavioral profle analysis of polysemous get in English. In S. Moran, D. S. Tanner, & M. Scanlon (Eds.), Proceedings of the 24th Northwest linguistics conference. University of Washington working papers in linguistics (Vol. 27, pp. 157–166). Seattle, WA: Department of Linguistics. 
Bernaisch, T., Gries, S. T., & Mukherjee, J. (2014). The dative alternation in South Asian English (Eds.), Modelling predictors and predicting prototypes. English World-Wide, 35(1), 7–31. 
Biber, D. (1988). Variation across speech and writing. Cambridge: Cambridge University Press. 
Biber, D. (1993). Representativeness in corpus design. Literary and linguistic computing, 8(4), 243–257. 
Biber, D., Reppen, R., Schnur, E., & Ghanem, R. (2016). On the (non) utility of Juilland’s D to measure lexical dispersion in large corpora. International Journal of Corpus Linguistics, 21(4), 439–464. 
Breiman, L. (2001). Random forests. Machine Learning, 45, 5–32. 
Brezina, V., & Gablasova, D. (2013). Is there a core general vocabulary? Introducing the new general service list. Applied Linguistics, 36(1), 1–22. 
Burch, B., Egbert, J., & Biber, D. (2017). Measuring and interpreting lexical dispersion in corpus linguistics. Journal of Research Design and Statistics in Linguistics and Communication Science, 3, 189–216. 
Chernick, M. R. (1999). Bootstrap methods: A practitioner’s guide (Wiley series in probability and statistics). Hoboken, NJ: Wiley. 
Davison, A. C., & Hinkley, D. V. (1997). Bootstrap methods and their application. Cambridge: Cambridge University Press. 
Deshors, S. C., & Gries, S. T. (2016). Profling verb complementation constructions across New Englishes. International Journal of Corpus Linguistics, 21(2), 192–218. 
Efron, B. (1979). Computers and the theory of statistics: Thinking the unthinkable. SIAM Review, 21(4), 460–480. 
Efron, B. (1987). Better bootstrap confdence intervals. Journal of the American Statistical Association, 82(397), 171–185. 
Efron, B. (1992). Bootstrap methods: Another look at the jackknife. In Breakthroughs in statistics (pp. 569–593). New York: Springer. 
Efron, B., & Gong, G. (1983). A leisurely look at the bootstrap, the jackknife, and cross-validation. The American Statistician, 37(1), 36–48. 
Efron, B., & Tibshirani, R. (1986). Bootstrap methods for standard errors, confdence intervals, and other measures of statistical accuracy. Statistical Science, 1, 54–75. 
Efron, B., & Tibshirani, R. J. (1993). An introduction to the bootstrap: Monographs on statistics and applied probability (Vol. 57). New York/London: Chapman and Hall/CRC. 
Efron, B., & Tibshirani, R. (1997). Improvements on cross-validation: The 632+ bootstrap method. Journal of the American Statistical Association, 92(438), 548–560. 
Egbert, J. (2015). Publication type and discipline variation in published academic writing: Investigating statistical interaction in corpus data. International Journal of Corpus Linguistics, 20(1), 1–29. 
Egbert, J., & LaFlair, G. T. (2018). Statistics for categorical and distribution-free data. In A. Phakiti, 
P. I. De Costa, L. Plonsky, & S. Starfeld (Eds.), The Palgrave handbook of applied linguistics research methodology. New York: Palgrave. Egbert, J., Biber, D., & Gray, B. (forthcoming). Towards representativeness in corpus design. Cambridge: Cambridge University Press. Gardner, D., & Davies, M. (2013). A new academic vocabulary list. Applied Linguistics, 35(3), 305–327. Gries, S. T. (2006). Exploring variability within and between corpora: Some methodological considerations. Corpora, 1(2), 109–151. Gries, S. T. (2010). Behavioral profles A fne-grained and quantitative approach. The Mental Lexicon, 5(3), 323–346. Gries, S. T. (2013). Statistics for linguistics with R: A practical introduction (2nd rev. ed.). Berlin: De Gruyter Mouton. 
Hastie, T., Tibshirani, R., & Friedman, J. (2008). The elements of statistical learning (2nd ed.). New York: Springer. 
J. Egbert and L. Plonsky 
Heller, B., Szmrecsanyi, B., & Grafmiller, J. (2017). Stability and fuidity in syntactic variation world-wide: The genitive alternation across varieties of English. Journal of English Linguistics, 45(1), 3–27. 
Hinneburg, A., Mannila, H., Kaislaniemi, S., Nevalainen, T., & Raumolin-Brunberg, H. (2007). How to handle small samples: Bootstrap and Bayesian methods in the analysis of linguistic change. Literary and linguistic computing, 22(2), 137–150. 
Ho, T. K. (2002). A data complexity analysis of comparative advantages of decision Forest constructors. Pattern Analysis and Applications, 5, 102–112. 
LaFlair, G. T., Egbert, J., & Plonsky, L. (2015). A practical guide to bootstrapping descriptive statistics, correlations, t tests, and ANOVAs. In Advancing quantitative methods in second language research (Vol. 46). New York: Routledge. 
Lijffjt, J., Papapetrou, P., Puolamäki, K., & Mannila, H. (2011). Analyzing word frequencies in large text corpora using inter-arrival times and bootstrapping. Machine Learning and Knowledge Discovery in Databases, 341–357. 
Lijffjt, J., Säily, T., & Nevalainen, T. (2012). CEECing the baseline: Lexical stability and signifcant change in a historical corpus. In Studies in variation, contacts and change in English (Vol. 10). Research unit for variation, contacts and change in English (VARIENG). 
Lijffjt, J., Nevalainen, T., Säily, T., Papapetrou, P., Puolamäki, K., & Mannila, H. (2016). Signifcance testing of word frequencies in corpora. Literary and Linguistic Computing, 31(2), 374–397. 
Mannila, H., Nevalainen, T., & Raumolin-Brunberg, H. (2013). Quantifying variation and esti­mating the effects of sample size on the frequencies of linguistic variables. In M. Krug &J.Schlter(Eds.), Research methods in language variation and change (pp. 337–360). Cambridge: Cambridge University Press. 
Nation, I. S. P. (2016). Making and using word lists for language learning and testing. Philadelphia, PA: John Benjamins Publishing. 
Plonsky, L., Egbert, J., & LaFlair, G. (2015). Bootstrapping in applied linguistics: Assessing its potential using shared data. Applied Linguistics, 36(5), 591–610. 
Säily, T. (2014). Sociolinguistic variation in English derivational productivity: Studies and methods in diachronic corpus linguistics. Mémoires de la Société Néophilologique de Helsinki. 
Szmrecsanyi, B., Biber, D., Egbert, J., & Franco, K. (2016a). Towards more accountability: Modeling ternary genitive variation in late modern English. Language Variation and Change, 28(1), 1–29. 
Szmrecsanyi, B., Grafmiller, J., Heller, B., & Rhlisberger, M. (2016b). Around the world in three alternations. English World-Wide, 37(2), 109–137. 
Wagner, S. E., Hesson, A., Bybel, K., & Little, H. (2015). Quantifying the referential function of general extenders in north American English. Language in Society, 44(5), 705–731. 
Wolk, C., Bresnan, J., Rosenbach, A., & Szmrecsanyi, B. (2013). Dative and genitive variability in Late Modern English: Exploring cross-constructional variation and change. Diachronica, 30(3), 382–419. 
Chapter 25 Conditional Inference Trees and Random 
Forests 
Natalia Levshina 
Abstract This chapter discusses popular non-parametric methods in corpus lin­guistics: conditional inference trees and conditional random forests. These methods, which allow the researcher to model and interpret the relationships between a numeric or categorical response variable and various predictors, are particularly attractive in ‘tricky’ situations, when the use of parametric methods (in particular, regression models) can be problematic, for example, in the situations of ‘small n, large p’, complex interactions, non-linearity and correlated predictors. For illustration, the chapter discusses a case study of T and V politeness forms in Russian based on a corpus of flm subtitles. 
25.1 Introduction 
Conditional inference trees (CITs) and conditional random forests (CRFs) are gaining popularity in corpus linguistics. They have been fruitfully used in models of linguistic variation, where the task is to fnd out which linguistic and extralinguistic factors determine the use of near-synonyms (e.g. let, allow or permit), alternating syntactic constructions (e.g. the double-object vs. to-dative) or sociolinguistic variants (e.g. the type of /r/ used by speakers of a particular dialect). The methods have been implemented in a user-friendly way in the packages party (Hothorn et al. 2006b; Strobl et al. 2007) and partykit (Hothorn and Zeileis 2015), which has contributed to the popularity of the methods. 
CITs belong to the family of recursive partitioning methods, which involve the following basic steps. 
Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_25) contains supplementary material, which is available to authorized users. 
N. Levshina (•) Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands e-mail: natalia.levshina@mpi.nl 
© Springer Nature Switzerland AG 2020 611 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_25 

Step 1. Select the predictor which helps best to distinguish between different values 
of the response variable, using some statistical criterion. Step 2. Make a split in this variable, splitting the data in several data sets. Most 
algorithms use binary partitioning, although non-binary splits have also been 
implemented. Step 3. Repeat Steps 1 and 2 recursively until no further splits can be made, based 
on certain pre-defned criteria. 
Figure 25.1 illustrates the main idea behind binary partitioning. Imagine a set of objects (circles and stars) that can be white or blue. The shape allows one to separate the white objects from the blue ones by splitting the entire data set into two subsets, one with stars and the other with circles. The goal is to achieve maximal purity (or minimal impurity) in the terminal nodes. In other words, we would like to have as few blue objects in the bottom left-hand node as possible, and as few white objects in the bottom right-hand node as possible. In our case, 80% of the objects in the node with stars are white, and 80% of the objects in the node with circles are blue. This means that splitting the data according to shape allows us to predict the colour correctly in 80% of the cases. 
Random forests, including CRFs, represent an ensemble method, by which many individual trees are ‘grown’, and their predictions are averaged. Each tree is based on a random sample of n observations from the original dataset, usually with replacement, and on a random sample of k predictors from all predictors in the model. Random forests usually produce more accurate predictions than single trees. 
CITs and CRFs have a number of advantages in some situations when the use of traditional approaches, such as regression analysis (see Chap. 21), may be inappropriate (see more details in Sect. 25.2). In addition, CITs allow one to interpret high-order interactions, which involve more than two predictors, in a very convenient and intuitive way. However, the methods also have their pitfalls (see Sect. 25.2.6). 
25.2 Fundamentals 
25.2.1 Types of Data 
Although most applications of these methods in corpus linguistics involve categor­ical response variables and predominantly categorical predictors, CITs and CRFs can model the relationships between the response variable and predictors at any scale of measurement. Also, one can ft models with multivariate response variables and censored data.1 Traditionally, models with numeric response variables are called regression trees, and models with categorical response variables are referred to as classifcation trees. 
CITs and CRFs can be particularly useful in the situations of small n, large 
p. This may be the case in many subfelds of corpus linguistics, where data are small and costly, e.g. analysis of spoken data (e.g. Tagliamonte and Baayen 2012), multilingual data (e.g. Levshina 2016) or data from less well documented languages. For such datasets, logistic regression models would not be suitable. There are no fxed rules regarding the minimum number of observations, but it is possible to have a relatively high number of predictors. As an extreme example, one can mention the study of Strobl et al. (2008), where CRFs are used to investigate a binding property of 310 amino acid sequences depending on 105 predictors. A multiple regression analysis of such data would be inappropriate. 
CRFs can be used in situations where predictors are highly intercorrelated. This happens quite often in corpus-linguistic research. For instance, one and the same underlying theoretical construct, such as transitivity in Hopper and Thompson’s (1980) sense, can be represented by several correlated semantic and morphosyntactic variables. Another example is entrenchment of a word, which can be operationalized as frequency, dispersion and contextual diversity of a word in a corpus (e.g. Baayen 2010). In such cases, a regression analysis may fail due to multicollinearity issues. 
In addition, CITs and CRFs represent an attractive alternative when some of the regression assumptions are not met, e.g. the assumptions of homoscedasticity (equal variability of the response variable across the range of values of the predictors) and non-linearity (lack of direct proportionality) of the relationship between a predictor and the outcome. This is particularly convenient if one wants to keep the original scale of the response variable rather than perform transformations (see, however, 
1Censored data are characterised by the presence of some observations that only specify an interval instead of an exact value, such as age older than 80 years or any size of clothes larger than XXL. 
Chap. 23 on generalized additive models, which represent a tool for modelling non­linear relationships). 
25.2.2 The Assumptions 
There are no traditional assumptions that should be met when ftting a CIT or a CRF, such as constant variance, non-linearity or normally distributed errors. However, a word of caution should be said about the independence of observations. At the moment of writing, the only working method for dealing with dependent observations seems to be including the grouping factor (e.g. the speaker or text IDs) on a par with all other predictors.2 This has been done, for instance, by Tagliamonte and Baayen (2012), who added the individual speakers as a covariate in their CITs and CRFs. This method cannot be regarded as a perfect solution, unfortunately, as will be shown in Sect. 25.2.6. 
25.2.3 Research Questions 
The research questions that can be answered with the help of CITs and CRFs are the same as the ones that are answered with the help of regression analysis. A typical question is which linguistic factors help to predict the use of particular linguistic variants. Examples are variation of was/were in York English (Tagliamonte and Baayen 2012), transitivity patterns with the verb give in South Asian Englishes (Bernaisch et al. 2014), fore-and backclipping (e.g. technology > tech,but racoon > coon, cf. Lohmann 2013) and positioning of adverbial concessive clauses (Wiechmann and Kerz 2012). Most of this work has been done on English data. Some exceptions are Baayen et al. (2013), who investigate Russian aspectual prefxes and suffxes, and Levshina’s (2016) study of causative constructions in 15 European languages. Usually, the task is to fnd the contextual (corpus) variables that are associated with the choice between two or more linguistic variants. CITs are particularly useful in multifactorial probabilistic grammar, when one investigates interactions between contextual variables and the variables representing language varieties, as in the study of several morphosyntactic alternations in World Englishes by Szmrecsanyi et al. (2016). 
Similar to regression modelling, one can use these methods for explanation and prediction (including classifcation). CITs can be particularly useful for explanation and interpretation, whereas CRFs are usually better in prediction. 
2A solution with random effects has been implemented in the package REEMtree, but it is available only for longitudinal data. 
25.2.4 The Algorithms 
25.2.4.1 The CIT Algorithm 
The method is based on testing the null hypothesis that the distribution of the response variable D(Y) is equal to the conditional distribution of the response variable given some predictor D(Y|X). The global null hypothesis says that this holds for all predictors (Hothorn et al. 2006a). In order to test the hypothesis, one uses permutation (reshuffing) of the labels in the response variable Y. By permuting Y, its association with the predictor X is broken. As a result of this permutation, the null distribution is derived directly from the data. In contrast, traditional tests impose assumptions on the distribution of the data, so that the null distribution of a test statistic can be derived analytically. 
As an illustration, consider the dative alternation in English. Some imaginary data are provided in Table 25.1. We investigate the dependence between the response 
(i.e. the use of the double object dative and the to-dative) and a number of predictors, including the information status of the Recipient (given or new). A permuted version of the response is shown in the third column of the table. 
Based on this information, one computes a statistic that involves the difference in the association between response Y and predictor X before and after the permutation. The greater this difference, the stronger the association between Y and X. The package party offers two types of test statistics: cquad (the default) and cmax.The difference in the results can be observed if there are categorical variables with more than two values (see Hothorn et al. 2006a or 2006b for the details). In most situations, there is no need to modify the default settings. One could then compare the statistics from different covariates and choose the largest one as the best candidate for the next split. However, this is not a very good idea when the predictors are on different measurement scales. In order to make the statistics comparable across different variables, the p-values are normally used (the default option). For the global null hypothesis test, the p-values are adjusted (a simple Bonferroni correction is used by default). 
There are several options for computing the p-values. By default, the algorithm returns the asymptotic p-values because the test statistics used in the algorithm are 
Table 25.1 Permutation of the response variable: an example 
Example ID  Observed response Y(dative variant)  Permuted response Y (dative variant)  Observed predictor X (information status of Recipient)  
1  To-dative  DO-dative  New  
2  DO-dative  DO-dative  New  
3  To-dative  To-dative  Given  
101  DO-dative  DO-dative  New  
102  To-dative  To-dative  Given  
103  DO-dative  To-dative  Given  
...  ...  ...  ...  

shown to tend to well-known distributions. More precisely, a . 2 distribution in the case of the test statistic cquad and a multivariate normal distribution in the case of cmax (Hothorn et al. 2006b). Alternatively, one can approximate the p-values by using a Monte-Carlo method and simulate the distribution of the test statistic by randomly reshuffing the data. The number of permutations can be specifed, as well. 
After the predictor has been selected, the next step is splitting the selected covariate into two disjoint sets. For variables with many possible splits, the algorithm computes a statistic for every possible binary split into subsets A and not-A, similar to how it was done during the process of variable selection. Next, the split with the maximal test statistic is chosen. The procedure is then repeated recursively until certain criteria are met, which are the following: 
• 
minimum criterion for a split, which equals 1 -a. If the global null hypothesis cannot be rejected at a certain level of statistical signifcance a (by default, 0.05), no further splits are made. This parameter performs two functions: as the signifcance criterion (and therefore it should not be changed without a very good reason, since it strikes a balance between Type I and Type II errors), and a hyperparameter (i.e. a parameter defned before running the algorithm) determining the size of a tree; 

• 
the minimum number of cases in a node before a split. If there are fewer cases than required, no split will be made; 

• 
the minimum number of cases in a node after a split. 


Importantly, one can obtain the values predicted by the model for the observa­tions in the original dataset or for new data. How are these predictions obtained? As an example, consider a white star in the imaginary data displayed in Fig. 25.1.The algorithm ‘puts’ all stars in the left-hand terminal node. In that case, the predicted category (i.e. white or blue) will be determined by the majority vote. Since the node contains more white objects than blue ones, the predicted colour is white. The predicted probability will be 0.8 (or 80%) because the white objects represent 80% from the entire number of objects in this node (eight out of ten). In case of regression trees the mean value of the response variable in all observations in the relevant terminal node is computed. To fnd out how to compute the median or another statistic of interest, see ?party::treeresponse. 
25.2.4.2 The CRF Algorithm 
A CRF is an ensemble of multiple CITs. The algorithm uses resampling with or without replacement to create a random sample for each tree. Importantly, only a sample of candidate predictors is randomly drawn for each individual CITs. Since only a restricted number of predictors is selected for every individual tree, each variable has the chance to appear in different contexts with different covariates. This may better refect its potentially complex effect on the response variable (Strobl et al. 2009). If some variable only matters in a very specifc type of contexts, it gets a chance to tell its story when the stronger variables are out of the competition. This is particularly important in situations of multicollinearity, where predictors are highly intercorrelated. 
To obtain the predicted values from a CRF, one needs to aggregate the results from the individual trees. To predict the response value for an individual observation with particular values of the predictors, the algorithm combines the information about all relevant observations that have the same properties as the observation of interest, in all trees. The predicted value for a given observation is then the average value of the response variable in all those observations (in case of regression trees with numeric response variables) or the most popular category (in case of classifcation trees with categorical response variables). 
Importantly, one can distinguish between predicted values for the training samples and those for the out-of-bag (OOB) samples. Recall that certain data points are left out during the bootstrap sampling or subsampling before a tree is ftted. The OOB samples can be used to assess the predictive performance of that specifc model since they were not used to build the model. 
Importantly, CRFs also provide a linguist with the so-called conditional variable importance scores, which show how important each variable is, taking into account all others and their interactions. To compute this measure for a predictor, the algorithm averages the results from many trees and measures the decrease in prediction power if one randomly permutes the predictor. If a predictor is associated with the response strongly, there will be a substantial decrease in prediction accuracy. More exactly, a conditional permutation method is applied, as described by Strobl et al. (2008). For example, if Y is the response, and X and Z are categorical predictors, the dependence between Y and X is measured within the levels of Z. Let us revisit the example with the dative alternation. Imagine we are testing two predictors: information status of the Recipient (given or new) and the semantics of the Recipient (animate or inanimate). In order to compute the conditional importance of X (information status) given Z (semantics) in an individual tree, we will reshuffe the value of X within the levels of Z, as shown in Table 25.2.Next, we will test how much worse the individual conditional tree with the permuted data will predict the observed scores of the response variable in comparison with the original data. The conditional importance score of X for the entire forest is computed as an average over all trees. One can use the OOB samples or the training data for this task. 
25.2.5 CITs and CRFs Compared with Other Recursive Partitioning Methods 
The methods described in this chapter belong to a large family of recursive parti­tioning methods used for regression and classifcation. Other approaches include 
Table 25.2 Conditional permutation scheme: an example 
ID of observation  Response  Observed X (information status of Recipient)  Permuted X|Z  Predictor Z (semantics of Recipient)  
1  To-dative  New  Given  Animate  
2  DO  New  New  Animate  
3  To-dative  Given  New  Animate  
101  DO  New  Given  Inanimate  
102  To-dative  Given  Given  Inanimate  
103  DO  Given  New  Inanimate  
...  ...  ...  ...  ...  

CART (Classifcation And Regression Trees), CHAID (CHi-squared Automatic Interaction Detector), and QUEST (Quick, Unbiased and Effcient Statistical Tree). An overview of these methods can be found in Loh (2014). 
One of the main differences of CITs from the other approaches is that the p-values are used as a stopping criterion and for choosing the next split (see Sect. 25.2.4.1). This enables one to compare the predictors at difference scales of measurement. The other well-known methods use other criteria, e.g. minimization of impurity in the nodes (the so-called Gini impurity measure) in CART. One of the consequences of using a statistical signifcance testing criterion is the decrease in statistical power as more and more splits are made in the data. As a result, overftting is avoided. This approach is quite convenient in practice because it does not require pruning, i.e. removal of the low branches that do not contribute to the predictive power of a tree (see Kuhn and Johnson 2016: 177–178). However, statistical hypothesis tests are not directly related to the predictive performance of the model and ‘cleanliness’ of the nodes. This should be kept in mind when comparing the results of a CIT with results of a different recursive partitioning method. 
Another important difference is that the CIT algorithm separates the steps of variable selection and making of a split, whereas most other methods merge this in one step. As a result, the variables with multiple splits (e.g. categorical variables with many different values) do not have advantages in comparison with the variables with few splits. The same holds for predictors with missing values. 
As for CRF, the best known alternative is probably Breiman’s (2001) random forests (see R package randomForest). An important distinctive feature of CRFs is that one can compute the conditional variable importance measures, which have some advantages in comparison with non-conditional ones (Strobl et al. 2008). Namely, they do not have a bias towards correlated predictors. If X is a real predictor and Z is a spurious one, and X and Z are correlated, the importance value of Z will increase if a traditional non-conditional method is used. This undesirable effect can be avoided if one uses the conditional permutation schema as described in Sect. 
25.2.4.2. A disadvantage of conditional variable importance, however, is that it is computationally intensive. 
Another difference between CRFs and other ensemble methods concerns the computation of predicted values. In many popular random forest algorithms, the predicted values are an aggregation of predictions from each individual tree (i.e. the mean predicted value or the most popular predicted outcome). In contrast, CRFs make predictions by retaining information about individual observations in each tree (see Sect. 25.2.4.2). As a result, terminal nodes with many observations play a greater role because they provide more data points for this calculation. 
The conditional inference approaches do not always outperform the other approaches in terms of prediction or explanation. For example, CART-based random forests sometimes provide better predictive power than CRFs (Kuhn and Johnson 2016: 200–201). One can also obtain similar results when using alternative, less computationally intensive methods, such as CART. The conditional inference framework has been preferred in linguistic studies for two main reasons. First, it provides realistic estimates of variable importance of highly correlated predictors. Second, it avoids overftting the data. A systematic comparison of the available methods and their practical advantages for corpus linguistics remains a task for future research. 
25.2.6 Situations When the Use of CITs and CRFs May Be Problematic 
There is no universal statistical method that can be used in all circumstances. CITs and CRFs are no exception to this rule. There are several cases where one might prefer to use a different method or perform additional checks. 
Paradoxically, although CITs can be more successful than regression models in tricky situations, e.g. with non-linear patterns and high-order interactions, which involve more than two predictors, CITs may be quite useless in very simple situations, i.e. when the relationships between the response and the predictors are linear (i.e. directly proportional) and additive (i.e. there are no interactions). As an illustration, consider Fig. 25.2. This tree represents an attempt of a CIT to analyse the relationships between a numeric response variable y and predictors x1 and x2, which were randomly generated from the normal distribution. The response variable was created using the following linear regression formula: 
(1) y = 0.7 + 5.4 x1–2.8 x2 + e 
where e contained random normally distributed errors. The predictors do not interact and the relationship between them and the response variable is linear. One can see that the tree presentation is not very helpful in modelling these simple relationships. It creates an illusion of interactions that are not present in the data. Moreover, the numeric predictors are split multiple times, which masks the linear relationships between them and the response variable. A multiple regression model would be a much better choice in such cases (see Chap. 21). 
10 0 –10 
–20 

Table 25.3 Distribution of imaginary corpus data of differential argument marking 
Role = Subject 
Response = marked  Response = unmarked  
Semantics = animate  5  20  
Semantics = inanimate  20  5  

Role = object 
Response = marked  Response = unmarked  
Semantics = animate  19  6  
Semantics = inanimate  5  20  

Moreover, CITs can run into problems when some predictors have a strong crossover interaction. As an illustration, consider Table 25.3, which contains imaginary corpus counts of different subjects and objects in a language with differential subject and object marking. The response variable refects the presence of case marking: marked or unmarked. There are two interacting predictors of case marking: the semantic class (animate or inanimate) and syntactic role (subject or object). If the argument is a subject, it is usually unmarked when it is animate and marked when it is inanimate. As for objects, it is the other way round, which is why we can speak of a near-perfect crossover interaction. 
When one fts a logistic regression model by using the lrm function in the rms package (Harrell Jr 2017), one obtains the coeffcients shown in Table 25.4.The interaction term is highly signifcant. Figure 25.3, which visualizes the interaction with the help of the package visreg (Breheny and Burchett 2016), demonstrates that the effect of semantics is almost perfectly reverse for subjects and objects. 
Table 25.4 The coeffcient table of a logistic regression model based on the data in Table 25.3. Positive coeffcients: increased likelihood of the marked form; negative coeffcients: increased likelihood of the unmarked form 
Term  Coeffcient  SE  Wald Z  P-value  
Intercept  -1.39  0.5  -2.77  0.0056  
Semantics = inanimate  2.77  0.71  3.92  <0.0001  
Role = object  2.54  0.69  3.71  0.0002  
Semantics = inanimate: Role = object  -5.31  0.98  -5.4  <0.0001  


In such cases, CITs may be unable to identify any splits. Figure 25.4 displays the result of a CIT analysis with the default settings. It only contains a bar plot with the marginal proportions of the response categories. No splits are made. The method thus fails to uncover the underlying relationships between the predictors and the response. 
Moreover, simple CITs may be unstable even when small changes in the data are made (Strobl et al. 2009; Kuhn and Johnson 2016: 174), especially if the model contains numerous correlated predictors. This is why individual trees need to be complemented by a random forest analysis, which aggregates the results from many individual trees (see an example in Sect. 25.3). 
CRFs have their pitfalls, as well. One of them is skewed response. Random forests of any kind do not predict very well when the response is very skewed (although other methods, e.g. logistic regression, can experience problems, as well). For example, if the proportions of two outcomes are 98% and 2%, it is possible to achieve 98% classifcation accuracy just by assigning the more frequent category. It will be diffcult to fnd the predictors that perform better than this (Berk 2006). As a result, the predicted frequency of the rare category may be zero, i.e. the model will not discriminate between the categories. In such situations, one can try to ‘upsample’ the small category and/or ‘downsample’ the large one in order to make the distribution more balanced. Note, however, that the interpretation of the p-values in the trees becomes problematic in that case because the statistical power of the independence tests will change in comparison with the original sample. 

As mentioned in Sect. 25.2.2, both CITs and CRFs have a problem with dependent observations, e.g. multiple examples from the same author, text or corpus segment. In such situations, one normally uses mixed-effects regression modelling (see Chap. 22). Some studies have treated the grouping factor as one of the predictors (e.g. Tagliamonte and Baayen 2012). However, this is not a perfect solution. First, CITs are based on the permutation framework which measures the association between the response and a predictor by permuting the response variable, without taking into account the levels of the grouping factor (see Sect. 25.2.4.1). Therefore, the information about the dependence of the data points is lost. Moreover, it is not clear how to use the same model with the grouping factor as a covariate on new data, where the grouping categories (e.g. texts or subcorpora) will be different. The model will therefore lose generalizability. Finally, as Baayen et al. (2013) demonstrate, the recursive partitioning methods do not perform very well when the grouping factor has many levels. In such cases, a mixed-effects model is the best choice. 
Finally, one can have computational problems when growing CRFs and com­puting conditional variable importance on very large datasets. One can run out of memory, or it may take the algorithm a very long time to complete the computations. Moreover, there is danger that the trees may become too large and overft the data. In that case, it is sometimes recommended to increase the minimum criterion, e.g. from 0.95 to 0.99. 
From all this, it follows that CITs and CRFs should be applied in tandem in order to counterbalance their strengths and weaknesses, and preferably in combination with other methods, most importantly, fxed-effects or mixed-effects (generalized) linear regression models (see Chaps. 21 and 22). 
Representative Study 1 
Tagliamonte, S., and Baayen, R.H. 2012. Models, forests and trees of York English: Was/were variation as a case study for statistical prac­tice. Language Variation and Change 24(2):135–178. doi:https://doi.org/ 10.1017/S0954394512000129. 
Research question 
The study investigates the factors that determine variation between was and were in plural past tense existential constructions in York English, as in There was/were a lot of people. 
Data 
The data come from a corpus of spoken York English at the turn of the twenty-frst century. All plural past tense existential constructions (e.g. There was/were + PL noun) were extracted. The dataset contains 489 tokens from 83 individuals. Nine covariates are tested, which represent such social factors as the sex, age and education level of the speakers, and such linguistic factors as polarity, type of determination and proximity of the copula to its referent. 
Methods 
The paper utilizes fxed-effects and mixed-effects logistic models, as well as CITs and CRFs, where the speakers’ IDs are added as a covariate. 
Results 
The CRFs provides the best prediction. The most important predictors are the speaker’s age, polarity, type of determination and proximity. There is also very substantial interspeaker variation. The CIT reveals that linguistic and social factors play a role only for a subset of speakers. For that subset, the non-standard form was is more likely to occur in affrmative contexts than in negative ones. Further, the differentiation by age is only relevant in affrmative contexts. Younger speakers are more likely to use was than the older speakers. 
Representative Study 2 
Szmrecsanyi, B., Grafmiller, J., Heller, B., and Rhlisberger, M. 2016. Around the world in three alternations: Modeling syntactic variation in varieties of English. English World-Wide 37(2):109–137. doi: https:// doi.org/10.1075/eww.37.2.01szm. 
Research questions 
This study focuses on three alternations in different geographic varieties of English: the dative alternation, the genitive alternation and variation in particle placement. The lects include two native varieties (Great Britain and Canada) and two non-native varieties (India and Singapore). The research questions are as follows: 
1. 
Do the varieties of English share a core probabilistic grammar? 

2. 
Is there a split between the native and non-native varieties of English? 

3. 
Do the alternations under study differ in terms of their probabilistic sensitivity to variety effects? 


Data 
The authors extract tokens of the alternations from the relevant components of the International Corpus of English. In addition, some frequency information was collected from the GloWbE corpus. Relevant predictors were coded manually. 
Methods 
For each of the three alternations, Szmrecsanyi et al. modelled the data using conditional inference trees. The varieties of English were tested as a categorical variable. Next, they performed conditional random forest analysis and computed the conditional variable importance scores of the predictors. In addition, in order to interpret the results of the CFR analysis of the particle placement, predicted probabilities of the alternating variants were computed for different language varieties. 
Results 
The results are as follows: 
1. 
The directions of the effects of the contextual variables are stable across the varieties in all three alternations, although there are quantitative differences with regard to the effect size. 

2. 
As for the second research question, the data provide no conclusive results. 

3. 
The particle placement alternation exhibits the most robust variety effects, and the genitive alternation the least. The authors explain this fnding by 


(continued) 
the fact that the particle placement alternation is more tightly associated with specifc lexical items (i.e. verb slots), which makes cross-varietal indigenization effects more likely. 
25.3 A Practical Guide with R 
25.3.1 T/V Forms in Russian: Theoretical Background and Research Question 
This case study is a part of a larger project on European T and V politeness forms (Levshina 2017), which represent different degrees of politeness in addressing the Hearer, e.g. French tu and vous,German du and Sie, Russian ty and vy, usually accompanied by a corresponding verb form. This cross-linguistic study is based on the parallel corpus of flm subtitles called ParTy.3 The observations come from several flms of different genres (see below). Since standard English has no politeness distinctions in the second person, the translators of subtitles to languages with the T/V distinction have to choose between these forms. This may be a diffcult thing to do because the norms of T/V use are multifactorial and fuid. 
According to Brown and Gilman (1960), the politeness forms in European languages can be described in terms of two dimensions: power and solidarity. Power means the ability of one person to control the behaviour of another one. In a one-to­one interaction, the participant with greater power addresses the participant with less power using T, while the participant with less power uses V. The power dimension used to play an important role earlier, but has been gradually replaced by solidarity semantics, with T for intimate communication, e.g. between family members and friends, and V for formal communication. Virtually any characteristic, e.g. gender, age, hobbies, political beliefs and even physical appearance (e.g. dreadlocks or tattoos) can be a basis for the perception of solidarity. See Levshina (2017) for an overview of these and more recent ideas. In what follows, we will discuss which factors infuence the use of the Russian T and V forms, which are ty (second person singular) and vy (second person plural), respectively. 
3Available at https://github.com/levshina/ParTy-1.0. Accessed 22 May 2019. 
25.3.2 Data: Film Subtitles 
The data for the present study come from online subtitles of nine popular flms of different genres. The flms are displayed in Table 25.5.The meta-information about the year and genres is taken from the International Movies Database.4 
The data set for the study was created as follows. First, 228 interactive contexts with the pronouns you or yourself were identifed in the English data. All plural references were excluded. Their translations were found in ten other languages, including Russian. It is important to mention that one should speak about T/V forms, rather than about T/V pronouns because there are many examples when the verb form is the only clue that helps us to distinguish between T and V. Consider an example in (2) from Russian, where the frst T form has no explicit pronominal subject, while the second one has both the pronoun and the verb form. 
(2) Duma-ješ, ona tut? Ty ošiba-eš-sja. think-PRES.2SG she here you.NOM be.mistaken-PRES.2SG-REFL5 “Do you really think she’s here? You’re mistaken.” 
25.3.3 Variables 
The flm situations with you or yourself were coded for 16 variables, which are presented in Table 25.6. 
Table 25.5 Films represented in the data set 
Film  Year  Genres  
Avatar  2009  Action, adventure, fantasy  
Black Swan  2010  Drama, thriller  
Bridge of Spies  2015  Drama, history, thriller  
Frozen  2013  Animation, adventure, comedy  
Inception  2010  Action, adventure, sci-f  
Spectre  2015  Action, adventure, thriller  
The Grand Budapest Hotel  2014  Adventure, comedy, crime  
The Imitation Game  2014  Biography, drama, thriller  
The Iron Lady  2011  Biography, drama, history  

4www.imdb.com. Accessed 22 May 2019. 5The abbreviations stand for the following. 2SG: second person singular; NOM: Nominative case; PRES: Present tense; REFL: Refexive. 

Rel_Age  Whether the hearer is older or younger than the speaker  “Same”, “Older” or “Younger”  
Rel_Power  Whether there is power asymmetry between the participants in general or in the given situation, e.g. a parent and a child, a general and a soldier, a boss and his/her employee  “Greater” (the hearer has power over the speaker), “less” (the speaker has power over the hearer) or “equal”  
Rel_Class  The social class difference in the dyad  “Higher” (the hearer belongs to a higher social class than the speaker), “lower” (the hearer belongs to a lower social class than the hearer) or “equal”  
Rel_Sex  The sex of the speaker and the hearer  “F_F” (female speaker and female hearer), “F_M” (female speaker and male hearer), “M_F” (male speaker and female hearer) and “M_M” (male speaker and male hearer)  
Rel_Circle  The social circle to which the speaker and the hearer belong  “Fam” (family), “Fri” (friends), “Rom” (romantic partners), “Work” (colleagues at work), “Str” (strangers) and “Acq” (acquaintances)  

Speaker-related and hearer-related variables 
S_Age, H_Age  The Speaker’s age, the Hearer’s age  “Child” (younger than 18), “Young” (approximately 18–35), “Middle” (approximately 35–60), “Old” (approximately older than 60)  
S_Class, H_Class  The Speaker’s social class, the Hearer’s social class  “Upper” (top-rank politicians and civil servants, owners of multinational corporations, etc.), “Middle” (white-collar workers, small business owners, military offcers, etc.), “Lower” (blue-collar workers, servants, etc.) and “Other” (aliens, animals, as well as gangsters, tramps, prostitutes and other declassed elements)  
S_Sex, H_Sex  The Speaker’s sex, the Hearer’s sex  “M” or “F” (there were no transgenders in the data)  

(continued) Table 25.6 (continued) 

Variables describing the communicative settings 
Others  The presence of other people who could hear the speaker  “Yes” or “No”  
Offce  Whether the interaction takes place in an offce, a government building, prison, school, etc.  “Yes” or “No”  
Before68  Whether the action takes place before 1968  “Yes” or “No”  
Place  Where the action of the flm takes place  “UK” (in the United Kingdom, “US” in the USA, “Fictive” (in an imaginary world), “Global” (in different parts of the world), “Europe” (somewhere in Europe)  


The dataset and R code (25_CIT_RF.r) are provided in the supple­mentary materials. In order to access the data, the comma-separated fle 25_CIT_RF_tv.csv should be frst saved locally in a directory on your computer. Next, you should read it in R as a data frame called tv. One of the ways to do so is to choose the fle interactively, as shown below. 
#read the data in R, choosing the file interactively tv <-read.csv(file = file.choose()) 
25.3.4 Software 
At the moment of writing, there are two add-on packages in R, in which conditional inference trees and random forests are implemented. One is party and the other one is partykit. The latter is a more recent version, which contains a new improved procedure for CITs. There are also some differences in the R syntax. The package partykit is still under development, though, and some functionalities available in party cannot be used at the moment in partykit. For example, one cannot use the Monte-Carlo resampling method of permutation. Only the default asymptotic method can be used. This is why the R code provided in the supplementary materials is based only on the functions from party. You will also need two other add-on packages: Hmisc and pdp. The packages should be frst installed, as shown below. 
install.packages(c("party", "Hmisc", "pdp")) library(party) library(Hmisc) library(pdp) 
25.3.5 Conditional Inference Tree 
In order to ft a CIT, the function ctree() should be used: 
#fit a CIT tv.cit <-ctree(Form ~ ., data = tv) 
The code, which uses the default settings, is identical to the following line: 
#Identical to: 
tv.cit <-ctree(Form ~ ., data = tv, controls = 
ctree_control(teststat = "quad", 
testtype = "Bonferroni", 
mincriterion = 0.95, 
minsplit = 20, 
minbucket = 7)) 
The default settings, which are recommended in most cases, can be changed, if necessary, in ctree_control(): 
• 
quadratic test statistic teststat = "quad". If necessary, one can use teststat = "max"; 

•use 
of p-values with the Bonferroni correction testtype = "Bonferroni". Alternatively, one can use testtype = "MonteCarlo", which performs actual reshuffing of the data the number of times specifed by nresample (9999 by default). It may be useful to try both the default test and run the Monte­

Carlo simulation and compare the results. Note, however, that the permutation may take a while if the dataset is large and the number of replications is high. In principle, it is also possible (but not advisable, unless the user knows well what she or he is doing) to take the p-values without the Bonferroni correction (testtype = "Univariate") or to use the test statistics themselves instead of the p-values (testtype = "Teststatistic"); 

• 
0.95 as the minimal 1 – p value needed to implement a split, defned by mincriterion = 0.95.Ifno p-values are computed, then this hyperparam­eter specifes the minimum score of the test statistic. In some situations, it may be useful to change this setting. For example, ftting a tree with a lower minimal criterion may be useful in a pilot study performed for exploratory purposes; 

• 
minimum 20 observations in a node for a split to be considered, specifed by minsplit = 20; 

• 
according to the default settings, there should be at least seven observations in one node after a split, minbucket = 7. 


In order to see the tree, one can use the following simple code: 
plot(tv.cit) 
Figure 25.5 shows an individual CIT ftted to the data with the help of ctree().The plot should be interpreted from the top down. The top split (Node 
1) is made in the variable Rel_Circle. This means that the predictor is the most important one with regard to the use of ty and vy. The variable selection criteria (i.e. 1– p) can be obtained with the help of the function node(). Only the frst seven predictors are shown: 
Check the variable selection criteria for Node 1: 
nodes(tv.cit, 1)[[1]]$criterion$criterion 
#Film Rel_Age Rel_Sex Rel_Power Rel_Circle 
#0.999800767 0.951730359 0.846906561 0.093920867 0.999999960 #S_Class H_Class 
#0.963888397 0.481392221 
The data are then split in two subsets. The one on the left includes the situations when the Speaker and the Hearer are friends, family members or romantic partners. These contexts are not split further and constitute together a terminal node (Node 2) with 45 observations. As one can see from the bar plot in that node, the proportion of ty is very high in those contexts. The right-hand branch from the top node represents the situations when the Speaker and the Hearer are colleagues, acquaintances or strangers. Further splits are made in these contexts. The frst split is made in H_Age (Node 3). Let us frst look at the right-hand branch, which includes middle-aged and old Hearers, and the resulting terminal Node 7. The proportions in the bar plot show that these contexts are characterized by a high chance of vy. No more splits are made here. As for the left-hand branch from Node 3, which includes the Hearers who are younger or have no human age at all (like magic creatures), then the relative social class (Rel_Class) plays a role, and another split is made (Node 4). If the social classes of the Speaker and Hearer are not equal, then the V form is slightly preferred (see the proportions in Node 5). If the Hearer and the Speaker belong to the same class or the class is not identifable, the T form is strongly preferred (Node 6). 

Since there are two non-fnal, internal splits in the tree, it may be diffcult to interpret them in terms of proportions of T and V. To facilitate the interpretation of those splits, one can create bar plots with proportions in each node, including the inner ones: 
plot(tv.cit, inner_panel = node_barplot) 

Such a plot is shown in Fig. 25.6. One can compare, for example, the proportions of T and V after the frst split, looking at the terminal Node 2 and the inner Node 
3. The bar plots show clearly that the proportion of V is greater in Node 3 than in Node 2. 
It is also easy to obtain the proportions of T and V in an inner or terminal node by using the function nodes(). For example, for Node 2 these proportions are as follows: 
#obtain the proportions of T and V in Node 2 nodes(tv.cit, 2)[[1]]$prediction #[1] 0.8888889 0.1111111 #proportion of T and proportion of V 
Finally, we need to estimate how well the tree fts the data. A popular measure for classifcation tasks is accuracy, which is defned as the number of correct predictions divided by the total number of observations. 
#compute the predicted(fitted) values 
pred.cit <-predict(tv.cit) #cross-tabulate the observed and predicted values table(pred.cit, tv$Form) #pred.ctree ty vy # ty7515 # vy 33 105 
#compute the proportion of correct predictions (75 + 105)/228 #[1] 0.7894737 
The accuracy is 0.79. Another popular measure, which can be used for binary response variables, is the C-index. It shows the proportion of times when the randomly sampled observation with outcome A also has a higher probability of A predicted by the model than a randomly sampled instance of B. It ranges from 0.5 (the model does not discriminate between the outcomes) to 1 (perfect discrimination). 
#create a vector of predicted probabilities 
prob.cit <-unlist(predict(tv.cit, type = "prob")) 
[c(FALSE, TRUE)] #compute the C-index using a function from the Hmisc package somers2(prob.cit, as.numeric(tv$Form) -1) # C Dxy n Missing # 0.8092593 0.6185185 228.0000000 0.0000000 
The C-index of our model is 0.81. One can use the same rule of thumb as the one for logistic regression models. Namely, a model has acceptable discrimination between the response categories if C is higher than 0.7, good if it is above 0.8, and excellent if it is above 0.9. 
25.3.6 Conditional Random Forest 
This subsection demonstrates how one can grow a CRF, compute the conditional variable importance scores, and visualize the partial effects of relevant predictors on the choice between ty and vy. There are numerous hyperparameters that should be set before a forest is grown: 
• 
ntree, which specifes the number of trees in the ensemble (500 by default). According to Strobl et al. (2009), one needs many trees when the number of predictor variables is large, in order to give each variable a chance to occur in enough trees; 

• 
mtry, which specifes the number of predictors randomly selected for each individual tree. For some technical reasons, mtry = 5 by default. One often sees a recommendation to use the square root of the total number of predictors. In the presence of many intercorrelated predictors, is may be useful to try a larger value, so that each variable occurs in a suffcient number of trees, and its importance is not due to some random variation (Strobl et al. 2009); 

• 
sampling with replacement (replace = TRUE) or without replacement (replace = FALSE). Traditional random forests use bootstrap, i.e. sampling with replacement (see Chap. 24). According to Strobl et al. (2007), only subsampling without replacement can guarantee unbiased variable importance measures. This is particularly important when a) categorical and continuous variables are combined, and b) there are categorical variables with different number of categories; 

• 
various parameters of the individual trees (see Sect. 25.3.5). 


These hyperparameters can be specifed with the help of different con­trols. Some default settings are implemented in cforest_unbiased and cforest_classical, which have the same defaults as cforest_control. The default settings in cforest_unbiased reproduce the approach in Strobl et al. (2007), who use subsampling without replacement instead of commonly accepted bootstrapping with replacement (replace = FALSE). In that case, the algorithm builds a tree based on a random sample 0.632 times the size of the original dataset. The number can be changed with the help of the parameter fraction. This approach also uses teststat = "quad" and testtype = "Univariate". The behaviour of cforest_classical, in contrast, mimics the algorithm in randomForest in the eponymous package (Liaw and Wiener 2002) and has the following default settings: testtype = "max", testtype = "Teststatistic", mincriterion = qnorm(0.9), which equals approximately 1.28 (a type of z-statistic), and replace = TRUE. 
In our data, we have categorical variables with the number of values rang­ing from two to nine. Following the conclusions made by Strobl et al. (2007) about the optimal way of treating such diverse predictors, we use the unbiased approach and will use subsampling without replacement. The default settings of cforest_unbiased are taken, with the exception of mtry = 4 (the square root of the total number of predictors) and ntree = 2000. 
#set the random seed if you want to reproduce the 
#results presented in this chapter 
set.seed(61) 
tv.crf <-cforest(Form ~ ., data = tv, 
controls = cforest_unbiased(mtry = 4, ntree = 2000)) 
Next, we compute the conditional variable importance scores. By default, the unconditional scores are computed by varimp(). To change that, add conditional = TRUE. 
set.seed(23) #compute the conditional variable importance scores. This may 
#take a while, depending on the number of trees 
#and the size of the dataset 
tv.varimp <-varimp(tv.crf, conditional = TRUE) #create a dot chart with varimp scores 
dotchart(sort(tv.varimp), xlab = "Conditional variable importance") #add a vertical line to separate important scores from #unimportant ones 
abline(v = abs(min(tv.varimp)), 
lty = 2, lwd = 2, col = "red") 
Figure 25.7 displays a dot chart with the sorted values. The horizontal axis represents the conditional variable importance for each predictor, i.e. the average decrease in the OOB prediction accuracy when this predictor is permuted using the conditional approach described in Sect. 25.2.4.2. The red line separates the important scores from the unimportant scores. Note that unimportant predictors fuctuate randomly around zero, with positive or negative values. Negative values appear when the randomly permuted versions of a predictor tend to be better than the original one. The rule of thumb is to take the absolute minimum value as a cut­off point. Note that the importance scores should only be interpreted with regard to their ranking, and not as absolute values. It would be a mistake to compare the scores across different models and data. 

The plot in Fig. 25.7 demonstrates that the variable Rel_Circle is the most important predictor. It is followed by the difference in the social class (Rel_Class) and the Hearer’s age (H_Age). Recall that these three variables were also the ones that played a role in the CIT. Also, the individual differences between the flms (Film) play a role. One can attribute that to some pragmatic variables that our course-grained schema does not take into account, or to the translators’ individual preferences. The place where the action is developing (Offce and Place)isofsome importance, as well, followed by the Speaker’s social class and age (S_Class and S_Age). The age differences (Rel_Age), time period (Before68) and the presence of others (Others) are only marginally important. Interestingly, the sex of the Speaker and Hearer seems to play hardly any role. The individual power relationships do not matter, either. 
It is also necessary to evaluate how well the model discriminates between T and 
V. As was mentioned in Sect. 25.2.4.2, one can use the learning samples and the OOB samples (i.e. those left out during the bootstrap sampling or subsampling). The OOB value is usually more realistic and is closer to the result one can fnd on new data or during cross-validation. The measures based on the learning samples can be naive and over-optimistic estimates of the error rate (Strobl et al. 2009). By default, the training sample is used. To change that, one should add OOB = TRUE in predict(). 
#measures based on the OOB sample #get predicted (fitted) values pred.crf.oob <-predict(tv.crf, OOB = TRUE) 
#cross-tabulate the predicted and observed values table(pred.crf.oob, tv$Form) #pred.rf ty vy #ty 80 24 #vy 28 96 #compute the proportion of correct predictions 
(accuracy) (80 + 96)/228 #[1] 0.7719298 #compute the predicted probabilities prob.crf.oob <-unlist(predict(tv.crf, 
type = "prob", OOB = TRUE))[c(FALSE, TRUE)] #compute C using a function from the Hmisc package somers2(prob.crf.oob, as.numeric(tv$Form) -1) 
#C Dxy n Missing #0.8689815 0.7379630 228.0000000 0.0000000 #for comparison, measures based on the learning sample: pred.crf.train <-predict(tv.crf, OOB = FALSE) table(pred.crf.train, tv$Form) 
#pred.rf ty vy # ty928 # vy 16 107 (92 + 107)/228 #[1] 0.872807 #accuracy based on the learning sample 
prob.crf.train <-unlist(predict(tv.crf, 
type = "prob"))[c(FALSE, TRUE)] somers2(prob.crf.train, as.numeric(tv$Form) -1) #C Dxy n Missing #0.9507716 0.9015432 228.0000000 0.0000000 

In our case, the learning sample accuracy is 0.87, and the corresponding C-index is 0.95. As expected, the OOB measures are lower: the OOB accuracy is 0.77, and the OOB C-index is 0.87. The OOB accuracy of the CRF is rather low, even in comparison with the accuracy of the CIT presented in Sect. 25.3.5. This may be due to the difference in the roles of the unbiased approach and the classical method. The unbiased approach with subsampling aims at providing the fairest evaluation of the variable importance, while the classical approach with bootstrapping is particularly good in prediction. Figure 25.8 illustrates the differences in the C-values and accuracy scores between the two methods for different values of mtry, i.e. the number of randomly sampled predictors. For most values of mtry, the classical method outperforms the unbiased approach in terms of predictive power. These results also demonstrate that it is useful to try different values of mtry and other settings. It is also recommended to run different models with different random seeds, in order to see whether the results are stable. In our case, as an inspection of several individual models suggests, the top 5 most infuential variables remain the same. The differences appear in the order of low-importance predictors. This is a usual result. 
25.3.7 Interpretation of the Predictor Effects: Partial Dependence Plots 
Unfortunately, the CRF does not return information similar to regression coeff­cients, which would enable us to interpret the effects of individual variables on the response. Instead, one can use partial dependence plots, which can help to visualize the relationships between different values of the predictors and the response while accounting for the average effect of the other predictors in the model. The plots shown here are created with the help of the package pdp (Greenwell 2017). 
#Figure 9a: predictor "Rel_Circle" 
pdp_circle <-partial(tv.crf, "Rel_Circle", prob = TRUE) 
plotPartial(pdp_circle, main = "Rel_Circle") #Figure 9b: predictor "Office" 
pdp_office <-partial(tv.crf, "Office", prob = TRUE) 
plotPartial(pdp_office, main = "Office") 
The left-hand plot in Fig. 25.9 displays the effects of Rel_Circle on the probability of ty. The vertical axis (‘y-hat’) corresponds to the average predicted probability of the T form for each value of the predictor. In order to understand better what the y-hat values represent, take the value “Acq” as an example. The algorithm copies the original dataset and replaces all original values of Rel_Cicle with “Acq”. Next, the predicted values are computed for each of the observations. Finally, the average predicted probabilities are computed. This procedure is repeated for all other values of the predictor (i.e. “Fam”, “Fri”, etc.). As a result of this procedure, one can obtain the average predicted probabilities that can be directly compared with each other because the remaining predictors in the dataset have the same values. It is also possible to obtain the predicted log-odds of the outcomes (see Chap. 21) instead of probabilities (not shown here). 
The plot suggests a cline of (in)formality or intimacy/distance shown in (3): 
(3) Family – Friends – Romance – Work – Acquaintances – Strangers 
The more to the left, the higher the probability of the T form. 
The right-hand plot shows the effect of Offce, which has not been found on the tree. Being in an offce increases the chances of vy. Note, however, that the difference is very small. 
Other plots, which are not shown here, reveal that the probability of ty is higher in Avatar, Black Swan, Frozen and Inception than in the other flms. The model also predicts a higher proportion of ty when the action takes place in the US or in a fctive world than in the other places. The probability of ty is the highest when the Speaker does not belong to any social class, and the lowest when an upper-class person is speaking. As for the Speaker’s age, young people, children and magic creatures tend to use ty more often than old people, while middle-aged people are the one who are the most likely to use vy. The V form is also more probable when in the presence of other people and before 1968. 

25.3.8 Conclusions and Recommendations for Reporting the Results 
The case study of T/V forms in Russian has revealed that the solidarity dimension is the strongest one. There is little evidence of the power dimension playing a role. Even the asymmetric variables (e.g. Rel_Class) are in fact more related to solidarity than power: if the communicators belong to the same class or do not belong to any class at all, as magic creatures, the T form is preferred. The translators’ perception of different places and time periods as involving more or less formal communication is refected in the choice of ty and vy, as well. 
When reporting the results, one should include the following information: 
• 
an individual tree (cf. Figure 25.5) and a verbal description of the splits made (see Sect. 25.3.5); 

• 
conditional variable importance scores of the forest (e.g. in the form of a plot, as in Fig. 25.8) and a description (see Sect. 25.3.6); 

• 
measures of predictive accuracy and goodness of ft of the tree and forest (in the latter case, it is preferable to report the statistics based on the OOB samples). For example, one can write the following: “The predictive power of the models is safsfactory. The CIT has the classifcation accuracy of 0.79 (with the baseline value of 0.51), whereas the concordance index C is 0.81. As for the CRF, its out-of-bag classifcation accuracy is 0.77, and the out-of-bag C is 0.87.” 


One also needs to mention in the Methods section the R package and the hyperparameters that were used to grow the trees and forests. For more general info about how to report the results of a quantitative corpus-based study, see also Chap. 26. 
To summarize, CITs and CRFs are very fexible and convenient tools that can be used in many situations when traditional parametric methods will fail. They provide easily interpretable results and do not require a tedious check of numerous assumptions. However, these methods may be misleading in some special cases and therefore must be used in a combination with other methods, most importantly, mixed-effects logistic models (see Chap. 23). There remain a few open questions. First of all, we need to have a more generalizable and appropriate way of dealing with the situations when the observations are not independent. Such situations are very common in corpus linguistics. In fact, this is a matter of ongoing research (Torsten Hothorn, p.c.), so hopefully we will see some important innovations in the near future. Second, the software is currently in a state of fux. We still have to wait until all functionalities that are available in the R package party are implemented in the optimized R package partykit. 
Further Reading 
Strobl, C., Malley, J., and Tutz, G. 2009. An introduction to recursive partition­ing: Rationale, application, and characteristics of classifcation and regres­sion trees, bagging, and random forests. Psychological Methods 14(4):323– 348. doi:10.1037/a0016973. 
This paper can be useful to those interested in the statistical details. It contains an excellent introduction to recursive partitioning methods. One can fnd the main prin­ciples of recursive partitioning, its advantages and methodological improvements, but also its limitations and pitfalls. The R code is provided, as well. 
Kuhn, M. & Johnson, K. 2016. Applied Predictive Modeling. New York: Springer. 
This book (more specifcally, Chap. 8) can be recommended to those who want to get a broader perspective on different popular recursive partitioning methods, including CITs and CRFs. The similarities and differences between these methods are discussed. 
Baayen, R.H., Endresen, A., Janda, L.A., Makarova, A., and Nesset, T. 2013. Making Choices in Russian: Pros and Cons of Statistical Methods for Rival Forms. Russian Linguistics 37:253–291. 
This paper, which deals with functionally similar Russian constructions, compares the advantages and disadvantages of several popular methods (CITs and CRFs, mixed-effects logistic regression models and naïve discriminative learning) along various dimensions, which are important to quantitative linguists, including classi­fcation accuracy, cognitive realism and ease of interpretation. 
References 
Baayen, R. H. (2010). Demythologizing the word frequency effect: A discriminative learning perspective. The Mental Lexicon, 5, 436–461. https://doi.org/10.1075/ml.5.3.10baa. Baayen, R. H., Endresen, A., Janda, L. A., Makarova, A., & Nesset, T. (2013). Making choices in Russian: Pros and cons of statistical methods for rival forms. Russian Linguistics, 37, 253–291. Berk, R. A. (2006). An introduction to ensemble methods for data analysis. Sociological Methods & Research, 34(3), 263–295. https://doi.org/10.1177/0049124105283119. Bernaisch, T., Gries, S. T., & Mukherjee, J. (2014). The dative alternation in south Asian English(es). English World-Wide, 35(1), 7–31. https://doi.org/10.1075/eww.35.1.02ber. Breheny, P., & Burchett, W. (2016). Visreg: Visualization of regression models. R package version 
2.3–0. https://CRAN.R-project.org/package=visreg. Accessed 22 May 2019. Breiman, L. (2001). Random forests. Machine Learning, 45, 5–32. Brown, R., & Gilman, A. (1960). The pronouns of power and solidarity. In T. A. Sebeok (Ed.), 
Style in language (pp. 253–276). Cambridge, MA: MIT Press. 
Greenwell, B. M. (2017). pdp: An R package for constructing partial dependence plots. The R Jour­nal, 9(1), 421–436. URL https://journal.r-project.org/archive/2017/RJ-2017-016/index.html. Accessed 22 May 2019. 
Harrell, F.E. Jr. 2017. rms: Regression modeling strategies. R package version 5.1–1. https:// CRAN.R-project.org/package=rms. Accessed 22 May 2019. Hopper, P., & Thompson, S. A. (1980). Transitivity in grammar and discourse. Language, 56, 251– 299. Hothorn, T., & Zeileis, A. (2015). partykit: A modular toolkit for recursive Partytioning in 
R. Journal of Machine Learning Research, 16, 3905–3909. URL http://jmlr.org/papers/v16/ hothorn15a.html. Accessed 22 May 2019. 
Hothorn, T., Hornik, K., & Zeileis, A. (2006a). Unbiased recursive partitioning: A conditional inference framework. Journal of Computational and Graphical Statistics, 15(3), 651–674. https://doi.org/10.1198/106186006X133933. 
Hothorn, T., Hornik, K., van de Wiel, M. A., & Zeileis, A. (2006b). A Lego system for conditional inference. The American Statistician, 60, 257–263. https://doi.org/10.1198/ 000313006X118430. 
Kuhn, M., & Johnson, K. (2016). Applied predictive modeling. New York: Springer. 
Levshina, N. (2016). Why we need a token-based typology: A case study of analytic and lexical causatives in ffteen European languages. Folia Linguistica, 50(2), 507–542. https://doi.org/ 10.1515/fin-2016-0019. 
Levshina, N. (2017). A multivariate study of T/V forms in European languages based on a parallel Corpus of flm subtitles. Research in Language, 15(2), 153–172. https://doi.org/10.1515/rela­2017-0010. 
Liaw, A., & Wiener, M. (2002). Classifcation and regression by randomForest. R News, 2(3), 18– 22. 
Loh, W.-Y. (2014). Fifty years of classifcation and regression trees. International Statistical Review, 82(3), 329–348. https://doi.org/10.1111/insr.12016. 
Lohmann, A. (2013). Is tree hugging the way to go? Classifcation trees and random forests in linguistic study. Vienna English Working Papers 22. https://anglistik.univie.ac.at/research/ views/archive/. Accessed 22 May 2019. 
Strobl, C., Boulesteix, A.-L., Zeileis, A., & Hothorn, T. (2007). Bias in random Forest variable importance measures: Illustrations, sources and a solution. BMC Bioinformatics, 8, 25. https:// doi.org/10.1186/1471-2105-8-25. 
Strobl, C., Boulesteix, A.-L., Kneib, T., Augustin, T., & Zeileis, A. (2008). Conditional variable importance for random forests. BMC Bioinformatics, 9, 307. https://doi.org/10.1186/1471­2105-9-307. 
Strobl, C., Malley, J., & Tutz, G. (2009). An introduction to recursive partitioning: Rationale, application, and characteristics of classifcation and regression trees, bagging, and random forests. Psychological Methods, 14(4), 323–348. https://doi.org/10.1037/a0016973. 
Szmrecsanyi, B., Grafmiller, J., Heller, B., & Rhlisberger, M. (2016). Around the world in three alternations: Modeling syntactic variation in varieties of English. English World-Wide, 37(2), 109–137. https://doi.org/10.1075/eww.37.2.01szm. 
Tagliamonte, S., & Baayen, R. H. (2012). Models, forests and trees of York English: Was/were variation as a case study for statistical practice. Language Variation and Change, 24(2), 135– 178. https://doi.org/10.1017/S0954394512000129. 
Wiechmann, D., & Kerz, E. (2012). The positioning of concessive adverbial clauses in English: Assessing the importance of discourse-pragmatic and processing-based constraints. English Language & Linguistics, 17(1), 1–23. 
Part VI Pulling Everything Together 
Chapter 26 Writing up a Corpus-Linguistic Paper 
Stefan Th. Gries 
and Magali Paquot 
Abstract In this chapter, we provide a brief characterization of what we consider the best and most common structure that empirical corpus-linguistic papers can and should have. In particular, we frst introduce the four major parts of a corpus linguistics paper: “Introduction”, “Methods”, “Results”, and “Discussion”. Since the nature of corpus data and corpus techniques makes the two sections very feld-specifc, we then focus more particularly on the “Methods” and “Discussion” sections of a typical quantitative corpus linguistic paper. We provide recommenda­tions that span the research cycle from data description to analyzing the dataset and reporting the results of statistical tests. 
26.1 The Structure of an Empirical Paper 
As shown in this volume, conducting an empirical analysis of language using corpus data requires that corpus linguists frst make informed decisions related to the type/s of corpus, variables and methods needed to answer their research question/s. Next, they need to analyze (quantitatively) corpus data in a scientifc and transparent way. Importantly, all the steps taken and decisions made will need to be reported in a corpus-linguistic paper. To report a quantitative/empirical study, researchers in a variety of disciplines typically adopt the overall structure represented in Table 26.1. 
The ‘Introduction’ aims at motivating the research question(s); typically, this is based on previous (published or presented) research and/or relevant observations of the phenomenon. For instance, previous studies may have come to results that are 
S. Th. Gries (•) University of California Santa Barbara, Santa Barbara, CA, USA 
Justus Liebig University Giessen, Giessen, Germany e-mail: stgries@linguistics.ucsb.edu 
M. Paquot FNRS -Université catholique de Louvain, Centre for English Corpus Linguistics Louvain-la-Neuve, Belgium e-mail: magali.paquot@uclouvain.be 
© Springer Nature Switzerland AG 2020 647 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_26 
S. Th. Gries and M. Paquot 
Table 26.1 Structure of a quantitative corpus-linguistic paper (based on Gries 2016b:174) 
Part  Content  
Introduction  What is the question?  
Motivation of the question: Why is this problem important?  
Overview of previous relevant work  
Formulation of hypotheses  
Methods  Choice of method (e.g. diachronic vs. synchronic corpus data; tagged vs. untagged corpora; etc.)  
Source of data (which corpus/corpora?)  
Operationalization of variables  
Retrieval algorithm or syntax  
Software that was used  
Data fltering/annotation (e.g., how were false hits identifed? What did you do to guarantee objective coding procedures? How did you annotate your data? etc.) Choice of statistical test(s) and how they are implemented  
Results  Summary statistics  
Graphic representation  
Signifcance test: Test statistic, degrees of freedom (where available), and p  
Effect size: The difference in means, the correlation, etc.  
Discussion  Implications of the results for your hypotheses  
Implications of the results for the research area  

diffcult to reconcile or previous studies may have not covered a certain part of the relevant population (in the statistical sense), or certain real-life observations do not appear to be explainable with the current state of the art in the feld, etc. Ideally, therefore, the introduction leads the reader to expect the author to address any of these scenarios with the present study. 
The ‘Methods’ section is concerned with which corpora are used, why and especially how variables, or factors or predictors of interest, are operationalized in the corpus data, how the relevant data points are extracted from the corpus and annotated as required by the questions/hypotheses outlined in the intro, and how they were statistically (or otherwise) analyzed. 
The ‘Results’ section contains all results of all steps of the analysis. This might begin with the number of hits from a corpus query using regular expressions to fnd matches of the phenomenon in question, how these were winnowed down by disregarding false positives, the result of sampling procedures or data transformation procedures, etc.; other results might include (co-occurrence) frequencies of anno­tated features. Most importantly, the ‘Results’ section will contain all results from the statistical exploration (in the case of exploratory/hypothesis-generating studies) or one’s evaluation of one’s hypotheses (in the case of hypothesis-testing studies). Ideally, one would not just report the results of signifcance tests, but also all relevant statistics such as effect directions, effects sizes (raw and/or standardized), indices of model/classifer quality and classifcation/prediction accuracies, as well as the results pertaining to model/classifer diagnostics and validation; also for most advanced analyses, this is the part where the main results should be visualized in a way that facilitates their comprehension even, but also especially, for readers whose statistical knowledge is more limited. 
Finally, the ‘Discussion’ section interprets the results against the background of the questions/hypotheses discussed in the introduction and contextualizes the results in the light of their bigger-picture implications for subsequent studies of the current or related phenomena, but also for the future development of data, theories, and methods. 
Although we will not discuss this further, this template can easily be extended to papers that report more than one empirical case study. Typically, after a general introduction, each case study would have its own ‘Methods’, ‘Results’, and ‘Discussion’ sections. The case studies would then be followed by a ‘General Discussion’ section that puts everything together and answers the main research questions on the basis of the combined results. 
In this chapter, we focus on how to write the ‘Methods’ and ‘Results’ sections of a quantitative corpus linguistic paper since the ‘Introduction’ and ‘Discussion’ sections are so phenomenon-dependent that, apart from the general guidelines above, they defy easy discipline-specifc characterization. Therefore, for more infor­mation about the general content of the ‘Introduction’ and ‘Discussion’ sections, we refer to (and strongly encourage students to read) Chap. 2 and the chapter “Manuscript Structure and Content” of the Publication Manual of the American Psychological Association (2010). However, the ‘Methods’ and ‘Results’ sections of many corpus-linguistic papers share some important commonalities that we believe can be usefully summarized for less experienced writers and/or beginning corpus researchers. 
26.2 The ‘Methods’ Section 
Given that we prefer to see corpus linguistics as a method rather than a theory (see the special issue of the International Journal of Corpus Linguistics 15(3) for a debate of these two views), we believe outlining the methodological details of a corpus study in a way that is comprehensive enough is absolutely central. At a very high level of abstractness, there is really only one rule, which says it all: The characterization of the methods employed in a paper needs to be so precise that the study is reproducible or, more explicitly, that someone who wanted to explore the same thing and had access to the same raw data would be able to follow the description in the methods section such that they end up with the same result (cf. Berez-Kroeker et al. 2017, Branco et al. 2017), and by extension, the study is replicable (cf. Porte 2012). 
To meet this objective, the methods section must include a number of compulsory parts. First, it should start with a detailed description of the corpus used. Each corpus type (e.g. diachronic corpora, web corpora, parallel corpora) comes with its own specifcities and these should be carefully described. For example, the reader of a 
S. Th. Gries and M. Paquot 
learner corpus study needs to know as much as possible about the learners who produced the language samples (what are their language background, profciency level in the foreign language, age, etc.) and the task settings (what were the learners requested to produce? A timed argumentative essay? A spontaneous dialogue with peers?) (cf. Part III for more about the specifcities of different corpus types). If relevant, this section should also specify which version of the corpus and what types of annotations available with the corpus were used to answer the research questions. For example, a corpus such as the British National Corpus World XML edition (BNC, BNC Data Consortium 2001, http://www.natcorp.ox.ac.uk/) comes with Part-of-Speech (POS) tags assigned automatically using CLAWS C5 tagset. As far as possible, this description should be accompanied with (a) a proper reference to the corpus used, in the form of a citation to a scientifc article or a data paper in which its authors introduced the dataset (corpus compilers often specify how they would like the corpus to be referred to),1 and, (b) if the dataset is available for research, a permanent link to where to fnd the corpus. 
After a general description of the corpus used, detailed information about corpus pre-processing should be presented. Two major types of pre-treatment can be distinguished, i.e. automatic annotation and sampling. As for the former, if answering the research questions requires an unannotated corpus to be automatically tagged or parsed, details about the tool used will need to be provided. These include: 
a. 
The full name of the tool and its version number. 

b. 
The selected parameters: Depending on the tool, it may be necessary to mention the tagset or the language model used (ideally with a URL). For example, the TreeTagger (Schmid 1994) can be used with two distinct English parameter fles that contain different tagsets trained on different corpora, https://www.cis.uni­
muenchen.de/~schmid/tools/TreeTagger/). 

c. 
General information on its reliability: If no information is available, it may be necessary to conduct a precision and recall study (see Chap. 2). 

d. 
A full reference and a download link. 


Today’s corpora can be huge and, depending on the linguistic feature under study, it may often be only possible to analyze a random sample of instances. The method used to select the fnal dataset should also be carefully described. A word of caution is warranted here: many off-the-shelf tools offer a ‘random selection’ option that makes it possible to retrieve randomly x instances of the searched item out of the total number of occurrences. While this is a common approach, it may not always be the best solution. Depending on the research question, it may be necessary to be able to statistically control for autocorrelation or priming effects. These notions refer to the fact that often the dependent, or response, variable is not just correlated with a variety of independent, or predictor, variables, but also with previous values of itself. For instance, speakers who have used one of a set of functionally very similar 
1This is also a means of bringing credit and recognition to all those involved in corpus compilation. 
constructions are, all other things being equal or at least very similar, more likely to use that construction again than they would be if they had not used it before. Such effects can be quite strong and predictive on their own: Gries (2016a)shows that future choices (will vs. going to vs. shall) can be predicted with more than 80% accuracy just on the basis of the last choice of a speaker, which means that most of the time the sampling unit should not be the individual usage event but the conversation, the (newspaper) article, the corpus fle, etc. so that information from previous choices can be accounted for. 
The next step is to report on the methods used to retrieve the fnal set of linguistic item/s. This also requires the author to describe how, for instance, false hits, i.e. instances of something in a corpus that fts the structural description or regular expression used for data retrieval, but that turn out to not actually be instances of the phenomenon in question, were identifed. For instance, in a shallowly-parsed corpus, one might search for instances of V NP NP with the goal of retrieving ditransitive constructions such as He gave him a book, but that search might also return instances of object complementation such as He called him a liar, which would have to be fltered out. Again, each corpus processing tool used at the retrieval stage should be listed, described and properly referred to. Importantly, the exact search expressions should be reported and the settings used should be specifed (e.g. list of word separators, minimum frequency or dispersion threshold for word lists; cf. Part II for the settings typically associated with different corpus methods). If a programming language was used, exact search expression (in particular more complex regular expressions) should always reported; depending on the complexity of all analytical procedures, even providing pseudocode can help readers comprehend the research reported on better. 
The ‘Methods’ section of a corpus-linguistic paper also needs to contain information on how the notions/concepts considered relevant for the analysis were operationalized as variables and how annotations were added with regard to the variables that may affect the linguistic phenomenon under study. Following Wilkinson et al. (1999: 595), a “variable is a method for assigning to a set of observations a value from a set of possible outcomes.” For example, a variable called “NP length” might assign to each NP one value quantifying its length. But if one considers the length of constituents (e.g. for a study of a syntactic alternation), the readers needs to be told how constituents’ lengths were measured: actual time in ms (from an audio/video corpus), length in characters, phonemes, syllables, morphemes, words, phrases, ... If one considers the animacy of referents of constituents for the same study, the readers needs to be told how many and which different levels of animacy were distinguished: two (animate vs. inanimate), three (human, non-human animate, inanimate), etc. 
The result of any annotation process should virtually always be a spreadsheet in the so-called case-by-variable, or long, format in which. 
• 
every row is one case, i.e. measurement of the dependent variable under investigation; 

S. Th. Gries and M. Paquot 

• 
every column is one variable – independent variable or otherwise – for which each case was annotated. (See Gries 2013: Sect. 1.3.3 for additional discussion). 


This is because, as repeatedly mentioned in the chapters on statistical testing (Part V), most statistical tests are easiest done on data in this format.2 
In many cases, it might be necessary to explain in the ‘Methods’ section how elements were treated whose classifcation is not obvious: If one measures the length of a subject in characters, are spaces or punctuation marks included? If one distinguished the above three animacy levels, how was God classifed or This virus or The police? Things can be even more complicated when the notion to be explored is harder to operationalize. For instance, the degree of givenness, or cognitive accessibility, of the referent of an NP is a graded notion and can be variously (and only imperfectly) operationalized via, for example, the number of times the referent has been mentioned, if at all, in the preceding 10 or 20 clauses or by the distance to the last mention of the referent, if any, in the same preceding context. But even then one has to consider tricky questions such as whether the word fower is an antecedent for the word rose (because it is a superordinate term and therefore arguably evokes rose to at least some degree), or whether the word car is an antecedent for the word tire (because a car is a whole of which a tire is a part), etc. If multiple annotators are involved, one should provide at least an indication of interrater reliability and/or how differences in annotation decisions were resolved (cf. Spooren and Degand 2010; Fuoli and Hommerberg 2015). If only one annotator was involved, it is still recommended to maintain a coding book/logs and report an intra-rater reliability score, i.e. a score that measures the reliability of the coding by a single researcher based on the repeated coding of the same set of data at a later time (cf. Loewen and Plonsky 2015: 164). Although this is still too rarely often done in the feld (including by us), reporting intra-rater reliability would appear good practice given that linguistic data are typically annotated by just one researcher, especially in M.A. and Ph.D. dissertations. The methods section is also the place to mention whether any instrument/material/coding scheme developed for the purposes of the study has been made publicly available (either in the form of an appendix to the article or uploaded on the author’s website or onto an online repository such as IRIS, i.e. a collection of instruments, materials, stimuli, data coding and analysis tools used for research into second languages (Marsden et al. 2016, https://www.iris-database. org). 
Often, the next kind of information the reader needs to learn about is some descriptive statistics of the data of the type discussed in Chap. 17. This might involve frequency tables of categorical variables as well as box plots and/or ecdf plots for numeric variables. The purpose of these descriptive summaries is that readers get a better overview of the data (including information about missing or unmeasurable/unclassifable data: how many such cases there were, how they were dealt with, etc.). This also means that care has to be taken to make sure the right 
2See Gries (in press) for more information about how to carry out the tasks of retrieval and annotation discussed above. 
kinds of statistics are reported because even descriptive statistics sometimes come with some assumptions that need to be borne in mind: For instance, (i) it does not make much sense to report one overall mean for a Zipfan or a bimodal distribution of a numeric variable and (ii) it does not make sense to report any measure of central tendency without a measure of dispersion. For categorical dependent variables, frequencies/percentages should be reported (as they constitute the baseline against which any models will be evaluated). 
Also, it is often helpful to discuss what, if any, other kinds of exploratory steps were undertaken and what, if any consequences they had for the analysis subsequently reported. For instance, in many studies, numeric variables have to be transformed to make them more ‘well-behaved’ in a subsequent statistical analysis so readers need to know which transformations were applied (logging, square-root, inverse, centering, z-standardizing, logit, etc.), why they were applied, how outliers were dealt with and so on. In the cases of categorical variables, readers should be told if certain categories that were distinguished at an earlier stage were then confated for conceptual/theoretical or statistical reasons (e.g., when one or more categories are so rare that their rarity would cause problems for subsequent statistical analyses); Zuur et al. (2010) provide a nice overview of many such exploratory steps. 
If more than just descriptive statistics are computed, i.e. statistics of the kinds discussed in Chaps. 20–25, then it is necessary to discuss how it was made sure that the data meet the assumptions of the method that was ultimately employed: If a chi-squared test for independence was computed, were all data points independent of each other and were the expected frequencies large enough? If a t-test for independent samples was computed, were the data checked for normality and what was the result? If a regression model with multiple predictors was computed, how was collinearity diagnosed (and addressed)? See Chap. 20 and following for more info about the assumptions of statistical tests. Also, the reader needs to get a precise explanation of all the often many steps of the statistical analysis. For example, for regression modeling, 
• 
did the analysis involve ftting and testing just a single model? If so, what was that model and why did it look the way it did – i.e., how did it test which hypotheses? 

• 
if the analysis involved ftting multiple models, was that a stepwise model selection process or a model amalgamation process? If it was the former, what was the direction of the selection process (forwards, backwards, hybrid) and which criterion was used (p, AIC(c), BIC, ...)? 

• 
did analyses have to be redone or changed because of problems emerging during the analysis or from initial results? For instance, did the analysis reveal that 0.5% of the data exhibit a degree of leverage on the results that distorted the general trend so it was decided to re-do everything without these 0.5% of the data? 


It is also important to explain how many tests were performed on one and the same data set to test which/how many hypotheses and which, if any, corrections for multiple testing were employed. 
S. Th. Gries and M. Paquot 
As is obvious from the above, a lot of corpus-linguistic work does not discuss all their methodological aspects in suffcient detail, but in order to at least begin to approach the ideal of reproducibility, it is essential that all this information be provided at a suffcient level of detail. This also applies to the results, whose presentation we discuss in the next section. 
26.3 The ‘Results’ Section 
If good ‘Methods’ sections help ensure reproducibility, good ‘Results’ sections ensure comprehensibility and go a long way towards making a reader accept one’s conclusions. If only simple descriptive overview statistics (Chap. 17) are computed, then they might be all that is required for a results section, but chances are that more than such overview statistics are computed. In such cases, quite a few kinds of results are required. In the case of monofactorial statistics of the kind discussed in Chap. 20 or the regression modeling approach discussed in Chaps. 21 and 22, it is usually a good idea to begin with some overall numeric summary statistics. For many monofactorial tests, these would be overall test statistics, degrees of freedom, and one or more p-values; for many regression models or similar kinds of predictive models or classifers, these would be the overall signifcance test (e.g., an F-ora G2-test with their degrees of freedom and p), overall (adjusted) R2-values, and, for methods involving categorical response variables, precision/recall/accuracy statistics on either the modeled data or, even better, data from cross-validation (see, e.g., Kuhn and Johnson 2013: 20–26, 69–71). In addition, readers should be presented with the following kinds of statistics for regression models or equivalents of those for other statistical methods: 
• 
signifcance values for each variable (often, these result from comparing a model with a predictor in question against one without it); 

• 
coeffcients and signifcance tests for each coeffcient in the model (often, these refect the change in prediction from a reference level to another treatment level or a planned contrast). Ideally, the contrasts that are represented by a coeffcient are also provided to the reader for each such coeffcient as exemplifed in Table 26.2 for an independent variable called Animacy with three levels (human, the reference level, animate, and inanimate) so as to make understanding regression results easier especially for complex models with (many) categorical predictors (with many levels). 


Table 26.2 Reporting regression coeffcients 
Predictor  coeffcient  Se  t  P  
Animacyhuman . animate  0.272  0.111  2.45  0.171  
Animacyhuman . inanimate  3.142  0.577  5.445  0.017  


(One should use reasonable numbers of decimals: just because R can provide 10 does not mean the readers needs 10 ...). Given that regression coeffcients correspond to raw/unstandardized effect sizes, it follows that for any other kinds of test – mono-or multifactorial – the actual effects should be provided: the differences in means/medians, the slopes of a regression, etc. Ideally, these effects should come with confdence intervals/bands or some other indication of their certainty (see the various chapters in Parts IV and V for more info about reporting specifc statistics) so as to paint a clearer picture of the reliability/robustness of the numerical point estimates. 
Finally, the more complex the statistical analysis, the more important it is to provide a proper visualization of the results; the purpose of visualization is to represent/explain what would be harder to represent/explain in prose. That means, one does not need a bar plot of two percentages: this is a statistical result simple enough to not require visualization. On the other hand, the numerical results of a multifactorial multinomial regression model are likely to be virtually incomprehensible without any visual aids. Visualization comes with its own set of guidelines, the maybe most important of which involves the notion that a graph should contain all the information it aims to present but no more. In this regard, the following plot in Fig. 26.1 is lacking. 
The data that this plot is supposed to represent is nothing more than ten per­centages adding up to 100%, i.e. one-dimensional vector/sequence of 10 numbers. However, the original graph that Fig. 26.1 is reproducing, which was in fact a three-dimensional version of Fig. 26.1, represented these data with an extremely bad data-ink ratio: 
• 
the original chart utilized four dimensions (a three-dimensional chart plus different colors) and a non-informative background-shading effect; 

S. Th. Gries and M. Paquot 

• 
the percentages are represented in a pie chart although humans are much worse at comparing information in angles than they are at comparing locations of points or lengths of lines (see Cleveland and McGill 1985,Tufte 2001: 178); 

• 
Figure 26.1 and its original version are extremely redundant, given that the countries and their percentages are listed around the pie and again in a legend. 



In other words, this graph contains much more visual ‘information’ than is merited by the actual data it is supposed to represent and the way that visual ‘information’ is provided is redundant and does not go well with how humans perceive data well. In terms of data-ink ratio and any other principle of data visualization, Fig. 26.2 is a better representation: Each percentage (i.e., one-dimensional data point) is represented as a point on a line (i.e., a one-dimensional geometric construct), and no redundant information detracts from the message: 
In the case of regression modeling, the probably most informative way to present results is to have effects plots (cf. Chap. 21; see also Fox 2003 and Fox and Hong 2009) for every predictor to be discussed, which show predictions on the y-axis against predictors on the x-axis (and maybe with different kinds of points and lines) together with confdence intervals, Fig. 26.3 is an example from a study on that-complementation (Wulff et al. 2018), i.e. the question of whether learners of English say I thought the Borg assimilate other species or I thought that the Borg assimilate other species. The plot shows one effect from a regression analysis on how similar the learner choices are to imputed native-speaker choices; that is, absolute values of the dependent variable Deviation on the y-axis indicate how much a learner choice differs from a native speaker choice; the plot represents the effect of an interaction between the length of the complement subject (here, the Borg,onthe x-axis) and the register/mode (speaking vs. writing, represented in colors and with s/w respectively). More specifcally, it shows how the effect of the length of the complement subject is different between speaking and writing: in speaking, there is essentially no effect (because the regression line is horizontal), but in writing there is an effect such that with increasing subject length, deviation scores approach 0 (i.e. the learner choices become more nativelike). The plot contains (raw) effect sizes (the slope of the regression lines), their uncertainty (the confdence bands), labels for everything important (x-axis, y-axis, positive and negative y-value labels etc.), and as a visual representation of ft, the actually observed data points are also included as grey circles. Note that, since this is an effects plot, the effect shown – the interaction of complement subject length and register/mode – is represented while every other effect in the regression model is controlled for, which is important because the frequently used plots of observed means/correlations do not do that. 

Similar recommendations hold for similar kinds of classifers such as trees and forests (Chap. 25) and other machine-learning algorithms. For some other methods, the resulting visualization might actually be the main result, as in cluster analyses (see Chap. 18) or classifcation trees, but there, too, it is important to be aware of the data-ink ratio and present everything that is required, but no more. 
Lastly, it is typically a good idea to provide some information on the validity of the results. For instance, regression models, but also many other statistical methods, are based on assumptions regarding the data, which means it is important to tell readers that these assumptions were checked (in a process called model validation or diagnostics). This part usually does not need to be long, but, for instance, providing the information that one’s model did not suffer from collinearity, overftting, heteroscedasticity, etc. (see Chaps. 22–25 for these notions) is strongly encouraged. 
S. Th. Gries and M. Paquot 
26.4 Concluding Remarks 
The level of details we advocate for in this chapter may seem a bit daunting at frst. It is however essential for at least the following reasons. First, a complete description of the study design (from data collection to data analysis) enables the reader to evaluate the appropriateness of the data and methods used for answering the research questions as well as the reliability and validity of the results. Second, and as already mentioned in the chapter, detailed information on the data and how it was analyzed is also a prerequisite for reproducibility and replicability. Third, it is only by paying more attention to methodology that the feld of corpus linguistics will answer repeated calls for developments in study quality, i.e. “the combination of (a) adherence to standards of contextually appropriate methodological rigor in research practices and (b) transparent and complete reporting of such practices” (Plonsky 2013: 657). Scholars have observed a signifcant number of weaknesses related to sampling practices, data analyses and reporting practices in corpus linguistics. Paquot and Plonsky (2017), for example, provided the frst empirical assessment of quantitative research methods and study quality in learner corpus research and reported high rates of both underreported and missing data. Fourth, improving reporting practices will also permit meta-analysts to conduct comprehensive and empirically grounded reviews of previous research, a practice that has only sparsely been adopted in the feld of corpus linguistics (see Chap. 27). 
References 
American Psychological Association. (2010). Publication manual of the American Psychological Association (6th ed.). Washington, DC: American Psychological Association. Berez-Kroeker, A., Gawne, L., Kung, S., et al. (2017). Reproducible research in linguistics: A position statement on data citation and attribution in our feld. Linguistics, 56(1), 1–18. 
BNC Consortium. (2001). The British National Corpus, version 2 (BNC World). Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http:// www.natcorp.ox.ac.uk/. Accessed 30 August 2019. 
Branco, A., Cohen, K. B., Vossen, P., Ide, N., & Calzolari, N. (2017). Replicability and reproducibility of research results for human language technology : Introducing an LRE special section. Language Resources and Evaluation, 51(1), 1–5. 
Cleveland, W., & McGill, R. (1985). Graphical perception and graphical methods for analyzing scientifc data. Science, 229(4716), 828–833. Fox, J. (2003). Effect displays in R for generalised linear models. Journal of Statistical Software, 8(15), 1–27. Fox, J., & Hong, J. (2009). Effect displays in R for multinomial and proportional-odds logit models: Extensions to the effects package. Journal of Statistical Software, 32(1), 1–24. 
Fuoli, M., & Hommerberg, C. (2015). Optimising transparency, reliability and replicability: Annotation principles and inter-coder agreement in the quantifcation of evaluation expressions. Corpora, 10(3), 315–349. 
Gries, S. Th. (2013). Statistics for linguistics with R (2nd rev. & ext. ed.). Boston/New York: De Gruyter Mouton. 
Gries, S. Th. (2016a). Variationist analysis: Variability due to random effects and autocorrelation. In P. Baker & J. A. Egbert (Eds.), Triangulating methodological approaches in corpus linguistic research (pp. 108–123). New York: Routledge, Taylor and Francis. 
Gries, S. Th. (2016b). Quantitative corpus linguistics with R. 2nd rev. & ext. ed. New York & London: Routledge, Taylor & Francis Group. 
Gries, S. Th. (in press). Managing synchronic corpus data with the British National Corpus (BNC). In A.L. Berez-Kroeker, B. McDonnell, E. Koller, & L. Collister (Eds.), MIT open handbook of linguistic data management. Cambridge, MA: The MIT Press 
Kuhn, M., & Johnson, K. (2013). Applied predictive modeling. Berlin/New York: Springer. 
Loewen, S., & Plonsky, L. (2015). An A-Z of applied linguistics research methods.New York: Palgrave. 
Marsden, E., Mackey, A., & Plonsky, L. (2016). The IRIS repository: Advancing research practice and methodology. In A. Mackey & E. Marsden (Eds.), Advancing methodology and practice: The IRIS repository of instruments for research into second languages (pp. 1–21). New York: Routledge. 
Paquot, M., & Plonsky, L. (2017). Quantitative research methods and study quality in learner corpus research. International Journal of Learner Corpus Research, 3(1), 61–94. 
Plonsky, L. (2013). Study quality in SLA: An assessment of designs, analyses, and reporting practices in quantitative L2 research. Studies in Second Language Acquisition, 35(4), 655–687. 
Porte, G. (2012). Replication research in applied linguistics. Cambridge: Cambridge University Press. 
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. Proceedings of international conference on new methods in language processing, Manchester, UK. 
Spooren, W., & Degand, L. (2010). Coding coherence relations: Reliability and validity. Corpus Linguistics and Linguistic Theory, 6(2), 241–266. 
Tufte, E. (2001). The visual display of quantitative information (2nd ed.). Graphics Press: Cheshire, CT. 
Wilkinson, L., & The Task Force on Statistical Inference. (1999). Statistical methods in psychology journals. American Psychologist, 54(8), 594–604. 
Wulff, S., Gries, S. Th., & Lester, N. A. (2018). Optional that in complementation by German and Spanish learners: Where and how German and Spanish learners differ from native speakers. In 
A. Tyler, L. Huan, & H. Jan (Eds.), What does applied cognitive linguistics look like? Answers from the L2 classroom and SLA studies (pp. 97–118). Berlin & Boston: De Gruyter Mouton. 
Zuur, A. F., Ieno, E. N., & Elphick, C. S. (2010). A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution, 1(1), 3–14. 
Chapter 27 Meta-analyzing Corpus Linguistic 
Research 
Atsushi Mizumoto, Luke Plonsky, and Jesse Egbert 
Abstract Research synthesis and meta-analysis (RS/MA) comprise a set of proce­dures for conducting comprehensive and empirically-grounded reviews of previous research. In contrast to traditional literature reviews, RS/MA provide enhanced objectivity and systematicity. It is not surprising, therefore, that researchers have turned in recent years to RS/MA as a means to bring together and examine previous research in a number of individual subdomains. Corpus researchers, however, have applied RS/MA only sparsely and in a very small number of subdomains. Considering the broad range of questions addressed in corpus linguistics, we believe there is massive potential for meta-analysis as a means to systematically synthesize fndings in the feld of corpus linguistics. In order to enable such efforts, the chapter provides both a conceptual and procedural introduction to RS/MA, addressing all major steps including (a) defning the domain and searching for primary literature, 
(b) 
developing and implementing a coding scheme, (c) calculating and aggregating effect sizes (using the langtest.jp web app and the metafor package in R), and 

(d) 
interpreting results. The discussion considers each step with respect to the challenges unique to meta-analyzing research in corpus linguistics. We also propose a novel use of corpus linguistic techniques as a means to enhance the effciency and perhaps accuracy of extracting synthetic data from primary studies. 


Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-46216-1_27) contains supplementary material, which is available to authorized users. 
A. Mizumoto Kansai University, Suita, Japan e-mail: mizumoto@kansai-u.ac.jp 
L. Plonsky (•) · J. Egbert Northern Arizona University, Flagstaff, AZ, USA e-mail: Luke.Plonsky@nau.edu; Jesse.Egbert@nau.edu 
© Springer Nature Switzerland AG 2020 
M. Paquot, S. Th. Gries (eds.), A Practical Handbook of Corpus Linguistics, 
https://doi.org/10.1007/978-3-030-46216-1_27 
A. Mizumoto et al. 
27.1 Introduction 
Meta-analysis comprises a set of procedures for conducting comprehensive and empirically grounded reviews of previous research. In contrast to traditional liter­ature reviews, meta-analysis provides enhanced objectivity and systematicity. It is not surprising, therefore, that applied linguists have turned in recent years to this technique as a means to bring together and examine previous research in several individual subdomains. In fact, we are aware of over 200 meta-analyses in the feld, largely within the realm of second language acquisition (see Plonsky and Oswald 2014). 
Corpus linguists, by contrast, have applied research synthesis and/or meta-analysis only sparsely and in very few subdomains. Among the few such studies, Durrant (2014) examined the correlation of collocational strength and L2 learner knowledge of multiword units across a number of studies. As is common in meta-analysis, the main relationship of interest was examined both overall and as a function of variables that were predicted to explain variability in that relationship (i.e., moderators) which in this case were the different collocational indices and the choices of reference corpora found in primary studies. Two recent studies have meta-analyzed the use of corpora and corpus-based teaching materials for their effects on second-language (L2) development (Boulton and Cobb 2017) one of which focused exclusively on vocabulary learning (Lee et al. 2019). Paquot and Plonsky (2017)also applied meta-analytic techniques in a methodological synthesis of learner corpus research. 
Considering the broad range of questions addressed in corpus linguistics, we believe massive potential exists for meta-analysis as a means to synthesize fndings systematically in this feld. Some areas within corpus linguistics that may be ripe for meta-analysis include research on register variation (see, e.g. Biber 2012; Egbert and Biber 2017), the effect of L1 on L2 acquisition (see, e.g., Gries and Wulff 2013; Paquot 2017;Wulff 2016), and the effect of persistence (priming) (see, e.g., Szmrecsanyi 2005; Gries and Kootstra 2017; Bernolet et al. 2016). 
To enable such efforts, this chapter provides both a conceptual and procedural introduction to RS/MA, addressing all major steps including: (a) defning the domain and searching for primary literature, (b) developing and implementing a coding scheme, (c) calculating and aggregating effect sizes (using R), and (d) interpreting results. At each step we consider and discuss both best practices as well as some of the many challenges unique to meta-analyzing research in corpus linguistics. But frst, our chapter begins with a conceptual introduction to this procedure and an explanation of some of its major benefts over traditional reviews. 
27.2 Fundamentals 
Meta-analysis is a type of literature review and a set of statistical methods used to synthesize the results of primary studies on a given topic. One way to understand 
meta-analysis is in parallel to primary research. Whereas most studies collect data from individual participants or texts, a meta-analysis does so from an exhaustively collected sample of studies, extracting substantive and methodological information from each report as well as statistical results (i.e., an effect size). (We would like to note here that it is this fnal feature—the statistical aggregation of results on a standardized index—that distinguishes meta-analysis from other types of syntheses and systematic reviews.) Because meta-analysis exhaustively collects and aggregates fndings across a number of studies, it can serve as a way to determine the validity and generalizability of a given effect or relationship across, for example, different languages or registers. Meta-analysis can also point to gaps and weaknesses, trends, and inconsistencies in the literature. 
However, using meta-analysis to quantitatively synthesize a body of research is not the only means by which to summarize previous research and show future prospects. What then are the advantages of meta-analysis? The most common or conventional method of summarizing previous research is the “narrative (literature) review.” In a narrative review, the researcher summarizes the previous research using his or her own way of making sense of the available literature. The researcher searches for relevant studies, reads them, likely takes notes in some fashion, and then summarizes his/her fndings. A narrative review based on studies of L2 writers’ accuracy and complexity might state, for example, that “whereas fve studies found a signifcant correlation between writing accuracy and writing complexity, one found a non-signifcant correlation.” The researcher may then conclude that there is likely to be a signifcant correlation between the two variables. There are a number of problems with such an approach, each of which meta-analysis seeks to improve on in several respects (see for example Cooper et al. 2009; Norris and Ortega 2006; and Oswald and Plonsky 2010 for more detail). 
The frst major distinction between a traditional and meta-analytic approach is that meta-analysis involves a higher level of systematicity in all phases of considering previous research. For instance, in a traditional review, there is likely a great deal of variability in how any single reviewer might identify relevant primary studies. The reviewer may be more likely to include studies carried out by researchers familiar to them or in more visible outlets. This approach, though common, will very likely yield a biased view of the relationships or phenomena in question. By contrast, in meta-analysis, procedures for collecting and aggregating previous research are comprehensive—if not exhaustive—and reported in detail. Therefore, the fnal paper has greater rigor and transparency, allowing for improved re-analysis and extension by other researchers as necessary (see Larson-Hall and Plonsky 2015 on the importance of reproducibility). 
The notions of systematicity and objectivity can also be observed in the meta-analytic approach to extracting information across the sample of studies that are obtained. More specifcally, meta-analysts consider and code for all substantive (e.g., corpus design, linguistic features) and methodological features that may be relevant to understanding the domain of interest (e.g., Johnson 2017). As the number of studies in a given domain grows, a variety of factors such as differing sample techniques or annotation tools are also likely to increase, meaning that 
A. Mizumoto et al. 
inconsistencies in research results are more likely to appear. By contrast, traditional reviews are generally much less systematic, leaving them more exposed to the limitations and idiosyncrasies of human memory and error. 
Another feature that distinguishes meta-analysis from more traditional reviews is the use and aggregation of effect sizes. Narrative reviews are often primarily concerned with whether the results of primary studies are statistically signifcant (e.g., Is there a difference in the use of feature X between text type A and B?). Statistical signifcance, usually expressed using p values, is problematic on a number of levels; for starters, the information it provides is generally unstable (i.e., fuctuates as a function of sample size) and binary (i.e., resulting in a dichotomous outcome and, therefore, not particularly informative; see Norris 2015; Plonsky 2015b). Effect sizes, by contrast, express the extent of such a difference or relationship. In most cases, the effect size is expressed using a standardized metric such as Cohen’s d. By doing so, results can be more directly compared and combined across studies (Plonsky 2012a). Consequently, there are at least eight journals in applied linguistics that now require the reporting of effect sizes for quantitative research (see e.g., Norris et al. 2015; Mahboob et al. 2016), a move that aligns practices in our feld with the recommendations of the American Psychological Association and other learned societies in the social sciences (APA Publications and Communications Board Working Group on Journal Article Reporting Standards 2008). Meta-analyses extract or compute effect size indices to then calculate an estimate of the overall (average) effect or relationship in question as well as an estimate of its variability (see Sect. 27.3.3). 
27.3 A Practical Guide to Meta-analysis with R 
Having laid out the conceptual foundation for meta-analysis, we now move to an explanation of meta-analytic procedures. More specifcally, in this section, we describe the following major stages involved in conducting a meta-analysis in general and specifcally within the realm of corpus linguistics: 
1. 
Defne the domain and search for primary literature 

2. 
Develop and implement a coding scheme 

3. 
Calculate and aggregate effect sizes 

4. 
Interpret the results 


In line with the rest of this volume, descriptions of analytical procedures as illus­trated are based on R. However, the majority of the stages involved in conducting a meta-analysis are not particular to any software package. 
27.3.1 Defning the Domain and Searching for Primary Literature 
The frst step in conducting a meta-analysis is to defne the research topic (domain) to be meta-analyzed. If possible, is it also advisable to devise concrete research questions, which will likely be very similar to but perhaps broader than the questions posed in primary studies conducted within the domain. For example, whereas an individual study may be concerned with the difference in use of a particular personal pronoun in two sub-registers, the research question posed at the meta-analytic level might be expanded to include differences in all personal pronouns across all registers. It is nearly impossible to overstate the importance or diffculty of this stage. The synthesist’s domain-specifc knowledge here is indispensable concerning what type(s) of questions have been suffciently addressed in the primary literature, for example, or what questions are theoretically and/or practically relevant to address. At the same time, even a veteran researcher will fnd him/herself adjusting their understanding of the domain in question as they delve into the literature. 
One question that often comes up at this stage is whether it is preferable to take on a broader or a narrower scope. Neither is necessarily superior. However, if the research question is too narrow, it will restrict the studies that can be included and, by consequence, the generalizability of the meta-analytic fndings. At the same time, a very broad research question limits the range of issues and fndings that can be addressed in detail. Broader domains also expose the study to ‘apples and oranges’­type critiques wherein the heterogeneity of the sample is such that aggregation of fndings across studies is deemed inappropriate. As in many other aspects of the meta-analytic process, there is no clear best option here; rather, domain breadth is a choice to be made and justifed by the researcher. 
Once the substantive and methodological scope are defned, the researcher sets out to comprehensively search for and collect the studies that fall within this domain. There are many different techniques for locating primary studies, many or all of which can and should be applied. This approach will certainly lead to redundancies; however, redundancy is a small price to pay to achieve a sample that is comprehensive and (nearly) free of bias. 
One initial and perhaps obvious choice involves searching academic databases such Educational Resources Information Center (https://eric.ed.gov), Linguistics and Language Behavior Abstracts (LLBA; http://www.proquest.com/products­
services/llba-set-c.html), and PsycINFO (https://www.apa.org/pubs/databases/ psycinfo/), all recommended by In’nami and Koizumi (2010), Oswald and Plonsky (2010), and Plonsky and Brown (2015). The keywords used in such databases must be chosen carefully, balancing the logistical challenge of false positives (i.e. irrelevant studies that are included in the search results) with the threat to validity presented by false negatives (i.e. relevant studies that are excluded from the search results). We particularly recommend searching both Google and Google Scholar as well which, in our experience, provide numerous sources not visible to traditional, academic databases. One reason for this might be the fact that most databases only 
A. Mizumoto et al. 
search for keywords in the title, abstract, and keywords of articles, books, and book chapters. Google and Google Scholar, by contrast, search full texts. A second technique is to the search within academic journals and publications in the relevant feld. For example, one might search for relevant studies using the search features of journals such as Corpora, Corpus Linguistics and Linguistic Theory, and the International Journal of Learner Corpus Research. Other techniques include (a) visiting the websites of prominent researchers in the target area, (b) requesting references on listservs, (c) forward-citations (i.e., searches for studies that have cited seminal studies in a given domain), and (d) ‘reference-digging’ (i.e., searching through the references of relevant studies and previous reviews), and (e) online bibliographies such as the Learner Corpus Bibliography; https://uclouvain.be/en/ research-institutes/ilc/cecl/learner-corpus-bibliography.html). In all searches, the principle to be adhered to is comprehensiveness. We acknowledge that this is much easier said than done. One of the current authors was recently involved in a project that required over 150 h of searches before determining that the sample was complete. 
During and following the search, the researcher must make decisions regarding which studies will be included and which will not. To the extent that it is possible, these decisions are best made apriori. However, there are inevitably novel cases that do not ft into the pre-determined decision rules. The meta-analyst must then expand his or her set of eligibility criteria, noting for inclusion in the manuscript all decisions and changes made along the way as with all search techniques applied. For example, in a recent meta-analysis of the effects of corpus use on L2 vocabulary development, Lee et al. (2019) only considered studies that included a comparison group and that accounted for any pretreatment differences by explicitly randomizing participation, pretesting, or other means. Lamentably, most meta-analyses are also often forced to eliminate large numbers of studies due to insuffcient reporting of data required to calculate an effect size, a problem that is as pervasive in corpus linguistics (see Paquot and Plonsky 2017) as in other linguistic domains (e.g., Larson-Hall and Plonsky 2015; Marsden et al. 2018; Plonsky and Gass 2011). 
27.3.2 Developing and Implementing a Coding Scheme 
After locating the set of primary literature to include in a meta-analysis, the data collection phase can begin. Retaining the parallel to primary research, meta-analytic research involves designing a data collection instrument that will be used to code three main types of data from each participant (study). We recommend conducting this phase of the study using whichever spreadsheet software authors prefer, with studies in rows and coding scheme items in columns. 
The researcher will frst code basic identifcation information related to each study. This will include paper titles, the type of publication (journal article, book chapter, doctoral dissertation), author names, year of publication, and so forth. This information can be considered comparable to metadata for texts in a corpus or to demographic data in a study with human participants. 
Study Identifcation  author(s), year, publication status, type of publication, outlet, title  
Study features and moderators  name of corpus; corpus size (number of texts, number or words); frst language; second language; profciency level(s); register(s); dialect(s); tagger; coding by hand or automatic; software  
Outcomes (effect sizes) and associated data  effect size (e.g., d, r, OR, frequency PMW); estimate of measurement error  

In addition to study identifcation, each study must also be coded for a number of different study features. Some features are worth coding for as a means to comprehensively describe and evaluate the domain of interest. Others are coded as potential moderators. Moderators are variables that, based on theoretical or practical concerns, are anticipated to infuence the overall, meta-analytic results. Such variables can be substantive in nature such as the language(s) being investigated, register/genre, the types of features in question; they can also be methodologically oriented. For example, the relationship between linguistic complexity and a particu­lar linguistic feature might vary as a function of (i.e., be moderated by) the measure of complexity in primary studies and/or the tools used to identify instances of the target features. Table 27.1 presents a set of items that might be included for meta-analyses of corpus linguistics research. 
Some of the items coded for will be very straightforward; others, however, will involve judgment and inference on the part of the coder. It is also very often the case that the meta-analyst will be forced to choose between one or more operational defnitions available in previous literature for key variables. In a vibrant feld such as corpus linguistics, there is no shortage of debate over fundamental terms such as collocation and dispersion (e.g., Burch et al. 2017). For these reasons, and in order to provide readers with an indication of the trustworthiness of the dataset, we recommend having at least two people involved in coding. An estimate of inter-rater reliability such as a Kappa coeffcient can then be reported (e.g., Plonsky 2011). For example, a subset of 20 studies included in Paquot and Plonsky’s (2017) synthesis were double-coded. The authors reported an interrater agreement of 88% (Cohen’s kappa .76), and they explained this value as pertaining to a small set of problematic items. These items were then re-visited and re-coded by another, more experienced research assistant, allowing the authors and readers to place greater confdence in the study’s fndings. If agreement on one or more items is not able to be reached through discussion and/or further coding, authors should alert readers to the potentially erroneous results. More generally, as Oswald and Plonsky (2010) assert, “researchers should be as transparent as possible about the process of meta-analysis that they undertook, so that readers can better understand and interpret the results, if not replicate them on their own” (p. 105). Therefore, we recommend making your coding scheme available online such as on the IRIS Repository of 
A. Mizumoto et al. 
Instruments and Materials for Research into Second Languages (https://www.iris­
database.org/) (Marsden et al. 2016). As of this writing, there are 90 records of materials on the IRIS Database with the tag ‘meta-analysis’, at least two of which are meta-analyses of research in corpus linguistics (Boulton and Cobb 2017; Paquot and Plonsky 2017). 
It should be noted here that there is a budding line of meta-analytic methods largely outside of linguistics that attempts to use text mining to carry out certain aspects of the synthetic coding process. Within linguistics, Plonsky (2015a) has also proposed a novel use of corpus linguistic techniques as a means of enhancing the effciency, reliability, and perhaps accuracy of extracting synthetic data from pri­mary studies. Specifcally, he built a 147-million-word corpus from approximately 22000 articles across 22 journals related to L2 research. He then used AntConc (Anthony 2014), a freeware corpus analytical toolkit for concordancing and text analysis, to extract frequencies of demographic features and variables, including learner information such as participants’ L1 and target language, educational and research contexts, and country of origin of the authors of studies. The study demonstrated tentatively that this method could be applied to automatically extract substantive and methodological features from primary studies. For a discussion of the potential of such techniques from outside of linguistics, see Jonnalagadda et al. (2015). 
The third type of data that must be recorded includes study outcomes expressed as effect sizes. Effect sizes are quantitative indices of an effect or relationship (Plonsky 2012a). Various types of effect sizes exist, but the three most prevalent ones include (a) the standardized mean difference (usually expressed as Cohen’s d), 
(b) the correlation coeffcient (r), and (c) the odds ratio (OR) (e.g., Wilson 2002). 
Cohen’s d is generally appropriate when two groups are being compared. The formula for this index is quite straightforward: 
Mgroup1 - Mgroup2 
d = 
SDpooled 
Dividing group differences by their combined standard deviation, in effect, yields a standardized index and allows differences to be understood much like z-scores. This process also enables researchers to compare and aggregate results across studies even if they are based on different scales. For example, Boulton and Cobb (2017), examined the effects of corpus-based interventions known as ‘data-driven learning’ (DDL) on L2 development. The 64 studies in their sample typically involved providing one group of learners with DDL-based instruction and another with traditional, non-corpus-based materials. The study revealed a meta-analytic result of d = .95, indicating that learners in corpus-based treatments scored on average approximately one standard deviation above those who received traditional instruction. 
We note that mean difference effects (d) are common within (quasi-)experiments, but they are not limited to such designs. The same type of contrast could be applied and d values could be calculated when comparing occurrences of linguistic features in different types of texts (e.g., across registers; between native and non-native speakers). 
Another commonly observed effect size is the correlation coeffcient (r). Corre­lations express the strength of the association between a pair of measured variables (see Norouzian and Plonsky 2018). As many readers will recognize, correlations are quite commonly found in primary studies within corpus linguistics. Imagine you are synthesizing studies that look at the relationship between L2 users’ collocational knowledge and frequency of certain collocations in L1 corpora, as Durrant (2014) did. Given this research question, the appropriate choice of effect size would be the correlation coeffcient (r), since it expresses the strength of the association between a pair of measured variables: L2 collocational knowledge and L1 frequency of collocations. This effect size would then speak to the validity of the underlying assumption that collocational knowledge may be due in part to the input that learners are exposed to. Following an extensive search, Durrant included correlational results of this relationship from 19 different tests (total N = 1568). 
A third effect size, the odds ratio (OR) is relevant when the dependent variable is binary. Such variables are common in variationist corpus linguistics that focus on predicting linguistic choices when two alternating variants of a particular feature are possible (e.g. Gries 2016). Learner corpus researchers also work regularly with binary outcomes (e.g., accurate vs. not accurate). To date, there are no meta-analyses of OR-based research in corpus linguistics. However, the prevalence of this type of data would suggest that such studies will be forthcoming as the use of meta-analysis in the feld expands. 
An additional index that is somewhat particular to the context of corpus linguis­tics and that we would propose to include in this list is normed frequency. Although frequency counts are not generally perceived as an effect size, they are quantitative and standardized and would therefore appear to meet our defnition. Most central for our purposes in this chapter, normed frequency counts can be compared and combined across studies via meta-analysis. Imagine, for example, that a researcher was interested in understanding the frequency of a given set of formulaic sequences. Imagine further that a number of corpus-based studies had examined this issue using different corpora, with each study yielding a result expressed in occurrences per million words. These normed frequencies could then be combined (averaged) to estimate the overall occurrence of the set of target sequences. A similar operation could be applied for measures of dispersion as well. A meta-analyst could also then apply moderator analysis, described below, to examine whether the use of the formulaic sequences in question varies across different study features such as text type (percentage of text in written vs. oral mode). 
Effect sizes are not always reported in spite of frequent recommendations to do so by methodologists and publication guidelines (see Paquot and Plonsky 2017; Plonsky 2013). In such cases, the researcher will need to calculate them. The main challenge to doing so is that many papers do not include the values necessary to calculate the effect size. In these cases, one can contact the author to request his/her data. Efforts to date to obtain data from primary authors have been mixed at best (see Nicklin and Plonsky 2020; Plonsky et al. 2015). If the meta-analyst is unable 
A. Mizumoto et al. 
to obtain the data necessary to calculate an effect size, the study must be excluded (e.g., Boulton and Cobb 2017). This is another reason why clear and transparent data reporting practices are critical to scientifc advancement (Larson-Hall and Plonsky 2015). 
In addition to the effect sizes themselves, we strongly advise researchers to collect information closely associated with effect sizes such as the sample size and estimates of reliability. When human ratings are involved, reliability might be expressed in the form of interrater agreement. Accuracy rates, if known, might be appropriate in the case of tagged corpora (see chap. 2; see also Larsson et al. in press). 
27.3.3 Aggregating Effect Sizes 
The foremost goal of meta-analysis is to aggregate effect sizes across a set of primary studies. This can be accomplished as simply as calculating the average of the observed effects and an estimate of its corresponding variance. More commonly, however, a weighting function based on the sample size or some other measure of precision is included in the calculation of the mean. We give greater weight to effects from studies with larger samples because they tend to provide more precise estimates. 
Consider, for example, a hypothetical meta-analysis of studies examining the relationship (correlation) between syntactic and lexical complexity in L2 writing. A mock set of results is presented in Table 27.2. As we can see at the bottom of the second column, the unweighted average of the correlations across the sample of primary studies is .38. We can also see in the next column that there is a fair amount of variability in the number of participants or texts in each of these studies. All other things being equal, we would likely place greater trust in the correlations based on larger samples. In order to build this into our model, we simply multiply the observed correlation by the weight which, in this case, is the sample size (N). The weighted average is then calculated by dividing the sum of the product of the correlation and the weight by the weight itself. This procedure yields an overall (weighted) effect size (r = .27) that is smaller than the unweighted effect. Such a 
Table 27.2 Example of unweighted and weighted overall meta-analytic effects 
Study  r  N (weight)  r × N  (r × N)/N  
1  .28  21  5.88  
2  .36  23  8.28  
3  .18  72  12.96  
4  .23  34  7.82  
5  .52  12  6.24  
6  .62  6  3.84  
.38 = Munweighted  168 = sum  45.02 = sum  .27 = Mweighted  

Study  N  r  Moderator  
01  175  0.78  Close  
02  53  0.38  Distant  
03  250  0.69  Close  
04  103  0.89  Close  
05  86  0.52  Distant  
06  182  0.83  Close  
07  591  0.81  Close  
08  37  0.47  Distant  
09  107  0.54  Distant  
10  217  0.86  Close  

difference is expected in this case considering the fact that the studies with larger samples tended to yield smaller correlations. 
Once you input the necessary data from the primary studies, software used in meta-analysis such as R or Comprehensive Meta-Analysis (2013) will often auto­matically compute overall and weighted effect sizes and corresponding measures of variance. We have described the weighting process, however, because it is critical that meta-analysts understand the calculations that go into such results. 
We will now perform a meta-analysis using R version 3.6.0 (R Core Team 2019) and explain how to interpret the results. Table 27.3 shows hypothetical data obtained from a learner corpus. In this meta-analysis exercise, we will examine the relationship between L2 writing performance and lexical complexity in L2 learners. Assume that, based on a comprehensive search for relevant research, we have identifed and obtained 10 primary studies that examine this relationship and that meet our eligibility criteria. A larger sample will be necessary for valid conclusions to be drawn. For didactic purposes, however, the sample of K = 10 will suffce. 
We have entered the names of these studies in the “Study” column of Table 27.3. We have also entered their sample sizes in the “N” column, and the rs they reported between L2 writing performance and lexical complexity in the “r” column. Finally, we have entered a moderator variable in the “Moderator” column. As a moderator variable, we have coded whether the native language of the learners in each study had a “close” or “distant” typological relationship to the target language, which is English in all cases. 
R includes many packages for meta-analysis (see Polanin et al. 2017). For this exercise, we will use the packages metafor (Viechtbauer 2010) and meta (Schwarzer 2007), which can perform the main meta-analytic procedures. We will also use the MAc package (Del Re and Hoyt 2012) to perform subgroup (moderator) analysis, which allows us to understand how overall results may vary as a function of study features. 
First, we use the read.csv function to export our data from the .csv fle into R (this is the data shown in Table 27.3). 
A. Mizumoto et al. 
x  <­ read.csv("27_data.csv",  header  =  TRUE)  # read the  csv  
file in R  
x  # show the data  

Next, we load the metafor package (see chap. 17 for more info about how to install and load packages). We then use the escalc function to transform r to Fisher’s z. When calculating means for r, Fisher’s z transformation is often used to allow for a more normal distribution of values. We also compute the sampling variances of these r-to-z transformed values. By setting the append argument of the escalc function to TRUE, we append columns to the original data for Fisher’s z as “yi” and sampling variances as “vi.” (Note that for other arguments of escalc, you can consult the help fle in the usual manner by entering help(escalc) or ?escalc). These values are not immediately necessary for the meta-analysis process described in the next section, but they will be needed for the purpose of confrming the calculation steps and for the subgroup (moderator) analysis discussed later. 
library(metafor) # load the metafor package 
# calculate z and variances 
dat <-escalc(measure = "ZCOR",ri = r, ni = n, data = x, 
append = TRUE) 
dat # show the data 
Study n r Moderator yi vi 
1 Study01 175 0.78 close 1.0454 0.0058 
2 Study02 53 0.38 distant 0.4001 0.0200 
3 Study03 250 0.69 close 0.8480 0.0040 
4 Study04 103 0.89 close 1.4219 0.0100 
5 Study05 86 0.52 distant 0.5763 0.0120 
6 Study06 182 0.83 close 1.1881 0.0056 
(continued) 
7 Study07 591 0.81 close 1.1270 0.0017 8 Study08 37 0.47 distant 0.5101 0.0294 9 Study09 107 0.54 distant 0.6042 0.0096 10 Study10 217 0.86 close 1.2933 0.0047 
27.3.4 Aggregating Effects and Interpreting Results 
Next, we load the meta package. We then formally aggregate the observed effects using the metacor function. 
library(meta) # load the meta package res <-metacor(dat$r, dat$n) # conduct a meta-analysis res # show the result 
COR 95%-CI 1 0.7800 [0.7143; 0.8321] 2 0.3800 [0.1223; 0.5897] 3 0.6900 [0.6189; 0.7499] 4 0.8900 [0.8414; 0.9243] 5 0.5200 [0.3463; 0.6592] 6 0.8300 [0.7785; 0.8704] 7 0.8100 [0.7803; 0.8360] 8 0.4700 [0.1722; 0.6891] 9 0.5400 [0.3901; 0.6620] 10 0.8600 [0.8208; 0.8911] 
%W(fixed) 9.7 2.8 13.9 5.6 4.7 10.1 33.2 1.9 5.9 12.1 
%W(random) 10.5 8.9 10.7 9.9 9.7 10.5 11.1 8.0 10.0 10.6 
(continued) 
A. Mizumoto et al. 
Number of studies combined: k = 10 
COR 95%-CI z p-value 
Fixed effect model 0.7756 [0.7564; 0.7935] 43.52 < 0.0001 
Random effects model 0.7270 [0.6328; 0.8000] 10.25 < 0.0001 
Quantifying heterogeneity: 
Tauˆ2 = 0.0714; H = 3.55 [2.83; 4.45]; Iˆ2 = 92.1% 
[87.5%; 95.0%] 
Test of heterogeneity: 
Q d.f. p-value 
113.38 9 < 0.0001 
Details on meta-analytical method: 
-Inverse variance method 
-DerSimonian-Laird estimator for tauˆ2 
-Fisher’s z transformation of correlations 
When we use the meta package’s metacor function to conduct a meta-analysis (i.e., to combine the effects across the sample of studies), results such as the aforementioned are displayed. The “COR” column is the original r of each study. The “95%-CI” column shows the lower and upper limits of the r’s 95% confdence interval (cf. chap. 20 for more information on confdence intervals). If this confdence interval of the r does not include zero, we interpret this to be the same as if a non-correlation test (a test in which the correlation is not zero) has indicated statistical signifcance, and consider the r to be non-zero (i.e., statistically signifcant). 
Depending on how you conceive of effect-size variance, one of two meta-analytic models can be applied. The fxed-effects model is one that assumes that there is a single population of effect sizes and that the variation observed among the effect sizes of different studies can be explained by sampling error alone. The random-effects model is one that assumes that the variation observed between the effect sizes of studies arises as a result of both sampling error as well as other factor(s) (including known and perhaps unknown moderators) (cf. chap. 22). The random effects model regards the primary studies included in the current meta-analysis as a sample of a larger universe of studies assumed to exist. To put it simply, if you fnd while reviewing the primary studies included in your meta-analysis that you can make an apriori hypothesis that all the studies have approximately the same effect size, you should use a fxed-effects model. However, if you cannot expect them all to have approximately the same effect size and you expect that one or more variables may be able to explain systematic variability in observed effects, a random-effects model would be more appropriate. Generally, we feel that a random effects model is more appropriate for meta-analyses in linguistics research, which is generally multifactorial in nature and therefore varies systematically and in often predictable ways (see discussions in Gries 2015; Plonsky and Oswald 2015, 2017). 
The “%W(fxed)” and “%W(random)” columns in our output show the per­centage weights of individual studies in fxed-effects and random-effects models, respectively. As described above, the greater the sample size of a study, the greater is the weight (i.e., the confdence interval is narrower). This is clearer for the fxed-effects model than the random-effects model because random-effects models consider not only the sampling error but also other factors. 
Now let us turn to interpreting the results we have arrived at thus far. Beneath the line that reads “Number of studies combined,” the overall effect is displayed under both fxed-effects and random-effects models, as well as each model’s respective rs, 95% confdence intervals (95%-CI), rz-scores (z), and p-values (p) based on those z-scores. If the absolute value of the z-score is greater than 1.96, it is deemed statistically signifcant at the 5% level (p < .05), and the r is interpreted to be (statistically signifcantly) non-zero. This is indeed the case for the present (hypothetical) study: The overall relationship between the two variables of interest is estimated to be quite strong at r = .73. This estimate is not only statistically signifcant but it is quite stable as an estimate of the true population effect, as indicated by the 95% confdence intervals falling in a fairly narrow range of [.63, .8]. 
Next, we must examine the statistics under “Quantifying heterogeneity.” Here, heterogeneity is expressed numerically. The tauˆ2 (tau-squared) value is the between-study variance. We refer to two indicators when assessing heterogeneity: H and Iˆ2 (I-squared). When the value of H is greater than 1, unexplained heterogeneity exists. (The bracketed values [2.83, 4.45] show the lower and upper limits of the 95% confdence interval.) Here, H is 3.55. Therefore, we assume “unexplained heterogeneity” and interpret the rs of between-studies to be considerably different. We interpret an I-squared value of 25–50% to mean that the effect sizes are a “little different” between studies; 50–75% indicates “quite different,” and 75–100% indicates “considerably different.” (The bracketed values [87.5%; 95.0%] show the lower and upper limits of the 95% confdence interval.) Here, I-squared is 92.1%, so again we interpret the rs of between-studies to be considerably different. 
Under “Quantifying heterogeneity” is “Test of heterogeneity.” Q is yet another value used to express sample heterogeneity; the larger the Q value, the greater the difference in effect sizes between studies. “Degrees of freedom” (or d.f.) is equal to the number of [primary] studies – 1. The p-value is the result of the test of hetero­geneity and tests the null hypothesis that “all effect sizes in the previous studies are the same size.” Because the p-value is .0001, this null hypothesis is rejected, and we 
A. Mizumoto et al. 

can state that the 10 rs are considerably different (i.e., inconsistencies exist) between studies. Given these multiple indicators, in this situation in which large differences exist between the effect sizes of the studies included in the meta-analysis, the mean of the effect sizes from all studies obscure patterns in the sample. Therefore, we must push our analysis further (see “Subgroup (moderator) analysis” later in this section). These statistics can be valuable indicators of heterogeneity. However, we strongly encourage researchers to not rely on such indices entirely. Rather, as in other methodological domains, statistics cannot and should not replace judicious examination of individual study effects by a knowledgeable synthesist. 
In addition to these strictly numeric analyses, there are several data visualization techniques that are useful in conducting a meta-analysis (see overview in Schild and Voracek 2013). Figure 27.1 displays a “forest plot,” which you can create using the forest function. A forest plot is a single graph that shows both the individual effect sizes of each study with their confdence intervals and weights as well as the pooled (overall) effect size and its confdence interval. A forest plot enables the researcher to search visually for patterns and variations between study effects, as incorporating all information needed to report your meta-analysis into one graphical representation is possible. 
Figure 27.1 displays not only the metacor function results previously explained, but also the names of the studies included in the meta-analysis (under “Study”), their respective sample sizes (“Total”), and their individual as well as overall effect sizes and confdence intervals (“Correlation”). The width of the confdence interval shows the precision of the effect size estimate; the narrower it is, the more accurate we know it is (confdence intervals become narrower as the sample size of the study increases). The size of each study’s square point expresses its weight within the meta-analysis, that is, how much weight it had relative to the rest of the sample. At the bottom of the forest plot, the respective overall effect sizes and confdence intervals under fxed-effects and random-effects models are displayed as rhombuses. 
forest(res) # draw a forest plot 
Because our analysis up to this point has revealed signifcant variability across observed correlations, let us now try to explain, in part, this variation by means of a moderator analysis (also referred to as “subgroup analysis”). Recall that we had coded each study for the categorical moderator of the distance of learners’ native language (L1) to English as the “Moderator” in this sample (see Table 27.3). For the purpose of this example, we have only included a single moderator. In meta-analyses, however, a number of moderators are examined based on the predictions of theory, practice, and/or previous research. 
When performing moderator analysis, you can apply a random-effects model to each subgroup. However, it often happens that the number of primary studies in each subgroup is small, making the estimated values unreliable. Therefore, it is considered preferable to use a pooled tau-squared for the entire subgroup and estimate using a mixed-effects model (Borenstein et al. 2009). Although using a mixed effects model on subgroup analysis in the metafor package (Viechtbauer 2010, 18–20) is possible, here we will use the macat function of the MAc package (Del Re and Hoyt 2012), which is more user-friendly. 
We will load the MAc package and conduct subgroup analysis using the macat function given as follows. 
library(MAc) macat(yi, vi, mod = Moderator, data = dat, ztor = TRUE, method = "random") 
x <-read.csv("27_data.csv", header = TRUE) # read the csv file in R # show the data 
A. Mizumoto et al. 
If you change the macat function’s fnal argument as shown previously from method = “random” to method = “fixed”, you can view the results under a fxed-effects model. In addition, because by entering ztor = TRUE,we are converting Fisher’s z back into rs for ease of interpretation, if you change it to ztor = FALSE, you can view the correct Fisher’s z and its p-value. 
Let us now examine the areas around which we have placed squares in these macat function subgroup analysis results. The estimate column shows the mean rs obtained from the subgroup analysis under a mixed-effects (i.e., a combination of fxed and random effects) model, the “ci.l” column shows the lower limit of the 95% confdence interval, and the “ci.u” column shows the upper limit of the 95% confdence interval. Thus, the overall r for studies in which the learners’ L1 was linguistically close to English was 0.816 [0.767, 0.855], and the overall r for studies in which the learners’ L1 was linguistically distant from English was 0.487 [0.328, 0.619]. 
Moreover, the confdence intervals of the “close” and “distant” L1 studies do not overlap indicating that the difference between the two correlations is statistically signifcant. This fact is also clear from the mixed-effects model heterogeneity test results in the “Heterogeneity” square. Looking at Q-between (Qb), degrees of freedom for Q-between (Qb.df ), and Q-between p-value (Qb.p), the Qb.p is p < .001, which also allows us to say that the mean rs of the two subgroups are signifcantly different (Q = 26.849, df = 1, p < .001). 
These (fabricated) results demonstrate that the strength of the association between L2 writing performance and lexical complexity differs between learners whose native language has a “close” linguistic relationship to English as opposed to a “distant” one, and that the association is particularly strong in the “close” case, while tending to be somewhat weaker in the “distant” case. In addition, whereas the data used in this sample meta-analysis is purely fctional, in a real meta-analysis, if you were to discover after exhaustively searching for and collecting previous research that the number of “distant” L1 studies was similarly small (e.g., k = 4), the study would also provide evidence of a need for further research in this area. 
27.4 Critical Assessment and Future Directions 
This chapter has sought to lay a foundation of meta-analysis for corpus linguists— for both those who might produce meta-analyses themselves and to improve the feld’s understanding of such studies. To ensure a balanced view, we would also like to address a few concerns and possible directions for scholars to keep in mind as the use of meta-analysis continues to expand. 
A frst concern is the potential for premature closure in domains that are subject to meta-analysis. Meta-analyses provide strong evidence of validity generalization (e.g., across text types, languages, linguistic features, registers). Nevertheless, it is not often the case that the fndings of a meta-analysis will provide conclusive answers to questions of interest, particularly in a young feld such as corpus linguistics. There are almost always additional questions to be addressed, many of which are actually pinpointed by meta-analytic evidence. In fact, it is our view that one of the duties of the meta-analyst is to be just as much prospective as retrospective, synthesizing but also providing clear guidance to future research both in terms of what remains to be studied (i.e., substance) and how to go about doing so (i.e., method). Once suffcient additional results have accumulated, replication at the meta-analytic level can then be applied, as we have seen on numerous occasions in the feld of second language acquisition (see discussion and examples in Kang et al. 2019; Plonsky 2012b, 2019; Saito and Plonsky 2019). 
A second challenge facing meta-analysts of research in corpus linguistics is the quality (i.e., rigor and transparency) in the primary literature. As we noted above, it is quite common for synthesists to lament poor design, analyses, and/or inadequate reporting among individual studies, thus necessarily limiting the quality and quantity of evidence available at the meta-analytic level. Boulton and Cobb (2017) noted, for example, that their “initial count of 205 DDL studies was reduced to 64 partly due to our inclusion criteria but also because of missing data and incomplete reporting.” For these reasons, we implore future meta-analysts to consider (and code for) methodological features in their sample just as thoroughly as the substantive ones. Doing so allows the researcher to not only describe and evaluate their domain of interest but, critically, to provide concrete, empirically-grounded direction to future studies. See Paquot and Plonsky (2017) for an example of a methodologically-focused synthesis in corpus linguistics. 
A third major challenge to meta-analysis in corpus linguistics and throughout many other felds is that of publication bias. Meta-analysis reaches conclusions by taking the effect sizes of primary study results and statistically pooling them. As we described in the Introduction, two of the most salient advantages meta-analysis provides over traditional narrative reviews are objectivity and systematicity. However, because meta-analysis necessarily relies on primary studies (previous research) for its data collection, before you fnd and interpret the overall effect size, it is critical to determine whether these studies are representative of the presumed larger (if hypothetical) universe of studies and not a biased sample of that universe. 
There are a number of reasons why a sample might be biased. For example, the theoretical orientations of editors, reviewers, and individual researchers themselves may lead these individuals to suppress results that support or fail to support a particular relationship or effect. The fndings in such a domain would, then, be biased in favor of the dominant orientation. 
Another main source of bias is more statistical in nature. It is widely recognized that quantitative research failing to reach statistical signifcance (e.g., p > .05) is less likely to be accepted for publication. When present, a publication bias (also referred to as “the fle drawer problem”) in favor of larger effects may be observed in a given sample of studies, leading to an infated overall effect. 
Completely avoiding this problem is impossible, but numerous methods of determining and correcting the effect of publication bias have been proposed. Here, we will show you how to perform two using R that are broadly used and easy to apply: the funnel plot and the trim-and-fll method. (A method of checking for 
A. Mizumoto et al. 

publication bias also exists called “fail-safe N,” which calculates the number of studies with null effects that would be required for the mean effect size to lose signifcance. However, this method has not been highly recommended in recent years, so we do not include it in our discussion here. For details, see Oswald and Plonsky 2010, 92.) 
The funnel plot is a type of scatter plot wherein each study is represented by its effect size estimate on the horizontal axis and sample size (or an indicator of error such as standard error) on the vertical axis. If studies have been gathered without bias, their effect size estimates should be distributed relatively symmetrically left and right around the mean effect size in the center. Further, as the estimate of precision (e.g. sample size) increases near the top of the graph, the variability of effects should narrow, thus resembling an upside-down funnel. Studies with small samples and thus greater error will fan out near the bottom. In a biased sample of primary effects, the image created in the plot will be asymmetrical. Specifcally, if there is a bias in favor of (positive and) statistically signifcant effects, the left-hand side of the fgure will appear to be missing data points due to the suppression or bias against smaller, non-statistically signifcant effects. 
Figure 27.2 shows: (a) a funnel plot of the data analyzed up until Sect. 27.3.4, with a random-effects model applied, and (b) a funnel plot of the subgroup analysis using the categorical moderator of learners’ L1 distance from English, with a mixed-effects model applied. For the random-effects model in (a), we take the data created in Sect. 27.3.3 containing the original data with Fisher’s z appended as “yi” and sampling variances appended as “vi” and apply a random-effects model through the metafor package’s rma function. We then execute the funnel function given as follows. 
res.RE <-rma(yi, vi, data = dat) 
funnel(res.RE, atrans = transf.ztor, main = "Random effects 
model") 
#The funnel function’s atrans = transf.ztor argument is used 
to convert Fisher’s z back to r 
In Sect. 27.3.4, we performed a subgroup (moderator) analysis because high heterogeneity was present between the effect sizes in the primary studies. We were able to draw a funnel plot (b), in which the mixed-effects model was applied to the subgroup analysis by executing the rma and funnel functions given as follows (for details of the functions’ arguments, see Viechtbauer 2010). 
res.MIX <-rma(yi, vi, mods = factor(Moderator) -1, data = dat) funnel(res.MIX, main = "Mixed effects model") 
Due to the fact that only 10 primary studies were included in this meta-analysis, making a defnitive statement about the bias is diffcult. However, the distribution on the funnel plots does appear to indicate some level of bias in favor of larger correlations. Such bias, though perhaps diffcult to discern in this small sample, can be seen in the lack of equilibrium lower (i.e., with smaller samples) in the fgure. A function in the metafor package exists for running tests for funnel plot asymmetry. Specifcally, the res.RE and res.MIX previously created can be used to check the funnel plots for asymmetry using the regtests function. 
regtest(res.RE) # random effects model without a moderator regtest(res.MIX) # mixed effects model with a moderator 
If the analysis yields a non-signifcant p-value, we interpret it to mean that funnel plot is not statistically asymmetrical (i.e., no publication bias was present). Because the random-effects model plot was p = 0.0130 and the mixed-effects model plot was p = 0.6644, we can probably say that the analysis using the mixed-effects model was statistically “free of publication bias.” Similarly, if we use the ranktest function and the results are p > .05 (a non-signifcant p-value), we interpret them to mean the 
A. Mizumoto et al. 
funnel plot is not statistically asymmetrical. There, the result was p = 0.1557, so again, we should be able to interpret this analysis as statistically “free of publication bias” (see the metafor manual (Viechtbauer 2017) for details). However, in cases showing high heterogeneity between the effect sizes in the primary studies included in a meta-analysis, these test results may be incorrect. Therefore, caution must be exercised in interpreting them. However, when the number of studies plotted is small as in our example here, the hallmarks of bias or lack of bias will not necessarily appear. When the difference in sample size between studies is small, the shape is particularly unclear and authors should avoid defnitive declarations in one direction or another. Once again, we urge researchers to balance statistical interpretations with their own domain-specifc knowledge as well as a careful consideration of the size and nature of the dataset at hand. The meta-analyst might ask him/herself whether the domain in question is likely to be susceptible to bias due to, for instance, strong positions among researchers, funder infuence, or other concerns beyond the impetus and infated importance of achieving statistical signifcance. 
27.5 Conclusion 
In this chapter, we have argued that meta-analysis should be more widely applied within corpus linguistics as a method of synthesizing and empirically reviewing previous corpus-based research. We have provided a conceptual and practical introduction to meta-analysis. In order to encourage and enable the application of meta-analysis in corpus linguistics, we have also introduced and demonstrated the main procedures involved including how to (1) search for primary studies, (2) develop and implement a coding scheme, (3) aggregate effect sizes using different R packages to obtain estimates of the relationship in question as well as estimates of variance and of different moderator effects. As we hope to have made clear, each of these stages entails a set of decisions to be made by the researcher. As such, although one of the strengths of meta-analysis is the enhanced objectivity it provides, the importance of researcher judgment and expertise is by no means diminished. We have also discussed in detail a number of concerns and future directions. These include the need to avoid premature closure, the importance of considering methodological practices in meta-analytic samples, and the threat of publication bias. Although meta-analysis has yet to take root in the feld of corpus linguistics, we look forward to future applications of meta-analysis in this domain. 
27.6 Tools and Resources 
In this chapter we have used the metafor package as it can perform, compared with other packages, most of the main meta-analytic procedures (Viechtbauer 2010). For readers who are interested in other R packages for conducting meta-analyses, a thorough overview/comparison by Task View for Meta-Analysis (https://cran. r-project.org/web/views/MetaAnalysis.html) (accessed 4 June 2019) of different packages is available. 
We also recommend the following online resources: 
• 
David B. Wilson’s “Meta-analysis stuff”: http://mason.gmu.edu/~dwilsonb/ma. html. Accessed 4 June 2019. 

• 
R code for meta-analysis and related procedures: http://cran.r-project.org/web/ views/MetaAnalysis.html. Accessed 4 June 2019. 

• 
Meta-analysis made easy (meta-analysis in Excel using MIX 2.0): http://www. meta-analysis-made-easy.com. Accessed 4 June 2019. 

• 
Plonsky’s bibliography of meta-analysis in applied linguistics: https:// lukeplonsky.wordpress.com/bibliographies/meta-analysis/. Accessed 4 June 2019. 

• 
MetaLab, a community-based meta-research site with data for public use: http:// metalab.stanford.edu/. Accessed 4 June 2019. 

• 
Meta-Research Innovation Center at Stanford (METRICS): https://metrics. stanford.edu/. Accessed 12 June 2019. 

• 
Langtest, a web-based application for running a variety of statistical analyses including meta-analysis: http://langtest.jp. (see Mizumoto and Plonsky 2016) 


Further Reading 
Appelbaum, M., Cooper, H., Kline, R.B., Mayo-Wilson, E., Nezu, A.M., and Rao, S.M. 2018. Journal article reporting standards for quantitative research in psychology: The APA Publications and Communications Board task force report. American Psychologist 73:3–25. 
This article contains current APA reporting guidelines for primary and secondary/meta-analytic research. 
Cooper, H. 2016. Research synthesis and meta-analysis: A step-by-step approach (5th ed.). Thousand Oaks, CA: Sage. 
Cooper’s popular book provides readers with an accessible and practical tutorial on research synthesis and meta-analysis. 
Cooper, H., Hedges, L.V., and Valentine, J.C. (Eds.). 2009. The handbook of research synthesis and meta-analysis (2nd ed.). New York, NY: Russel Sage Foundation. 
This edited volume covers a wide range of meta-analytic techniques of a more specialized nature. 
A. Mizumoto et al. 
References 
Anthony, L. (2014). AntConc (Version 3.4.3) [Computer Software]. Tokyo: Waseda University. Available from http://www.laurenceanthony.net/. Accessed 4 June 2019. 
APA Publications and Communications Board Working Group on Journal Article Reporting Standards. (2008). Reporting standards for research in psychology: Why do we need them? What might they be? American Psychologist, 63, 839–851. https://doi.org/10.1037/0003­066X.63.9.839. 
Bernolet, S., Collina, S., & Hartsuiker, R. J. (2016). The persistence of syntactic priming revisited. Journal of Memory and Language, 91, 99–116. 
Biber, D. (2012). Register as a predictor of linguistic variation. Corpus Linguistics and Linguistic Theory, 8(1), 9–37. 
Borenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). Introduction to meta-analysis. West Sussex: Wiley. 
Boulton, A., & Cobb, T. (2017). Corpus use in language learning: A meta-analysis. Language Learning, 67, 348–393. https://doi.org/10.1111/lang.12224. 
Burch, B., Egbert, J., & Biber, D. (2017). Measuring and interpreting lexical dispersion in corpus linguistics. Journal of Research Design and Statistics in Linguistics and Communication Science, 3, 189–216. 
Comprehensive Meta-analysis Version 3. (2013). Biostat. Englewood. 
Cooper, H., Hedges, L. V., & Valentine, J. C. (Eds.). (2009). The handbook of research synthesis and meta-analysis (2nd ed.). New York: Russel Sage Foundation. 
Del Re, A. C., & Hoyt, W. T. (2012). MAc: Meta-analysis with correlations (Version 1.1) [Software]. Retrieved from http://cran.r-project.org/web/packages/MAc/MAc.pdf . Accessed 4 June 2019. 
Durrant, P. (2014). Corpus frequency and second language learners’ knowledge of collocations: A meta-analysis. International Journal of Corpus Linguistics, 19, 443–477. https://doi.org/ 10.1075/ijcl.19.4.01dur. 
Egbert, J., & Biber, D. (2017). Do all roads lead to Rome?: Modeling register variation with factor analysis and discriminant analysis. Corpus Linguistics and Linguistic Theory. 
Gries, S. T. (2015). Quantitative designs and statistical techniques. In D. Biber & R. Reppen (Eds.), The Cambridge handbook of English corpus linguistics (pp. 50–71). Cambridge: Cambridge University Press. 
Gries, S. T. (2016). Variationist analysis: Variability due to random effects and autocorrelation. In 
P. Baker & J. A. Egbert (Eds.), Triangulating methodological approaches in corpus linguistic research (pp. 108–123). New York: Routledge. Gries, S. T., & Kootstra, G. J. (2017). Structural priming within and across languages: A corpus-based perspective. Bilingualism: Language and Cognition, 20(2), 235–250. 
Gries, S. T., & Wulff, S. (2013). The genitive alternation in Chinese and German ESL learners: Towards a multifactorial notion of context in learner corpus research. International Journal of Corpus Linguistics, 18(3), 327–356. 
In’nami, Y., & Koizumi, R. (2010). Database selection guidelines for meta-analysis in applied linguistics. TESOL Quarterly, 44, 169–184. 
Johnson, M. D. (2017). Cognitive task complexity and L2 written syntactic complexity, accuracy, lexical complexity, and fuency: A research synthesis and meta-analysis. Journal of Second Language Writing, 37, 13–38. https://doi.org/10.1016/j.jslw.2017.06.001. 
Jonnalagadda, S. R., Goyal, P., & Huffman, M. D. (2015). Automating data extraction in systematic reviews: A systematic review. Systematic Reviews, 4, 78. 
Kang, E.Y., Sok, S., and Han, Z. (2019). Thirty-fve years of ISLA on form-focused instruc­tion: A meta-analysis. Language Teaching Research. 23, 428–453. https://doi.org/10.1177/ 1362168818776671. 
Larson-Hall, J., & Plonsky, L. (2015). Reporting and interpreting quantitative research fndings: What gets reported and recommendations for the feld. Language Learning, 65, 127–159. https:/ /doi.org/10.1111/lang.12115. 
Larsson, T., Paquot, M., & Plonsky, L. (in press). Inter-rater reliability in learner corpus research: Insights from a collaborative study on adverb placement. International Journal of Learner Corpus Research. 
Lee, H., Warschauer, M., and Lee, J.H. (2019). The effects of corpus use on second language vocabulary learning: A multilevel meta-analysis. Applied Linguistics, 40, 721–753. https:// doi.org/10.1093/applin/amy012. 
Mahboob, A., Paltridge, B., Phakiti, A., Wagner, E., Starfeld, S., Burns, A., et al. (2016). TESOL quarterly research guidelines. TESOL Quarterly, 50, 42–65. https://doi.org/10.1002/tesq.288. 
Marsden, E., Mackey, A., & Plonsky, L. (2016). The IRIS repository: Advancing research practice and methodology. In A. Mackey & E. Marsden (Eds.), Advancing methodology and practice: The IRIS repository of instruments for research into second languages (pp. 1–21). New York: Routledge. 
Marsden, E., Thompson, S., & Plonsky, L. (2018). A methodological synthesis of self-paced reading in second language research. Applied PsychoLinguistics, 39, 861–904. 
Mizumoto, A., & Plonsky, L. (2016). R as a lingua franca: Advantages of using R for quantitative research in applied linguistics. Applied Linguistics, 37, 284–291. https://doi.org/10.1093/ applin/amv025. 
Nicklin, C., & Plonsky, L. (2020). Outliers in L2 research: A synthesis and data re-analysis from self-paced reading. Annual Review of Applied Linguistics, 40, 26–55. https://doi.org/10.1017/ S0267190520000057 
Norouzian, R., & Plonsky, L. (2018). Correlation and simple linear regression in applied linguistics. In A. Phakiti, P. I. De Costa, L. Plonsky, & S. Starfeld (Eds.), The Palgrave handbook of applied linguistics research methodology (pp. 395–421). New York: Palgrave. 
Norris, J. M. (2015). Statistical signifcance testing in second language research: Basic problems and suggestions for reform. Language Learning, 65(Supp 1), 97–126. 
Norris, J. M., & Ortega, L. (Eds.). (2006). Synthesizing research on language learning and teaching. Philadelphia: John Benjamins. 
Norris, J. M., Plonsky, L., Ross, S. J., & Schoonen, R. (2015). Guidelines for reporting quantitative methods and results in primary research. Language Learning, 65, 470–476. https://doi.org/ 10.1111/lang.12104. 
Oswald, F. L., & Plonsky, L. (2010). Meta-analysis in second language research: Choices and challenges. Annual Review of Applied Linguistics, 30, 85–110. https://doi.org/10.1017/ S0267190510000115. 
Paquot, M. (2017). L1 frequency in foreign language acquisition: Recurrent word combinations in French and Spanish EFL learner writing. Second Language Research, 33(1), 13–32. 
Paquot, M., & Plonsky, L. (2017). Quantitative research methods and study quality in learner corpus research. International Journal of Learner Corpus Research, 3, 61–94. https://doi.org/ 10.1075/ijlcr.3.1.03paq. 
Plonsky, L. (2011). The effectiveness of second language strategy instruction: A meta-analysis. Language Learning, 61, 993–1038. https://doi.org/10.1111/j.1467-9922.2011.00663.x. 
Plonsky, L. (2012a). Effect size. In P. Robinson (Ed.), The Routledge encyclopedia of second language acquisition (pp. 200–202). New York: Routledge. 
Plonsky, L. (2012b). Replication, meta-analysis, and generalizability. In G. Porte (Ed.), Replication research in applied linguistics (pp. 116–132). New York: Cambridge University Press. 
Plonsky, L. (2013). Study quality in SLA: An assessment of designs, analyses, and reporting practices in quantitative L2 research. Studies in Second Language Acquisition, 35, 655–687. Plonsky, L. (2015a). Demographics in SLA: A systematic review of sampling practices in L2 research. Paper presented at the Second Language Research Forum (SLRF), Atlanta, GA. 
Plonsky, L. (2015b). Quantitative considerations for improving replicability in CALL and applied linguistics. CALICO Journal, 32, 232–244. 
A. Mizumoto et al. 
Plonsky, L. (2019). Recent research on language learning strategy instruction. In A. U. Chamot & V. Harris (Eds.), Learning strategy instruction in the language classroom: Issues and implementation (pp. 3–21). Bristol: Multilingual Matters. 
Plonsky, L., & Brown, D. (2015). Domain defnition and search techniques in meta-analyses of L2 research (or why 18 meta-analyses of feedback have different results). Second Language Research, 31, 267–278. https://doi.org/10.1177/0267658314536436. 
Plonsky, L., & Gass, S. (2011). Quantitative research methods, study quality, and outcomes: The case of interaction research. Language Learning, 61, 325–366. 
Plonsky, L., & Oswald, F. L. (2014). How big is “big”? Interpreting effect sizes in L2 research. Language Learning, 64, 878–912. https://doi.org/10.1111/lang.12079. 
Plonsky, L., & Oswald, F. L. (2015). Meta-analyzing second language research. In L. Plonsky (Ed.), Advancing quantitative methods in second language research (pp. 106–128). New York: Routledge. 
Plonsky, L., & Oswald, F. L. (2017). Multiple regression as a fexible alternative to ANOVA in L2 research. Studies in Second Language Acquisition, 39, 579–592. 
Plonsky, L., Egbert, J., & LaFlair, G. T. (2015). Bootstrapping in applied linguistics: Assessing its potential using shared data. Applied Linguistics, 36, 591–610. https://doi.org/10.1093/applin/ amu001. 
Polanin, J. R., Hennessy, E. A., & Tanner-Smith, E. E. (2017). A review of meta-analysis packages in R. Journal of Educational and Behavioral Statistics, 42, 206–242. https://doi.org/10.3102/ 1076998616674315. 
R Core Team. (2019). R: A language and environment for statistical computing (Version 3.4.2) [Computer software].Vienna,Austria.Retrieved from http://www.r-project.org/. Accessed 4 June 2019. 
Saito, K., & Plonsky, L. (2019). Effects of second language pronunciation teaching revisited: A proposed measurement framework and meta-analysis. Language Learning, 69, 652–708. https:/ /doi.org/10.1111/lang.12345. 
Schild, A. H. E., & Voracek, M. (2013). Less is less: A systematic review of graph use in meta-analyses. Research Synthesis Methods, 4, 209–219. 
Schwarzer, G. (2007). Meta: An R package for meta-analysis. R News, 7(3), 40–45. 
Szmrecsanyi, B. (2005). Language users as creatures of habit: A corpus-based analysis of persistence in spoken English. Corpus Linguistics and Linguistic Theory, 1(1), 113–150. 
Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. Journal of Statistical Software, 36(3), 1–48. http://www.jstatsoft.org/v36/i03/. Accessed 4 June 2019. 
Viechtbauer, W. (2017). Package “metafor”. https://cran.r-project.org/web/packages/metafor/ metafor.pdf. Accessed 4 June 2019. 
Wilson, D. B. (2002). Meta-analysis stuff. http://mason.gmu.edu/~dwilsonb/ma.html. Accessed 4 June 2019. 
Wulff, S. (2016). What learner corpus research can contribute to multilingualism research? International Journal of Bilingualism, 21(6), 734–753. 





